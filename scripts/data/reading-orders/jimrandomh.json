{
  "author": "jimrandomh",
  "chapters": [
    {
      "title": "Top-5",
      "description": "A good pace is one or two posts per day, so that you have time to digest and internalize them. If you enjoyed the top five, take a moment to bookmark this page before continuing to the rest of the top 25. If you have questions or thoughts on a post, leave a comment; readers who follow the [recent comments](https://www.lesswrong.com/comments) page will see it and may reply.",
      "children": [
        {
          "name": "What Do We Mean By \"Rationality\"",
          "type": "post",
          "slug": "what-do-we-mean-by-rationality-1",
          "_id": "RcZCwxFiZzE6X7nsv",
          "url": null,
          "title": "What Do We Mean By \"Rationality\"?",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Definitions"
            },
            {
              "name": "Distinctions"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "I mean two things:\n\n1\\. **Epistemic rationality**: systematically improving the accuracy of your beliefs.\n\n2\\. **Instrumental rationality**: systematically achieving your values.\n\nThe first concept is simple enough. When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed.\n\nThis is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead. This correspondence between belief and reality is commonly called “truth,” and I’m happy to call it that.^1^\n\nInstrumental rationality, on the other hand, is about *steering* reality—sending the future where you want it to go. It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.”\n\nSo rationality is about forming true beliefs and making decisions that help you win.\n\n(Where truth doesn't mean “certainty,” since we can do plenty to increase the *probability* that our beliefs are accurate even though we're uncertain; and winning doesn't mean “winning at others' expense,” since our values include *everything* we care about, including other people.)\n\nWhen people say “X is rational!” it’s usually just a more strident way of saying “I think X is true” or “I think X is good.” So why have an additional word for “rational” as well as “true” and “good”?\n\nAn analogous argument can be given against using “true.” There is no need to say “it is true that snow is white” when you could just say “snow is white.” What makes the idea of truth useful is that it allows us to talk about the general features of map-territory correspondence. “True models usually produce better experimental predictions than false models” is a useful generalization, and it’s not one you can make without using a concept like “true” or “accurate.”\n\nSimilarly, “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function” is the kind of thought that depends on a concept of (instrumental) rationality, whereas “It’s rational to eat vegetables” can probably be replaced with “It’s useful to eat vegetables” or “It’s in your interest to eat vegetables.” We need a concept like “rational” in order to note general facts about those ways of thinking that systematically produce truth or value—and the systematic ways in which we fall short of those standards.\n\nAs we’ve observed in the previous essays, experimental psychologists sometimes uncover human reasoning that seems very strange. For example, someone rates the probability “Bill plays jazz” as *less* than the probability “Bill is an accountant who plays jazz.” This seems like an odd judgment, since any particular jazz-playing accountant is obviously a jazz player. But to what higher vantage point do we appeal in saying that the judgment is *wrong* ?\n\nExperimental psychologists use two gold standards: *probability theory*, and *decision theory*.\n\nProbability theory is the set of laws underlying rational belief. The mathematics of probability applies equally to “figuring out where your bookcase is” and “estimating how many hairs were on Julius Caesars head,” even though our evidence for the claim “Julius Caesar was bald” is likely to be more complicated and indirect than our evidence for the claim “theres a bookcase in my room.” It’s all the same problem of how to process the evidence and observations to update one’s beliefs. Similarly, decision theory is the set of laws underlying rational action, and is equally applicable regardless of what one’s goals and available options are.\n\nLet “P(such-and-such)” stand for “the probability that such-and-such happens,” and “P(A,B)” for “the probability that both A and B happen.” Since it is a universal law of probability theory that P(A) ≥ P(A,B), the judgment that P(Bill plays jazz) is less than P(Bill plays jazz, Bill is an accountant) is labeled incorrect.\n\nTo keep it technical, you would say that this probability judgment is *non-Bayesian*. Beliefs that conform to a coherent probability distribution, and decisions that maximize the probabilistic expectation of a coherent utility function, are called “Bayesian.”\n\nI should emphasize that this *isn't *the notion of rationality thats common in popular culture. People may use the same string of sounds, “ra-tio-nal,” to refer to “acting like Mr. Spock of *Star Trek*” and “acting like a Bayesian”; but this doesn't mean that acting Spock-like helps one hair with epistemic or instrumental rationality.^2^\n\nAll of this does not quite exhaust the problem of what is meant in practice by “rationality,” for two major reasons:\n\nFirst, the Bayesian formalisms in their full form are computationally intractable on most real-world problems. No one can *actually *calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.\n\nThis is why there is a whole site called “Less Wrong,” rather than a single page that simply states the formal axioms and calls it a day. There’s a whole further art to finding the truth and accomplishing value *from inside a human mind*: we have to learn our own flaws, overcome our biases, prevent ourselves from self-deceiving, get ourselves into good emotional shape to confront the truth and do what needs doing, et cetera, et cetera.\n\nSecond, sometimes the meaning of the math itself is called into question. The exact rules of probability theory are called into question by, e.g., [anthropic problems](http://www.anthropic-principle.com/?q=anthropic_principle/primer) in which the number of observers is uncertain. The exact rules of decision theory are called into question by, e.g., Newcomblike problems in which other agents may predict your decision before it happens.^3^\n\nIn cases where our best formalizations still come up short, we can return to simpler ideas like “truth” and “winning.” If you are a scientist just beginning to investigate fire, it might be a lot wiser to point to a campfire and say “Fire is that orangey-bright hot stuff over there,” rather than saying “I define fire as an alchemical transmutation of substances which releases phlogiston.” You certainly shouldn’t ignore something just because you can’t define it. I can't quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I'll fall. And we can say the same of cognitive biases and other obstacles to truth—they won't hit any less hard if it turns out we can't define compactly what “irrationality” is.\n\nIn cases like these, it is futile to try to settle the problem by coming up with some new definition of the word “rational” and saying, “Therefore my preferred answer, *by definition*, is what is meant by the word ‘rational.’ ” This simply raises the question of why anyone should pay attention to your definition. I’m not interested in probability theory because it is the holy word handed down from Laplace. I’m interested in Bayesian-style belief-updating (with Occam priors) because I expect that this style of thinking gets us systematically closer to, you know, *accuracy*, the map that reflects the territory.\n\nAnd then there are questions of how to think that seem not quite answered by either probability theory or decision theory—like the question of how to feel about the truth once you have it. Here, again, trying to define “rationality” a particular way doesn’t support an answer, but merely presumes one.\n\nI am not here to argue the meaning of a word, not even if that word is “rationality.” The point of attaching sequences of letters to particular concepts is to let two people *communicate*—to help transport thoughts from one mind to another. You cannot change reality, or prove the thought, by manipulating which meanings go with which words.\n\nSo if you understand what concept I am *generally getting at *with this word “rationality,” and with the sub-terms “epistemic rationality” and “instrumental rationality,” we *have communicated*: we have accomplished everything there is to accomplish by talking about how to define “rationality.” What’s left to discuss is not *what meaning *to attach to the syllables “ra-tio-na-li-ty”; what’s left to discuss is *what is a good way to think*.\n\nIf you say, “It’s (epistemically) rational for me to believe X, but the truth is Y,” then you are probably using the word “rational” to mean something other than what I have in mind. (E.g., “rationality” should be *consistent under reflection*—“rationally” looking at the evidence, and “rationally” considering how your mind processes the evidence, shouldn’t lead to two different conclusions.)\n\nSimilarly, if you find yourself saying, “The (instrumentally) rational thing for me to do is X, but the right thing for me to do is Y,” then you are almost certainly using some other meaning for the word “rational” or the word “right.” I use the term “rationality” *normatively*, to pick out desirable patterns of thought.\n\nIn this case—or in any other case where people disagree about word meanings—you should substitute more specific language in place of “rational”: “The self-benefiting thing to do is to run away, but I hope I would at least try to drag the child off the railroad tracks,” or “Causal decision theory as usually formulated says you should two-box on Newcomb’s Problem, but I’d rather have a million dollars.”\n\nIn fact, I recommend reading back through this essay, replacing every instance of “rational” with “foozal,” and seeing if that changes the connotations of what I’m saying any. If so, I say: strive not for rationality, but for foozality.\n\nThe word “rational” has potential pitfalls, but there are plenty of *non*-borderline cases where “rational” works fine to communicate what I’m getting at. Likewise “irrational.” In these cases I’m not afraid to use it.\n\nYet one should be careful not to *overuse *that word. One receives no points merely for pronouncing it loudly. If you speak overmuch of the Way, you will not attain it.\n\n* * *\n\n^1^ For a longer discussion of truth, see “[The Simple Truth](https://www.lesswrong.com/rationality/the-simple-truth)” at the very end of this volume.\n\n^2^ The idea that rationality is about strictly privileging verbal reasoning over feelings is a case in point. Bayesian rationality applies to urges, hunches, perceptions, and wordless intuitions, not just to assertions.\n\nI gave the example of opening your eyes, looking around you, and building a mental model of a room containing a bookcase against the wall. The modern idea of rationality is general enough to include your eyes and your brains visual areas as things-that-map, and to include instincts and emotions in the belief-and-goal calculus.\n\n^3^ For an informal statement of Newcomb’s Problem, see Jim Holt, “Thinking Inside the Boxes,” *Slate*, 2002, [http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside\\_the\\_boxes.single.html](http://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.single.html)."
          },
          "voteCount": 263
        },
        {
          "name": "Scientific Self-Help: The State of Our Knowledge",
          "type": "post",
          "slug": "scientific-self-help-the-state-of-our-knowledge",
          "_id": "33KewgYhNSxFpbpXg",
          "url": null,
          "title": "Scientific Self-Help: The State of Our Knowledge",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Practical"
            },
            {
              "name": "Well-being"
            },
            {
              "name": "Happiness"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "The industry and the literature",
                "anchor": "The_industry_and_the_literature",
                "level": 1
              },
              {
                "title": "A sampling of scientific self-help advice",
                "anchor": "A_sampling_of_scientific_self_help_advice",
                "level": 1
              },
              {
                "title": "Conclusions",
                "anchor": "Conclusions",
                "level": 1
              },
              {
                "title": "Notes",
                "anchor": "Notes",
                "level": 1
              },
              {
                "title": "References",
                "anchor": "References",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "502 comments"
              }
            ],
            "headingsCount": 7
          },
          "contents": {
            "markdown": "##### Part of the sequence: [The Science of Winning at Life](http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life)\n\n[Some](/lw/9p/rationality_its_not_that_great/) [have](/lw/bg/instrumental_rationality_is_a_chimera/) [suggested](/lw/2po/selfimprovement_or_shiny_distraction_why_less/) [that](/lw/2p5/humans_are_not_automatically_strategic/2l5h) the Less Wrong community could improve readers' [instrumental rationality](/lw/2p5/humans_are_not_automatically_strategic) more effectively if it first [caught up with the scientific literature](/lw/3m3/the_neglected_virtue_of_scholarship/) on productivity and self-help, and then enabled readers to deliberately practice [self-help skills](/lw/3kv/working_hurts_less_than_procrastinating_we_fear/) and apply what they've learned [in real life](/lw/34m/what_ive_learned_from_less_wrong/).\n\nI think that's a good idea. My contribution today is a quick overview of scientific self-help: what professionals call \"the psychology of adjustment.\" First I'll review the state of the industry and the scientific literature, and then I'll briefly summarize the scientific data available on three topics in self-help: study methods, productivity, and happiness.\n\n#### The industry and the literature\n\nAs you probably know, much of [the self-help](http://www.amazon.com/Self-Help-Inc-Makeover-Culture-American/dp/0195171241/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [industry](http://www.amazon.com/Oracle-Supermarket-American-Preoccupation-Self-Help/dp/0765809648/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) is a [sham](http://www.amazon.com/Sham-Self-Help-Movement-America-Helpless/dp/1400054095/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20), [ripe](http://www.amazon.com/Lost-Cosmos-Last-Self-Help-Book/dp/0312253990/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) for [parody](http://www.amazon.com/Secrets-SuperOptimist-W-R-Morton/dp/0977480704/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20). Most self-help books are written to _sell_, [not to _help_](http://www.amazon.com/Im-Dysfunctional-Youre-Recovery-Self-Help/dp/0201570629/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20). [Pop psychology](http://www.amazon.com/Fools-Paradise-Unreal-World-Psychology/dp/1566636280/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) may be [more myth](http://www.amazon.com/Great-Myths-Popular-Psychology-Misconceptions/dp/1405131128/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [than fact](http://www.amazon.com/House-Cards-Robyn-Dawes/dp/0684830914/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20). As Christopher Buckley (1999) writes, \"The more people read \\[self-help books\\], the more they think they need them... \\[it's\\] more like an addiction than an alliance.\"\n\nWhere can you turn for reliable, empirically-based self-help advice? A few leading therapeutic psychologists (e.g., [Albert Ellis](http://www.amazon.com/Albert-Ellis/e/B000APLUAE/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20), [Arnold Lazarus](http://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=arnold+lazarus&x=0&y=0&tag=lesswrong-20), [Martin Seligman](http://www.amazon.com/Martin-E.-P.-Seligman/e/B001ILOB78/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)) have written self-help books based on decades of research, but even these works tend to give recommendations that are still debated, because they aren't yet part of [_settled_ science](/lw/ow/the_beauty_of_settled_science/).\n\nLifelong self-help researcher [Clayton Tucker-Ladd](http://psychcentral.com/blog/archives/2010/01/08/a-psychologist-pioneer-clay-tucker-ladd-phd-78/) wrote and updated _[Psychological Self-Help](http://www.psychologicalselfhelp.org/)_ ([pdf](http://www.psychologicalselfhelp.org/download/)) over several decades. It's a summary of what scientists do and don't know about self-help methods (as of about 2003), but it's also more than 2,000 pages long, and much of it surveys scientific _opinion_ rather than experimental results, because on many subjects there _aren't_ any experimental results yet. The book is associated with an [internet community](http://forums.psychcentral.com/) of people sharing what does and doesn't work for them.\n\nMore immediately useful is Richard Wiseman's _[59 Seconds](http://www.amazon.com/59-Seconds-Change-Minute-Vintage/dp/0307474860/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. [Wiseman](http://en.wikipedia.org/wiki/Richard_Wiseman) is an experimental psychologist and paranormal investigator who gathered together what little self-help research _is_ part of settled science, and put it into a short, fun, and useful [Malcolm Gladwell-ish](http://en.wikipedia.org/wiki/Malcolm_Gladwell) book. The next best popular-level _general_ self-help book is perhaps Martin Seligman's _[What You Can Change and What You Can't](http://www.amazon.com/What-You-Change-Cant-Self-Improvement/dp/1400078407/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_.\n\nTwo [large](http://www.amazon.com/Authoritative-Self-Help-Resources-Revised-Clinicians/dp/1572308397/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [books](http://www.amazon.com/dp/0838906524/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) rate hundreds of popular self-help books according to what professional psychologists think of them, and offer advice on [how to choose self-help books](http://lukeprog.com/selfhelp/self_help_books.html). Unfortunately, this may not mean much because even professional psychologists very often have opinions that depart from the empirical data, as documented extensively by [Scott Lilienfeld](http://www.psychology.emory.edu/clinical/lilienfeld/index.html) and others in _[Science and Pseudoscience in Clinical Psychology](http://www.amazon.com/Science-Pseudoscience-Clinical-Psychology-Lilienfeld/dp/1593850700/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ and _[Navigating the Mindfield](http://www.amazon.com/Navigating-Mindfield-Separating-Science-Pseudoscience/dp/1591024676/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. These two books are helpful in assessing what is and isn't known according to _empirical research_ (rather than according to _expert opinion_). Lilienfeld also edits the useful journal [Scientific Review of Mental Health Practice](http://www.srmhp.org/current-issue.html), and has compiled a list of [harmful psychological treatments](http://commonsenseatheism.com/wp-content/uploads/2011/01/Lilienfeld-Psychological-Treatments-That-Cause-Harm.pdf). Also see Nathan and Gorman's _[A Guide to Treatments That Work](http://www.amazon.com/Guide-Treatments-that-Work/dp/0195304144/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, Roth & Fonagy's [What Works for Whom?](http://www.amazon.com/What-Works-Whom-Second-Psychotherapy/dp/159385272X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_,_ and, more generally, Stanovich's _[How to Think Straight about Psychology](http://www.amazon.com/How-Think-Straight-About-Psychology/dp/0205685900/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_.\n\nMany self-help books are written as \"one size fits all,\" but of course this is rarely appropriate in psychology, and this leads to reader disappointment (Norem & Chang, 2000). But psychologists _have_ tested the effectiveness of reading particular _problem-focused_ self-help books (\"bibliotherapy\").^1^ For example, [it appears that](http://commonsenseatheism.com/wp-content/uploads/2011/01/Anderson-Self-Help-Books-for-Depression.pdf) reading David Burns' _[Feeling Good](http://www.amazon.com/Feeling-Good-Therapy-Revised-Updated/dp/0380810336/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ can be as effective for treating depression as individual or group therapy. Results vary from book to book.\n\nThere are at least four university textbooks that teach basic scientific self-help. The first is Weiten, Dunn, and Hammer's _[Psychology Applied to Modern Life: Adjustment in the 21st Century](http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. It's expensive, but you can [preview it here.](http://www.coursesmart.com/9781111186630/Ch02) Others are are Santrock's [Human Adjustment](http://www.amazon.com/Human-Adjustment--Psych-CD-ROM-Santrock/dp/0073111910/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20), Duffy et al.'s _[Psychology for Living](http://www.amazon.com/Psychology-Living-Adjustment-Behavior-MyPsychKit/dp/0205790364/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_, and Nevid & Rathus' _[Psychology and the Challenges of Life](http://www.amazon.com/Psychology-Challenges-Life-Jeffrey-Nevid/dp/0470383623/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_.\n\nIf you read only one book of self-help in your life, I recommend Weiten, Dunn, and Hammer's [Psychology Applied to Modern Life](http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20).^2^ Unfortunately, like Tucker-Ladd's _Psychological Self-Help_, many sections of the book are an overview of scientific _opinion_ rather than _experimental result_, because so few experimental studies on the subject have been done!\n\nIn private correspondance with me, Weiten remarked:\n\n> You are looking for substance in what is ultimately a black hole of empirical research ...Basically, almost everything written on the topic emphasizes the complete lack of evidence.\n> \n> Perhaps I am overly cynical, but I suspect that empirical tests are nonexistent because the authors of self-help and time-management titles are not at all confident that the results would be favorable. Hence, they have no incentive to pursue such research because it is likely to undermine their sales and their ability to write their next book. Another issue is that many of the authors who crank out these titles have little or no background in research. In a less cynical vein, another issue is that this research would come with all the formidable complexities of the research evaluating the effectiveness of different approaches to therapy. Efficacy trials for therapies are extremely difficult to conduct in a clean fashion and because of these complexities require big bucks in the way of grants.\n\nOther leading researchers in the psychology of adjustment expressed much the same opinion of the field when I contacted them.\n\n#### A sampling of scientific self-help advice\n\nStill, perhaps scientific psychology can offer _some_ useful self-help advice. I'll focus on two areas of _particular_ interest to the Less Wrong community - [studying](/lw/2un/references_resources_for_lesswrong/) and [productivity](/lw/3kv/working_hurts_less_than_procrastinating_we_fear/) \\- and on one area of _general_ interest: [happiness](/lw/lb/not_for_the_sake_of_happiness_alone/).\n\n_Study methods_\n\nOrganize for clarity the information you want to learn, for example in an outline (Einstein & McDaniel 2004; Tigner 1999; McDaniel et al. 1996). Cramming doesn't work (Wong 2006). Set up a schedule for studying (Allgood et al. 2000). _Test_ yourself on the material (Karpicke & Roediger 2003; Roediger & Karpicke 2006a; Roediger & Karpicke 2006b; Agarwal et al. 2008; Butler & Roediger 2008), and do so repeatedly, with 24 hours or more between study sessions (Rohrer & Taylor 2006; Seabrook et al 2005; Cepeda et al. 2006; Rohrer et al. 2005; Karpicke & Roediger 2007). Basically: [**use Anki**](/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/).\n\nTo retain studied information more effectively, try [acrostics](http://en.wikipedia.org/wiki/Acrostic) (Hermann et al. 2002), the [link method](http://www.brighthub.com/education/homework-tips/articles/41610.aspx) (Iaccino 1996; Worthen 1997); and the [method of loci](http://en.wikipedia.org/wiki/Method_of_loci) (Massen & Vaterrodt-Plunnecke 2006; Moe & De Beni 2004; Moe & De Beni 2005).\n\n_Productivity_\n\nUnfortunately, there have been fewer experimental studies on effective productivity and time management methods than there have been on effective study methods. For an overview of scientific _opinion_ on productivity, I recommend pages 121-126 of _[Psychology Applied to Modern Life](http://www.amazon.com/gp/product/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. According to those pages, common advice from professionals includes:\n\n1.  Doing the _right_ tasks is more important than doing your tasks _efficiently_. In fact, too much concern for efficiency is a leading cause of procrastination. Say \"no\" more often, and use your time for tasks that really matter.\n2.  Delegate responsibility as often as possible. Throw away unimportant tasks and items.\n3.  Keep a record of your time use. ([Quantified Self](http://quantifiedself.com/) can help.)\n4.  Write down your goals. Break them down into smaller goals, and break these into manageable tasks. Schedule these tasks into your calendar.\n5.  Process notes and emails only once. Tackle one task at a time, and group similar tasks together.\n6.  Make use of your downtime (plane rides, bus rides, doctor's office waitings). These days, many of your tasks can be completed on your smartphone.\n\nWhy the dearth of experimental research on productivity? A leading researcher on the topic, [Piers Steel](http://haskayne.ucalgary.ca/profiles/piers-steel), explained to me in personal communication:\n\n> Fields tend to progress from description to experimentation, and the procrastination field is just starting to move towards that direction. There really isn’t very much directly done on procrastination, but there is more for the broader field of self-regulation... it should transfer as the fundamentals are the same. For example, I would bet everything I own that goal setting works, as there \\[are\\] about \\[a thousand studies\\] on it in the motivational field (just not specifically on procrastination). On the other hand, we are building a behavioral lab so we can test many of these techniques head to head, something that sorely needs to be done.\n\nSteel's book on the subject is [The Procrastination Equation](http://webapps2.ucalgary.ca/~steel/), which I highly recommend.\n\n_Happiness_\n\nThere is an abundance of research on factors that correlate with _subjective well-being_ (individuals' own assessments of their happiness and life satisfaction).\n\nFactors that _don't correlate_ much with happiness include: age,^3^ gender,^4^ parenthood,^5^ intelligence,^6^ physical attractiveness,^7^ and money^8^ (as long as you're above the poverty line). Factors that _correlate moderately_ with happiness include: health,^9^ social activity,^10^ and religiosity.^11^ Factors that _correlate strongly_ with happiness include: genetics,^12^ love and relationship satisfaction,^13^ and work satisfaction.^14^\n\nFor many of these factors, a _causal_ link to happiness has also been demonstrated with some confidence, but that story is too complicated to tell in this short article.\n\n#### Conclusions\n\nMany compassionate professionals have modeled their careers after George Miller's (1969) call to \"[give psychology away](http://commonsenseatheism.com/wp-content/uploads/2011/01/Epstein-Giving-Psychology-Away.pdf)\" to the masses as a means of promoting human welfare. As a result, hundreds of experimental studies have been done to test which self-help methods work, and which do not. We humans can use this knowledge to achieve our goals.\n\nBut much work remains to be done. Many features of human psychology and behavior are not well-understood, and many self-help methods recommended by popular and academic authors have not yet been experimentally tested. If you are considering psychology research as a career path, and you want to (1) improve human welfare, (2) get research funding, (3) explore an under-developed area of research, and (4) have the chance to write a best-selling self-help book once you've done some of your research, then please consider a career of _experimentally testing different self-help methods_. Humanity will thank you for it.\n\nNext post: [How to Beat Procrastination](/lw/3w3/how_to_beat_procrastination/)\n\n#### Notes\n\n^1^Read a nice overview of the literature in Bergsma, \"[Do Self-Help Books Help?](http://commonsenseatheism.com/wp-content/uploads/2011/01/Bergsma-Do-Self-Help-Books-Help.pdf)\" (2008).\n\n^2^ I recommend the 10th edition, which has large improvements over the 9th edition, including 4500 new citations.\n\n^3^ Age and happiness are unrelated (Lykken 1999), age accounting for less than 1% of the variation in people's happiness (Inglehart 1990; Myers & Diener 1997).\n\n^4^ Despite being treated for depressive disorders twice as often as men (Nolen-Hoeksema 2002), women report just as high levels of well-being as men do (Myers 1992).\n\n^5^ Apparently, the joys and stresses of parenthood balance each other out, as people with and without children are equally happy (Argyle 2001).\n\n^6^ Both IQ and educational attainment appear to be unrelated to happiness (Diener et al. 2009; Ross & Van Willigen 1997).\n\n^7^ Good-looking people enjoy huge advantages, but do not report greater happiness than others (Diener et al. 1995).\n\n^8^ The correlation between income and happiness is surprisingly weak (Diener & Seligman 2004; Diener et al. 1993; Johnson & Krueger 2006). One problem may be that higher income contributes to greater materialism, which impedes happiness (Frey & Stutzer 2002; Kasser et al. 2004; Solberg et al. 2002; Kasser 2002; Van Boven 2005; Nickerson et al. 2003; Kahneman et al. 2006).\n\n^9^ Those with disabling health conditions are happier than you might think (Myers 1992; Riis et al. 2005; Argyle 1999).\n\n^10^ Those who are satisfied with their social life are moderately more happy than others (Diener & Seligman 2004; Myers 1999; Diener & Seligman 2002).\n\n^11^ Religiosity correlates with happiness (Abdel-Kahlek 2005; Myers 2008), though it may be religious attendance and not religious belief that matters (Chida et al. 2009).\n\n^12^ Past happiness is the best predictor of future happiness (Lucas & Diener 2008). Happiness is surprisingly unmoved by external factors (Lykken & Tellegen 1996), because the genetics accounts for about 50% of the variance in happiness (Lyubomirsky et al. 2005; Stubbe et al. 2005).\n\n^13^ Married people are happier than those who are single or divorced (Myers & Diener 1995; Diener et al. 2000), and marital satisfaction predicts happiness (Proulx et al. 2007).\n\n^14^ Unemployment makes people very unhappy (Argyle 2001), and job satisfaction is strongly correlated with happiness (Judge & Klinger 2008; Warr 1999).\n\n#### References\n\nAbdel-Khalek (2006). \"[Happiness, health, and religiosity: Significant relations](http://commonsenseatheism.com/wp-content/uploads/2011/01/Abdel-Khalek-Happiness-health-and-religiosity-Significant-relations.pdf).\" _Mental Health_, 9(1): 85-97.\n\nAgarwal, Karpicke, Kang, Roediger, & McDermott (2008). \"[Examining the testing effect with open- and closed-book tests.](http://commonsenseatheism.com/wp-content/uploads/2011/01/Agarwal-Examining-the-Testing-Effect.pdf)\" _Applied Cognitive Psychology_, 22: 861-876.\n\nAllgood, Risko, Alvarez, & Fairbanks (2000). \"Factors that influence study.\" In Flippo & Caverly, (Eds.), _Handbook of college reading and study strategy research_. Mahwah, NJ: Erlbaum.\n\nArgyle (1999). \"Causes and correlates of happiness.\" In Kahneman, Diener, & Schwartz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Sage.\n\nArgyle (2001). _The Psychology of Happiness_ (2nd ed.). New York: Routledge.\n\nBuckley (1998). _God is My Broker: A Monk-Tycoon Reveals the 7 1/2 Laws of Spiritual and Financial Growth_. New York: Random House.\n\nButler & Roediger (2008). \"[Feedback enhances the positive effects and reduces the negative effects of multiple-choice testing](http://commonsenseatheism.com/wp-content/uploads/2011/01/Butler-Feedback-enhances-the-positive-effects.pdf).\" _Memory & Cognition_, 36(3).\n\nChida, Steptoe, & Powell (2009). \"[Religiosity/Spirituality and Mortality](http://commonsenseatheism.com/wp-content/uploads/2011/01/Chida-Religiosity-Spirituality-and-Mortality.pdf).\" _Psychotherapy and Psychosomatics_, 78(2): 81-90.\n\nCepeda, Pashler, Vul, Wixted, & Rohrer (2006). \"[Distributed practice in verbal recall tasks: A review and quantitative synthesis](http://commonsenseatheism.com/wp-content/uploads/2011/01/Cepeda-Distributed-Practice-in-Verbal-Recall-Trasks.pdf).\" _Psychological Bulletin_, 132: 354-380.\n\nDiener, Sandvik, Seidlitz, & Diener (1993). \"[The relationship between income and subjective well-being: Relative or absolute?](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-The-relationship-between-income-and-subjective-well-being-Relative-or-absolute.pdf)\" _Social Indicators Research_, 28: 195-223.\n\nDiener, Wolsic, & Fujita (1995). \"[Physical attractiveness and subjective well-being](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Physical-Attractiveness-and-Subjective-Well-Being.pdf).\" _Journal of Personality and Social Psychology_, 69: 120-129.\n\nDiener, Gohm, Suh, & Oishi (2000). \"[Similarity of the relations between marital status and subjective well-being across cultures](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Similarity-of-the-relations-between-marital-status-and-subjective-well-being-across-cultures.pdf).\" _Journal of Cross-Cultural Psychology_, 31: 419-436.\n\nDiener & Seligman (2002). \"[Very happy people](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Very-Happy-People.pdf).\" _Psychological Science_, 13: 80-83.\n\nDiener & Seligman (2004). \"[Beyond money: Toward an economy of well-being](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Beyond-money.pdf).\" _Psychological Science in the Public Interest_, 5(1): 1-31.\n\nDiener, Kesebir, & Tov (2009). \"Happiness\" In Leary & Hoyle (Eds.), _Handbook of Individual Differences in Social Behavior_ (pp. 147-160). New York: Guilford.\n\nEinstein & McDaniel (2004). _Memory Fitness: A Guide for Successful Aging_. New Haven, CT: Yale University Press.\n\nFrey & Stutzer (2002). \"[What can economists learn from happiness research?](http://commonsenseatheism.com/wp-content/uploads/2011/01/Frey-What-can-economists-learn-from-happiness-research.pdf)\" _Journal of Economic Literature_, 40: 402-435.\n\nHermann, Raybeck, & Gruneberg (2002). _Improving memory and study skills: Advances in theory and practice._ Ashland, OH: Hogrefe & Huber.\n\nIaccino (1996). \"A further examination of the bizarre imagery mnemonic: Its effectiveness with mixed context and delayed testing. _Perceptual & Motor Skills_, 83: 881-882.\n\nInglehart (1990). _Culture shift in advanced industrial society_. Princeton, NJ: Princeton University Press.\n\nJohnson & Krueger (2006). \"How money buys happiness: Genetic and environmental processes linking finances and life satisfaction.\" _Journal of Personality and Social Psychology_, 90: 680-691.\n\nJudge & Klinger (2008). \"Job satisfaction: Subjective well-being at work.\" In Eid & Larsen (Eds.), _The science of subjective well-being_ (pp. 393-413). New York: Guilford.\n\nKahneman, Krueger, Schkade, Schwarz, & Stone (2006). \"[Would you be happier if you were richer? A focusing illusion](http://commonsenseatheism.com/wp-content/uploads/2011/01/Kahneman-Would-you-be-happier-if-you-were-richer-A-focusing-illusion.pdf).\" _Science_, 312: 1908-1910.\n\nKasser (2002). _The high prices of materialism_. Cambridge, MA: MIT Press.\n\nKasser, Ryan, Couchman, & Sheldon (2004). \"Materialistic values: Their causes and consequences.\" In Kasser & Kanner (Eds.), _Psychology and consumer culture: The struggle for a good life in a materialistic world_. Washington DC: American Psychological Association.\n\nKarpicke & Roediger (2003). \"[The critical importance of retrieval for learning](http://commonsenseatheism.com/wp-content/uploads/2011/01/Karpicke-The-critical-importance-of-retrieval-for-learning.pdf).\" _Science_, 319: 966-968. \n\nKarpicke & Roediger (2007). \"[Expanding retrieval practice promotes short-term retention, but equally spaced retrieval enhances long-term retention](http://commonsenseatheism.com/wp-content/uploads/2011/01/Karpicke-Expanding-retrieval-practice-promotes-short-term-retention-but-equally-spaced-retrieval-enhances-long-term-retention.pdf).\" _Journal of Experimental Psychology: Learning, Memory, and Cognition_, 33(4): 704-719.\n\nLucas & Diener (2008). \"Personality and subjective well-being.\" In John, Robins, & Pervin (Eds.), _Handbook of personality: Theory and research_ (pp. 796-814). New York: Guilford.\n\nLyubomirsky, Sheldon, & Schkade (2005). \"[Pursuing happiness: The architecture of sustainable change](http://commonsenseatheism.com/wp-content/uploads/2011/01/Lyubomirsky-Pursuing-happiness-The-architecture-of-sustainable-change.pdf).\" _Review of General Psychology_, 9(2), 111-131.\n\nLykken & Tellegen (1996). \"[Happiness is a stochastic phenomenon](http://commonsenseatheism.com/wp-content/uploads/2011/01/Lykken-Happiness-Is-a-Stochastic-Phenomenon.pdf).\" _Psychological Science_, 7: 186-189.\n\nLykken (1999). _Happiness: The nature and nurture of joy and contentment_. New York: St. Martin's.\n\nMassen & Vaterrodt-Plunnecke (2006). \"The role of proactive interference in mnemonic techniques.\" _Memory_, 14: 189-196.\n\nMcDaniel, Waddill, & Shakesby (1996). \"Study strategies, interest, and learning from Text: The application of material appropriate processing.\" In Herrmann, McEvoy, Hertzog, Hertel, & Johnson (Eds.), _Basic and applied memory research: Theory in context_ (Vol 1). Mahwah, NJ: Erlbaum.\n\nMiller (1969). \"On turning psychology over to the unwashed.\" _Psychology Today_, 3(7), 53–54, 66–68, 70, 72, 74.\n\nMoe & De Beni (2004). \"Studying passages with the loci method: Are subject-generated more effective than experimenter-supplied loci?\" _Journal of Mental Imagery_, 28(3-4): 75-86.\n\nMoe & De Beni (2005). \"[Stressing the efficacy of the Loci method: oral presentation and the subject-generation of the Loci pathway with expository passages](http://commonsenseatheism.com/wp-content/uploads/2011/01/Moe-Stressing-the-efficacy-of-the-Loci-method.pdf).\" _Applied Cognitive Psychology_, 19(1): 95-106.\n\nMyers (1992). _The pursuit of happiness: Who is happy, and why_. New York: Morrow.\n\nMyers & Diener (1995). \"[Who is happy?](http://commonsenseatheism.com/wp-content/uploads/2011/01/Myers-who-is-happy.pdf)\" _Psychological Science_, 6: 10-19.\n\nMyers & Diener (1997). \"The pursuit of happiness.\" _Scientific American, Special Issue 7_: 40-43.\n\nMyers (1999). \"Close relationships and quality of life.\" In Kahnemann, Diener, & Schwarz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Sage.\n\nMyers (2008). \"Religion and human flourishing.\" In Eid & Larsen (Eds.), _The science of subjective well-being_ (pp. 323-346). New York: Guilford.\n\nNickerson, Schwartz, Diener, & Kahnemann (2003). \"[Zeroing in on the dark side of the American dream: A closer look at the negative consequences of the goal for financial success](http://commonsenseatheism.com/wp-content/uploads/2011/01/Nickerson-Zeroing-in-on-the-dark-side-of-the-american-dream.pdf).\" _Psychological Science_, 14(6): 531-536.\n\nNolen-Hoeksema (2002). \"[Gender differences in depression](http://commonsenseatheism.com/wp-content/uploads/2011/01/Nolen-Hoeksema-Gender-differences-in-depression.pdf).\" In Gotlib & Hammen (Eds.), _Handbook of Depression_. New York: Guilford.\n\nProulx, Helms, & Cheryl (2007). \"[Marital quality and personal well-being: A Meta-analysis](http://commonsenseatheism.com/wp-content/uploads/2011/01/Proulx-Marital-Quality-and-Personal-Well‐Being-A-Meta‐Analysis.pdf).\" _Journal of Marriage and Family_, 69: 576-593.\n\nRoediger & Karpicke (2006a). \"[Test-enhanced learning: Taking memory tests improves long-term retention](http://commonsenseatheism.com/wp-content/uploads/2011/01/Roediger-Test-Enhanced-Learning.pdf).\" _Psychological Science_, 17: 249-255.\n\nRoediger & Karpicke (2006b). \"[The power of testing memory: Basic research and implications for educational practice](http://commonsenseatheism.com/wp-content/uploads/2011/01/Roediger-The-power-of-testing-memory.pdf).\" _Perspectives on Psychological Science_, 1(3): 181-210.\n\nRiis, Loewenstein, Baron, Jepson, Fagerlin, & Ubel (2005). \"Ignorance of hedonic adaptation to hemodialysis: A study using ecological momentary assessment.\" _Journal of Experimental Psychology: General_, 134: 3-9.\n\nRohrer & Taylor (2006). \"The effects of over-learning and distributed practice on the retention of mathematics knowlege. _Applied Cognitive Psychology_, 20: 1209-1224. \n\nRohrer, Taylor, Pashler, Wixted, & Cepeda (2005). \"[The Effect of Overlearning on Long-Term Retention](http://commonsenseatheism.com/wp-content/uploads/2011/01/Rohrer-The-Effect-of-Overlearning-on-Long-Term-Retention.pdf).\" _Applied Cognitive Psychology_, 19: 361-374.\n\nRoss & Van Willigen (1997). \"[Education and the subjective quality of life](http://commonsenseatheism.com/wp-content/uploads/2011/01/Ross-Education-and-the-subjective-quality-of-life.pdf).\" _Journal of Health & Social Behavior_, 38: 275-297.\n\nSeabrook, Brown, & Solity (2005). \"[Distributed and massed practice: From laboratory to class-room](http://commonsenseatheism.com/wp-content/uploads/2011/01/Saebrook-Distributed-and-massed-practice-From-laboratory-to-classroom.pdf).\" _Applied Cognitive Psychology_, 19(1): 107-122.\n\nSolberg, Diener, Wirtz, Lucas, & Oishi (2002). \"[Wanting, having, and satisfaction: Examining the role of desire discrepancies in satisfaction with income](http://commonsenseatheism.com/wp-content/uploads/2011/01/Solberg-Wanting-Having-and-Satisfaction-Examining-the-Role-of-Desire-Discrepancies-in-Satisfaction-With-Income.pdf).\" _Journal of Personality and Social Psychology_, 83(3): 725-734.\n\nStubbe, Posthuma, Boomsa, & De Geus (2005). \"[Heritability and life satisfaction in adults: A twin-family study](http://commonsenseatheism.com/wp-content/uploads/2011/01/Stubbe-Heritability-of-life-satisfaction-in-adults-A-twin-family-study.pdf).\" _Psychological Medicine_, 35: 1581-1588.\n\nTigner (1999). \"[Putting memory research to good use: Hints from cognitive psychology](http://commonsenseatheism.com/wp-content/uploads/2011/01/Tigner-Putting-Memory-Research-to-Good-Use.pdf).\" _College Teaching_, 47(4): 149-151.\n\nVan Boven (2005). \"Experientialism, materialism, and the pursuit of happiness.\" _Review of General Psychology_, 9(2): 132-142.\n\nWarr (1999). \"Well-being and the workplace.\" In Kahneman, Diener, & Schwartz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Sage.\n\nWong (2006). _Essential Study Skills_. Boston: Houghton Mifflin.\n\nWorthen (1997). \"Resiliency of bizarreness effects under varying conditions of verbal and imaginal elaboration and list composition. _Journal of Mental Imagery_, 21: 167-194."
          },
          "voteCount": 184
        },
        {
          "name": "Cached Selves",
          "type": "post",
          "slug": "cached-selves",
          "_id": "BHYBdijDcAKQ6e45Z",
          "url": null,
          "title": "Cached Selves",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Identity"
            },
            {
              "name": "Cached Thoughts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "by Anna Salamon and Steve Rayhawk (joint authorship)\n\nRelated to: [Beware identity](http://www.overcomingbias.com/2008/05/beware-identity.html)  \n\n***Update, 2021:** I believe a large majority of the priming studies failed replication, though I haven't looked into it in depth.  I still personally do a great many of the \"possible strategies\" listed at the bottom; and they subjectively seem useful to me; but if you end up believing that it should not be on the basis of the claimed studies.*\n  \nA few days ago, Yvain [introduced](/lw/3b/never_leave_your_room/) us to priming, the effect where, in Yvain’s words, \"any random thing that happens to you can hijack your judgment and personality for the next few minutes.\"  \n  \nToday, I’d like to discuss a related effect from the social psychology and marketing literatures: “commitment and consistency effects”, whereby any random thing _you say or do_ in the absence of obvious outside pressure, can hijack your self-concept _for the medium- to long-term future_.   \n  \nTo sum up the principle briefly: your brain builds you up a self-image. You are the kind of person who says, and does... whatever it is your brain remembers you saying and doing.  So if you say you believe X... especially if no one’s holding a gun to your head, and it looks superficially as though you endorsed X “by choice”... you’re liable to “go on” believing X afterwards.  Even if you said X because you were lying, or because a salesperson tricked you into it, or because your neurons and the wind just happened to push in that direction at that moment.  \n  \nFor example, if I hang out with a bunch of Green Sky-ers, and I make small remarks that accord with the Green Sky position so that they’ll like me, I’m liable to end up a Green Sky-er myself.  If my friends ask me what I think of their poetry, or their rationality, or of how they look in that dress, and I choose my words slightly on the positive side, I’m liable to end up with a falsely positive view of my friends.  If I get promoted, and I start telling my employees that of course rule-following is for the best (because I want them to follow my rules), I’m liable to start believing in rule-following in general.  \n  \nAll familiar phenomena, right?  You probably already discount other peoples’ views of their friends, and you probably already know that _other_ people mostly stay stuck in their own bad initial ideas.  But if you’re like me, you might not have looked carefully into the mechanisms behind these phenomena.  And so you might not realize how much arbitrary influence consistency and commitment is having on [your own](http://www.overcomingbias.com/2007/04/knowing_about_b.html) beliefs, or how you can reduce that influence.  (Commitment and consistency isn’t the only mechanism behind the above phenomena; but it is _a_ mechanism, and it’s one that’s more likely to persist even after you decide to [value](http://hanson.gmu.edu/belieflikeclothes.html) truth.)  \n  \nConsider the following research.  \n  \nIn the classic 1959 [study](http://psychclassics.yorku.ca/Festinger/) by Festinger and Carlsmith, test subjects were paid to tell others that a tedious experiment has been interesting.  Those who were paid $20 to tell the lie continued to believe the experiment boring; those paid a mere $1 to tell the lie were liable later to report the experiment interesting.  The theory is that the test subjects remembered calling the experiment interesting, and either:\n\n1.  Honestly figured they must have found the experiment interesting -- why else would they have said so for only $1?  (This interpretation is called [self-perception](http://en.wikipedia.org/wiki/Self-perception_theory) theory.), or\n2.  Didn’t want to think they were the type to lie for just $1, and so deceived themselves into thinking their lie had been true.  (This interpretation is one strand within [cognitive dissonance](http://en.wikipedia.org/wiki/Cognitive_dissonance) theory.)\n\n  \nIn a [follow-up](http://osil.psy.ua.edu:16080/~Rosanna/Soc_Inf/week9/693_long.pdf), Jonathan Freedman used threats to convince 7- to 9-year old boys not to play with an attractive, battery-operated robot.  He also told each boy that such play was “wrong”.  Some boys were given big threats, or were kept carefully supervised while they played -- the equivalents of Festinger’s $20 bribe.  Others were given mild threats, and left unsupervised -- the equivalent of Festinger’s $1 bribe.  Later, instead of asking the boys about their verbal beliefs, Freedman arranged to test their actions.  He had an apparently unrelated researcher leave the boys alone with the robot, this time giving them explicit permission to play.  The results were as predicted.  Boys who’d been given big threats or had been supervised, on the first round, mostly played happily away.  Boys who’d been given only the mild threat mostly refrained.  Apparently, their brains had looked at their earlier restraint, seen no harsh threat and no experimenter supervision, and figured that not playing with the attractive, battery-operated robot was the way they wanted to act.  \n  \nOne interesting take-away from Freedman’s experiment is that consistency effects change what we do -- they change the “near thinking” beliefs that drive our decisions -- and not just our verbal/propositional claims about our beliefs.  A second interesting take-away is that this belief-change happens even if we aren’t thinking much -- Freedman’s subjects were children, and a [related](http://jbd.sagepub.com/cgi/content/refs/1/4/355) “forbidden toy” experiment found a similar effect even in pre-schoolers, who just barely have propositional reasoning at all.  \n  \nOkay, so how large can such “consistency effects” be?  And how obvious are these effects -- now that you know the concept, are you likely to notice when consistency pressures change your beliefs or actions?  \n  \nIn what is perhaps the most unsettling study I’ve heard along these lines, Freedman and Fraser had an ostensible “volunteer” go door-to-door, asking homeowners to put a big, ugly “Drive Safely” sign in their yard.  In the control group, homeowners were just asked, straight-off, to put up the sign.  Only 19% said yes.  With this baseline established, Freedman and Fraser tested out some commitment and consistency effects.  First, they chose a similar group of homeowners, and they got a new “volunteer” to ask these new homeowners to put up a tiny three inch “Drive safely” sign; nearly everyone said yes.  Two weeks later, the original volunteer came along to ask about the big, badly lettered signs -- and 76% of the group said yes, perhaps moved by their new self-image as people who cared about safe driving.  Consistency effects were working.  \n  \nThe unsettling part comes next; Freedman and Fraser wanted to know how _apparently unrelated_ the consistency prompt could be.  So, with a third group of homeowners, they had a “volunteer” for an ostensibly unrelated non-profit ask the homeowners to sign a petition to “keep America beautiful”.  The petition was innocuous enough that nearly everyone signed it.  And two weeks later, when the original guy came by with the big, ugly signs, nearly half of the homeowners said yes -- a significant boost above the 19% baseline rate.  Notice that the “keep America beautiful” petition that prompted these effects was: (a) a tiny and un-memorable choice; (b) on an apparently unrelated issue (“keeping America beautiful” vs. “driving safely”); and (c) _two weeks_ before the second “volunteer”’s sign request (so we are observing medium-term attitude change from a single, brief interaction).   \n  \nThese consistency effects are reminiscent of Yvain’s large, unnoticed priming effects -- except that they’re based on your actions rather than your sense-perceptions, and the influences last over longer periods of time.  Consistency effects make us likely to stick to our past ideas, good or bad.  They make it easy to freeze ourselves into our initial postures of [disagreement](/lw/3h/why_our_kind_cant_cooperate/), or agreement.  They leave us vulnerable to a variety of sales tactics.  They mean that if I’m working on a cause, even a “rationalist” cause, and I say things to try to engage new people, befriend potential donors, or get core group members to collaborate with me, my beliefs are liable to move toward whatever my allies want to hear.  \n  \nWhat to do?  \n  \nSome possible strategies (I’m not recommending these, just putting them out there for consideration):\n\n1.  **Reduce external pressures on your speech and actions**, so that you won’t make so many pressured decisions, and your brain won’t cache those pressure-distorted decisions as indicators of your real beliefs or preferences.  For example:\n    *   **1a.  Avoid petitions, and other socially prompted or incentivized speech.**  Cialdini takes this route, in part.  He writes: “\\[The Freedman and Fraser study\\] scares me enough that I am rarely willing to sign a petition anymore, even for a position I support.  Such an action has the potential to influence not only my future behavior but also my self-image in ways I may not want.”\n    *   **1b.  Tenure, or independent wealth.**\n    *   **1c.  Anonymity.**\n    *   **1d.  Leave yourself “[social lines of retreat](/lw/3k/how_to_not_lose_an_argument/)”**: avoid making definite claims of a sort that would be embarrassing to retract later.  Another tactic here is to tell people in advance that you often change your mind, so that you’ll be under less pressure not to.\n2.  **Only say things you don’t mind being consistent with.**  For example:\n    *   **2a.  Hyper-vigilant honesty.**  Take care never to say anything but what is best supported by the evidence, aloud or to yourself, lest you come to believe it.\n    *   **2b.  Positive hypocrisy.**  Speak and act like the person you wish you were, in hopes that you’ll come to be them.  ([Apparently](http://www.washingtonpost.com/wp-dyn/content/article/2009/01/05/AR2009010501863.html) this works.)\n3.  **Change or weaken your brain’s notion of “consistent”.**  Your brain has to be using prediction and classification methods in order to generate “[consistent](http://www.overcomingbias.com/2008/08/unnatural-categ.html)” behavior, and these can be hacked. \n    *   **3a.  Treat $1 like a gun.**  Regard the decisions you made under slight monetary or social incentives as like decisions you made at gunpoint -- decisions that say more about the external pressures you were under, or about random dice-rolls in your brain, than about the truth.  Take great care not to rationalize your past actions.\n    *   **3b.  [](http://www.overcomingbias.com/2008/12/devils-offers.html?cid=143466704#comment-143466704)** **[Build emotional comfort with lying](http://www.overcomingbias.com/2008/12/devils-offers.html?cid=143466704#comment-143466704)**, so you won’t be tempted to rationalize your last week’s false claim, or your next week’s political convenience.  Perhaps follow Michael Vassar’s suggestion to lie on purpose in some unimportant contexts. \n    *   **3c.  Reframe your past behavior as having occurred in a different context, and as not bearing on today’s decisions.**  Or add context cues to trick your brain into regarding today's decision as belonging to a different category than past decisions.  This is, for example, part of how conversion experiences can help people change their behavior.  (For a cheap hack, try traveling.)\n    *   **3d.  More specifically, visualize your life as something you just [inherited](http://www.overcomingbias.com/2007/08/the-importance-.html?cid=83687583#comment-83687583) from someone else**; ignore sunk words as you would aspire to ignore sunk costs.\n    *   **3e.  Re-conceptualize your actions into schemas you don’t mind propagating.**  If you’ve just had some conversations and come out believing the Green Sky Platform, don’t say “so, I’m a green sky-er”.  Say “so, I’m someone who changes my opinions based on conversation and reasoning”.  If you’ve incurred repeated library fines, don’t say “I’m so disorganized, always and everywhere”.  Say “I have a pattern of forgetting library due dates; still, I’ve been getting more organized with other areas of my life, and I’ve changed harder habits many times before.”\n4.  **Make a list of the most important consistency pressures on your beliefs, and consciously compensate for them.**  You might either consciously move in the opposite direction (I know I’ve been hanging out with singularitarians, so I somewhat distrust my singularitarian impressions) or take extra pains to apply [rationalist](http://www.overcomingbias.com/2008/02/leave-retreat.html) [tools](http://www.overcomingbias.com/2007/10/avoiding-your-b.html) to any opinions you’re under consistency pressure to have.  Perhaps write [public](/lw/u/the_ethic_of_handwashing_and_community_epistemic/gq#comments) or [private](http://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html) critiques of your consistency-reinforced views (though Eliezer [notes](http://www.overcomingbias.com/2008/06/against-devils.html) reasons for caution with this one).\n5.  **Build more reliably truth-indicative types of thought.**  Ultimately, both priming and consistency effects suggest that our [baseline sanity level](/lw/1e/raising_the_sanity_waterline/) is low; if small interactions can have large, arbitrary effects, our thinking is likely pretty arbitrary to [to begin with](/lw/3b/never_leave_your_room/2c9#comments).  Some avenues of approach:\n    *   **5a.  Improve your general rationality skill**, so that your thoughts have something else to be driven by besides your random cached selves.  (It wouldn’t surprise me if OB/LW-ers are less vulnerable than average to some kinds of consistency effects.  We could test this.)\n    *   **5b.  Take your equals’ opinions as seriously as you take the opinions of your ten-minutes-past self.**  If you often discuss topics with a comparably rational friend, and you two usually end with the same opinion-difference you began with, ask yourself why. An _obvious first hypothesis_ should be “irrational consistency effects”: maybe you’re holding onto particular conclusions, modes of analysis, etc., just because [your self-concept says you believe them](/lw/s/belief_in_selfdeception/).  \n    *   **5c.  Work more often from the raw data**; explicitly distrust your beliefs about what you previously saw the evidence as implying.  Re-derive the wheel, animated by a [core distrust](http://www.overcomingbias.com/2008/05/no-defenses.html) in your past self or [cached conclusions](http://www.overcomingbias.com/2007/10/cached-thoughts.html).  Look for [new thoughts](http://www.overcomingbias.com/2007/10/original-seeing.html)."
          },
          "voteCount": 192
        },
        {
          "name": "Efficient Charity: Do Unto Others...",
          "type": "post",
          "slug": "efficient-charity-do-unto-others",
          "_id": "pC47ZTsPNAkjavkXs",
          "url": null,
          "title": "Efficient Charity: Do Unto Others...",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Altruism"
            },
            {
              "name": "World Optimization"
            },
            {
              "name": "Cause Prioritization"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_This was originally posted as part of the [efficient charity](/lw/35o/100_for_the_best_article_on_efficient_charity/) contest back in November. Thanks to Roko, multifoliaterose, Louie, jmmcd, jsalvatier, and others I forget for help, corrections, encouragement, and bothering me until I finally remembered to post this here._  \n  \nImagine you are setting out on a dangerous expedition through the Arctic on a limited budget. The grizzled old prospector at the general store shakes his head sadly: you can't afford everything you need; you'll just have to purchase the bare essentials and hope you get lucky. But what is essential? Should you buy the warmest parka, if it means you can't afford a sleeping bag? Should you bring an extra week's food, just in case, even if it means going without a rifle? Or can you buy the rifle, leave the food, and hunt for your dinner?  \n  \nAnd how about the field guide to Arctic flowers? You like flowers, and you'd hate to feel like you're failing to appreciate the harsh yet delicate environment around you. And a digital camera, of course - if you make it back alive, you'll have to put the Arctic expedition pics up on Facebook. And a hand-crafted scarf with authentic Inuit tribal patterns woven from organic fibres! Wicked!  \n  \n...but of course buying any of those items would be insane. The problem is what economists call opportunity costs: buying one thing costs money that could be used to buy others. A hand-crafted designer scarf might have some value in the Arctic, but it would cost so much it would prevent you from buying much more important things. And when your life is on the line, things like impressing your friends and buying organic pale in comparison. You have one goal - staying alive - and your only problem is how to distribute your resources to keep your chances as high as possible. These sorts of economics concepts are natural enough when faced with a journey through the freezing tundra.\n\n  \nBut they are decidedly not natural when facing a decision about charitable giving. Most donors say they want to \"help people\". If that's true, they should try to distribute their resources to help people as much as possible. Most people don't. In the [\"Buy A Brushstroke\"](http://www.artfund.org/savebluerigi/Introduction.html) campaign, eleven thousand British donors gave a total of £550,000 to keep the famous painting \"Blue Rigi\" in a UK museum. If they had given that £550,000 to buy better sanitation systems in African villages instead, the latest statistics suggest it would have saved the lives of about one thousand two hundred people from disease. Each individual $50 donation could have given a year of normal life back to a Third Worlder afflicted with a disabling condition like blindness or limb deformity..  \n  \nMost of those 11,000 donors genuinely wanted to help people by preserving access to the original canvas of a beautiful painting. And most of those 11,000 donors, if you asked, would say that a thousand people's lives are more important than a beautiful painting, original or no. But these people didn't have the proper mental habits to realize that was the choice before them, and so a beautiful painting remains in a British museum and somewhere in the Third World a thousand people are dead.  \n  \nIf you are to \"love your neighbor as yourself\", then you should be as careful in maximizing the benefit to others when donating to charity as you would be in maximizing the benefit to yourself when choosing purchases for a polar trek. And if you wouldn't buy a pretty picture to hang on your sled in preference to a parka, you should consider not helping save a famous painting in preference to helping save a thousand lives.  \n  \nNot all charitable choices are as simple as that one, but many charitable choices do have right answers. GiveWell.org, a site which collects and interprets data on the effectiveness of charities, predicts that antimalarial drugs save one child from malaria per $5,000 worth of medicine, but insecticide-treated bed nets save one child from malaria per $500 worth of netting. If you want to save children, donating bed nets instead of antimalarial drugs is the objectively right answer, the same way buying a $500 TV instead of an identical TV that costs $5,000 is the right answer. And since saving a child from diarrheal disease costs $5,000, donating to an organization fighting malaria instead of an organization fighting diarrhea is the right answer, unless you are donating based on some criteria other than whether you're helping children or not.  \n  \nSay all of the best Arctic explorers agree that the three most important things for surviving in the Arctic are good boots, a good coat, and good food. Perhaps they have run highly unethical studies in which they release thousands of people into the Arctic with different combination of gear, and consistently find that only the ones with good boots, coats, and food survive. Then there is only one best answer to the question \"What gear do I buy if I want to survive\" - good boots, good food, and a good coat. Your preferences are irrelevant; you may choose to go with alternate gear, but only if you don't mind dying.  \n  \nAnd likewise, there is only one best charity: the one that helps the most people the greatest amount per dollar. This is vague, and it is up to you to decide whether a charity that raises forty children's marks by one letter grade for $100 helps people more or less than one that prevents one fatal case of tuberculosis per $100 or one that saves twenty acres of rainforest per $100. But you cannot abdicate the decision, or you risk ending up like the 11,000 people who accidentally decided that a pretty picture was worth more than a thousand people's lives.  \n  \nDeciding which charity is the best is hard. It may be straightforward to say that one form of antimalarial therapy is more effective than another. But how do both compare to financing medical research that might or might not develop a \"magic bullet\" cure for malaria? Or financing development of a new kind of supercomputer that might speed up all medical research? There is no easy answer, but the question has to be asked.  \n  \nWhat about just comparing charities on overhead costs, the one easy-to-find statistic that's universally applicable across all organizations? This solution is simple, elegant, and wrong. High overhead costs are only one possible failure mode for a charity. Consider again the Arctic explorer, trying to decide between a $200 parka and a $200 digital camera. Perhaps a parka only cost $100 to make and the manufacturer takes $100 profit, but the camera cost $200 to make and the manufacturer is selling it at cost. This speaks in favor of the moral qualities of the camera manufacturer, but given the choice the explorer should still buy the parka. The camera does something useless very efficiently, the parka does something vital inefficiently. A parka sold at cost would be best, but in its absence the explorer shouldn't hesitate to choose the the parka over the camera. The same applies to charity. An antimalarial net charity that saves one life per $500 with 50% overhead is better than an antidiarrheal drug charity that saves one life per $5000 with 0% overhead: $10,000 donated to the high-overhead charity will save ten lives; $10,000 to the lower-overhead will only save two. Here the right answer is to donate to the antimalarial charity while encouraging it to find ways to lower its overhead. In any case, [examining the financial practices of a charity](http://www.charitynavigator.org/) is helpful but not enough to answer the \"which is the best charity?\" question.  \n  \nJust as there is only one best charity, there is only one best way to donate to that charity. Whether you [volunteer versus donate money](/lw/65/money_the_unit_of_caring/) versus raise awareness is your own choice, but that choice has consequences. If a high-powered lawyer who makes $1,000 an hour chooses to take an hour off to help clean up litter on the beach, he's wasted the opportunity to work overtime that day, make $1,000, donate to a charity that will hire a hundred poor people for $10/hour to clean up litter, and end up with a hundred times more litter removed. If he went to the beach because he wanted the sunlight and the fresh air and the warm feeling of personally contributing to something, that's fine. If he actually wanted to help people by beautifying the beach, he's chosen an objectively wrong way to go about it. And if he wanted to help people, period, he's chosen a very wrong way to go about it, since that $1,000 could save two people from malaria. Unless the litter he removed is really worth more than two people's lives to him, he's erring even according to his own value system.  \n  \n...and the same is true if his philanthropy leads him to work full-time at a nonprofit instead of going to law school to become a lawyer who makes $1,000 / hour in the first place. Unless it's one HELL of a nonprofit.  \n  \nThe Roman historian Sallust said of Cato \"He preferred to be good, rather than to seem so\". The lawyer who quits a high-powered law firm to work at a nonprofit organization certainly seems like a good person. But if we define \"good\" as helping people, then the lawyer who stays at his law firm but donates the profit to charity is taking Cato's path of maximizing how much good he does, rather than how good he looks.  \n  \nAnd this dichotomy between being and seeming good applies not only to looking good to others, but to ourselves. When we donate to charity, one incentive is the [warm glow of a job well done](/lw/6z/purchase_fuzzies_and_utilons_separately/). A lawyer who spends his day picking up litter will feel a sense of personal connection to his sacrifice and relive the memory of how nice he is every time he and his friends return to that beach. A lawyer who works overtime and donates the money online to starving orphans in Romania may never get that same warm glow. But concern with a warm glow is, at root, concern about seeming good rather than being good - albeit seeming good to yourself rather than to others. There's nothing wrong with donating to charity as a form of entertainment if it's what you want - giving money to the Art Fund may well be a quicker way to give yourself a warm feeling than seeing a romantic comedy at the cinema - but charity given by people who genuinely want to be good and not just to feel that way requires more forethought.  \n  \nIt is important to be rational about charity for the same reason it is important to be rational about Arctic exploration: it requires the same awareness of opportunity costs and the same hard-headed commitment to investigating efficient use of resources, and it may well be a matter of life and death. Consider going to [www.GiveWell.org](http://www.givewell.org) and making use of the excellent resources on effective charity they have available."
          },
          "voteCount": 158
        },
        {
          "name": "Making Beliefs Pay Rent (in Anticipated Experiences)",
          "type": "post",
          "slug": "making-beliefs-pay-rent-in-anticipated-experiences",
          "_id": "a7n8GdKiAZRX86T5A",
          "url": null,
          "title": "Making Beliefs Pay Rent (in Anticipated Experiences)",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Epistemology"
            },
            {
              "name": "Empiricism"
            },
            {
              "name": "Principles"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Thus begins the ancient parable:\n\n*If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”*\n\nIf there’s a foundational skill in the martial art of rationality, a mental stance on which all other technique rests, it might be this one: the ability to spot, inside your own head, psychological signs that you have a mental map of something, and signs that you don’t.\n\nSuppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?\n\nThough the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will *happen to* them; their maps of the world do not diverge in any sensory detail.\n\nIt’s tempting to try to eliminate this mistake class by insisting that the only legitimate kind of belief is an anticipation of sensory experience. But the world does, in fact, contain much that is not sensed directly. We don’t see the atoms underlying the brick, but the atoms are in fact there. There is a floor beneath your feet, but you don’t *experience* the floor directly; you see the light *reflected* from the floor, or rather, you see what your retina and visual cortex have processed of that light. To infer the floor from seeing the floor is to step back into the unseen causes of experience. It may seem like a very short and direct step, but it is still a step.\n\nYou stand on top of a tall building, next to a grandfather clock with an hour, minute, and ticking second hand. In your hand is a bowling ball, and you drop it off the roof. On which tick of the clock will you hear the crash of the bowling ball hitting the ground?\n\nTo answer precisely, you must use beliefs like *Earth’s gravity is 9.8 meters per second per second,* and *This building is around 120 meters tall.* These beliefs are not wordless anticipations of a sensory experience; they are verbal-ish, propositional. It probably does not exaggerate much to describe these two beliefs as sentences made out of words. But these two beliefs have an inferential *consequence* that is a direct sensory anticipation—if the clock’s second hand is on the 12 numeral when you drop the ball, you anticipate seeing it on the 1 numeral when you hear the crash five seconds later. To anticipate sensory experiences as precisely as possible, we must process beliefs that are not anticipations of sensory experience.\n\nIt is a great strength of *Homo sapiens* that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal.\n\nThe same brain that builds a network of inferred causes behind sensory experience can also build a network of causes that is not connected to sensory experience, or poorly connected. Alchemists believed that phlogiston caused fire—we could simplistically model their minds by drawing a little node labeled “Phlogiston,” and an arrow from this node to their sensory experience of a crackling campfire—but this belief yielded no advance predictions; the link from phlogiston to experience was always configured after the experience, rather than constraining the experience in advance.\n\nOr suppose your English professor teaches you that the famous writer Wulky Wilkinsen is actually a “retropositional author,” which you can tell because his books exhibit “alienated resublimation.” And perhaps your professor knows all this because their professor told them; but all they're able to say about resublimation is that it's characteristic of retropositional thought, and of retropositionality that it's marked by alienated resublimation. What does this mean you should expect from Wulky Wilkinsen’s books?\n\nNothing. The belief, if you can call it that, doesn’t connect to sensory experience at all. But you had better remember the propositional assertions that “Wulky Wilkinsen” has the “retropositionality” attribute and also the “alienated resublimation” attribute, so you can regurgitate them on the upcoming quiz. The two beliefs are connected to each other, though still not connected to any anticipated experience.\n\nWe can build up whole networks of beliefs that are connected only to each other—call these “floating” beliefs. It is a uniquely human flaw among animal species, a perversion of *Homo sapiens*’s ability to build more general and flexible belief networks.\n\nThe rationalist virtue of *empiricism* consists of constantly asking which experiences our beliefs predict—or better yet, prohibit. Do you believe that phlogiston is the cause of fire? Then what do you expect to see happen, because of that? Do you believe that Wulky Wilkinsen is a retropositional author? Then what do you expect to see because of that? No, not “alienated resublimation”; *what experience will happen to you?* Do you believe that if a tree falls in the forest, and no one hears it, it still makes a sound? Then what experience must therefore befall you?\n\nIt is even better to ask: what experience *must not* happen to you? Do you believe that *Élan vital* explains the mysterious aliveness of living beings? Then what does this belief *not* allow to happen—what would definitely falsify this belief? A null answer means that your belief does not *constrain* experience; it permits *anything* to happen to you. It floats.\n\nWhen you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about. If you can’t find the difference of anticipation, you’re probably arguing about labels in your belief network—or even worse, floating beliefs, barnacles on your network. If you don’t know what experiences are implied by Wulky Wilkinsens writing being retropositional, you can go on arguing forever.\n\nAbove all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. Every guess of belief should begin by flowing to a specific guess of anticipation, and should continue to pay rent in future anticipations. If a belief turns deadbeat, evict it."
          },
          "voteCount": 277
        }
      ]
    },
    {
      "title": "Top-25",
      "children": [
        {
          "name": "About Less Wrong",
          "type": "post",
          "slug": "about-less-wrong",
          "_id": "2om7AHEHtbogJmT5s",
          "url": null,
          "title": "About Less Wrong",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Site Meta"
            },
            {
              "name": "Conjunction Fallacy"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_**Edit: **This post refers to the original version of LessWrong, which ran between February 2009 and March 2018. The About page referring to the period from March 2018 to the present can be found [here](https://www.lesswrong.com/about)._\n\nOver the last decades, new experiments have changed science's picture of the way we think - the ways we succeed or fail to obtain the truth, or fulfill our goals. The heuristics and biases program, in cognitive psychology, has exposed dozens of major flaws in human reasoning. Social psychology shows how we succeed or fail in groups. Probability theory and decision theory have given us new mathematical foundations for understanding minds.\n\nLess Wrong is devoted to refining the art of human [rationality](/lw/31/what_do_we_mean_by_rationality/) \\- the art of thinking. The new math and science deserves to be applied to our daily lives, and heard in our public voices.\n\nLess Wrong consists of three areas: The [main community blog](/), the [Less Wrong wiki](http://wiki.lesswrong.com/) and the [Less Wrong discussion area](/r/discussion/).\n\nLess Wrong is a partially moderated community blog that allows general authors to contribute posts as well as comments. Users vote posts and comments up and down (with code based on [Reddit's open source](http://code.reddit.com/)). \"Promoted\" posts (appearing on the front page) are chosen by the editors on the basis of substantive new content, clear argument, good writing, popularity, and importance.\n\nWe suggest submitting links with a _short_ description. Recommended books should have longer descriptions. Links will not be promoted unless they are truly excellent - the \"promoted\" posts are intended as a filtered stream for the casual/busy reader.\n\nThe Less Wrong discussion area is for topics not yet ready or not suitable for normal top level posts. To post a new discussion, select \"Post to: Less Wrong Discussion\" from the Create new article page. Comment on discussion posts as you would elsewhere on the site.\n\nVotes on posts are worth ±10 points on the main site and ±1 point in the discussion area. Votes on comments are worth ±1 point. Users with sufficient karma can publish posts. You need 20+ points to post to the main area and 2+ points to post to the discussion area. You can only down vote up to four times your current karma (thus if you never comment, you cannot downvote). Comments voted to -3 or lower will be collapsed by default for most readers (if you log in, you can change this setting in your Preferences). Please keep this in mind before writing long, thoughtful, intelligent responses to trolls: most readers will never see your work, and your effort may be better spent elsewhere, in more visible threads. Similarly, if many of your comments are heavily downvoted, please take the hint and change your approach, or choose a different venue for your comments. (Failure to take the hint may lead to moderators deleting future comments.) Spam comments will be deleted immediately. Off-topic top-level posts may be removed.\n\nWe reserve the right for moderators to change contributed posts or comments to fix HTML problems or other misfeatures. Moderators may add or remove tags.\n\nLess Wrong is brought to you by the [Future of Humanity Institute at Oxford University](http://www.fhi.ox.ac.uk/). Neither FHI nor Oxford University necessarily endorses any specific views appearing anywhere on Less Wrong. Copyright is retained by each author, but we reserve the non-exclusive right to move, archive, or otherwise reprint posts and comments.\n\nSample posts:\n\n<table border=\"0\" cellspacing=\"10\"><tbody><tr valign=\"top\"><td><a href=\"/lw/gw/politics_is_the_mindkiller/\">Politics is the Mind-Killer</a></td><td><a href=\"http://www.overcomingbias.com/2007/05/policy_tugowar.html\">Policy Tug-O-War</a></td><td><a href=\"/lw/i0/are_your_enemies_innately_evil/\">Are Your Enemies Innately Evil?</a></td></tr><tr valign=\"top\"><td><a href=\"/lw/lg/the_affect_heuristic/\">The Affect Heuristic</a></td><td><a href=\"/lw/jx/we_change_our_minds_less_often_than_we_think/\">We Change Our Minds Less Often Than We Think</a></td><td><a href=\"/lw/jj/conjunction_controversy_or_how_they_nail_it_down/\">Conjunction Controversy</a></td></tr><tr valign=\"top\"><td><a href=\"/lw/hp/feeling_rational/\">Feeling Rational</a></td><td><a href=\"/lw/he/knowing_about_biases_can_hurt_people/\">Knowing About Biases Can Hurt You</a></td><td><a href=\"/lw/nc/newcombs_problem_and_regret_of_rationality/\">Newcomb's Problem</a></td></tr><tr valign=\"top\"><td><a href=\"/lw/oj/probability_is_in_the_mind/\">Probability is in the Mind</a></td><td><a href=\"/lw/ih/absence_of_evidence_is_evidence_of_absence/\">Absence of Evidence is Evidence of Absence</a></td><td><a href=\"/lw/jo/einsteins_arrogance/\">Einstein's Arrogance</a></td></tr><tr valign=\"top\"><td><a href=\"/lw/js/the_bottom_line/\">The Bottom Line</a></td><td><a href=\"/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\">Making Beliefs Pay Rent</a></td><td><a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">Tsuyoku Naritai</a></td></tr></tbody></table>\n\n**To read through Less Wrong systematically, use the [Sequences](http://wiki.lesswrong.com/wiki/Sequences).**\n\nLess Wrong was established as a sister site to the blog [Overcoming Bias](http://www.overcomingbias.com/), where Eliezer Yudkowsky originally began blogging under the chief editorship of Robin Hanson, and contains many old posts imported from OB.\n\nTo some extent, the software of LW is still under development. The code that runs this site is forked from the open source [Reddit base](http://code.reddit.com/), and is [hosted on Github](http://github.com/tricycle/lesswrong/). Our public issue tracker is hosted at [Google Code](http://code.google.com/p/lesswrong/). Contributions of [code](http://github.com/tricycle/lesswrong/) or [issues](http://code.google.com/p/lesswrong/) are welcome and volunteer Python developers are cordially solicited (see [this post](/lw/1t/wanted_python_open_source_volunteers/)). More information on how to contribute is available at the [LW Github wiki](https://github.com/tricycle/lesswrong/wiki).\n\nLess Wrong is hosted and maintained by [Trike Apps](http://trikeapps.com/)."
          },
          "voteCount": 57
        },
        {
          "name": "Hindsight Devalues Science",
          "type": "post",
          "slug": "hindsight-devalues-science",
          "_id": "WnheMGAka4fL99eae",
          "url": null,
          "title": "Hindsight Devalues Science",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Fallacies"
            },
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Hindsight Bias"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "This essay is closely based on an [excerpt](https://web.archive.org/web/20170801042830/http://csml.som.ohio-state.edu:80/Music829C/hindsight.bias.html) from Meyers’s *Exploring Social Psychology*; the excerpt is worth reading in its entirety.\n\nCullen Murphy, editor of *The Atlantic*, said that the social sciences turn up “no ideas or conclusions that can’t be found in \\[any\\] encyclopedia of quotations . . . Day after day social scientists go out into the world. Day after day they discover that people’s behavior is pretty much what you’d expect.”\n\nOf course, the “expectation” is all [hindsight](http://lesswrong.com/lw/il/hindsight_bias/). (Hindsight bias: Subjects who know the actual answer to a question assign much higher probabilities they “would have” guessed for that answer, compared to subjects who must guess without knowing the answer.)\n\nThe historian Arthur Schlesinger, Jr. dismissed scientific studies of World War II soldiers’ experiences as “ponderous demonstrations” of common sense. For example:\n\n1.  Better educated soldiers suffered more adjustment problems than less educated soldiers. (Intellectuals were less prepared for battle stresses than street-smart people.) \n2.  Southern soldiers coped better with the hot South Sea Island climate than Northern soldiers. (Southerners are more accustomed to hot weather.) \n3.  White privates were more eager to be promoted to noncommissioned officers than Black privates. (Years of oppression take a toll on achievement motivation.) \n4.  Southern Blacks preferred Southern to Northern White officers. (Southern officers were more experienced and skilled in interacting with Blacks.) \n5.  As long as the fighting continued, soldiers were more eager to return home than after the war ended. (During the fighting, soldiers knew they were in mortal danger.)\n\nHow many of these findings do you think you *could have* predicted in advance? Three out of five? Four out of five? Are there any cases where you would have predicted the opposite—where your model takes a hit? Take a moment to think before continuing . . .\n\n. . .\n\nIn this demonstration (from Paul Lazarsfeld by way of Meyers), all of the findings above are the *opposite* of what was actually found.^1^ How many times did you think your model took a hit? How many times did you admit you would have been wrong? That’s how good your model really was. The measure of your strength as a rationalist is your ability to be more confused by fiction than by reality.\n\nUnless, of course, I reversed the results again. What do you think?\n\nDo your thought processes at this point, where you *really don’t* know the answer, feel different from the thought processes you used to rationalize either side of the “known” answer?\n\nDaphna Baratz exposed college students to pairs of supposed findings, one true (“In prosperous times people spend a larger portion of their income than during a recession”) and one the truth’s opposite.^2^ In both sides of the pair, students rated the supposed finding as what they “would have predicted.” Perfectly standard hindsight bias.\n\nWhich leads people to think they have no need for science, because they “could have predicted” that.\n\n(Just as you would expect, right?)\n\nHindsight will lead us to systematically undervalue the surprisingness of scientific findings, especially the discoveries we *understand*—the ones that seem real to us, the ones we can retrofit into our models of the world. If you understand neurology or physics and read news in that topic, then you probably underestimate the surprisingness of findings in those fields too. This unfairly devalues the contribution of the researchers; and worse, will prevent you from noticing when you are seeing evidence that doesn’t fit what you *really* would have expected.\n\nWe need to make a conscious effort to be shocked *enough*.\n\n* * *\n\n^1^ Paul F. Lazarsfeld, “The American Solidier—An Expository Review,” *Public Opinion* *Quarterly* 13, no. 3 (1949): 377–404.\n\n^2^ Daphna Baratz, *How Justified Is the “Obvious” Reaction?* (Stanford University, 1983)."
          },
          "voteCount": 165
        },
        {
          "name": "Tsuyoku Naritai! (I Want To Become Stronger)",
          "type": "post",
          "slug": "tsuyoku-naritai-i-want-to-become-stronger",
          "_id": "DoLQN5ryZ9XkZjq5h",
          "url": null,
          "title": "Tsuyoku Naritai! (I Want To Become Stronger)",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Ambition"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Something To Protect"
            },
            {
              "name": "Tsuyoku Naritai"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "In Orthodox Judaism there is a saying: “The previous generation is to the next one as angels are to men; the next generation is to the previous one as donkeys are to men.” This follows from the Orthodox Jewish belief that all Judaic law was given to Moses by God at Mount Sinai. After all, it’s not as if you could do an experiment to gain new halachic knowledge; the only way you can know is if someone tells you (who heard it from someone else, who heard it from God). Since there is no new source of information; it can only be degraded in transmission from generation to generation.\n\nThus, modern rabbis are not allowed to overrule ancient rabbis. Crawly things are ordinarily unkosher, but it is permissible to eat a worm found in an apple—the ancient rabbis believed the worm was spontaneously generated inside the apple, and therefore was part of the apple. A modern rabbi cannot say, “Yeah, well, the ancient rabbis knew diddly-squat about biology. Overruled!” A modern rabbi cannot possibly know a halachic principle the ancient rabbis did not, because how could the ancient rabbis have passed down the answer from Mount Sinai to him? Knowledge derives from authority, and therefore is only ever lost, not gained, as time passes.\n\nWhen I was first exposed to the angels-and-donkeys proverb in (religious) elementary school, I was not old enough to be a full-blown atheist, but I still thought to myself: “Torah loses knowledge in every generation. Science gains knowledge with every generation. No matter where they started out, sooner or later science must surpass Torah.”\n\nThe most important thing is that there should be progress. So long as you keep moving forward you will reach your destination; but if you stop moving you will never reach it.\n\n_Tsuyoku naritai_ is Japanese. _Tsuyoku_ is “strong”; _naru_ is “becoming,” and the form _naritai_ is “want to become.” Together it means, “I want to become stronger,” and it expresses a sentiment embodied more intensely in Japanese works than in any Western literature I’ve read. You might say it when expressing your determination to become a professional Go player—or after you lose an important match, but you haven’t given up—or after you win an important match, but you’re not a ninth-dan player yet—or after you’ve become the greatest Go player of all time, but you still think you can do better. That is _tsuyoku naritai_, the will to transcendence.\n\nEach year on Yom Kippur, an Orthodox Jew recites a litany which begins _Ashamnu, bagadnu, gazalnu, dibarnu dofi_, and goes on through the entire Hebrew alphabet: _We have acted shamefully, we have betrayed, we have stolen, we have slandered . . ._\n\nAs you pronounce each word, you strike yourself over the heart in penitence. There’s no exemption whereby, if you manage to go without stealing all year long, you can skip the word _gazalnu_ and strike yourself one less time. That would violate the community spirit of Yom Kippur, which is about _confessing_ sins—not _avoiding_ sins so that you have less to confess.\n\nBy the same token, the _Ashamnu_ does not end, “But that was this year, and next year I will do better.”\n\nThe _Ashamnu_ bears a remarkable resemblance to the notion that the way of rationality is to beat your fist against your heart and say, “We are all biased, we are all irrational, we are not fully informed, we are overconfident, we are poorly calibrated . . .”\n\nFine. Now tell me how you plan to become _less_ biased, _less_ irrational, _more_ informed, _less_ overconfident, _better_ calibrated.\n\nThere is an old Jewish joke: During Yom Kippur, the rabbi is seized by a sudden wave of guilt, and prostrates himself and cries, “God, I am nothing before you!” The cantor is likewise seized by guilt, and cries, “God, I am nothing before you!” Seeing this, the janitor at the back of the synagogue prostrates himself and cries, “God, I am nothing before you!” And the rabbi nudges the cantor and whispers, “Look who thinks he’s nothing.”\n\nTake no pride in your confession that you too are biased; do not glory in your self-awareness of your flaws. This is akin to the principle of not taking pride in confessing your ignorance; for if your ignorance is a source of pride to you, you may become loath to relinquish your ignorance when evidence comes knocking. Likewise with our flaws—we should not gloat over how self-aware we are for confessing them; the occasion for rejoicing is when we have a little less to confess.\n\nOtherwise, when the one comes to us with a plan for _correcting_ the bias, we will snarl, “Do you think to set yourself above us?” We will shake our heads sadly and say, “You must not be very self-aware.”\n\nNever confess to me that you are just as flawed as I am unless you can tell me what you plan to do about it. Afterward you will still have plenty of flaws left, but that’s not the point; the important thing is to _do better_, to keep moving ahead, to take one more step forward. _Tsuyoku naritai!_"
          },
          "voteCount": 219
        },
        {
          "name": "Mysterious Answers to Mysterious Questions",
          "type": "post",
          "slug": "mysterious-answers-to-mysterious-questions",
          "_id": "6i3zToomS86oj9bS6",
          "url": null,
          "title": "Mysterious Answers to Mysterious Questions",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Map and Territory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Imagine looking at your hand, and knowing nothing of cells, nothing of biochemistry, nothing of DNA. You’ve learned some anatomy from dissection, so you know your hand contains muscles; but you don’t know why muscles move instead of lying there like clay. Your hand is just . . . stuff . . . and for some reason it moves under your direction. Is this not magic?\n\n> It seemed to me then, and it still seems to me, most probable that the animal body does not act as a thermodynamic engine . . . The influence of animal or vegetable life on matter is infinitely beyond the range of any scientific inquiry hitherto entered on. Its power of directing the motions of moving particles, in the demonstrated daily miracle of our human free-will, and in the growth of generation after generation of plants from a single seed, are infinitely different from any possible result of the fortuitous concourse of atoms\\[.\\]^1^\n> \n> \\[C\\]onsciousness teaches every individual that they are, to some extent, subject to the direction of his will. It appears, therefore, that animated creatures have the power of immediately applying, to certain moving particles of matter within their bodies, forces by which the motions of these particles are directed to produce desired mechanical effects.^2^\n> \n> Modern biologists are coming once more to a firm acceptance of something beyond mere gravitational, chemical, and physical forces; and that unknown thing is a vital principle.^3^\n> \n> —Lord Kelvin\n\nThis was the theory of *vitalism* ; that the mysterious difference between living matter and non-living matter was explained by an *Élan vital* or *vis vitalis*. *Élan vital* infused living matter and caused it to move as consciously directed. *Élan vital* participated in chemical transformations which no mere non-living particles could undergo—Wöhler’s later synthesis of urea, a component of urine, was a major blow to the vitalistic theory because it showed that mere *chemistry* could duplicate a product of biology.\n\nCalling “Élan vital” an explanation, even a fake explanation like phlogiston, is probably giving it too much credit. It functioned primarily as a curiosity-stopper. You said “Why?” and the answer was “Élan vital!”\n\nWhen you say “Élan vital!” it *feels* like you know why your hand moves. You have a little causal diagram in your head that says:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1586123558/MysteriousAnswersToMysteriousQuestions_diagram_1_dr5tbq.svg)\n\nBut actually you know nothing you didn’t know before. You don’t know, say, whether your hand will generate heat or absorb heat, unless you have observed the fact already; if not, you won’t be able to predict it in advance. Your curiosity feels sated, but it hasn’t been fed. Since you can say “Why? Élan vital!” to any possible observation, it is equally good at explaining all outcomes, a disguised hypothesis of maximum entropy, et cetera.\n\nBut the greater lesson lies in the vitalists’ reverence for the *Élan vital*, their eagerness to pronounce it a mystery beyond all science. Meeting the great dragon Unknown, the vitalists did not draw their swords to do battle, but bowed their necks in submission. They took pride in their ignorance, made biology into a *sacred* mystery, and thereby became loath to [relinquish their ignorance](https://www.lesswrong.com/rationality/twelve-virtues-of-rationality) when evidence came knocking.\n\nThe Secret of Life was *infinitely beyond the reach of science!* Not just a *little* beyond, mind you, but *infinitely* beyond! Lord Kelvin sure did get a tremendous emotional kick out of *not knowing something.*\n\nBut ignorance exists in the map, not in the territory. If I am ignorant about a phenomenon, that is a fact about my own state of mind, not a fact about the phenomenon itself. A phenomenon can *seem* mysterious to some particular person. There are no phenomena which are mysterious of themselves. To worship a phenomenon because it seems so wonderfully mysterious is to worship your own ignorance.\n\nVitalism shared with phlogiston the error of *encapsulating the mystery as a substance.* Fire was mysterious, and the phlogiston theory encapsulated the mystery in a mysterious substance called “phlogiston.” Life was a sacred mystery, and vitalism encapsulated the sacred mystery in a mysterious substance called “Élan vital.” Neither answer helped concentrate the model’s probability density—helped make some outcomes easier to explain than others. The “explanation” just wrapped up the question as a small, hard, opaque black ball.\n\nIn a comedy written by Molière, a physician explains the power of a soporific by saying that it contains a “dormitive potency.” Same principle. It is a failure of human psychology that, faced with a mysterious phenomenon, we more readily postulate mysterious inherent substances than complex underlying processes.\n\nBut the deeper failure is supposing that an *answer* can be mysterious. If a phenomenon feels mysterious, that is a fact about our state of knowledge, not a fact about the phenomenon itself. The vitalists saw a mysterious gap in their knowledge, and postulated a mysterious stuff that plugged the gap. In doing so, they mixed up the map with the territory. All confusion and bewilderment exist in the mind, not in encapsulated substances.\n\nThis is the ultimate and fully general explanation for why, again and again in humanity’s history, people are shocked to discover that an incredibly mysterious question has a non-mysterious answer. Mystery is a property of questions, not answers.\n\nTherefore I call theories such as vitalism *mysterious answers to mysterious questions*.\n\nThese are the signs of mysterious answers to mysterious questions:\n\n*   First, the explanation acts as a curiosity-stopper rather than an anticipation-controller.\n*   Second, the hypothesis has no moving parts—the model is not a specific complex mechanism, but a blankly solid substance or force. The mysterious substance or mysterious force may be said to be here or there, to cause this or that; but the reason why the mysterious force behaves thus is wrapped in a blank unity.\n*   Third, those who proffer the explanation cherish their ignorance; they speak proudly of how the phenomenon defeats ordinary science or is unlike merely mundane phenomena.\n*   Fourth, *even after the answer is given, the phenomenon is still a mystery* and possesses the same quality of wonderful inexplicability that it had at the start.\n\n* * *\n\n^1^ Lord Kelvin, “On the Dissipation of Energy: Geology and General Physics,” in *Popular* *Lectures and Addresses, vol. ii* (London: Macmillan, 1894).\n\n^2^ Lord Kelvin, “On the Mechanical action of Heat or Light: On the Power of Animated Creatures over Matter: On the Sources available to Man for the production of Mechanical Effect,” *Proceedings of the Royal Society of Edinburgh* 3, no. 1 (1852): 108–113.\n\n^3^ Silvanus Phillips Thompson, *The Life of Lord Kelvin* (American Mathematical Society, 2005)."
          },
          "voteCount": 140
        },
        {
          "name": "Twelve Virtues of Rationality",
          "type": "post",
          "slug": "twelve-virtues-of-rationality",
          "_id": "7ZqGiPHTpiDMwqMN2",
          "url": null,
          "title": "Twelve Virtues of Rationality",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Virtues"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance.\n\nThere is a time to confess your ignorance and a time to relinquish your ignorance. The second virtue is relinquishment. P. C. Hodgell said: “That which can be destroyed by the truth should be.”\\[1\\] Do not flinch from experiences that might destroy your beliefs. The thought you cannot think controls you more than thoughts you speak aloud. Submit yourself to ordeals and test yourself in fire. Relinquish the emotion which rests upon a mistaken belief, and seek to feel fully that emotion which fits the facts. If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear. If the iron approaches your face, and you believe it is cool, and it is hot, the Way opposes your calm. Evaluate your beliefs first and then arrive at your emotions. Let yourself say: “If the iron is hot, I desire to believe it is hot, and if it is cool, I desire to believe it is cool.” Beware lest you become attached to beliefs you may not want.\n\nThe third virtue is lightness. Let the winds of evidence blow you about as though you are a leaf, with no direction of your own. Beware lest you fight a rearguard retreat against the evidence, grudgingly conceding each foot of ground only when forced, feeling cheated. Surrender to the truth as quickly as you can. Do this the instant you realize what you are resisting, the instant you can see from which quarter the winds of evidence are blowing against you. Be faithless to your cause and betray it to a stronger enemy. If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims. For you cannot make a true map of a city by sitting in your bedroom with your eyes shut and drawing lines upon paper according to impulse. You must walk through the city and draw lines on paper that correspond to what you see. If, seeing the city unclearly, you think that you can shift a line just a little to the right, just a little to the left, according to your caprice, this is just the same mistake.\n\nThe fourth virtue is evenness. One who wishes to believe says, “Does the evidence permit me to believe?” One who wishes to disbelieve asks, “Does the evidence force me to believe?” Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: “But it is good to be skeptical.” If you attend only to favorable evidence, picking and choosing from your gathered data, then the more data you gather, the less you know. If you are selective about which arguments you inspect for flaws, or how hard you inspect for flaws, then every flaw you learn how to detect makes you that much stupider. If you first write at the bottom of a sheet of paper “And therefore, the sky is green!” it does not matter what arguments you write above it afterward; the conclusion is already written, and it is already correct or already wrong. To be clever in argument is not rationality but rationalization. Intelligence, to be useful, must be used for something other than defeating itself. Listen to hypotheses as they plead their cases before you, but remember that you are not a hypothesis; you are the judge. Therefore do not seek to argue for one side or another, for if you knew your destination, you would already be there.\n\nThe fifth virtue is argument. Those who wish to fail must first prevent their friends from helping them. Those who smile wisely and say “I will not argue” remove themselves from help and withdraw from the communal effort. In argument strive for exact honesty, for the sake of others and also yourself: the part of yourself that distorts what you say to others also distorts your own thoughts. Do not believe you do others a favor if you accept their arguments; the favor is to you. Do not think that fairness to all sides means balancing yourself evenly between positions; truth is not handed out in equal portions before the start of a debate. You cannot move forward on factual questions by fighting with fists or insults. Seek a test that lets reality judge between you.\n\nThe sixth virtue is empiricism. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes,” and one saying “No,” the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.”\\[2\\] Do not be blinded by words. When words are subtracted, anticipation remains.\n\nThe seventh virtue is simplicity. Antoine de Saint-Exupéry said: “Perfection is achieved not when there is nothing left to add, but when there is nothing left to take away.”\\[3\\] Simplicity is virtuous in belief, design, planning, and justification. When you profess a huge belief with many details, each additional detail is another chance for the belief to be wrong. Each specification adds to your burden; if you can lighten your burden you must do so. There is no straw that lacks the power to break your back. Of artifacts it is said: The most reliable gear is the one that is designed out of the machine. Of plans: A tangled web breaks. A chain of a thousand links will arrive at a correct conclusion if every step is correct, but if one step is wrong it may carry you anywhere. In mathematics a mountain of good deeds cannot atone for a single sin. Therefore, be careful on every step.\n\nThe eighth virtue is humility. To be humble is to take specific actions in anticipation of your own errors. To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty. Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans. Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.\n\nThe ninth virtue is perfectionism. The more errors you correct in yourself, the more you notice. As your mind becomes more silent, you hear more noise. When you notice an error in yourself, this signals your readiness to seek advancement to the next level. If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors. In every art, if you do not seek perfection you will halt before taking your first steps. If perfection is impossible that is no excuse for not trying. Hold yourself to the highest standard you can imagine, and look for one still higher. Do not be content with the answer that is almost right; seek one that is exactly right.\n\nThe tenth virtue is precision. One comes and says: The quantity is between 1 and 100. Another says: The quantity is between 40 and 50. If the quantity is 42 they are both correct, but the second prediction was more useful and exposed itself to a stricter test. What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world. The narrowest statements slice deepest, the cutting edge of the blade. As with the map, so too with the art of mapmaking: The Way is a precise Art. Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot. Each piece of evidence shifts your beliefs by exactly the right amount, neither more nor less. What is exactly the right amount? To calculate this you must study probability theory. Even if you cannot do the math, knowing that the math exists tells you that the dance step is precise and has no room in it for your whims.\n\nThe eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. It is especially important to eat math and science which impinge upon rationality: evolutionary psychology, heuristics and biases, social psychology, probability theory, decision theory. But these cannot be the only fields you study. The Art must have a purpose other than itself, or it collapses into infinite recursion.\n\nBefore these eleven virtues is a virtue which is nameless.\n\nMiyamoto Musashi wrote, in _The Book of Five Rings_:\\[4\\]\n\n> The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy’s cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him. More than anything, you must be thinking of carrying your movement through to cutting him.\n\nEvery step of your reasoning must cut through to the correct answer in the same movement. More than anything, you must think of carrying your map through to reflecting the territory.\n\nIf you fail to achieve a correct answer, it is futile to protest that you acted with propriety.\n\nHow can you improve your conception of rationality? Not by saying to yourself, “It is my duty to be rational.” By this you only enshrine your mistaken conception. Perhaps your conception of rationality is that it is rational to believe the words of the Great Teacher, and the Great Teacher says, “The sky is green,” and you look up at the sky and see blue. If you think, “It may look like the sky is blue, but rationality is to believe the words of the Great Teacher,” you lose a chance to discover your mistake.\n\nDo not ask whether it is “the Way” to do this or that. Ask whether the sky is blue or green. If you speak overmuch of the Way you will not attain it. You may try to name the highest principle with names such as “the map that reflects the territory” or “experience of success and failure” or “Bayesian decision theory.” But perhaps you describe incorrectly the nameless virtue. How will you discover your mistake? Not by comparing your description to itself, but by comparing it to that which you did not name.\n\nIf for many years you practice the techniques and submit yourself to strict constraints, it may be that you will glimpse the center. Then you will see how all techniques are one technique, and you will move correctly without feeling constrained. Musashi wrote: “When you appreciate the power of nature, knowing the rhythm of any situation, you will be able to hit the enemy naturally and strike naturally. All this is the Way of the Void.”\n\nThese then are twelve virtues of rationality:\n\nCuriosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.\n\n* * *\n\n###### 1\\. Patricia C. Hodgell, _Seeker’s Mask_ (Meisha Merlin Publishing, Inc., 2001).\n\n###### 2\\. Cleaver, _Immediate Fiction: A Complete Writing Course_.\n\n###### 3\\. Antoine de Saint-Exupéry, _Terre des Hommes_ (Paris: Gallimard, 1939).\n\n###### 4\\. Musashi, _Book of Five Rings_.\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/rational/virtues/)_._"
          },
          "voteCount": 86
        },
        {
          "name": "Bayes' rule: Guide",
          "type": "post",
          "comment": "The original post is deprecated in favor of this explanation.",
          "href": "https://arbital.com/p/bayes_rule/?l=1zq"
        },
        {
          "name": "Diseased thinking: dissolving questions about disease",
          "type": "post",
          "slug": "diseased-thinking-dissolving-questions-about-disease",
          "_id": "895quRDaK6gR2rM82",
          "url": null,
          "title": "Diseased thinking: dissolving questions about disease",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Health / Medicine / Disease"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Carving / Clustering Reality"
            },
            {
              "name": "Reversal Test"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "What is Disease?",
                "anchor": "What_is_Disease_",
                "level": 1
              },
              {
                "title": "Hidden Inferences From Disease Concept",
                "anchor": "Hidden_Inferences_From_Disease_Concept",
                "level": 1
              },
              {
                "title": "Sympathy or Condemnation?",
                "anchor": "Sympathy_or_Condemnation_",
                "level": 1
              },
              {
                "title": "The Ethics of Treating Marginal Conditions",
                "anchor": "The_Ethics_of_Treating_Marginal_Conditions",
                "level": 1
              },
              {
                "title": "Summary",
                "anchor": "Summary",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "354 comments"
              }
            ],
            "headingsCount": 7
          },
          "contents": {
            "markdown": "**Related to:** [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/), [Words as Hidden Inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/), [Dissolving the Question](https://www.lesswrong.com/lw/of/dissolving_the_question/), [Eight Short Studies on Excuses](https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/)\n\n> _Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity_\n\n_\\-\\- George Will, [townhall.com](http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c)_\n\nSandy is a morbidly obese woman looking for advice.\n\nHer husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while?\n\nHer doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass.\n\nHer sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.\n\nWhen she tells each of her friends about the opinions of the others, things really start to heat up.\n\nHer husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.\n\nHer doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.\n\nHer sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.\n\nSandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.\n\n  \n\nSandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.\n\nThe debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.\n\n  \n\n**What is Disease?**\n\n  \n\nIn [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/) , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.\n\n\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:\n\n1\\. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.\n\n2\\. Something involuntary and completely immune to the operations of free will\n\n3\\. Something rare; the vast majority of people don't have it\n\n4\\. Something unpleasant; when you have it, you want to get rid of it\n\n5\\. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.\n\n6\\. Something commonly treated with science-y interventions like chemicals and radiation.\n\nCancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, [the sparrow as opposed to the ostrich](https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/). The same is true of heart attack, the flu, diabetes, and many more.\n\nSome conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant.\n\nThe marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).\n\nSo, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.\n\nIf it weren't for those pesky [hidden inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more)...\n\n**Hidden Inferences From Disease Concept**\n\nThe state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.\n\nIf something is a real disease, the patient deserves our sympathy and support; for example, [cancer sufferers must universally be described as \"brave\"](http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/). If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.\n\nAnd if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.\n\n![](http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif)\n\nWhile a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.\n\nIf we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.\n\n**Sympathy or Condemnation?**\n\nOur attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.\n\nUnder this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.\n\nDeterminist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.\n\nBut determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.\n\nThe consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus [is it written](http://yudkowsky.net/obsolete/tmol-faq.html#theo_free): \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.\n\nSo here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.\n\nThe question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...\n\n**The Ethics of Treating Marginal Conditions**\n\nIf a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason.\n\nIn a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern.\n\nOthers complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here [we see the status-quo bias at work, and so can apply a preference reversal test](http://www.nickbostrom.com/ethics/statusquo.pdf). If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way.\n\nBut the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On [her blog](http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/), she says:\n\n> _...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don’t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don’t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people._\n\nA case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.\n\nThere are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see [here](https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p)).\n\nI see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.\n\n**Summary**\n\nPeople commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.\n\nInstead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective."
          },
          "voteCount": 367
        },
        {
          "name": "Humans are not automatically strategic",
          "type": "post",
          "slug": "humans-are-not-automatically-strategic",
          "_id": "PBRWb2Em5SNeWYwwB",
          "url": null,
          "title": "Humans are not automatically strategic",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Goodhart's Law"
            },
            {
              "name": "Introspection"
            },
            {
              "name": "General Intelligence"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Reply to: [A \"Failure to Evaluate Return-on-Time\" Fallacy](/lw/2p1/a_failure_to_evaluate_returnontime_fallacy/)\n\nLionhearted writes:\n\n> \\[A\\] large majority of otherwise smart people spend time doing semi-productive things, when there are massively productive opportunities untapped.\n> \n> A somewhat silly example: Let's say someone aspires to be a comedian, the best comedian ever, and to make a living doing comedy. He wants nothing else, it is his purpose. And he decides that in order to become a better comedian, he will watch re-runs of the old television cartoon 'Garfield and Friends' that was on TV from 1988 to 1995....\n> \n> I’m curious as to why.\n\nWhy will a randomly chosen eight-year-old fail a calculus test?  Because most possible answers are wrong, and there is no force to guide him to the correct answers.  (There is no need to postulate a “fear of success”; _most_ ways writing or not writing on a calculus test constitute failure, and so people, and rocks, fail calculus tests by default.)\n\nWhy do most of us, most of the time, choose to \"pursue our goals\" through routes that are far less effective than the routes we could find if we tried?\\[1\\]  My guess is that here, as with the calculus test, the main problem is that _most_ courses of action are extremely ineffective, and that there has been no strong evolutionary or cultural force sufficient to focus us on the very narrow behavior patterns that would actually be effective. \n\nTo be more specific: there are clearly at least some limited senses in which we have goals.  We: (1) tell ourselves and others stories of how we’re aiming for various “goals”; (2) search out modes of activity that are consistent with the role, and goal-seeking, that we see ourselves as doing (“learning math”; “becoming a comedian”; “being a good parent”); and sometimes even (3) feel glad or disappointed when we do/don’t achieve our “goals”.\n\nBut there are clearly also heuristics that would be useful to goal-achievement (or that would be part of what it means to “have goals” at all) that we do _not_ automatically carry out.  We do _not_ automatically:\n\n*   (a) Ask ourselves what we’re trying to achieve; \n*   (b) Ask ourselves how we could tell if we achieved it (“what does it look like to be a good comedian?”) and how we can track progress; \n*   (c) Find ourselves strongly, intrinsically curious about information that would help us achieve our goal; \n*   (d) Gather that information (e.g., by asking as how folks commonly achieve our goal, or similar goals, or by tallying which strategies have and haven’t worked for us in the past); \n*   (e) Systematically test many different conjectures for how to achieve the goals, including methods that aren’t habitual for us, while tracking which ones do and don’t work; \n*   (f) Focus most of the energy that \\*isn’t\\* going into systematic exploration, on the methods that work best;\n*   (g) Make sure that our \"goal\" is really our goal, that we coherently want it and are not constrained by fears or by uncertainty as to whether it is worth the effort, and that we have thought through any questions and decisions in advance so they won't continually sap our energies;\n*   (h) Use environmental cues and social contexts to bolster our motivation, so we can keep working effectively in the face of intermittent frustrations, or temptations based in hyperbolic discounting;\n\n.... or carry out any number of other useful techniques.  Instead, we mostly just do things.  We act from habit; we act from impulse or convenience when primed by the activities in front of us; we remember our goal and choose an action that _feels associated_ with our goal.  We do any number of things.  But we do not systematically choose the narrow sets of actions that would effectively optimize for our claimed goals, or for any other goals.\n\nWhy?  Most basically, because humans are only just on the cusp of general intelligence.  Perhaps 5% of the population has enough abstract reasoning skill to _verbally understand_ that the above heuristics would be useful _once these heuristics are pointed out_.  That is not at all the same as the ability to _automatically implement these heuristics_.  Our verbal, conversational systems are much better at abstract reasoning than are the motivational systems that pull our behavior.  I have enough abstract reasoning ability to understand that I’m safe on the glass floor of a tall building, or that ice cream is not healthy, or that exercise furthers my goals... but this _doesn’t_ lead to an automatic updating of the reward gradients that, absent rare and costly conscious overrides, pull my behavior.  I can train my automatic systems, for example by visualizing ice cream as disgusting and artery-clogging and yucky, or by walking across the glass floor often enough to persuade my brain that I can’t fall through the floor... but systematically training one’s motivational systems in this way is _also_ not automatic for us.  And so it seems far from surprising that most of us have not trained ourselves in this way, and that most of our “goal-seeking” actions are far less effective than they could be.\n\nStill, I’m keen to train.  I know people who are far more strategic than I am, and there seem to be clear avenues for becoming far more strategic than they are.  It also seems that having goals, in a much more pervasive sense than (1)-(3), is part of what “rational” should mean, will help us achieve what we care about, and hasn't been taught in much detail on LW.\n\nSo, to second Lionhearted's questions: does this analysis seem right?  Have some of you trained yourselves to be substantially more strategic, or goal-achieving, than you started out?  How did you do it?  Do you agree with (a)-(h) above?  Do you have some good heuristics to add?  Do you have some good ideas for how to train yourself in such heuristics?\n\n\\[1\\] For example, why do many people go through long training programs “to make money” without spending a few hours doing salary comparisons ahead of time?  Why do many who type for hours a day remain two-finger typists, without bothering with a typing tutor program?  Why do people spend their Saturdays “enjoying themselves” without bothering to track which of their habitual leisure activities are \\*actually\\* enjoyable?  Why do even unusually numerate people fear illness, car accidents, and bogeymen, and take safety measures, but not bother to look up statistics on the relative risks? Why do most of us settle into a single, stereotyped mode of studying, writing, social interaction, or the like, without trying alternatives to see if they work better -- even when such experiments as we have tried have sometimes given great boosts?"
          },
          "voteCount": 316
        },
        {
          "name": "Ugh Fields",
          "type": "post",
          "slug": "ugh-fields",
          "_id": "EFQ3F6kmt4WHXRqik",
          "url": null,
          "title": "Ugh fields",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Akrasia"
            },
            {
              "name": "Motivations"
            },
            {
              "name": "Emotions"
            },
            {
              "name": "Aversion"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_Tl;Dr version: Pavlovian conditioning can cause humans to unconsciously flinch from even thinking about a serious personal problem they have, we call it an \"Ugh Field\"^1^. The Ugh Field forms a self-shadowing blind spot covering an area desperately in need of optimization, imposing huge costs._\n\nA problem with the human mind — your human mind — is that it's a horrific kludge that will fail when you most need it not to. The _Ugh Field failure mode_ is one of those really annoying failures. The idea is simple: if a person receives constant negative conditioning via unhappy thoughts whenever their mind goes into a certain zone of thought, they will begin to develop a psychological flinch mechanism around the thought. The \"Unhappy Thing\" — the source of negative thoughts — is typically some part of your model of the world that relates to bad things being likely to happen to you.\n\nA key part of the Ugh Field phenomenon is that, to start with, there is no flinch, only negative real consequences resulting from real physical actions in the problem area. Then, gradually, you begin to feel the emotional hit when you are planning to take physical actions in the problem area. Then eventually, the emotional hit comes when you even begin to think about the problem. The reason for this may be that your brain operates a [temporal difference learning](http://en.wikipedia.org/wiki/Temporal_difference_learning#TD_algorithm_in_neuroscience) (TDL) algorithm. Your brain propagates the psychological pain \"back to the earliest reliable stimulus for the punishment\". If you fail or are punished sufficiently many times in some problem area, and acting in that area is always preceeded by thinking about it, your brain will propagate the psychological pain right back to the moment you first begin to entertain a thought about the problem, and hence cut your conscious optimizing ability right out of the loop. Related to this is engaging in a [displacement activity](http://en.wikipedia.org/wiki/Displacement_activity): this is some activity that usually involves comfort, done instead of confronting the problem. Perhaps (though this is speculative) the comforting displacement activity is there to counterbalance the psychological pain that you experienced just because you thought about the problem.\n\nFor example, suppose that you started off in life with a wandering mind and were punished a few times for failing to respond to official letters. Your TDL algorithm began to propagate the pain back to the moment you looked at an official letter or bill. As a result, you would be less effective than average at responding, so you got punished a few more times. Henceforth, when you received a bill, you got the pain before you even opened it, and it laid unpaid on the mantelpiece until a Big Bad Red late payment notice with an $25 fine arrived. More negative conditioning. Now even thinking about a bill, form or letter invokes the flinch response, and your [lizard brain](http://en.wikipedia.org/wiki/Lizard_brain) has fully cut you out out. You find yourself spending time on internet time-wasters, comfort food, TV, computer games, etc. Your life may not obviously be a disaster, but this is only because you can't see the alternative paths that it could have taken if you had been able to take advantage of the opportunities that came as letters and forms with deadlines.\n\nThe subtlety with the Ugh Field is that the flinch occurs **_before you start to consciously think_** about how to deal with the Unhappy Thing, meaning that you never deal with it, and you don't even have the option of dealing with it in the normal run of things. I find it frightening that my lizard brain could implicitly be making life decisions for me, without even asking my permission!\n\nPossible antidotes to Ugh Field problem:\n\n*   Actively look out for the flinch, preferably when you are in a motivationally \"high\" state. Better still, do this when you are both motivationally high, not under time pressure, and when you are undertaking an overview of your life. This overview exercise will tend to make your mind range over all of the relevant parts of your life, and hopefully \"throw up\" some \"Ugh!\" reactions.\n*   Concretely visualize how your life could be much better if you oust control of it from your lizarrd brain. Imagine, in [near-mode](http://wiki.lesswrong.com/wiki/Near/far_thinking), how much better your future life could be if you can find and \"pick off\" your Ugh Fields and optimize the relevant part of your life. If you haven't yet identified these areas of your life, imagine that some concrete good thing (such as eating ice-cream, laughing with friends, etc) will happen to you in the future if you can honestly face these areas.\n*   Identifying these reactions, writing them down in a list, and affirming that you want to take control of them will help you to distance yourself from them. Once your conscious mind has a positive desire to take control, the offending stimulus will hopefully activate this \"take-control\" reaction, rather than the \"flinch\" reaction. Key to this is framing the \"take control\" action as a \"positive\" outcome enabler will facilitate action, [as Kaj and PJ have already told us](/lw/21r/pain_and_gain_motivation/).\n\n1: (Credit for this idea goes to Anna Salamon and Jennifer Rodriguez-Müller. Upvotes go to me, as I wrote the darn article)"
          },
          "voteCount": 275
        },
        {
          "name": "Probability is in the Mind",
          "type": "post",
          "slug": "probability-is-in-the-mind",
          "_id": "f6ZLxEWaankRZ2Crv",
          "url": null,
          "title": "Probability is in the Mind",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Bayes' Theorem"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Bayesianism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[![Monsterwithgirl_2](/static/imported/2007/08/10/monsterwithgirl_2.jpg \"Monsterwithgirl_2\")](/static/imported/2007/08/10/monsterwithgirl_2.jpg)\n\nYesterday I spoke of the Mind Projection Fallacy, giving the example of the alien monster who carries off a girl in a torn dress for intended ravishing—a mistake which I imputed to the artist's tendency to think that a woman's sexiness is a property of the woman herself, woman.sexiness, rather than something that exists in the mind of an observer, and probably wouldn't exist in an alien mind.\n\nThe term \"Mind Projection Fallacy\" was coined by the late great Bayesian Master, E. T. Jaynes, as part of his long and hard-fought battle against the accursèd frequentists.  Jaynes was of the opinion that probabilities were in the mind, not in the environment—that probabilities express ignorance, states of partial information; and if I am ignorant of a phenomenon, that is a fact about my state of mind, not a fact about the phenomenon.\n\nI cannot do justice to this ancient war in a few words—but the classic example of the argument runs thus:\n\nYou have a coin.  \nThe coin is biased.  \nYou don't know which way it's biased or how much it's biased.  Someone just told you, \"The coin is biased\" and that's all they said.  \nThis is all the information you have, and the only information you have.\n\nYou draw the coin forth, flip it, and slap it down.\n\nNow—before you remove your hand and look at the result—are you willing to say that you assign a 0.5 probability to the coin having come up heads?\n\nThe frequentist says, \"No.  Saying 'probability 0.5' means that the coin has an inherent propensity to come up heads as often as tails, so that if we flipped the coin infinitely many times, the ratio of heads to tails would approach 1:1.  But we know that the coin is biased, so it can have any probability of coming up heads _except_ 0.5.\"\n\nThe Bayesian says, \"Uncertainty exists in the map, not in the territory.  In the real world, the coin has either come up heads, or come up tails.  Any talk of 'probability' must refer to the _information_ that I have about the coin—my state of partial ignorance and partial knowledge—not just the coin itself.  Furthermore, I have all sorts of theorems showing that if I don't treat my partial knowledge a [certain way](http://www.overcomingbias.com/2008/01/something-to-pr.html), I'll make stupid bets.  If I've got to plan, I'll plan for a 50/50 state of uncertainty, where I don't weigh outcomes conditional on heads any more heavily in my mind than outcomes conditional on tails.  You can call that number whatever you like, but it has to obey the probability laws on pain of stupidity.  So I don't have the slightest hesitation about calling my outcome-weighting a probability.\"\n\nI side with the Bayesians.  You may have noticed that about me.\n\nEven before a fair coin is tossed, the notion that it has an _inherent_ 50% probability of coming up heads may be just plain wrong.  Maybe you're holding the coin in such a way that it's just about guaranteed to come up heads, or tails, given the force at which you flip it, and the air currents around you.  But, if you don't know which way the coin is biased on this one occasion, so what?\n\nI believe there was a lawsuit where someone alleged that the draft lottery was unfair, because the slips with names on them were not being mixed thoroughly enough; and the judge replied, \"To whom is it unfair?\"\n\nTo make the coinflip experiment repeatable, as frequentists are wont to demand, we could build an automated coinflipper, and verify that the results were 50% heads and 50% tails.  But maybe a robot with extra-sensitive eyes and a good grasp of physics, watching the autoflipper prepare to flip, could predict the coin's fall in advance—not with certainty, but with 90% accuracy.  Then what would the _real_ probability be?\n\nThere is no \"real probability\".  The robot has one state of partial information.  You have a different state of partial information.  The coin itself has no mind, and doesn't assign a probability to anything; it just flips into the air, rotates a few times, bounces off some air molecules, and lands either heads or tails.\n\nSo that is the Bayesian view of things, and I would now like to point out a couple of classic brainteasers that derive their brain-_teasing_ ability from the tendency to think of probabilities as inherent properties of objects.\n\nLet's take the old classic:  You meet a mathematician on the street, and she happens to mention that she has given birth to two children on two separate occasions.  You ask:  \"Is at least one of your children a boy?\"  The mathematician says, \"Yes, he is.\"\n\nWhat is the probability that she has two boys?  If you assume that the prior probability of a child being a boy is 1/2, then the probability that she has two boys, on the information given, is 1/3.  The prior probabilities were:  1/4 two boys, 1/2 one boy one girl, 1/4 two girls.  The mathematician's \"Yes\" response has probability ~1 in the first two cases, and probability ~0 in the third.  Renormalizing leaves us with a 1/3 probability of two boys, and a 2/3 probability of one boy one girl.\n\nBut suppose that instead you had asked, \"Is your eldest child a boy?\" and the mathematician had answered \"Yes.\"  Then the probability of the mathematician having two boys would be 1/2.  Since the eldest child is a boy, and the younger child can be anything it pleases.\n\nLikewise if you'd asked \"Is your youngest child a boy?\"  The probability of their being both boys would, again, be 1/2.\n\nNow, if at least one child is a boy, it must be either the oldest child who is a boy, or the youngest child who is a boy.  So how can the answer in the first case be different from the answer in the latter two?\n\nOr here's a very similar problem:  Let's say I have four cards, the ace of hearts, the ace of spades, the two of hearts, and the two of spades.  I draw two cards at random.  You ask me, \"Are you holding at least one ace?\" and I reply \"Yes.\"  What is the probability that I am holding a pair of aces?  It is 1/5.  There are six possible combinations of two cards, with equal prior probability, and you have just eliminated the possibility that I am holding a pair of twos.  Of the five remaining combinations, only one combination is a pair of aces.  So 1/5.\n\nNow suppose that instead you asked me, \"Are you holding the ace of spades?\"  If I reply \"Yes\", the probability that the other card is the ace of hearts is 1/3.  (You know I'm holding the ace of spades, and there are three possibilities for the other card, only one of which is the ace of hearts.)  Likewise, if you ask me \"Are you holding the ace of hearts?\" and I reply \"Yes\", the probability I'm holding a pair of aces is 1/3.\n\nBut then how can it be that if you ask me, \"Are you holding at least one ace?\" and I say \"Yes\", the probability I have a pair is 1/5?  Either I must be holding the ace of spades or the ace of hearts, as you know; and either way, the probability that I'm holding a pair of aces is 1/3.\n\nHow can this be?  Have I miscalculated one or more of these probabilities?\n\nIf you want to figure it out for yourself, do so now, because I'm about to reveal...\n\nThat all stated calculations are correct.\n\nAs for the paradox, there isn't one.  The _appearance_ of paradox comes from thinking that the probabilities must be properties of the cards themselves.  The ace I'm holding has to be either hearts or spades; but that doesn't mean that your _knowledge about_ my cards must be the same as if you _knew_ I was holding hearts, or _knew_ I was holding spades.\n\nIt may help to think of Bayes's Theorem:\n\n> P(H|E) = P(E|H)P(H) / P(E)\n\nThat last term, where you divide by P(E), is the part where you throw out all the possibilities that have been eliminated, and renormalize your probabilities over what remains.\n\nNow let's say that you ask me, \"Are you holding at least one ace?\"  _Before_ I answer, your probability that I say \"Yes\" should be 5/6.\n\nBut if you ask me \"Are you holding the ace of spades?\", your prior probability that I say \"Yes\" is just 1/2.\n\nSo right away you can see that you're _learning_ something very different in the two cases.  You're going to be eliminating some different possibilities, and renormalizing using a different P(E).  If you learn two different items of evidence, you shouldn't be surprised at ending up in two different states of partial information.\n\nSimilarly, if I ask the mathematician, \"Is at least one of your two children a boy?\" I expect to hear \"Yes\" with probability 3/4, but if I ask \"Is your eldest child a boy?\" I expect to hear \"Yes\" with probability 1/2.  So it shouldn't be surprising that I end up in a different state of partial knowledge, depending on which of the two questions I ask.\n\nThe only reason for seeing a \"paradox\" is thinking as though the probability of holding a pair of aces is _a property of cards_ that have at least one ace, or a property _of cards_ that happen to contain the ace of spades.  In which case, it would be paradoxical for card-sets containing at least one ace to have an inherent pair-probability of 1/5, while card-sets containing the ace of spades had an inherent pair-probability of 1/3, and card-sets containing the ace of hearts had an inherent pair-probability of 1/3.\n\nSimilarly, if you think a 1/3 probability of being both boys is an _inherent property_ of child-sets that include at least one boy, then that is not consistent with child-sets of which the eldest is male having an _inherent_ probability of 1/2 of being both boys, and child-sets of which the youngest is male having an inherent 1/2 probability of being both boys.  It would be like saying, \"All green apples weigh a pound, and all red apples weigh a pound, and all apples that are green or red weigh half a pound.\"\n\nThat's what happens when you start thinking as if probabilities are _in_ things, rather than probabilities being states of partial information _about_ things.\n\nProbabilities express uncertainty, and it is only agents who can be uncertain.  A blank map does not correspond to a blank territory.  Ignorance is in the mind."
          },
          "voteCount": 95
        },
        {
          "name": "How An Algorithm Feels From Inside",
          "type": "post",
          "slug": "how-an-algorithm-feels-from-inside",
          "_id": "yA4gF5KrboK2m2Xu7",
          "url": null,
          "title": "How An Algorithm Feels From Inside",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Cognitive Reduction"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "\"If a tree falls in the forest, and no one hears it, does it make a sound?\"  I remember seeing an actual argument get started on this subject—a fully naive argument that went nowhere near Berkeleyan subjectivism.  Just:\n\n> \"It makes a sound, just like any other falling tree!\"  \n> \"But how can there be a sound that no one hears?\"\n\nThe standard rationalist view would be that the first person is speaking as if \"sound\" means acoustic vibrations in the air; the second person is speaking as if \"sound\" means an auditory experience in a brain.  If you ask \"Are there acoustic vibrations?\" or \"Are there auditory experiences?\", the answer is at once obvious.  And so the argument is really about the definition of the word \"sound\".\n\nI think the standard analysis is essentially correct.  So let's accept that as a premise, and ask:  Why do people get into such an argument?  What's the underlying psychology?\n\nA key idea of the heuristics and biases program is that mistakes are often more revealing of cognition than correct answers.  Getting into a heated dispute about whether, if a tree falls in a deserted forest, it makes a sound, is traditionally considered a mistake.\n\nSo what kind of mind design corresponds to that error?\n\nIn [Disguised Queries](/lw/nm/disguised_queries/) I introduced the blegg/rube classification task, in which Susan the Senior Sorter explains that your job is to sort objects coming off a conveyor belt, putting the blue eggs or \"bleggs\" into one bin, and the red cubes or \"rubes\" into the rube bin.  This, it turns out, is because bleggs contain small nuggets of vanadium ore, and rubes contain small shreds of palladium, both of which are useful industrially.\n\nExcept that around 2% of blue egg-shaped objects contain palladium instead.  So if you find a blue egg-shaped thing that contains palladium, should you call it a \"rube\" instead?  You're going to put it in the rube bin—why not call it a \"rube\"?\n\nBut when you switch off the light, nearly all bleggs glow faintly in the dark.  And blue egg-shaped objects that contain palladium are just as likely to glow in the dark as any other blue egg-shaped object.\n\nSo if you find a blue egg-shaped object that contains palladium, and you ask \"Is it a blegg?\", the answer depends on what you have to do with the answer:  If you ask \"Which bin does the object go in?\", then you choose as if the object is a rube.  But if you ask \"If I turn off the light, will it glow?\", you predict as if the object is a blegg.  In one case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Which bin does it go in?\".  In the other case, the question \"Is it a blegg?\" stands in for the [disguised query](/lw/nm/disguised_queries/), \"Will it glow in the dark?\"\n\nNow suppose that you have an object that is blue and egg-shaped and contains palladium; and you have already observed that it is furred, flexible, opaque, and glows in the dark.\n\nThis answers _every_ query, observes every observable introduced.  There's nothing left for a disguised query to stand _for._\n\nSo why might someone feel an impulse to go on arguing whether the object is _really_ a blegg?\n\n[![Blegg3](/static/imported/2008/02/10/blegg3.png \"Blegg3\")](/static/imported/2008/02/10/blegg3.png)\n\nThis diagram from [Neural Categories](/lw/nn/neural_categories/) shows two different neural networks that might be used to answer questions about bleggs and rubes.  Network 1 has a number of disadvantages—such as potentially oscillating/chaotic behavior, or requiring O(N^2^) connections—but Network 1's structure does have one major advantage over Network 2:  Every unit in the network corresponds to a testable query.  If you observe every observable, clamping every value, there are no units in the network left over.\n\nNetwork 2, however, is a far better candidate for being something vaguely like how the human brain works:  It's fast, cheap, scalable—and has an extra dangling unit in the center, whose activation can still vary, even after we've observed every single one of the surrounding nodes.\n\nWhich is to say that even after you know whether an object is blue or red, egg or cube, furred or smooth, bright or dark, and whether it contains vanadium or palladium, it _feels_ like there's a leftover, unanswered question:  _But is it really a blegg?_\n\nUsually, in our daily experience, acoustic vibrations and auditory experience go together.  But a tree falling in a deserted forest unbundles this common association.  And even after you know that the falling tree creates acoustic vibrations but not auditory experience, it _feels_ like there's a leftover question:  _Did it make a sound?_  \n  \nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?\n\nNow remember:  When you look at Network 2, as I've laid it out here, you're seeing the algorithm from the outside.  People don't think to themselves, \"Should the central unit fire, or not?\" any more than you think \"Should neuron #12,234,320,242 in my visual cortex fire, or not?\"\n\nIt takes a deliberate effort to visualize your brain from the outside—and then you still don't see your actual brain; you imagine what you _think_ is there, hopefully based on science, but regardless, you don't have any direct access to neural network structures from introspection.  That's why the ancient Greeks didn't invent computational neuroscience.\n\nWhen you look at Network 2, you are seeing from the _outside;_ but the way that neural network structure feels from the _inside,_ if you yourself _are_ a brain running that algorithm, is that even after you know every characteristic of the object, you still find yourself wondering:  \"But is it a blegg, or not?\"\n\nThis is a great gap to cross, and I've seen it stop people in their tracks.  Because we don't instinctively see our intuitions as \"intuitions\", we just see them as the world.  When you look at a green cup, you don't think of yourself as seeing a picture reconstructed in your visual cortex—although that _is_ what you are seeing—you just see a green cup.  You think, \"Why, look, this cup is green,\" not, \"The picture in my visual cortex of this cup is green.\"\n\nAnd in the same way, when people argue over whether the falling tree makes a sound, or whether Pluto is a planet, they don't see themselves as arguing over whether a categorization should be active in their neural networks.  It seems like either the tree makes a sound, or not.\n\nWe know where Pluto is, and where it's going; we know Pluto's shape, and Pluto's mass—but is it a planet?  And yes, there were people who said this was a fight over definitions—but even that is a Network 2 sort of perspective, because you're arguing about how the central unit ought to be wired up.  If you were a mind constructed along the lines of Network 1, you wouldn't say \"It depends on how you define 'planet',\" you would just say, \"Given that we know Pluto's orbit and shape and mass, there is no question left to ask.\"  Or, rather, that's how it would _feel_—it would _feel_ like there was no question left—if you were a mind constructed along the lines of Network 1.\n\nBefore you can question your intuitions, you have to realize that what your mind's eye is looking at _is_ an intuition—some cognitive algorithm, as seen from the inside—rather than a direct perception of the Way Things Really Are.\n\nPeople [cling to their intuitions](/lw/n1/allais_malaise/), I think, not so much because they believe their cognitive algorithms are perfectly reliable, but because they can't see their intuitions _as the way their cognitive algorithms happen to look from the inside._\n\nAnd so everything you try to say about how the native cognitive algorithm goes astray, ends up being contrasted to their direct perception of the Way Things Really Are—and discarded as obviously wrong."
          },
          "voteCount": 179
        },
        {
          "name": "Knowing About Biases Can Hurt People",
          "type": "post",
          "slug": "knowing-about-biases-can-hurt-people",
          "_id": "AdYdLP2sRqPMoe8fb",
          "url": null,
          "title": "Knowing About Biases Can Hurt People",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationalization"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Information Hazards"
            },
            {
              "name": "Pitfalls of Rationality"
            },
            {
              "name": "Fallacies"
            },
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Valley of Bad Rationality"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Once upon a time I tried to tell my mother about the problem of expert calibration, saying: “So when an expert says they’re 99% confident, it only happens about 70% of the time.” Then there was a pause as, suddenly, I realized I was talking to my mother, and I hastily added: “Of course, you’ve got to make sure to apply that skepticism evenhandedly, including to yourself, rather than just using it to argue against anything you disagree with—”\n\nAnd my mother said: “Are you kidding? This is great! I’m going to use it all the time!”\n\nTaber and Lodge’s “Motivated Skepticism in the Evaluation of Political Beliefs” describes the confirmation of six predictions:\n\n1.  Prior attitude effect. Subjects who feel strongly about an issue—even when encouraged to be objective—will evaluate supportive arguments more favorably than contrary arguments.\n2.  Disconfirmation bias. Subjects will spend more time and cognitive resources denigrating contrary arguments than supportive arguments.\n3.  Confirmation bias. Subjects free to choose their information sources will seek out supportive rather than contrary sources.\n4.  **Attitude polarization. Exposing subjects to an apparently balanced set of pro and con arguments will exaggerate their initial polarization.**\n5.  Attitude strength effect. Subjects voicing stronger attitudes will be more prone to the above biases.\n6.  **Sophistication effect. Politically knowledgeable subjects, because they possess greater ammunition with which to counter-argue incongruent facts and arguments, will be more prone to the above biases.**\n\nIf you’re irrational to start with, having *more* knowledge can *hurt* you. For a true Bayesian, information would never have negative expected utility. But humans aren’t perfect Bayes-wielders; if we’re not careful, we can cut ourselves.\n\nI’ve *seen* people severely messed up by their own knowledge of biases. They have more ammunition with which to argue against anything they don’t like. And that problem—too much ready ammunition—is one of the primary ways that people with high mental agility end up stupid, in Stanovich’s “dysrationalia” sense of stupidity.\n\nYou can think of people who fit this description, right? People with high g-factor who end up being *less* effective because they are too sophisticated as arguers? Do you think you’d be helping them—making them more effective rationalists—if you just told them about a list of classic biases?\n\nI recall someone who learned about the calibration/overconfidence problem. Soon after he said: “Well, you can’t trust experts; they’re wrong so often—as experiments have shown. So therefore, when I predict the future, I prefer to assume that things will continue historically as they have—” and went off into this whole complex, error-prone, highly questionable extrapolation. Somehow, when it came to trusting his own preferred conclusions, all those biases and fallacies seemed much less *salient*—leapt much less readily to mind—than when he needed to counter-argue someone else.\n\nI told the one about the problem of disconfirmation bias and sophisticated argument, and lo and behold, the next time I said something he didn’t like, he accused me of being a sophisticated arguer. He didn’t try to point out any particular sophisticated argument, any particular flaw—just shook his head and sighed sadly over how I was apparently using my own intelligence to defeat itself. He had acquired yet another Fully General Counterargument.\n\nEven the notion of a “sophisticated arguer” can be deadly, if it leaps all too readily to mind when you encounter a seemingly intelligent person who says something you don’t like.\n\nI endeavor to learn from my mistakes. The last time I gave a talk on heuristics and biases, I started out by introducing the general concept by way of the conjunction fallacy and representativeness heuristic. And then I moved on to confirmation bias, disconfirmation bias, sophisticated argument, motivated skepticism, and other attitude effects. I spent the next thirty minutes *hammering* on that theme, reintroducing it from as many different perspectives as I could.\n\nI wanted to get my audience interested in the subject. Well, a simple description of conjunction fallacy and representativeness would suffice for that. But suppose they did get interested. Then what? The literature on bias is mostly cognitive psychology for cognitive psychology’s sake. I had to give my audience their dire warnings during that one lecture, or they probably wouldn’t hear them at all.\n\nWhether I do it on paper, or in speech, I now try to never mention calibration and overconfidence unless I have first talked about disconfirmation bias, motivated skepticism, sophisticated arguers, and dysrationalia in the mentally agile. First, do no harm!"
          },
          "voteCount": 140
        },
        {
          "name": "A Fable of Science and Politics",
          "type": "post",
          "slug": "a-fable-of-science-and-politics",
          "_id": "6hfGNLf4Hg5DXqJCF",
          "url": null,
          "title": "A Fable of Science and Politics",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Politics"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Fiction"
            },
            {
              "name": "Blues & Greens (metaphor)"
            },
            {
              "name": "Tribalism"
            },
            {
              "name": "Parables & Fables"
            },
            {
              "name": "Litany of Tarski"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "In the time of the Roman Empire, civic life was divided between the Blue and Green factions. The Blues and the Greens murdered each other in single combats, in ambushes, in group battles, in riots. Procopius said of the warring factions: “So there grows up in them against their fellow men a hostility which has no cause, and at no time does it cease or disappear, for it gives place neither to the ties of marriage nor of relationship nor of friendship, and the case is the same even though those who differ with respect to these colors be brothers or any other kin.”^1^ Edward Gibbon wrote: “The support of a faction became necessary to every candidate for civil or ecclesiastical honors.”^2^\n\nWho were the Blues and the Greens? They were sports fans—the partisans of the blue and green chariot-racing teams.\n\nImagine a future society that flees into a vast underground network of caverns and seals the entrances. We shall not specify whether they flee disease, war, or radiation; we shall suppose the first Undergrounders manage to grow food, find water, recycle air, make light, and survive, and that their descendants thrive and eventually form cities. Of the world above, there are only legends written on scraps of paper; and one of these scraps of paper describes the *sky*, a vast open space of air above a great unbounded floor. The sky is cerulean in color, and contains strange floating objects like enormous tufts of white cotton. But the meaning of the word “cerulean” is controversial; some say that it refers to the color known as “blue,” and others that it refers to the color known as “green.”\n\nIn the early days of the underground society, the Blues and Greens contested with open violence; but today, truce prevails—a peace born of a growing sense of pointlessness. Cultural mores have changed; there is a large and prosperous middle class that has grown up with effective law enforcement and become unaccustomed to violence. The schools provide some sense of historical perspective; how long the battle between Blues and Greens continued, how many died, how little changed as a result. Minds have been laid open to the strange new philosophy that people are people, whether they be Blue or Green.\n\nThe conflict has not vanished. Society is still divided along Blue and Green lines, and there is a “Blue” and a “Green” position on almost every contemporary issue of political or cultural importance. The Blues advocate taxes on individual incomes, the Greens advocate taxes on merchant sales; the Blues advocate stricter marriage laws, while the Greens wish to make it easier to obtain divorces; the Blues take their support from the heart of city areas, while the more distant farmers and watersellers tend to be Green; the Blues believe that the Earth is a huge spherical rock at the center of the universe, the Greens that it is a huge flat rock circling some other object called a Sun. Not every Blue or every Green citizen takes the “Blue” or “Green” position on every issue, but it would be rare to find a city merchant who believed the sky was blue, and yet advocated an individual tax and freer marriage laws.\n\nThe Underground is still polarized; an uneasy peace. A few folk genuinely think that Blues and Greens should be friends, and it is now common for a Green to patronize a Blue shop, or for a Blue to visit a Green tavern. Yet from a truce originally born of exhaustion, there is a quietly growing spirit of tolerance, even friendship.\n\nOne day, the Underground is shaken by a minor earthquake. A sightseeing party of six is caught in the tremblor while looking at the ruins of ancient dwellings in the upper caverns. They feel the brief movement of the rock under their feet, and one of the tourists trips and scrapes her knee. The party decides to turn back, fearing further earthquakes. On their way back, one person catches a whiff of something strange in the air, a scent coming from a long-unused passageway. Ignoring the well-meant cautions of fellow travellers, the person borrows a powered lantern and walks into the passageway. The stone corridor wends upward . . . and upward . . . and finally terminates in a hole carved out of the world, a place where all stone ends. Distance, endless distance, stretches away into forever; a gathering space to hold a thousand cities. Unimaginably far above, too bright to look at directly, a searing spark casts light over all visible space, the naked filament of some huge light bulb. In the air, hanging unsupported, are great incomprehensible tufts of white cotton. And the vast glowing ceiling above . . . the *color* . . . is . . .\n\nNow history branches, depending on which member of the sightseeing party decided to follow the corridor to the surface.\n\n> Aditya the Blue stood under the blue forever, and slowly smiled. It was not a pleasant smile. There was hatred, and wounded pride; it recalled every argument she’d ever had with a Green, every rivalry, every contested promotion. *“You were right all along,”* the sky whispered down at her, *“and now you can prove it.”* For a moment Aditya stood there, absorbing the message, glorying in it, and then she turned back to the stone corridor to tell the world. As Aditya walked, she curled her hand into a clenched fist. “The truce,” she said, “is over.”\n\n> Barron the Green stared uncomprehendingly at the chaos of colors for long seconds. Understanding, when it came, drove a pile-driver punch into the pit of his stomach. Tears started from his eyes. Barron thought of the Massacre of Cathay, where a Blue army had massacred every citizen of a Green town, including children; he thought of the ancient Blue general, Annas Rell, who had declared Greens “a pit of disease; a pestilence to be cleansed”; he thought of the glints of hatred he’d seen in Blue eyes and something inside him cracked. *“How can you be on their side?”* Barron screamed at the sky, and then he began to weep; because he knew, standing under the malevolent blue glare, that the universe had always been a place of evil.\n\n> Charles the Blue considered the blue ceiling, taken aback. As a professor in a mixed college, Charles had carefully emphasized that Blue and Green viewpoints were equally valid and deserving of tolerance: The sky was a metaphysical construct, and cerulean a color that could be seen in more than one way. Briefly, Charles wondered whether a Green, standing in this place, might not see a green ceiling above; or if perhaps the ceiling would be green at this time tomorrow; but he couldn’t stake the continued survival of civilization on that. This was merely a natural phenomenon of some kind, having nothing to do with moral philosophy or society . . . but one that might be readily misinterpreted, Charles feared. Charles sighed, and turned to go back into the corridor. Tomorrow he would come back alone and block off the passageway.\n\n> Daria, once Green, tried to breathe amid the ashes of her world. *I will not flinch*, Daria told herself, *I will not look away*. She had been Green all her life, and now she must be Blue. Her friends, her family, would turn from her. *Speak the truth, even if your voice trembles,* her father had told her; but her father was dead now, and her mother would never understand. Daria stared down the calm blue gaze of the sky, trying to accept it, and finally her breathing quietened. *I was wrong*, she said to herself mournfully; *it’s not so complicated, after all*. She would find new friends, and perhaps her family would forgive her . . . or, she wondered with a tinge of hope, rise to this same test, standing underneath this same sky? “The sky is blue,” Daria said experimentally, and nothing dire happened to her; but she couldn’t bring herself to smile. Daria the Blue exhaled sadly, and went back into the world, wondering what she would say.\n\n> Eddin, a Green, looked up at the blue sky and began to laugh cynically. The course of his world’s history came clear at last; even he couldn’t believe they’d been such fools. “Stupid,” Eddin said, “stupid, *stupid,* and all the time it was right here.” Hatred, murders, wars, and all along it was just a *thing* somewhere, that someone had written about like they’d write about any other thing. No poetry, no beauty, nothing that any sane person would ever care about, just one pointless thing that had been blown out of all proportion. Eddin leaned against the cave mouth wearily, trying to think of a way to prevent this information from blowing up the world, and wondering if they didn’t all deserve it.\n\n> Ferris gasped involuntarily, frozen by sheer wonder and delight. Ferris’s eyes darted hungrily about, fastening on each sight in turn before moving reluctantly to the next; the blue *sky*, the white *clouds*, the vast unknown *outside*, full of places and things (and people?) that no Undergrounder had ever seen. “Oh, so *that’s* what color it is,” Ferris said, and went exploring.\n\n* * *\n\n^1^ Procopius, *History of the Wars*, ed. Henry B. Dewing, vol. 1 (Harvard University Press, 1914).\n\n^2^ Edward Gibbon, *The History of the Decline and Fall of the Roman Empire*, vol. 4 (J. & J. Harper, 1829)."
          },
          "voteCount": 222
        },
        {
          "name": "Taboo Your Words",
          "type": "post",
          "slug": "taboo-your-words",
          "_id": "WBdvyyHLdxZSAMmoz",
          "url": null,
          "title": "Taboo Your Words",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "World Modeling"
            },
            {
              "name": "Disagreement"
            },
            {
              "name": "Conversation (topic)"
            },
            {
              "name": "Techniques"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Rationalist Taboo"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "In the game Taboo (by Hasbro), the objective is for a player to have their partner guess a word written on a card, without using that word or five additional words listed on the card.  For example, you might have to get your partner to say \"baseball\" without using the words \"sport\", \"bat\", \"hit\", \"pitch\", \"base\" or of course \"baseball\".\n\nAs soon as I see a problem like that, I at once think, \"An artificial group conflict in which you use a long wooden cylinder to whack a thrown spheroid, and then run between four safe positions.\"  It might not be the most efficient strategy to convey the word 'baseball' under the stated rules - that might be, \"It's what the Yankees play\" - but the general skill of _blanking a word out of my mind_ was one I'd practiced for years, albeit with a different purpose.\n\nYesterday we saw how replacing terms with definitions could reveal the [empirical unproductivity](/lw/nf/the_parable_of_hemlock/) of the classical Aristotelian syllogism.  All humans are mortal (and also, apparently, featherless bipeds); Socrates is human; therefore Socrates is mortal.  When we replace the word 'human' by its apparent definition, the following underlying reasoning is revealed:\n\n> All \\[mortal, ~feathers, biped\\] are mortal;  \n> Socrates is a \\[mortal, ~feathers, biped\\];  \n> Therefore Socrates is mortal.\n\nBut the principle of replacing words by definitions applies much more broadly:\n\n> Albert:  \"A tree falling in a deserted forest makes a sound.\"  \n> Barry:  \"A tree falling in a deserted forest does not make a sound.\"\n\nClearly, since one says \"sound\" and one says \"not sound\", we must have a contradiction, right?  But suppose that they both dereference their pointers before speaking:\n\n> Albert:  \"A tree falling in a deserted forest matches \\[membership test: this event generates acoustic vibrations\\].\"  \n> Barry:  \"A tree falling in a deserted forest does not match \\[membership test: this event generates auditory experiences\\].\"\n\nNow there is no longer an apparent collision—all they had to do was prohibit themselves from using the word _sound_. If \"acoustic vibrations\" came into dispute, we would just play Taboo again and say \"pressure waves in a material medium\"; if necessary we would play Taboo again on the word \"[wave](/lw/iq/guessing_the_teachers_password/)\" and replace it with the wave equation.  (Play Taboo on \"auditory experience\" and you get \"That form of sensory processing, within the human brain, which takes as input a linear time series of frequency mixes...\")\n\nBut suppose, on the other hand, that Albert and Barry were to have the argument:\n\n> Albert:  \"Socrates matches the concept \\[membership test: this person will die after drinking hemlock\\].\"  \n> Barry:  \"Socrates matches the concept \\[membership test: this person will not die after drinking hemlock\\].\"\n\nNow Albert and Barry have a substantive clash of expectations; a difference in what they anticipate seeing after Socrates drinks hemlock.  But they might not notice this, if they happened to use the same word \"human\" for their different concepts.\n\nYou get a very different picture of what people agree or disagree about, depending on whether you take a label's-eye-view (Albert says \"sound\" and Barry says \"not sound\", so they must disagree) or taking the test's-eye-view (Albert's membership test is acoustic vibrations, Barry's is auditory experience).\n\nGet together a pack of _soi-disant_ futurists and ask them if they believe we'll have Artificial Intelligence in thirty years, and I would guess that at least half of them will say yes.  If you leave it at that, they'll shake hands and congratulate themselves on their consensus.  But make the term \"Artificial Intelligence\" taboo, and ask them to describe _what_ they expect to see, without ever using words like \"computers\" or \"think\", and you might find quite a conflict of expectations hiding under that featureless standard word.  Likewise [that other term](http://intelligence.org/blog/2007/09/30/three-major-singularity-schools/).  And see also Shane Legg's compilation of [71 definitions of \"intelligence\"](http://arxiv.org/abs/0706.3639).\n\nThe illusion of unity across religions can be dispelled by making the term \"God\" taboo, and asking them to say what it is they believe in; or making the word \"faith\" taboo, and asking them why they believe it. Though mostly they won't be able to answer at all, because it is mostly [profession](/lw/i4/belief_in_belief/) in the first place, and you cannot cognitively zoom in on an audio recording.\n\nWhen you find yourself in philosophical difficulties, _the first line of defense is not to define your problematic terms, but to see whether you can think without using those terms at all._  Or any of their short synonyms.  And be careful not to let yourself invent a new word to use instead.  Describe outward observables and interior mechanisms; don't use a single handle, whatever that handle may be.\n\nAlbert says that people have \"free will\".  Barry says that people don't have \"free will\".  Well, that will certainly generate an apparent conflict.  Most philosophers would advise Albert and Barry to try to define exactly what they mean by \"free will\", on which topic they will certainly be able to discourse at great length.  I would advise Albert and Barry to describe what it is that they think people do, or do not have, without using the phrase \"free will\" at all.  (If you want to try this at home, you should also avoid the words \"choose\", \"act\", \"decide\", \"determined\", \"responsible\", or any of their synonyms.)\n\nThis is one of the nonstandard tools in my toolbox, and in my humble opinion, it works _way way_ better than the standard one.  It also requires more effort to use; you get what you pay for."
          },
          "voteCount": 136
        },
        {
          "name": "The Fallacy of Gray",
          "type": "post",
          "slug": "the-fallacy-of-gray",
          "_id": "dLJv2CoRCgeC2mPgj",
          "url": null,
          "title": "The Fallacy of Gray",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Fallacies"
            },
            {
              "name": "Fallacy of Gray"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> The Sophisticate: “The world isn’t black and white. No one does pure good or pure bad. It’s all gray. Therefore, no one is better than anyone else.”\n> \n> The Zetet: “Knowing only gray, you conclude that all grays are the same shade. You mock the simplicity of the two-color view, yet you replace it with a one-color view . . .”\n> \n> —Marc Stiegler, _David’s Sling_\n\nI don’t know if the Sophisticate’s mistake has an official name, but I call it the Fallacy of Gray. We saw it manifested in the previous essay—the one who believed that odds of two to the power of seven hundred and fifty million to one, against, meant “there was still a chance.” All probabilities, to him, were simply “uncertain” and that meant he was licensed to ignore them if he pleased.\n\n“The Moon is made of green cheese” and “the Sun is made of mostly hydrogen and helium” are both uncertainties, but they are not the same uncertainty.\n\nEverything is shades of gray, but there are shades of gray so light as to be very nearly white, and shades of gray so dark as to be very nearly black. Or even if not, we can still compare shades, and say “it is darker” or “it is lighter.”\n\nYears ago, one of the strange little formative moments in my career as a rationalist was reading this paragraph from _Player of Games_ by Iain M. Banks, especially the sentence in bold:\n\n> A guilty system recognizes no innocents. As with any power apparatus which thinks everybody’s either for it or against it, we’re against it. You would be too, if you thought about it. The very way you think places you amongst its enemies. This might not be your fault, because **every society imposes some of its values on those raised within it, but the point is that some societies try to maximize that effect, and some try to minimize it**. You come from one of the latter and you’re being asked to explain yourself to one of the former. Prevarication will be more difficult than you might imagine; neutrality is probably impossible. You cannot choose not to have the politics you do; they are not some separate set of entities somehow detachable from the rest of your being; they are a function of your existence. I know that and they know that; you had better accept it.\n\nNow, don’t write angry comments saying that, if societies impose fewer of their values, then each succeeding generation has more work to start over from scratch. That’s not what I got out of the paragraph.\n\nWhat I got out of the paragraph was something which seems so obvious in retrospect that I could have conceivably picked it up in a hundred places; but something about that one paragraph made it click for me.\n\nIt was the whole notion of the Quantitative Way applied to life-problems like moral judgments and the quest for personal self-improvement. That, even if you couldn’t switch something from on to off, you could still tend to increase it or decrease it.\n\nIs this too obvious to be worth mentioning? I say it is not too obvious, for many bloggers have said of _Overcoming Bias_: “It is impossible, no one can completely eliminate bias.” I don’t care if the one is a professional economist, it is clear that they have not yet grokked the Quantitative Way as it applies to everyday life and matters like personal self-improvement. That which I cannot _eliminate_ may be well worth _reducing_.\n\nOr consider an exchange between Robin Hanson and Tyler Cowen.^[1](#fn1x8)^ Robin Hanson said that he preferred to put at least 75% weight on the prescriptions of economic theory versus his intuitions: “I try to mostly just straightforwardly apply economic theory, adding little personal or cultural judgment.” Tyler Cowen replied:\n\n> In my view there is no such thing as “straightforwardly applying economic theory” . . . theories are always applied through our personal and cultural filters and there is no other way it can be.\n\nYes, but you can try to minimize that effect, or you can do things that are bound to increase it. And _if_ you try to minimize it, then in many cases I don’t think it’s unreasonable to call the output “straightforward”—even in economics.\n\n“Everyone is imperfect.” Mohandas Gandhi was imperfect and Joseph Stalin was imperfect, but they were not the same shade of imperfection. “Everyone is imperfect” is an excellent example of replacing a two-color view with a one-color view. If you say, “No one is perfect, but _some people are less imperfect than others_,” you may not gain applause; but for those who strive to do better, you have held out hope. No one is _perfectly_ imperfect, after all.\n\n(Whenever someone says to me, “Perfectionism is bad for you,” I reply: “I think it’s okay to be imperfect, but not so imperfect that other people notice.”)\n\nLikewise the folly of those who say, “Every scientific paradigm imposes some of its assumptions on how it interprets experiments,” and then act like they’d proven science to occupy the same level with witchdoctoring. Every worldview imposes some of its structure on its observations, but the point is that there are worldviews which try to minimize that imposition, and worldviews which glory in it. There is no white, but there are shades of gray that are far lighter than others, and it is folly to treat them as if they were all on the same level.\n\nIf the Moon has orbited the Earth these past few billion years, if you have seen it in the sky these last years, and you expect to see it in its appointed place and phase tomorrow, then that is not a certainty. And if you expect an invisible dragon to heal your daughter of cancer, that too is not a certainty. But they are rather different degrees of uncertainty—this business of expecting things to happen yet again in the same way you have previously predicted to twelve decimal places, versus expecting something to happen that _violates_ the order previously observed. Calling them both “faith” seems a little too un-narrow.\n\nIt’s a most peculiar psychology—this business of “Science is based on faith too, so there!” Typically this is said by people who claim that faith is a _good_ thing. Then why do they say “Science is based on faith too!” in that angry-triumphal tone, rather than as a compliment? And a rather _dangerous_ compliment to give, one would think, from their perspective. If science is based on “faith,” then science is of the same kind as religion—directly comparable. If science is a religion, it is the religion that heals the sick and reveals the secrets of the stars. It would make sense to say, “The priests of science can blatantly, publicly, verifiably walk on the Moon as a faith-based miracle, and your priests’ faith can’t do the same.” Are you sure you wish to go there, oh faithist? Perhaps, on further reflection, you would prefer to retract this whole business of “Science is a religion too!”\n\nThere’s a strange dynamic here: You try to purify your shade of gray, and you get it to a point where it’s pretty light-toned, and someone stands up and says in a deeply offended tone, “But it’s not white! It’s gray!” It’s one thing when someone says, “This isn’t as light as you think, because of specific problems X, Y, and Z.” It’s a different matter when someone says angrily “It’s not white! It’s gray!” without pointing out any specific dark spots.\n\nIn this case, I begin to suspect psychology that is more imperfect than usual—that someone may have made a devil’s bargain with their own mistakes, and now refuses to hear of any possibility of improvement. When someone finds an excuse not to try to do better, they often refuse to concede that anyone else _can_ try to do better, and every mode of improvement is thereafter their enemy, and every claim that it is possible to move forward is an offense against them. And so they say in one breath proudly, “I’m glad to be gray,” and in the next breath angrily, “And _you’re gray too!_”\n\nIf there is no black and white, there is yet lighter and darker, and not all grays are the same.\n\nThe commenter G2 points us to Asimov’s “The Relativity of Wrong”:\n\n> When people thought the earth was flat, they were wrong. When people thought the earth was spherical, they were wrong. But if you think that thinking the earth is spherical is just as wrong as thinking the earth is flat, then your view is wronger than both of them put together.\n\n^[1](#fn1x8-bk)^Hanson (2007), “Economist Judgment,” [http://www.overcomingbias.com/2007/12/economist-judgm.html](http://www.overcomingbias.com/2007/12/economist-judgm.html). Cowen (2007), “Can Theory Override Intuition?”, [http://marginalrevolution.com/marginalrevolution/2007/12/how-my-views-di.html](http://marginalrevolution.com/marginalrevolution/2007/12/how-my-views-di.html)."
          },
          "voteCount": 185
        },
        {
          "name": "Mind Projection Fallacy",
          "type": "post",
          "slug": "mind-projection-fallacy",
          "_id": "ZTRiSNmeGQK8AkdN2",
          "url": null,
          "title": "Mind Projection Fallacy",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Fallacies"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Map and Territory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[![Monsterwithgirl_2](/static/imported/2007/08/10/monsterwithgirl_2.jpg \"Monsterwithgirl_2\")](/static/imported/2007/08/10/monsterwithgirl_2.jpg)In the dawn days of science fiction, alien invaders would occasionally kidnap a girl in a torn dress and carry her off for intended ravishing, as lovingly depicted on many ancient magazine covers.  Oddly enough, the aliens never go after men in torn shirts.\n\nWould a non-humanoid alien, with a different evolutionary history and [evolutionary psychology](/lw/l1/evolutionary_psychology/), sexually desire a human female?  It seems rather unlikely.  To put it mildly.\n\nPeople don't make mistakes like that by deliberately reasoning:  \"All possible minds are likely to be wired pretty much the same way, therefore a bug-eyed monster will find human females attractive.\"  Probably the artist did not even think to ask whether an alien _perceives_ human females as attractive.  Instead, a human female in a torn dress _is sexy_—inherently so, as an intrinsic property.\n\nThey who went astray did not think about the alien's evolutionary history; they focused on the woman's torn dress.  If the dress were not torn, the woman would be less sexy; the alien monster doesn't enter into it.\n\nApparently we instinctively represent Sexiness as a direct attribute of the Woman object, Woman.sexiness, like Woman.height or Woman.weight.\n\nIf your brain uses that data structure, or something metaphorically similar to it, then [from the inside](/lw/no/how_an_algorithm_feels_from_inside/) it feels like sexiness is an inherent property of the woman, not a property of the alien looking at the woman.  Since the woman _is attractive,_ the alien monster will be _attracted_ to her—isn't that logical?\n\nE. T. Jaynes used the term [Mind Projection Fallacy](http://citeseer.ist.psu.edu/6330.html) to denote the error of projecting your own mind's properties into the external world.  Jaynes, as a late grand master of the Bayesian Conspiracy, was most concerned with the mistreatment of _probabilities_ as inherent properties of objects, rather than states of partial knowledge in some particular mind.  More about this shortly.\n\nBut the Mind Projection Fallacy generalizes as an error.  It is in the argument over [the real meaning of the word sound](/lw/np/disputing_definitions/), and in the magazine cover of the monster carrying off a woman in the torn dress, and Kant's declaration that space by its very nature is flat, and Hume's definition of [a priori](/lw/k2/a_priori/) ideas as those \"discoverable by the mere operation of thought, without dependence on what is anywhere existent in the universe\"...\n\n(Incidentally, I once read an SF story about a human male who entered into a sexual relationship with a sentient alien plant of appropriately squishy fronds; discovered that it was an [androecious](http://en.wikipedia.org/wiki/Plant_sexuality) (male) plant; agonized about this for a bit; and finally decided that it didn't really matter at that point.  And in Foglio and Pollotta's _Illegal Aliens,_ the humans land on a planet inhabited by sentient insects, and see a movie advertisement showing a human carrying off a bug in a delicate chiffon dress.  Just thought I'd mention that.)"
          },
          "voteCount": 58
        },
        {
          "name": "Reductionism",
          "type": "post",
          "slug": "reductionism",
          "_id": "tPqQdLCuxanjhoaNs",
          "url": null,
          "title": "Reductionism",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Truth, Semantics, & Meaning"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Reductionism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Almost one year ago, in April 2007, Matthew C submitted the following suggestion for an Overcoming Bias topic:\n\n> \"How and why the current reigning philosophical hegemon (reductionistic materialism) is obviously correct \\[...\\], while the reigning philosophical viewpoints of all past societies and civilizations are obviously suspect—\"\n\nI remember this, because I looked at the request and deemed it legitimate, but I knew I couldn't do that topic until I'd started on the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/) sequence, which wouldn't be for a while...\n\nBut now it's time to begin addressing this question.  And while I haven't yet come to the \"materialism\" issue, we can now start on \"reductionism\".\n\nFirst, let it be said that I do indeed hold that \"reductionism\", according to the [meaning](/lw/nr/the_argument_from_common_usage/) I will give for that word, is obviously correct; and [to perdition with any past civilizations that disagreed](/lw/lz/guardians_of_the_truth/).\n\nThis seems like a strong statement, at least the first part of it.  General Relativity seems well-supported, yet who knows but that some future physicist may overturn it?\n\nOn the other hand, we are never going _back_ to Newtonian mechanics.  The ratchet of science turns, but it does not turn in reverse_._  There are cases in scientific history where a theory suffered a wound or two, and then bounced back; but when a theory takes as many arrows through the chest as Newtonian mechanics, it _stays dead._\n\n\"To hell with what past civilizations thought\" seems safe enough, when past civilizations believed in something that has been falsified to the trash heap of history.\n\nAnd reductionism is not so much a positive hypothesis, as the _absence_ of belief—in particular, disbelief in a form of the Mind Projection Fallacy.\n\nI once met a fellow who claimed that he had experience as a Navy gunner, and he said, \"When you fire artillery shells, you've got to compute the trajectories using Newtonian mechanics.  If you compute the trajectories using relativity, you'll get the wrong answer.\"\n\nAnd I, and another person who was present, said flatly, \"No.\"  I added, \"You might not be able to compute the trajectories fast enough to get the answers in time—maybe that's what you mean?  But the relativistic answer will always be more accurate than the Newtonian one.\"\n\n\"No,\" he said, \"I mean that relativity will give you the _wrong answer,_ because things moving at the speed of artillery shells are governed by Newtonian mechanics, not relativity.\"\n\n\"If that were really true,\" I replied, \"you could publish it in a physics journal and collect your Nobel Prize.\"\n\nStandard physics uses the same _fundamental_ theory to describe the flight of a Boeing 747 airplane, and collisions in the Relativistic Heavy Ion Collider.  Nuclei and airplanes alike, according to our understanding, are obeying special relativity, quantum mechanics, and chromodynamics.\n\nBut we use entirely different _models_ to understand the aerodynamics of a 747 and a collision between gold nuclei in the RHIC.  A computer modeling the aerodynamics of a 747 may not contain a single token, a single bit of RAM, that represents a quark.\n\nSo is the 747 made of something other than quarks?  No, you're just _modeling_ it with _representational elements_ that do not have a one-to-one correspondence with the quarks of the 747.  The map is not the territory.\n\nWhy _not_ model the 747 with a chromodynamic representation?  Because then it would take a gazillion years to get any answers out of the model.  Also we could not store the model on all the memory on all the computers in the world, as of 2008.\n\nAs the saying goes, \"The map is not the territory, but you can't fold up the territory and put it in your glove compartment.\"  Sometimes you need a smaller map to fit in a more cramped glove compartment—but this does not change the territory.  The scale of a map is not a fact about the territory, it's a fact about the map.\n\nIf it _were_ possible to build and run a chromodynamic model of the 747, it would yield accurate predictions.  Better predictions than the aerodynamic model, in fact.\n\nTo build a fully accurate model of the 747, it is not necessary, in principle, for the model to contain explicit descriptions of things like airflow and lift.  There does not have to be a single token, a single bit of RAM, that corresponds to the position of the wings.  It is possible, in principle, to build an accurate model of the 747 that makes no mention of anything _except_ elementary particle fields and fundamental forces.\n\n\"What?\" cries the antireductionist.  \"Are you telling me the 747 _doesn't really have wings?_  I can see the wings right there!\"\n\nThe notion here is a subtle one.  It's not _just_ the notion that an object can have different descriptions at different levels.\n\nIt's the notion that \"having different descriptions at different levels\" is _itself_ something you say that belongs in the realm of Talking About Maps, not the realm of Talking About Territory.\n\nIt's not that the _airplane itself,_ the _laws of physics themselves,_ use different descriptions at different levels—as yonder artillery gunner thought.  Rather _we,_ for our convenience, use different simplified models at different levels.\n\nIf you looked at the ultimate chromodynamic model, the one that contained only elementary particle fields and fundamental forces, that model would contain all the facts about airflow and lift and wing positions—but these facts would be implicit, rather than explicit.\n\nYou, looking _at_ the model, and thinking _about_ the model, would be able to figure out where the wings were.  Having figured it out, there would be an explicit representation in your mind of the wing position—an explicit computational object, there in your neural RAM.  _In your mind._\n\nYou might, indeed, deduce all sorts of explicit descriptions of the airplane, at various levels, and even explicit rules for how your models at different levels interacted with each other to produce combined predictions—\n\nAnd the way that [algorithm feels from inside](/lw/no/how_an_algorithm_feels_from_inside/), is that the airplane would _seem_ to be made up of many levels at once, interacting with each other.\n\nThe way a belief _feels from inside_, is that you seem to be looking straight at reality.  When it actually _seems_ that you're looking at a belief, as such, you are really [experiencing a belief about belief](/lw/om/qualitatively_confused/).\n\nSo when your mind simultaneously believes explicit descriptions of many different levels, and believes explicit rules for transiting between levels, as part of an efficient combined model, it _feels like_ you are seeing a system that is _made of_ different level descriptions and their rules for interaction.\n\nBut this is just the brain trying to be efficiently compress an object that it cannot remotely begin to model on a fundamental level.  The airplane is too large.  Even a hydrogen atom would be too large.  Quark-to-quark interactions are insanely intractable.  You can't handle the _truth._\n\nBut the way physics _really_ works, as far as we can tell, is that there is _only_ the most basic level—the elementary particle fields and fundamental forces.  You can't handle the raw truth, but reality can handle it without the slightest simplification.  (I wish I knew where Reality got its computing power.)\n\nThe laws of physics do not contain distinct additional causal entities that correspond to lift or airplane wings, the way that _the mind of an engineer_ contains distinct additional _cognitive_ entities that correspond to lift or airplane wings.\n\nThis, as I see it, is the thesis of reductionism.  Reductionism is not a positive belief, but rather, a disbelief that the higher levels of simplified multilevel models are out there in the territory.  Understanding this on a gut level [dissolves the question](/lw/of/dissolving_the_question/) of \"How can you say the airplane doesn't really have wings, when I can see the wings right there?\"  The critical words are _really_ and _see_."
          },
          "voteCount": 80
        },
        {
          "name": "Privileging the Hypothesis",
          "type": "post",
          "slug": "privileging-the-hypothesis",
          "_id": "X2AD2LgtKgkRNPj2a",
          "url": null,
          "title": "Privileging the Hypothesis",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Confirmation Bias"
            },
            {
              "name": "Motivated Reasoning"
            },
            {
              "name": "Practice & Philosophy of Science"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Suppose that the police of Largeville, a town with a million inhabitants, are investigating a murder in which there are few or no clues—the victim was stabbed to death in an alley, and there are no fingerprints and no witnesses.\n\nThen, one of the detectives says, “Well… we have no idea who did it… no particular evidence singling out any of the million people in this city… but let’s _consider the hypothesis_ that this murder was committed by Mortimer Q. Snodgrass, who lives at 128 Ordinary Ln. It _could_ have been him, after all.”\n\nI’ll label this _the fallacy of privileging the hypothesis_. (Do let me know if it already has an official name—I can’t recall seeing it described.)\n\nNow the detective may perhaps have some form of [rational evidence](https://wiki.lesswrong.com/wiki/Rational_evidence) that is not legal evidence admissible in court—hearsay from an informant, for example. But if the detective does not have _some_ justification _already in hand_ for promoting Mortimer to the police’s special attention—if the name is pulled entirely out of a hat—then Mortimer’s rights are being violated.\n\nAnd this is true even if the detective is not claiming that Mortimer “did” do it, but only asking the police to spend time pondering that Mortimer _might_ have done it—unjustifiably promoting that particular hypothesis to attention. It’s human nature to [look for confirmation rather than disconfirmation](https://www.lesswrong.com/posts/rmAbiEKQDpDnZzcRf/positive-bias-look-into-the-dark). Suppose that three detectives each suggest their hated enemies, as names to be considered; and Mortimer is brown-haired, Frederick is black-haired, and Helen is blonde. Then a witness is found who says that the person leaving the scene was brown-haired. “Aha!” say the police. “We previously had no evidence to distinguish among the possibilities, but _now_ we know that Mortimer did it!”\n\nThis is related to the principle I’ve started calling “[locating the hypothesis](https://wiki.lesswrong.com/wiki/Locate_the_hypothesis),” which is that if you have a billion boxes only one of which contains a diamond (the truth), and your detectors only provide [1 bit of evidence](https://wiki.lesswrong.com/wiki/Amount_of_evidence) apiece, then it takes much more evidence to promote the truth to your particular attention—to narrow it down to ten good possibilities, each deserving of our individual attention—than it does to figure out _which_ of those ten possibilities is true. It takes 27 bits to narrow it down to ten, and just another 4 bits will give us better than even odds of having the right answer.\n\nThus the detective, in calling Mortimer to the particular attention of the police, for no reason out of a million other people, is skipping over _most of the evidence_ that needs to be supplied against Mortimer.\n\nAnd the detective ought to have this evidence in their possession, at the first moment when they bring Mortimer to the police’s attention _at all_. It may be mere rational evidence rather than legal evidence, but if there’s _no evidence_ then the detective is harassing and persecuting poor Mortimer.\n\nDuring my recent [diavlog with Scott Aaronson on quantum mechanics](http://bloggingheads.tv/videos/2220), I did manage to corner Scott to the extent of getting Scott to admit that there was no concrete evidence whatsoever that favors a [collapse postulate](https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ/p/xsZnufn3cQw7tJeQ3) or [single-world quantum mechanics](https://www.lesswrong.com/s/Kqs6GR7F5xziuSyGZ/p/S8ysHqeRGuySPttrS). But, said Scott, we might encounter _future_ evidence in favor of single-world quantum mechanics, and many-worlds still has [the open question of the Born probabilities](http://lesswrong.com/lw/py/the_born_probabilities/).\n\nThis is indeed what I would call the fallacy of privileging the hypothesis. There must be a trillion better ways to answer the Born question without adding a collapse postulate that would be the only non-linear, non-unitary, discontinous, non-differentiable, non-CPT-symmetric, non-local in the configuration space, Liouville’s-Theorem-violating, privileged-space-of-simultaneity-possessing, faster-than-light-influencing, acausal, informally specified law in all of physics. Something that unphysical is not worth _saying out loud_ or even _thinking about as a possibility_without a rather large weight of evidence—far more than the current grand total of zero.\n\nBut because of a historical accident, collapse postulates and single-world quantum mechanics are indeed on everyone’s lips and in everyone’s mind to be thought of, and so the open question of the Born probabilities is offered up (by Scott Aaronson no less!) as evidence that many-worlds can’t yet offer a complete picture of the world. Which is taken to mean that single-world quantum mechanics is still in the running somehow.\n\nIn the minds of human beings, if you can get them to think about this particular hypothesis rather than the trillion other possibilities that are no more complicated or unlikely, you really _have_ done a huge chunk of the work of persuasion. Anything thought about is treated as “in the running,” and if other runners seem to fall behind in the race a little, it’s assumed that this runner is edging forward or even entering the lead.\n\nAnd yes, this is just the same fallacy committed, on a much more blatant scale, by the theist who points out that modern science does not offer an absolutely complete explanation of the entire universe, and takes this as evidence for the existence of Jehovah. Rather than Allah, the Flying Spaghetti Monster, or a trillion other gods no less complicated—never mind the space of naturalistic explanations!\n\nTo talk about “intelligent design” whenever you point to a purported flaw or open problem in evolutionary theory is, again, privileging the hypothesis—you must have evidence _already in hand_ that points to intelligent design specifically in order to justify _raising that particular idea to our attention_, rather than a thousand others.\n\nSo that’s the _sane_ rule. And the corresponding [anti-epistemology](https://wiki.lesswrong.com/wiki/Anti-epistemology) is to talk endlessly of “possibility” and how you “can’t disprove” an idea, to hope that future evidence may confirm it without presenting past evidence already in hand, to dwell and dwell on _possibilities_ without evaluating possibly unfavorable evidence, to draw glowing word-pictures of confirming observations that _could_ happen but haven’t happened _yet_, or to try and show that piece after piece of negative evidence is “not conclusive.”\n\nJust as [Occam’s Razor](https://wiki.lesswrong.com/wiki/Occam%27s_razor) says that more complicated propositions require more evidence to believe, more complicated propositions also ought to require more work to raise to attention. Just as the principle of [burdensome details](https://wiki.lesswrong.com/wiki/Burdensome_details) requires that each part of a belief be separately justified, it requires that each part be separately raised to attention.\n\nAs discussed in [Perpetual Motion Beliefs](https://www.lesswrong.com/posts/zFuCxbY9E2E8HTbfZ/perpetual-motion-beliefs), faith and type 2 perpetual motion machines (water < refresh to render LaTeX > ice cubes < refresh to render LaTeX > electricity) have in common that they purport to _manufacture improbability from nowhere_, whether the improbability of water forming ice cubes or the improbability of arriving at correct beliefs without observation. Sometimes most of the anti-work involved in manufacturing this improbability is getting us to _pay attention_ to an unwarranted belief—thinking on it, dwelling on it. In large answer spaces, attention without evidence is more than halfway to belief without evidence.\n\nSomeone who spends all day thinking about whether the _Trinity_ does or does not exist, rather than Allah or Thor or the Flying Spaghetti Monster, is more than halfway to Christianity. If leaving, they’re less than half departed; if arriving, they’re more than halfway there.\n\nAn oft-encountered mode of privilege is to try to make uncertainty within a space, slop outside of that space onto the privileged hypothesis. For example, a creationist seizes on some (allegedly) debated aspect of contemporary theory, argues that scientists are _uncertain about evolution_, and then says, “We don’t really know which theory is right, so maybe intelligent design is right.” But the uncertainty is uncertainty _within_ the realm of naturalistic theories of evolution—we have no reason to believe that we’ll need to leave that realm to deal with our uncertainty, still _less_ that we would jump out of the realm of standard science and land _on Jehovah in particular_. That is privileging the hypothesis—taking doubt _within_ a normal space, and trying to slop doubt _out_ of the normal space, onto a privileged (and usually discredited) _extremely_ abnormal target.\n\nSimilarly, our uncertainty about where the Born statistics come from should be uncertainty _within_ the space of quantum theories that are continuous, linear, unitary, slower-than-light, local, causal, naturalistic, et cetera—the usual character of physical law. Some of that uncertainty might slop outside the standard space onto theories that violate _one_ of these standard characteristics. It’s indeed possible that we might have to think outside the box. But single-world theories violate _all_ these characteristics, and there is no reason to privilege that hypothesis."
          },
          "voteCount": 80
        },
        {
          "name": "Conservation of Expected Evidence",
          "type": "post",
          "slug": "conservation-of-expected-evidence",
          "_id": "jiBFC7DcCrZjGmZnJ",
          "url": null,
          "title": "Conservation of Expected Evidence",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Conservation of Expected Evidence"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Bayes' Theorem"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Friedrich Spee von Langenfeld, a priest who heard the confessions of condemned witches, wrote in 1631 the _Cautio Criminalis_ (“prudence in criminal cases”), in which he bitingly described the decision tree for condemning accused witches: If the witch had led an evil and improper life, she was guilty; if she had led a good and proper life, this too was a proof, for witches dissemble and try to appear especially virtuous. After the woman was put in prison: if she was afraid, this proved her guilt; if she was not afraid, this proved her guilt, for witches characteristically pretend innocence and wear a bold front. Or on hearing of a denunciation of witchcraft against her, she might seek flight or remain; if she ran, that proved her guilt; if she remained, the devil had detained her so she could not get away.\n\nSpee acted as confessor to many witches; he was thus in a position to observe _every_ branch of the accusation tree, that no matter _what_ the accused witch said or did, it was held as proof against her. In any individual case, you would only hear one branch of the dilemma. It is for this reason that scientists write down their experimental predictions in advance.\n\nBut _you can’t have it both ways_ —as a matter of probability theory, not mere fairness. The rule that “absence of evidence _is_ evidence of absence” is a special case of a more general law, which I would name Conservation of Expected Evidence: the _expectation_ of the posterior probability, after viewing the evidence, must equal the prior probability.\n\n_Therefore,_ for every expectation of evidence, there is an equal and opposite expectation of counterevidence.\n\nIf you expect a strong probability of seeing weak evidence in one direction, it must be balanced by a weak expectation of seeing strong evidence in the other direction. If you’re very confident in your theory, and therefore anticipate seeing an outcome that matches your hypothesis, this can only provide a very small increment to your belief (it is already close to 1); but the unexpected failure of your prediction would (and must) deal your confidence a huge blow. On _average_, you must expect to be _exactly_ as confident as when you started out. Equivalently, the mere _expectation_ of encountering evidence—before you’ve actually seen it—should not shift your prior beliefs.\n\nSo if you claim that “no sabotage” is evidence _for_ the existence of a Japanese-American Fifth Column, you must conversely hold that seeing sabotage would argue _against_ a Fifth Column. If you claim that “a good and proper life” is evidence that a woman is a witch, then an evil and improper life must be evidence that she is not a witch. If you argue that God, to test humanity’s faith, refuses to reveal His existence, then the miracles described in the Bible must argue against the existence of God.\n\nDoesn’t quite sound right, does it? Pay attention to that feeling of _this seems a little forced_, that quiet strain in the back of your mind. It’s important.\n\nFor a true Bayesian, it is impossible to seek evidence that _confirms_ a theory. There is no possible plan you can devise, no clever strategy, no cunning device, by which you can legitimately expect your confidence in a fixed proposition to be higher (on _average_) than before. You can only ever seek evidence to _test_ a theory, not to confirm it.\n\nThis realization can take quite a load off your mind. You need not worry about how to interpret every possible experimental result to confirm your theory. You needn’t bother planning how to make _any_ given iota of evidence confirm your theory, because you know that for every expectation of evidence, there is an equal and oppositive expectation of counterevidence. If you try to weaken the counterevidence of a possible “abnormal” observation, you can only do it by weakening the support of a “normal” observation, to a precisely equal and opposite degree. It is a zero-sum game. No matter how you connive, no matter how you argue, no matter how you strategize, you can’t possibly expect the resulting game plan to shift your beliefs (on average) in a particular direction.\n\nYou might as well sit back and relax while you wait for the evidence to come in.\n\n. . . Human psychology is _so_ screwed up."
          },
          "voteCount": 147
        },
        {
          "name": "The Apologist and the Revolutionary",
          "type": "post",
          "slug": "the-apologist-and-the-revolutionary",
          "_id": "ZiQqsgGX6a42Sfpii",
          "url": null,
          "title": "The Apologist and the Revolutionary",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationalization"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Psychiatry"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Rationalists complain that most people are too willing to [make excuses](http://www.overcomingbias.com/2007/10/fake-justificat.html) for their positions, and too unwilling to abandon those positions for ones that better fit the evidence. And most people really _are_ pretty bad at this. But certain stroke victims called anosognosiacs are much, much worse.  \n  \n[Anosognosia](http://en.wikipedia.org/wiki/Anosognosia) is the condition of not being aware of your own disabilities. To be clear, we're not talking minor disabilities here, the sort that only show up during a comprehensive clinical exam. We're talking paralysis or even blindness^1^. Things that should be pretty hard to miss.  \n  \nTake the example of the woman discussed in Lishman's [Organic Psychiatry](http://books.google.ie/books?id=Jsq-O12Ydo8C&pg=PA69&lpg=PA69&dq=anosognosia+shoulder+ring&source=bl&ots=_JDmzV0NJe&sig=_8rVlu85GNnsSkYEDRmAaUhWL1Q&hl=en&ei=Utu2SfjDL5SIjAfQpeSlCQ&sa=X&oi=book_result&resnum=1&ct=result). After a right-hemisphere stroke, she lost movement in her left arm but continuously denied it. When the doctor asked her to move her arm, and she observed it not moving, she claimed that it wasn't actually her arm, it was her daughter's. Why was her daughter's arm attached to her shoulder? The patient claimed her daughter had been there in the bed with her all week. Why was her wedding ring on her daughter's hand? The patient said her daughter had borrowed it. Where was the patient's arm? The patient \"turned her head and searched in a bemused way over her left shoulder\".  \n  \nWhy won't these patients admit they're paralyzed, and what are the implications for neurotypical humans? Dr. [Vilayanur Ramachandran](http://en.wikipedia.org/wiki/Vilayanur_S._Ramachandran#Scientific_career), leading neuroscientist and current holder of the world land-speed record for hypothesis generation, has a theory.\n\nOne immediately plausible hypothesis: the patient is unable to cope psychologically with the possibility of being paralyzed, so he responds with denial. Plausible, but according to Dr. Ramachandran, wrong. He notes that patients with left-side strokes almost never suffer anosognosia, even though the left side controls the right half of the body in about the same way the right side controls the left half. There must be something special about the right hemisphere.  \n  \nAnother plausible hypothesis: the part of the brain responsible for thinking about the affected area was damaged in the stroke. Therefore, the patient has lost access to the area, so to speak. Dr. Ramachandran doesn't like this idea either. The lack of right-sided anosognosia in left-hemisphere stroke victims argues against it as well. But how can we disconfirm it?  \n  \nDr. Ramachandran performed an experiment^2^ where he \"paralyzed\" an anosognosiac's good right arm. He placed it in a clever system of mirrors that caused a research assistant's arm to look as if it was attached to the patient's shoulder. Ramachandran told the patient to move his own right arm, and the false arm didn't move. What happened? The patient claimed he could see the arm moving - a classic anosognosiac response. This suggests that the anosognosia is not specifically a deficit of the brain's left-arm monitoring system, but rather some sort of failure of rationality.\n\nSays Dr. Ramachandran:\n\n> The reason anosognosia is so puzzling is that we have come to regard the 'intellect' as primarily propositional in character and one ordinarily expects propositional logic to be internally consistent. To listen to a patient deny ownership of her arm and yet, in the same breath, admit that it is attached to her shoulder is one of the most perplexing phenomena that one can encounter as a neurologist.\n\nSo what's Dr. Ramachandran's solution? He [posits two different reasoning modules](http://psych.utoronto.ca/~peterson/psy430s2001/Ramachandran%20VS%20Evolution%20of%20self-deception%20Med%20Hypoth%201996.pdf) located in the two different hemispheres. The left brain tries to fit the data to the theory to preserve a coherent internal narrative and prevent a person from jumping back and forth between conclusions upon each new data point. It is primarily an apologist, there to explain why any experience is exactly what its own theory would have predicted. The right brain is the seat of the [second virtue](http://yudkowsky.net/rational/virtues). When it's had enough of the left-brain's confabulating, it initiates a Kuhnian paradigm shift to a completely new narrative. Ramachandran describes it as \"a left-wing revolutionary\".  \n  \nNormally these two systems work in balance. But if a stroke takes the revolutionary offline, the brain loses its ability to change its mind about anything significant. If your left arm was working before your stroke, the little voice that ought to tell you it might be time to reject the \"left arm works fine\" theory goes silent. The only one left is the poor apologist, who must tirelessly invent stranger and stranger excuses for why all the facts really fit the \"left arm works fine\" theory perfectly well.  \n  \nIt gets weirder. For some reason, [squirting cold water into the left ear canal](http://www.neurology.org/cgi/content/abstract/65/8/1278) wakes up the revolutionary. Maybe the intense sensory input from an unexpected source makes the right hemisphere unusually aroused. Maybe distoring the balance sense causes the eyes to move rapidly, activating a latent system for inter-hemisphere co-ordination usually restricted to REM sleep^3^. In any case, a patient who has been denying paralysis for weeks or months will, upon having cold water placed in the ear, admit to paralysis, admit to having been paralyzed the past few weeks or months, and express bewilderment at having ever denied such an obvious fact. And then the effect wears off, and the patient not only denies the paralysis but denies ever having admitted to it.  \n  \nThis divorce between the apologist and the revolutionary might also explain some of the odd behavior of [split-brain](http://en.wikipedia.org/wiki/Split-brain) patients. Consider [the following experiment](http://books.google.ie/books?id=_rkKxbevFZEC&pg=PA10&lpg=PA10&dq=split-brain+chicken+shovel&source=bl&ots=9gVX7xBkJq&sig=yKnpOKg1jdzifungp7VgIXMLMcA&hl=en&ei=muu2SZKXA-LBjAeupOCvCQ&sa=X&oi=book_result&resnum=8&ct=result): a split-brain patient was shown two images, one in each visual field. The left hemisphere received the image of a chicken claw, and the right hemisphere received the image of a snowed-in house. The patient was asked verbally to describe what he saw, activating the left (more verbal) hemisphere. The patient said he saw a chicken claw, as expected. Then the patient was asked to point with his left hand (controlled by the right hemisphere) to a picture related to the scene. Among the pictures available were a shovel and a chicken. He pointed to the shovel. So far, no crazier than what we've come to expect from neuroscience.  \n  \nNow the doctor verbally asked the patient to describe why he just pointed to the shovel. The patient verbally (left hemisphere!) answered that he saw a chicken claw, and of course shovels are necessary to clean out chicken sheds, so he pointed to the shovel to indicate chickens. The apologist in the left-brain is helpless to do anything besides explain why the data fits its own theory, and its own theory is that whatever happened had something to do with chickens, dammit!  \n  \nThe logical follow-up experiment would be to ask the right hemisphere to explain the left hemisphere's actions. Unfortunately, the right hemisphere is either non-linguistic or as close as to make no difference. Whatever its thoughts, it's keeping them to itself.  \n  \n...you know, my mouth is _still_ agape at that whole cold-water-in-the-ear trick. I have this fantasy of gathering all the leading creationists together and squirting ice cold water in each of their left ears. All of a sudden, one and all, they admit their mistakes, and express bafflement at ever having believed such nonsense. And then ten minutes later the effect wears off, and they're all back to talking about irreducible complexity or whatever. I don't mind. I've already run off to upload the video to YouTube.  \n  \nThis is surely so great an exaggeration of Dr. Ramachandran's theory as to be a parody of it. And in any case I don't know how much to believe all this about different reasoning modules, or how closely the intuitive understanding of it I take from his paper matches the way a neuroscientist would think of it. Are the apologist and the revolutionary active in normal thought? Do anosognosiacs demonstrate the same pathological inability to change their mind on issues other than their disabilities? What of the argument that [confabulation](http://books.google.ie/books?id=_rkKxbevFZEC&dq=brain+fiction+confabulation&printsec=frontcover&source=bl&ots=9gVX7xFiMo&sig=ecS9mLduiZePctwU8lly9DejYjo&hl=en&ei=jvq2SYigB9nHjAefq62dCQ&sa=X&oi=book_result&resnum=1&ct=result#PPP11,M1) is a rather common failure mode of the brain, shared by some conditions that have little to do with right-hemisphere failure? Why does the effect of the cold water wear off so quickly? I've yet to see any really satisfying answers to any of these questions.\n\nBut whether Ramachandran is right or wrong, I give him enormous credit for doing serious research into the neural correlates of human rationality. I can think of few other fields that offer so many potential benefits.\n\n**Footnotes**\n\n1: See [Anton-Babinski syndrome](http://en.wikipedia.org/wiki/Anton%27s_syndrome)\n\n2: See Ramachandran's \"The Evolutionary Biology of Self-Deception\", the link from \"posits two different reasoning modules\" in this article.\n\n3: For Ramachandran's thoughts on REM, again see \"The Evolutionary Biology of Self Deception\""
          },
          "voteCount": 196
        }
      ]
    },
    {
      "title": "Top-50",
      "children": [
        {
          "name": "Your Strength as a Rationalist",
          "type": "post",
          "slug": "your-strength-as-a-rationalist",
          "_id": "5JDkW4MYXit2CquLs",
          "url": null,
          "title": "Your Strength as a Rationalist",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationalization"
            },
            {
              "name": "Noticing"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The following happened to me in an IRC chatroom, long enough ago that I was still hanging around in IRC chatrooms. Time has fuzzed the memory and my report may be imprecise.\n\nSo there I was, in an IRC chatroom, when someone reports that a friend of his needs medical advice. His friend says that he’s been having sudden chest pains, so he called an ambulance, and the ambulance showed up, but the paramedics told him it was nothing, and left, and now the chest pains are getting worse. What should his friend do?\n\nI was confused by this story. I remembered reading about homeless people in New York who would call ambulances just to be taken someplace warm, and how the paramedics always had to take them to the emergency room, even on the 27th iteration. Because if they didn’t, the ambulance company could be sued for lots and lots of money. Likewise, emergency rooms are legally obligated to treat anyone, regardless of ability to pay.^1^ So I didn’t quite understand how the described events could have happened. *Anyone* reporting sudden chest pains should have been hauled off by an ambulance instantly.\n\nAnd this is where I fell down as a rationalist. I remembered several occasions where my doctor would completely fail to panic at the report of symptoms that seemed, to me, very alarming. And the Medical Establishment was always right. Every single time. I had chest pains myself, at one point, and the doctor patiently explained to me that I was describing chest muscle pain, not a heart attack. So I said into the IRC channel, “Well, if the paramedics told your friend it was nothing, it must *really be* nothing—they’d have hauled him off if there was the tiniest chance of serious trouble.”\n\nThus I managed to explain the story within my existing model, though the fit still felt a little forced . . .\n\nLater on, the fellow comes back into the IRC chatroom and says his friend made the whole thing up. Evidently this was not one of his more reliable friends.\n\nI should have realized, perhaps, that an unknown acquaintance of an acquaintance in an IRC channel might be less reliable than a published journal article. Alas, belief is easier than disbelief; we believe instinctively, but disbelief requires a conscious effort.^2^\n\nSo instead, by dint of mighty straining, I forced my model of reality to explain an anomaly that *never actually happened.* And I *knew* how embarrassing this was. I *knew* that the usefulness of a model is not what it can explain, but what it can’t. A hypothesis that forbids nothing, permits everything, and thereby fails to constrain anticipation.\n\nYour strength as a rationalist is your ability to be more confused by fiction than by reality. If you are equally good at explaining any outcome, you have zero knowledge.\n\nWe are all weak, from time to time; the sad part is that I *could* have been stronger. I had all the information I needed to arrive at the correct answer, I even *noticed* the problem, and then I ignored it. My feeling of confusion was a Clue, and I threw my Clue away.\n\nI should have paid more attention to that sensation of *still feels a little forced.* It’s one of the most important feelings a truthseeker can have, a part of your strength as a rationalist. It is a design flaw in human cognition that this sensation manifests as a quiet strain in the back of your mind, instead of a wailing alarm siren and a glowing neon sign reading:\n\n**Either Your Model Is False Or This Story Is Wrong**.\n\n^1^ And the hospital absorbs the costs, which are enormous, so hospitals are closing their emergency rooms . . . It makes you wonder what’s the point of having economists if we’re just going to ignore them.\n\n^2^ From McCluskey (2007), “Truth Bias”: “\\[P\\]eople are more likely to correctly judge that a truthful statement is true than that a lie is false. This appears to be a fairly robust result that is not just a function of truth being the correct guess where the evidence is weak—it shows up in controlled experiments where subjects have good reason not to assume truth\\[.\\]” [http://www.overcomingbias.com/2007/08/truth-bias.html](http://www.overcomingbias.com/2007/08/truth-bias.html) .\n\nAnd from Gilbert et al. (1993), “You Can’t Not Believe Everything You Read”: “Can people comprehend assertions without believing them? \\[...\\] Three experiments support the hypothesis that comprehension includes an initial belief in the information comprehended.”"
          },
          "voteCount": 144
        },
        {
          "name": "Practical Advice Backed By Deep Theories",
          "type": "post",
          "slug": "practical-advice-backed-by-deep-theories",
          "_id": "LqjKP255fPRY7aMzw",
          "url": null,
          "title": "Practical Advice Backed By Deep Theories",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Practical"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Scholarship & Learning"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Once upon a time, Seth Roberts took a European vacation and found that [he started losing weight while drinking unfamiliar-tasting caloric fruit juices](/lw/a6/the_unfinished_mystery_of_the_shangrila_diet/).\n\nNow suppose Roberts had not known, and never did know, anything about metabolic set points or flavor-calorie associations—all this high-falutin' scientific experimental research that had been done on rats and occasionally humans.\n\nHe would have posted to his blog, \"Gosh, everyone!  You should try these amazing fruit juices that are making me lose weight!\"  And that would have been the end of it.  Some people would have tried it, it would have worked _temporarily_ for some of them (until the flavor-calorie association kicked in) and there never would have been a Shangri-La Diet _per se_.\n\nThe existing Shangri-La Diet is visibly incomplete—for some people, like me, it doesn't seem to work, and there is no apparent reason for this or any logic permitting it.  But the reason why as many people have benefited as they have—the reason why there was more than just one more blog post describing a trick that seemed to work for one person and didn't work for anyone else—is that Roberts _knew the experimental science that let him interpret what he was seeing, in terms of deep factors that actually did exist._\n\nOne of the pieces of advice on OB/LW that was frequently cited as [the most important thing](/lw/9/the_most_important_thing_you_learned/) learned, was the idea of \"[the bottom line](http://www.overcomingbias.com/2007/09/the-bottom-line.html)\"—that once a conclusion is written in your mind, it is already true or already false, already wise or already stupid, and no amount of later argument can change that except by changing the conclusion.  And this ties directly into another oft-cited most important thing, which is the idea of \"[engines of cognition](http://www.overcomingbias.com/2008/02/second-law.html)\", minds as mapping engines that require evidence as fuel.\n\nIf I had merely written one more blog post that said, \"You know, you really should be more open to changing your mind—it's pretty important—and oh yes, you should pay attention to the evidence too.\"  And this would not have been as useful.  Not just because it was _less persuasive,_ but because the _actual operations_ would have been much less clear without the explicit theory backing it up.  What constitutes _evidence,_ for example?  Is it anything that seems like a forceful argument?  Having an explicit probability theory and an explicit causal account of what makes reasoning effective, makes a _large_ difference in the forcefulness and implementational details of the old advice to \"Keep an open mind and pay attention to the evidence.\"\n\nIt is also important to realize that _causal theories_ are much more likely to be true when they are picked up from a science textbook than when invented on the fly—it is very easy to invent cognitive structures that look like causal theories but are not even [anticipation-controlling](http://www.overcomingbias.com/2007/07/making-beliefs-.html), let alone true.\n\nThis is the signature style I want to convey from all those posts that entangled cognitive science experiments and probability theory and epistemology with the practical advice—that practical advice actually becomes practically more powerful if you go out and read up on cognitive science experiments, or probability theory, or even materialist epistemology, and _realize what you're seeing._  This is the brand that can distinguish LW from ten thousand other blogs purporting to offer advice.\n\nI could tell you, \"You know, how much you're satisfied with your food probably depends more on the quality of the food than on how much of it you eat.\"  And you would read it and forget about it, and the impulse to finish off a whole plate would still feel just as strong.  But if I tell you about [scope insensitivity](http://www.overcomingbias.com/2007/05/scope_insensiti.html), and [duration neglect](http://web.mit.edu/ariely/www/MIT/Papers/duration1.pdf) and the [Peak/End rule](http://en.wikipedia.org/wiki/Peak-end_rule), you are suddenly aware in a very concrete way, looking at your plate, that you will form almost exactly the same retrospective memory whether your portion size is large or small; you now possess a deep theory about the _rules_ governing your memory, and you know that this is what the rules say.  (You also know to save the dessert for last.)\n\nI want to hear how I can overcome akrasia—how I can have more willpower, or get more done with less mental pain.  But there are ten thousand people purporting to give advice on this, and for the most part, it is on the level of that alternate Seth Roberts who just tells people about the amazing effects of drinking fruit juice.  Or actually, somewhat worse than that—it's people trying to describe internal mental levers that they pulled, for which there are no standard words, and which they do not actually know how to point to.  See also the [illusion of transparency](http://www.overcomingbias.com/2007/10/illusion-of-tra.html), [inferential distance](http://www.overcomingbias.com/2007/10/inferential-dis.html), and [double illusion of transparency](http://www.overcomingbias.com/2007/10/double-illusion.html).  (Notice how \"You overestimate how much you're explaining and your listeners overestimate how much they're hearing\" becomes _much more forceful_ as advice, after I back it up with a cognitive science experiment and some evolutionary psychology?)\n\nI think that the advice I _need_ is from someone who reads up on a whole lot of experimental psychology dealing with willpower, mental conflicts, ego depletion, preference reversals, hyperbolic discounting, the breakdown of the self, picoeconomics, etcetera, and who, in the process of overcoming their own akrasia, manages to understand what they did in _truly general terms_—thanks to experiments that give them a vocabulary of cognitive phenomena that _actually exist_, as opposed to phenomena they just made up.  And moreover, someone who can _explain_ what they did to someone else, thanks again to the experimental and theoretical vocabulary that lets them point to replicable experiments that ground the ideas in very concrete results, or mathematically clear ideas.\n\nNote the grade of increasing difficulty in citing:\n\n*   _Concrete_ _experimental results_ (for which one need merely consult a paper, hopefully one that reported p < 0.01 because p < 0.05 may fail to replicate)\n*   _Causal accounts that are actually true_ (which may be most reliably obtained by looking for the theories that are used by a majority within a given science)\n*   _Math validly interpreted_ (on which I have trouble offering useful advice because so much of my own math talent is intuition that kicks in before I get a chance to deliberate)\n\nIf you don't know who to trust, or you don't trust yourself, you should concentrate on experimental results to start with, move on to thinking in terms of causal theories that are widely used within a science, and dip your toes into math and epistemology with extreme caution.\n\nBut practical advice really, really _does_ become a lot more powerful when it's backed up by _concrete experimental results_, _causal accounts that are actually true,_ and _math validly interpreted._"
          },
          "voteCount": 56
        },
        {
          "name": "Scientific Evidence, Legal Evidence, Rational Evidence",
          "type": "post",
          "slug": "scientific-evidence-legal-evidence-rational-evidence",
          "_id": "fhojYBGGiYAFcryHZ",
          "url": null,
          "title": "Scientific Evidence, Legal Evidence, Rational Evidence",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Practice & Philosophy of Science"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Empiricism"
            },
            {
              "name": "Law and Legal systems"
            },
            {
              "name": "Distinctions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Suppose that your good friend, the police commissioner, tells you in strictest confidence that the crime kingpin of your city is Wulky Wilkinsen. As a rationalist, are you licensed to believe this statement? Put it this way: if you go ahead and insult Wulky, I’d call you foolhardy. Since it is prudent to act as if Wulky has a substantially higher-than-default probability of being a crime boss, the police commissioner’s statement must have been strong Bayesian evidence.\n\nOur legal system will not imprison Wulky on the basis of the police commissioner’s statement. It is not admissible as *legal evidence*. Maybe if you locked up every person accused of being a crime boss by a police commissioner, you’d *initially* catch a lot of crime bosses, and relatively few people the commissioner just didn’t like. But unrestrained power attracts corruption like honey attracts flies: over time, you’d catch fewer and fewer real crime bosses (who would go to greater lengths to ensure anonymity), and more and more innocent victims.\n\nThis does not mean that the police commissioner’s statement is not rational evidence. It still has a lopsided likelihood ratio, and you’d still be a fool to insult Wulky. But on a *social* level, in pursuit of a social goal, we deliberately define “legal evidence” to include only particular kinds of evidence, such as the police commissioner’s own observations on the night of April 4th. All legal evidence should ideally be rational evidence, but not the other way around. We impose special, strong, additional standards before we anoint rational evidence as “legal evidence.”\n\nAs I write this sentence at 8:33 p.m., Pacific time, on August 18th, 2007, I am wearing white socks. As a rationalist, are you licensed to believe the previous statement? Yes. Could I testify to it in court? Yes. Is it a *scientific* statement? No, because there is no experiment you can perform yourself to verify it. Science is made up of *generalizations* which apply to many particular instances, so that you can run new real-world experiments which test the generalization, and thereby verify for yourself that the generalization is true, without having to trust anyone’s authority. Science is the *publicly reproducible* knowledge of humankind.\n\nLike a court system, science as a social process is made up of fallible humans. We want a protected pool of beliefs that are *especially* reliable. And we want social rules that encourage the generation of such knowledge. So we impose special, strong, additional standards before we canonize rational knowledge as “scientific knowledge,” adding it to the protected belief pool. Is a rationalist licensed to believe in the historical existence of Alexander the Great? Yes. We have a rough picture of ancient Greece, untrustworthy but better than maximum entropy. But we are dependent on authorities such as Plutarch; we cannot discard Plutarch and verify everything for ourselves. Historical knowledge is not scientific knowledge.\n\nIs a rationalist licensed to believe that the Sun will rise on September 18th, 2007? Yes—not with absolute certainty, but that’s the way to bet.^1^ Is this statement, as I write this essay on August 18th, 2007, a *scientific* belief?\n\nIt may seem perverse to deny the adjective “scientific” to statements like “The Sun will rise on September 18th, 2007.” If Science could not make predictions about future events—events which have *not yet* happened—then it would be useless; it could make no prediction in advance of experiment. The prediction that the Sun will rise is, definitely, an *extrapolation* from scientific generalizations. It is based upon models of the Solar System that you could test for yourself by experiment.\n\nBut imagine that you’re constructing an experiment to verify prediction #27, in a new context, of an accepted theory Q. You may not have any concrete reason to suspect the belief is wrong; you just want to test it in a new context. It seems dangerous to say, *before* running the experiment, that there is a “scientific belief” about the result. There is a “conventional prediction” or “theory Q’s prediction.” But if you already know the “scientific belief” about the result, why bother to run the experiment?\n\nYou begin to see, I hope, why I identify Science with *generalizations*, rather than the history of any one experiment. A historical event happens once; generalizations apply over many events. History is not reproducible; scientific generalizations are.\n\nIs my definition of “scientific knowledge” *true*? That is not a well-formed question. The special standards we impose upon science are pragmatic choices. Nowhere upon the stars or the mountains is it written that p < 0.05 shall be the standard for scientific publication. Many now argue that 0.05 is too weak, and that it would be *useful* to lower it to 0.01 or 0.001.\n\nPerhaps future generations, acting on the theory that science is the *public*, *reproducible* knowledge of humankind, will only label as “scientific” papers published in an open-access journal. If you charge for access to the knowledge, is it part of the knowledge of *humankind*? Can we fully trust a result if people must pay to criticize it?\n\nFor myself, I think scientific practice would be better served by the dictum that only open, public knowledge counts. But however we choose to define “science,” information in a $20,000/year closed-access journal will still count as Bayesian evidence; and so too, the police commissioner’s private assurance that Wulky is the kingpin.\n\n* * *\n\n^1^ Pedants: interpret this as the Earth’s rotation and orbit remaining roughly constant relative to the Sun."
          },
          "voteCount": 85
        },
        {
          "name": "Guessing the Teacher's Password",
          "type": "post",
          "slug": "guessing-the-teacher-s-password",
          "_id": "NMoLJuDJEms7Ku9XS",
          "url": null,
          "title": "Guessing the Teacher's Password",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Problem-solving (skills and techniques)"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Education"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "When I was young, I read popular physics books such as Richard Feynman’s *QED: The Strange Theory of Light and Matter*. I knew that light was waves, sound was waves, matter was waves. I took pride in my scientific literacy, when I was nine years old.\n\nWhen I was older, and I began to read the *Feynman Lectures on Physics*, I ran across a gem called “the wave equation.” I could follow the equation’s derivation, but, [looking back](http://www.math.utah.edu/~pa/math/polya.html), I couldn’t see its truth at a glance. So I thought about the wave equation for three days, on and off, until I saw that it was embarrassingly obvious. And when I finally understood, I realized that the whole time I had accepted the honest assurance of physicists that light was waves, sound was waves, matter was waves, I had not had the vaguest idea of what the word “wave” meant to a physicist.\n\nThere is an instinctive tendency to think that if a physicist says “light is made of waves,” and the teacher says “What is light made of?” and the student says “Waves!”, then the student has made a true statement. That’s only fair, right? We accept “waves” as a correct answer from the physicist; wouldn’t it be unfair to reject it from the student? Surely, the answer “Waves!” is either *true* or *false*, right?\n\nWhich is one more bad habit to [unlearn from school](http://lesswrong.com/lw/i2/two_more_things_to_unlearn_from_school/). Words do not have intrinsic definitions. If I hear the syllables “bea-ver” and think of a large rodent, that is a fact about my own state of mind, not a fact about the syllables “bea-ver.” The sequence of syllables “made of waves” (or “because of heat conduction”) is not a *hypothesis*; it is a pattern of vibrations traveling through the air, or ink on paper. It can *associate* to a hypothesis in someone’s mind, but it is not, of itself, right or wrong. But in school, the teacher hands you a gold star for *saying* “made of waves,” which must be the correct answer because the teacher heard a physicist emit the same sound-vibrations. Since verbal behavior (spoken or written) is what gets the gold star, students begin to think that verbal behavior has a truth-value. After all, either light is made of waves, or it isn’t, right?\n\nAnd this leads into an even worse habit. Suppose the teacher asks you why the far side of a metal plate feels warmer than the side next to the radiator. If you say “I don’t know,” you have *no* chance of getting a gold star—it won’t even count as class participation. But, during the current semester, this teacher has used the phrases “because of heat convection,” “because of heat conduction,” and “because of radiant heat.” One of these is probably what the teacher wants. You say, “Eh, maybe because of heat conduction?”\n\nThis is not a hypothesis *about* the metal plate. This is not even a proper belief. It is an attempt to *guess the teacher’s password.*\n\nEven visualizing the symbols of the diffusion equation (the math governing heat conduction) doesn’t mean you’ve formed a hypothesis *about* the metal plate. This is not school; we are not testing your memory to see if you can write down the diffusion equation. This is Bayescraft; we are scoring your anticipations of experience. If you *use* the diffusion equation, by measuring a few points with a thermometer and then trying to predict what the thermometer will say on the next measurement, then it is definitely connected to experience. Even if the student just visualizes something *flowing*, and therefore holds a match near the cooler side of the plate to try to measure where the heat goes, then this mental image of flowing-ness connects to experience; it controls anticipation.\n\nIf you aren’t *using* the diffusion equation—putting in numbers and getting out results that control your anticipation of particular experiences—then the connection between map and territory is severed as though by a knife. What remains is not a belief, but a verbal behavior.\n\nIn the school system, it’s all about verbal behavior, whether written on paper or spoken aloud. Verbal behavior gets you a gold star or a failing grade. Part of unlearning this bad habit is becoming consciously aware of the difference between an explanation and a password.\n\nDoes this seem too harsh? When you’re faced by a confusing metal plate, can’t “heat conduction?” be a first step toward finding the answer? Maybe, but only if you don’t fall into the trap of thinking that you are looking for a password. What if there is no teacher to tell you that you failed? Then you may think that “Light is wakalixes” is a good explanation, that “wakalixes” is the correct password. It happened to me when I was nine years old—not because I was stupid, but because this is what happens *by default.* This is how human beings think, unless they are trained *not* to fall into the trap. Humanity stayed stuck in holes like this for thousands of years.\n\nMaybe, if we drill students that *words don’t count, only anticipation-controllers,* the student will *not* get stuck on “Heat conduction? No? Maybe heat convection? That’s not it either?” Maybe *then*, thinking the phrase “heat conduction” will lead onto a genuinely helpful path, like:\n\n*   “Heat conduction?”\n*   But that’s only a phrase—what does it mean?\n*   The diffusion equation?\n*   But those are only symbols—how do I apply them?\n*   What does applying the diffusion equation lead me to anticipate?\n*   It sure doesn’t lead me to anticipate that the side of a metal plate farther away from a radiator would feel warmer.\n*   I notice that I am confused. Maybe the near side just *feels* cooler, because it’s made of more insulative material and transfers less heat to my hand? I’ll try measuring the temperature . . .\n*   Okay, that wasn’t it. Can I try to verify whether the diffusion equation holds true of this metal plate, at all? Is heat *flowing* the way it usually does, or is something else going on?\n*   I could hold a match to the plate and try to measure how heat spreads over time . . .\n\nIf we are *not* strict about “Eh, maybe because of heat conduction?” being a fake explanation, the student will very probably get stuck on some wakalixes-password. *This happens by default: it happened to the whole human species for thousands of years.*"
          },
          "voteCount": 178
        },
        {
          "name": "The Sword of Good",
          "type": "post",
          "slug": "the-sword-of-good",
          "_id": "XuLG6M7sHuenYWbfC",
          "url": null,
          "title": "The Sword of Good",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Fiction"
            },
            {
              "name": "Moral Uncertainty"
            },
            {
              "name": "Tribalism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_..fragments of a book that would never be written..._\n\n\\*      *      *\n\nCaptain Selena, late of the pirate ship _Nemesis,_ quietly extended the very tip of her blade around the corner, staring at the tiny reflection on the metal.  At once, but still silently, she pulled back the sword; and with her other hand made a complex gesture.\n\nThe translation spell told Hirou that the handsigns meant:  \"Orcs.  Seven.\"\n\nDolf looked at Hirou.  \"My Prince,\" the wizard signed, \"do not waste yourself against mundane opponents.  Do not draw the Sword of Good as yet.  Leave these to Selena.\"\n\nHirou's mouth was very dry.  He didn't know if the translation spell could understand the difference between wanting to talk and wanting to make gestures; and so Hirou simply nodded.\n\nNot for the first time, the thought occurred to Hirou that if he'd actually _known_ he was going to be transported into a magical universe, informed he was the long-lost heir to the Throne of Bronze, handed the legendary Sword of Good, and told to fight evil, he would have spent less time reading fantasy novels.  Joined the army, maybe.  Taken fencing lessons, at least.  If there was one thing that _didn't_ prepare you for fantasy real life, it was sitting at home reading fantasy fiction.\n\nDolf and Selena were looking at Hirou, as if waiting for something more.\n\n_Oh.  That's right.  I'm the prince._\n\nHirou raised a finger and pointed it around the corner, trying to indicate that they should go ahead -\n\nWith a sudden burst of motion Selena plunged around the corner, Dolf following hard on her heels, and Hirou, startled and hardly thinking, moving after.\n\n_(This story ended up too long for a single LW post, so I put it [on yudkowsky.net](http://yudkowsky.net/other/fiction/the-sword-of-good).  \nDo read [the rest of the story](http://yudkowsky.net/other/fiction/the-sword-of-good) there, before continuing to the Acknowledgments below.)_\n\n* * *\n\n**Acknowledgments:**\n\nI had the idea for this story during a conversation with Nick Bostrom and Robin Hanson about an awful little facet of human nature I call \"suspension of moral disbelief\".  The archetypal case in my mind will always be the Passover Seder, watching my parents and family and sometimes friends reciting the Ten Plagues that God is supposed to have visited on Egypt.  You take drops from the wine glass - or grape juice in my case - and drip them onto the plate, to symbolize your sadness at God slaughtering the first-born male children of the Egyptians.  So the Seder actually _points out_ the awfulness, and yet no one says:  \"This is wrong; God should not have done that to innocent families in retaliation for the actions of an unelected Pharaoh.\"  I forget when I first realized how horrible that was - the real horror being not the Plagues, of course, since they never happened; the real horror is watching your family _not notice_ that they're swearing allegiance to an evil God in a happy wholesome family Cthulhu-worshiping ceremony.  Arbitrarily hideous evils can be wholly concealed by a social atmosphere in which no one is expected to point them out and it would seem awkward and out-of-place to do so.\n\nIn writing it's even simpler - the author gets to _create_ the whole social universe, and the readers are immersed in the hero's own internal perspective.  And so _anything_ the heroes do, which no character notices as wrong, won't be noticed by the readers as unheroic.  Genocide, mind-rape, eternal torture, _anything._\n\nExplicit inspiration was taken from [this XKCD](http://xkcd.com/549/) (warning: spoilers for _The Princess Bride_), [this Boat Crime](http://www.boatcrime.com/2009/07/19/wizards-are-assholes/), and [this Monty Python](http://www.youtube.com/watch?v=5Xd_zkMEgkI), not to mention [that essay by David Brin](http://dir.salon.com/story/ent/feature/2002/12/17/tolkien_brin/index.html) and [the entire Goblins webcomic](http://goblins.keenspot.com/d/20051217.html).  This [Looking For Group](http://lfgcomic.com/page/104) helped inspire the story's title, and everything else flowed downhill from there."
          },
          "voteCount": 112
        },
        {
          "name": "Fake Explanations",
          "type": "post",
          "slug": "fake-explanations",
          "_id": "fysgqk4CjAwhBgNYT",
          "url": null,
          "title": "Fake Explanations",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Once upon a time, there was an instructor who taught physics students. One day the instructor called them into the classroom and showed them a wide, square plate of metal, next to a hot radiator. The students each put their hand on the plate and found the side next to the radiator cool, and the distant side warm. And the instructor said, *Why do you think this happens?* Some students guessed convection of air currents, and others guessed strange metals in the plate. They devised many creative explanations, none stooping so low as to say “I don’t know” or “This seems impossible.”\n\nAnd the answer was that before the students entered the room, the instructor turned the plate around.^1^\n\nConsider the student who frantically stammers, “Eh, maybe because of the heat conduction and so?” I ask: Is this answer a proper belief? The words are easily enough professed—said in a loud, emphatic voice. But do the words actually control anticipation?\n\nPonder that innocent little phrase, “because of,” which comes before “heat conduction.” Ponder some of the *other* things we could put after it. We could say, for example, “Because of phlogiston,” or “Because of magic.”\n\n“Magic!” you cry. “That’s not a *scientific* explanation!” Indeed, the phrases “because of heat conduction” and “because of magic” are readily recognized as belonging to different *literary genres.* “Heat conduction” is something that Spock might say on *Star Trek*, whereas “magic” would be said by Giles in *Buffy the Vampire Slayer*.\n\nHowever, as Bayesians, we take no notice of literary genres. For us, the substance of a model is the control it exerts on anticipation. If you say “heat conduction,” what experience does that lead you to *anticipate*? Under normal circumstances, it leads you to anticipate that, if you put your hand on the side of the plate near the radiator, that side will feel warmer than the opposite side. If “because of heat conduction” can also explain the radiator-adjacent side feeling *cooler*, then it can explain pretty much *anything.*\n\nAnd as we all know by this point (I do hope), if you are equally good at explaining any outcome, you have zero knowledge. “Because of heat conduction,” used in such fashion, is a disguised hypothesis of maximum entropy. It is anticipation-isomorphic to saying “magic.” It feels like an explanation, but it’s not.\n\nSuppose that instead of guessing, we measured the heat of the metal plate at various points and various times. Seeing a metal plate next to the radiator, we would ordinarily expect the point temperatures to satisfy an equilibrium of the diffusion equation with respect to the boundary conditions imposed by the environment. You might not know the exact temperature of the first point measured, but after measuring the first points—I’m not physicist enough to know how many would be required—you could take an excellent guess at the rest.\n\nA true master of the art of using numbers to constrain the anticipation of material phenomena—a “physicist”—would take some measurements and say, “This plate was in equilibrium with the environment two and a half minutes ago, turned around, and is now approaching equilibrium again.”\n\nThe deeper error of the students is not simply that they failed to constrain anticipation. Their deeper error is that they thought they were doing physics. They said the phrase “because of,” followed by the sort of words Spock might say on *Star Trek*, and thought they thereby entered the magisterium of science.\n\nNot so. They simply moved their magic from one literary genre to another.\n\n* * *\n\n^1^ Joachim Verhagen, *Science Jokes*, 2001, [http://web.archive.org/web/20060424082937/http://www.nvon.nl/scheik/best/diversen/scijokes/scijokes.txt](http://web.archive.org/web/20060424082937/http://www.nvon.nl/scheik/best/diversen/scijokes/scijokes.txt)"
          },
          "voteCount": 120
        },
        {
          "name": "The Proper Use of Humility",
          "type": "post",
          "slug": "the-proper-use-of-humility",
          "_id": "GrDqnMjhqoxiqpQPw",
          "url": null,
          "title": "The Proper Use of Humility",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Humility"
            },
            {
              "name": "Distinctions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "It is widely recognized that good science requires some kind of humility. _What sort_ of humility is more controversial.\n\nConsider the creationist who says: “But who can really know whether evolution is correct? It is just a theory. You should be more humble and open-minded.” Is this humility? The creationist practices a very selective underconfidence, refusing to integrate massive weights of evidence in favor of a conclusion they find uncomfortable. I would say that whether you call this “humility” or not, it is the wrong step in the dance.\n\nWhat about the engineer who humbly designs fail-safe mechanisms into machinery, even though they’re damn sure the machinery won’t fail? This seems like a good kind of humility to me. Historically, it’s not unheard-of for an engineer to be damn sure a new machine won’t fail, and then it fails anyway.\n\nWhat about the student who humbly double-checks the answers on their math test? Again I’d categorize that as good humility. The student who double-checks their answers _wants to become stronger_; they react to a possible inner flaw by doing what they can to repair the flaw.\n\nWhat about a student who says, “Well, no matter how many times I check, I can’t ever be _certain_ my test answers are correct,” and therefore doesn’t check even once? Even if this choice stems from an emotion similar to the emotion felt by the previous student, it is less wise.\n\nYou suggest studying harder, and the student replies: “No, it wouldn’t work for me; I’m not one of the smart kids like you; nay, one so lowly as myself can hope for no better lot.” This is social modesty, not humility. It has to do with regulating status in the tribe, rather than scientific process. If you ask someone to “be more humble,” by default they’ll associate the words to social modesty—which is an intuitive, everyday, ancestrally relevant concept. Scientific humility is a more recent and rarefied invention, and it is not inherently social. Scientific humility is something you would practice even if you were alone in a spacesuit, light years from Earth with no one watching. Or even if you received an absolute guarantee that no one would ever criticize you again, no matter what you said or thought of yourself. You’d still double-check your calculations if you were wise.\n\nThe student says: “But I’ve seen other students double-check their answers and then they still turned out to be wrong. Or what if, by the problem of induction, 2 + 2 = 5 this time around? No matter what I do, I won’t be sure of myself.” It sounds very profound, and very modest. But it is not coincidence that the student wants to hand in the test quickly, and go home and play video games.\n\nThe end of an era in physics does not always announce itself with thunder and trumpets; more often it begins with what seems like a small, small flaw . . . But because physicists have this arrogant idea that their models should work _all_ the time, not just _most_ of the time, they follow up on small flaws. Usually, the small flaw goes away under closer inspection. Rarely, the flaw widens to the point where it blows up the whole theory. Therefore it is written: “If you do not seek perfection you will halt before taking your first steps.”\n\nBut think of the social audacity of trying to be right _all_ the time! I seriously suspect that if Science claimed that evolutionary theory is true most of the time but not all of the time—or if Science conceded that maybe on some days the Earth _is_ flat, but who really knows—then scientists would have better social reputations. Science would be viewed as less confrontational, because we wouldn’t have to argue with people who say the Earth is flat—there would be room for compromise. When you argue a lot, people look upon you as confrontational. If you repeatedly refuse to compromise, it’s even worse. Consider it as a question of tribal status: scientists have certainly earned some extra status in exchange for such socially useful tools as medicine and cellphones. But this social status does not justify their insistence that _only_ scientific ideas on evolution be taught in public schools. Priests also have high social status, after all. Scientists are getting above themselves—they won a little status, and now they think they’re chiefs of the whole tribe! They ought to be more humble, and compromise a little.\n\nMany people seem to possess rather hazy views of “rationalist humility.” It is dangerous to have a prescriptive principle which you only vaguely comprehend; your mental picture may have so many degrees of freedom that it can adapt to justify almost any deed. Where people have vague mental models that can be used to argue anything, they usually end up believing whatever they started out wanting to believe. This is so convenient that people are often reluctant to give up vagueness. But the purpose of our ethics is to move us, not be moved by us.\n\n“Humility” is a virtue that is often misunderstood. This doesn’t mean we should discard the concept of humility, but we should be careful using it. It may help to look at the _actions_ recommended by a “humble” line of thinking, and ask: “Does acting this way make you stronger, or weaker?” If you think about the problem of induction as applied to a bridge that needs to stay up, it may sound reasonable to conclude that nothing is certain no matter what precautions are employed; but if you consider the real-world difference between adding a few extra cables, and shrugging, it seems clear enough what makes the stronger bridge.\n\nThe vast majority of appeals that I witness to “rationalist’s humility” are excuses to shrug. The one who buys a lottery ticket, saying, “But you can’t _know_ that I’ll lose.” The one who disbelieves in evolution, saying, “But you can’t _prove_ to me that it’s true.” The one who refuses to confront a difficult-looking problem, saying, “It’s probably too hard to solve.” The problem is motivated skepticism a.k.a. disconfirmation bias—more heavily scrutinizing assertions that we don’t want to believe.^[1](#fn1x2)^ Humility, in its most commonly misunderstood form, is a fully general excuse not to believe something; since, after all, you can’t be _sure_. Beware of fully general excuses!\n\nA further problem is that humility is all too easy to _profess._ Dennett, in _Breaking the Spell: Religion as a Natural Phenomenon_, points out that while many religious assertions are very hard to believe, it is easy for people to believe that they _ought_ to believe them. Dennett terms this “belief in belief.” What would it mean to really assume, to really believe, that three is equal to one? It’s a lot easier to believe that you _should_, somehow, believe that three equals one, and to make this response at the appropriate points in church. Dennett suggests that much “religious belief” should be studied as “religious profession”—what people think they should believe and what they know they ought to say.\n\nIt is all too easy to meet every counterargument by saying, “Well, of course I could be wrong.” Then, having dutifully genuflected in the direction of Modesty, having made the required obeisance, you can go on about your way without changing a thing.\n\nThe temptation is always to claim the most points with the least effort. The temptation is to carefully integrate all incoming news in a way that lets us change our beliefs, and above all our _actions_, as little as possible. John Kenneth Galbraith said: “Faced with the choice of changing one’s mind and proving that there is no need to do so, almost everyone gets busy on the proof.”^[2](#fn2x2)^ And the greater the _inconvenience_ of changing one’s mind, the more effort people will expend on the proof.\n\nBut y’know, if you’re gonna _do_ the same thing anyway, there’s no point in going to such incredible lengths to rationalize it. Often I have witnessed people encountering new information, apparently accepting it, and then carefully explaining why they are going to do exactly the same thing they planned to do previously, but with a different justification. The point of thinking is to _shape_ our plans; if you’re going to keep the same plans anyway, why bother going to all that work to justify it? When you encounter new information, the hard part is to _update_, to _react_, rather than just letting the information disappear down a black hole. And humility, properly misunderstood, makes a wonderful black hole—all you have to do is admit you could be wrong. Therefore it is written: “To be humble is to take specific actions in anticipation of your own errors. To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty.”\n\n^[1](#fn1x2-bk)^Charles S. Taber and Milton Lodge, “Motivated Skepticism in the Evaluation of Political Beliefs,” _American Journal of Political Science_ 50, no. 3 (2006): 755–769.\n\n^[2](#fn2x2-bk)^John Kenneth Galbraith, _Economics, Peace and Laughter_ (Plume, 1981), 50."
          },
          "voteCount": 130
        },
        {
          "name": "\"What Is Wrong With Our Thoughts\"",
          "type": "post",
          "slug": "what-is-wrong-with-our-thoughts",
          "_id": "EdyDGRLNFScEt5uDz",
          "url": null,
          "title": "\"What Is Wrong With Our Thoughts\"",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> \"But let us never forget, either, as all conventional history of philosophy conspires to make us forget, what the 'great thinkers' really are: proper objects, indeed, of pity, but even more, of horror.\"\n\nDavid Stove's \"[What Is Wrong With Our Thoughts](http://web.maths.unsw.edu.au/~jim/wrongthoughts.html)\" is a critique of philosophy that I can only call epic.\n\nThe astute reader will of course find themselves objecting to Stove's notion that we _should_ be catologuing every possible way to do philosophy _wrong_.  It's not like there's some originally pure mode of thought, being tainted by only a small library of poisons.  It's just that there are exponentially more possible crazy thoughts than sane thoughts, c.f. entropy.\n\nBut Stove's list of 39 different classic crazinesses applied to _the number three_ is absolute pure epic gold.  (Scroll down about halfway through if you want to jump there directly.)\n\nI especially like #8:  \"There is an integer between two and four, but it is not three, and its true name and nature are not to be revealed.\""
          },
          "voteCount": 37
        },
        {
          "name": "Doing your good deed for the day",
          "type": "post",
          "slug": "doing-your-good-deed-for-the-day",
          "_id": "r8stxYL29NF9w53am",
          "url": null,
          "title": "Doing your good deed for the day",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Academic Papers"
            },
            {
              "name": "Rationalization"
            },
            {
              "name": "Heuristics & Biases"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[Interesting](http://blog.newsweek.com/blogs/nurtureshock/archive/2009/10/12/is-good-behavior-a-license-to-misbehave.aspx) new [study](http://www.google.ie/url?sa=t&source=web&ct=res&cd=6&ved=0CBoQFjAF&url=http%3A%2F%2Fwww.rotman.utoronto.ca%2Fnewthinking%2Fgreenproducts.pdf&ei=WkHmSqbdPMOMjAfGmtyhBA&usg=AFQjCNEmQvSQvbVgEJOzxl4oZw7Z8zaq4w&sig2=iOMp6AtL_6QF0IzQsvf67g) out on moral behavior. The one sentence summary of the most interesting part is that people who did one good deed were less likely to do another good deed in the near future. They had, quite literally, done their good deed for the day.  \n  \nIn the first part of the study, they showed that people exposed to environmentally friendly, \"green\" products were more likely to behave nicely. Subjects were asked to rate products in an online store; unbeknownst to them, half were in a condition where the products were environmentally friendly, and the other half in a condition where the products were not. Then they played a Dictator Game. Subjects who had seen environmentally friendly products shared more of their money.  \n  \nIn the second part, instead of just rating the products, they were told to select $25 worth of products to buy from the store. One in twenty five subjects would actually receive the products they'd purchased. Then they, too, played the [Dictator Game](http://en.wikipedia.org/wiki/Dictator_Game). Subjects who had bought environmentally friendly products shared less of their money.  \n  \nIn the third part, subjects bought products as before. Then, they participated in a \"separate, completely unrelated\" experiment \"on perception\" in which they earned money by identifying dot patterns. The experiment was designed such that participants could lie about their perceptions to earn more. People who purchased the green products were more likely to do so.  \n  \nThis does not prove that environmentalists are actually bad people - remember that whether a subject purchased green products or normal products was completely randomized. It does suggest that people who have done one nice thing feel less of an obligation to do another.  \n  \nThis meshes nicely with a self-signalling conception of morality. If part of the point of behaving morally is to convince yourself that you're a good person, then once you're convinced, behaving morally loses a lot of its value.  \n  \nBy coincidence, a few days after reading this study, I found [this article](http://experimentaltheology.blogspot.com/2009/08/bait-and-switch-of-contemporary.html) by Dr. Beck, a theologian, complaining about the behavior of churchgoers on Sunday afternoon lunches. He says that in his circles, it's well known that people having lunch after church tend to abuse the waitstaff and tip poorly. And he blames the same mechanism identified by Mazar and Zhong in their Dictator Game. He says that, having proven to their own satisfaction that they are godly and holy people, doing something _else_ godly and holy like being nice to others would be overkill.  \n  \nIt sounds...strangely plausible.  \n  \nIf this is true, then anything that makes people feel moral without actually doing good is no longer a harmless distraction. All those biases that lead people to give time and money and thought to causes that don't really merit them waste not only time and money, but an exhaustible supply of moral fiber (compare to Baumeister's idea of [willpower as a limited resource](http://www.psychologytoday.com/files/attachments/584/baumeisteretal1998.pdf)).  \n  \nPeople here probably don't have to worry about church. But some of the other activities Dr. Beck mentions as morality sinkholes seem appropriate, with a few of the words changed:\n\n> Bible study  \n> Voting Republican  \n> Going on spiritual retreats  \n> Reading religious books  \n> Arguing with evolutionists  \n> Sending your child to a Christian school or providing education at home  \n> Using religious language  \n> Avoiding R-rated movies  \n> Not reading Harry Potter.\n\n  \nLet's not get too carried away with the evils of spiritual behavior - after all, data do show that religious people still give more to non-religious charities than the nonreligious do. But the points in and of themselves are valid. I've seen [Michael Keenan](http://michaelkeenan.blogspot.com/2009/02/dont-vote.html) and Patri Friedman say exactly the same thing regarding voting, and I would add to the less religion-o-centric list:\n\n> Joining \"[1000000 STRONG AGAINST WORLD HUNGER](http://www.google.ie/url?sa=t&source=web&ct=res&cd=4&ved=0CBkQFjAD&url=http%3A%2F%2Fgl-es.facebook.com%2Fgroup.php%3Fgid%3D6812867316&ei=MELmSqrvEqK5jAeap9ShBA&usg=AFQjCNHZx3sQ9MBnDQWe0_MDY7ckffPrgQ&sig2=X6txcBNPuCaU0AjIPWskew)\" type Facebook groups  \n> Reading a book about the struggles faced by poor people, and telling people how emotional it made you  \n> \"Raising awareness of problems\" without raising awareness of any practical solution  \n> Taking (or teaching) college courses about the struggles of the less fortunate  \n> Many forms of political, religious, and philosophical arguments\n\nMy preferred solution to this problem is to consciously try not to count anything I do as charitable or morally relevant except actually donating money to organizations. It is a bit extreme, but, like Eliezer's utilitarian foundation for deontological ethics, sometimes to escape the problems inherent in [running on corrupted hardware](http://www.google.ie/url?sa=t&source=web&ct=res&cd=2&ved=0CBcQFjAB&url=http%3A%2F%2Flesswrong.com%2Flw%2Fuv%2Fends_dont_justify_means_among_humans%2F&ei=T0LmSsqnB6PbjQeBusShBA&usg=AFQjCNHMc-TZlS8SyK_xwyYRuI6GsaPwaw&sig2=HMuT4NtYXYWoRuqn0AbXfw) you have to jettison all the bathwater, even knowing it contains a certain number of babies. A lot probably slips by subconsciously, but I find it better than nothing (at least, I did when I was actually making money; it hasn't worked since I went back to school. Your mileage may vary.  \n  \nIt may be tempting to go from here to a society where we talk much less about morality, especially little bits of morality that have no importance on their own. That might have unintended consequences. Remember that the participants in the study who saw lots of environmentally friendly products but couldn't buy any ended up nicer. The urge to be moral seems to build up by anything priming us with thoughts of morality.  \n  \nBut to prevent that urge from being discharged, we need to plug up the moral sinkholes Dr. Beck mentions, and any other moral sinkholes we can find. We need to give people less moral recognition and acclaim for performing only slightly moral acts. Only then can we concentrate our limited moral fiber on truly improving the world.  \n  \nAnd by, \"we\", I mean \"you\". I've done my part just by writing this essay."
          },
          "voteCount": 141
        },
        {
          "name": "Beware Trivial Inconveniences",
          "type": "post",
          "slug": "beware-trivial-inconveniences",
          "_id": "reitXJgJXFzKpdKyd",
          "url": null,
          "title": "Beware Trivial Inconveniences",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Practical"
            },
            {
              "name": "Censorship"
            },
            {
              "name": "Dark Arts"
            },
            {
              "name": "Trivial Inconvenience"
            },
            {
              "name": "Shaping Your Environment"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[The Great Firewall of China](http://en.wikipedia.org/wiki/Great_Firewall_of_China). A massive system of centralized censorship purging the Chinese version of the Internet of all potentially subversive content. Generally agreed to be a great technical achievement and political success even by the vast majority of people who find it morally abhorrent.  \n  \nI spent a few days in China. I got around it at the Internet cafe by using a free online proxy. Actual Chinese people have dozens of ways of getting around it with a minimum of technical knowledge or just the ability to read some instructions.  \n  \nThe Chinese government isn't losing any sleep over this (although they also don't lose any sleep over murdering political dissidents, so maybe they're just very sound sleepers). Their theory is that by making it a little inconvenient and time-consuming to view subversive sites, they will discourage casual exploration. No one will bother to circumvent it unless they already seriously distrust the Chinese government and are specifically looking for foreign websites, and these people probably know what the foreign websites are going to say anyway.  \n  \nThink about this for a second. The human longing for freedom of information is a terrible and wonderful thing. It delineates a pivotal difference between mental emancipation and slavery. It has launched protests, rebellions, and revolutions. Thousands have devoted their lives to it, thousands of others have even died for it. And it can be stopped dead in its tracks by requiring people to search for \"how to set up proxy\" before viewing their anti-government website.\n\nI was reminded of this recently by Eliezer's [Less Wrong Progress Report](http://www.overcomingbias.com/2009/04/less-wrong-progress-report.html). He mentioned how surprised he was that so many people were posting so much stuff on Less Wrong, when very few people had ever taken advantage of Overcoming Bias' policy of accepting contributions if you emailed them to a moderator and the moderator approved. Apparently all us folk brimming with ideas for posts didn't want to deal with the aggravation.  \n  \nOkay, in my case at least it was a bit more than that. There's a sense of going out on a limb and drawing attention to yourself, of arrogantly claiming some sort of equivalence to Robin Hanson and Eliezer Yudkowsky. But it's still interesting that this potential embarrassment and awkwardness was enough to keep the several dozen people who have blogged on here so far from sending that \"I have something I'd like to post...\" email.  \n  \nCompanies frequently offer \"free rebates\". For example, an $800 television with a $200 rebate. There are a few reasons companies like rebates, but one is that you'll be attracted to the television because it appears to have a net cost only $600, but then filling out the paperwork to get the rebate is too inconvenient and you won't get around to it. This is basically a free $200 for filling out an annoying form, but companies can predict that customers will continually fail to complete it. This might make some sense if you're a high-powered lawyer or someone else whose time is extremely valuable, but most of us have absolutely no excuse.  \n  \nOne last example: It's become a truism that people spend more when they use credit cards than when they use money. This particular truism happens to be true: in a study by Prelec and Simester^1^, auction participants bid twice as much for the same prize when using credit than when using cash. The trivial step of getting the money and handing it over has a major inhibitory effect on your spending habits.  \n  \nI don't know of any unifying psychological theory that explains our problem with trivial inconveniences. It seems to have something to do with loss aversion, and with the brain's general use of emotion-based hacks instead of serious cost-benefit analysis. It might be linked to akrasia; for example, you might not have enough willpower to go ahead with the unpleasant action of filling in a rebate form, and your brain may assign it low priority because it's hard to imagine the connection between the action and the reward.  \n  \nBut these trivial inconveniences have major policy implications. Countries like China that want to oppress their citizens are already using \"soft\" oppression to make it annoyingly difficult to access subversive information. But there are also benefits for governments that want to help their citizens.  \n  \n\"Soft paternalism\" means a lot of things to a lot of different people. But one of the most interesting versions is the idea of \"opt-out\" government policies. For example, it would be nice if everyone put money into a pension scheme. Left to their own devices, many ignorant or lazy people might never get around to starting a pension, and in order to prevent these people's financial ruin, there is strong a moral argument for a government-mandated pension scheme. But there's also a strong libertarian argument against that idea; if someone for reasons of their own doesn't want a pension, or wants a different kind of pension, their status as a free citizen should give them that right.  \n  \nThe \"soft paternalist\" solution is to have a government-mandated pension scheme, but allow individuals to opt-out of it after signing the appropriate amount of paperwork. Most people, the theory goes, would remain in the pension scheme, because they understand they're better off with a pension and it was only laziness that prevented them from getting one before. And anyone who actually goes through the trouble of opting out of the government scheme would either be the sort of intelligent person who has a good reason not to want a pension, or else deserve what they get^2^.  \n  \nThis also reminds me of Robin's IQ-gated, test-requiring [would-have-been-banned store](http://www.overcomingbias.com/2007/03/paternalism_is_.html), which would discourage people from certain drugs without making it impossible for the true believers to get their hands on them. I suggest such a store be located way on the outskirts of town accessible only by a potholed road with a single traffic light that changes once per presidential administration, have a surly clerk who speaks heavily accented English, and be open between the hours of two and four on weekdays.\n\n**Footnotes**\n\n**1:** See Jonah Lehrer's book _How We Decide._ In fact, do this anyway. It's very good.\n\n**2:** Note also the clever use of the status quo bias here."
          },
          "voteCount": 159
        },
        {
          "name": "The Fable of the Dragon-Tyrant",
          "href": "https://nickbostrom.com/fable/dragon",
          "type": "post"
        },
        {
          "name": "The Bias You Didn't Expect",
          "type": "post",
          "slug": "the-bias-you-didn-t-expect",
          "_id": "L8dB6yoMEWofoeDNt",
          "url": null,
          "title": "The Bias You Didn't Expect",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Self-Deception"
            },
            {
              "name": "Politics"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "There are few places where society values rational, objective decision making as much as it values it in judges. While there is a rather cynical discipline called [legal realism](http://en.wikipedia.org/wiki/Legal_realism) that says the law is really based on quirks of individual psychology, \"what the judge had for breakfast,\" there's a broad social belief that the decision of judges are unbiased. And where they aren't unbiased, they're biased for Big, Important, Bad reasons, like [racism](http://www.freakonomics.com/2008/01/30/can-bail-bond-dealers-reduce-discrimination-a-guest-post/) or classism or politics.\n\nIt turns out that legal realism is totally wrong. It's not what the judge had for breakfast. It's _how recently_ the judge had breakfast. A [a new study](http://www.pnas.org/content/early/2011/03/29/1018033108) ([media coverage](http://www.miller-mccune.com/legal-affairs/judges-decisions-more-lenient-after-lunch-30179/)) on Israeli judges shows that, when making parole decisions, they grant about 65% after meal breaks, and almost all the way down to 0% right before breaks and at the end of the day (i.e. as far from the last break as possible). There's a relatively linear decline between the two points.\n\nThink about this for a moment. A tremendously important decision, determining whether a person will go free or spend years in jail, appears to be substantially determined by an arbitrary factor. Also, note that we don't know if it's the lack of food, the anticipation of a break, or some other factor that is responsible for this. More interestingly, we don't know where the optimal result occurred. It's probably not the near 0% at the end of each work period. But is it the post-break high of 65%? Or were judges being too nice? We know there was bias, but we still don't know when bias occurred.\n\nThere are at least two lessons from this. The little, obvious one is to be aware of one's own physical limitations. Avoid making big decisions when tired or hungry - though this doesn't mean you should try to make decisions right after eating. For particularly important decisions, consider contemplating them at different times, if you can. Think about one thing Monday morning, then Wednesday afternoon, then Saturday evening, going only to the point of getting an overall feel for an answer, and not to the point of really making a solid conclusion. Take notes, and then compare them. This may not work perfectly, but it may help you realize inconsistencies, which could help. For big questions, the wisdom of crowds may be helpful - unless it's been a while since most of the crowd had breakfast.\n\nThe bigger lesson is one of humility. This provides rather stark evidence that our decisions are not under our control to the extent we believe. We can be influenced by factors we don't even suspect. Even knowing we have been biased, we may still be unable to identify what the correct answer was. While using formal rules and logic may be one of the best approaches to minimizing such errors, even formal rules can fail when applied by biased agents. The biggest, most condemnable biases - like racism - are in some ways less dangerous, because we know we need to look out for them. It's the bias you don't even suspect that can get you. The authors of the study think they basically got lucky with these results - if the effect had been to make decisions arbitrary rather than to increase rejections, this would not have shown up.\n\nWhen those charged with making impartial decisions that control people's lives are subject to arbitrary forces they never suspected, it shows how important it is and much more we can do to be less wrong."
          },
          "voteCount": 101
        },
        {
          "name": "Hold Off On Proposing Solutions",
          "type": "post",
          "slug": "hold-off-on-proposing-solutions",
          "_id": "uHYYA32CKgKT3FagE",
          "url": null,
          "title": "Hold Off On Proposing Solutions",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Practical"
            },
            {
              "name": "Problem-solving (skills and techniques)"
            },
            {
              "name": "Problem Formulation & Conceptualization"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "From Robyn Dawes’s _Rational Choice in an Uncertain World_.^[1](#fn1x47)^ Bolding added.\n\n> Norman R. F. Maier noted that when a group faces a problem, the natural tendency of its members is to propose possible solutions as they begin to discuss the problem. Consequently, the group interaction focuses on the merits and problems of the proposed solutions, people become emotionally attached to the ones they have suggested, and superior solutions are not suggested. Maier enacted an edict to enhance group problem solving: **“Do not propose solutions until the problem has been discussed as thoroughly as possible without suggesting any.”** It is easy to show that this edict works in contexts where there are objectively defined good solutions to problems.\n> \n> Maier devised the following “role playing” experiment to demonstrate his point. Three employees of differing ability work on an assembly line. They rotate among three jobs that require different levels of ability, because the most able—who is also the most dominant—is strongly motivated to avoid boredom. In contrast, the least able worker, aware that he does not perform the more difficult jobs as well as the other two, has agreed to rotation because of the dominance of his able co-worker. An “efficiency expert” notes that if the most able employee were given the most difficult task and the least able the least difficult, productivity could be improved by 20%, and the expert recommends that the employees stop rotating. The three employees and . . . a fourth person designated to play the role of foreman are asked to discuss the expert’s recommendation. Some role-playing groups are given Maier’s edict not to discuss solutions until having discussed the problem thoroughly, while others are not. Those who are not given the edict immediately begin to argue about the importance of productivity versus worker autonomy and the avoidance of boredom. Groups presented with the edict have a much higher probability of arriving at the solution that the two more able workers rotate, while the least able one sticks to the least demanding job—a solution that yields a 19% increase in productivity.\n> \n> I have often used this edict with groups I have led—**particularly when they face a very tough problem, which is when group members are most apt to propose solutions immediately**. While I have no objective criterion on which to judge the quality of the problem solving of the groups, Maier’s edict appears to foster better solutions to problems.\n\nThis is so true it’s not even funny. And it gets worse and worse the tougher the problem becomes. Take artificial intelligence, for example. A surprising number of people I meet seem to know exactly how to build an artificial general intelligence, without, say, knowing how to build an optical character recognizer or a collaborative filtering system (much easier problems). And as for building an AI with a positive impact on the world—a Friendly AI, loosely speaking—why, _that_ problem is so incredibly difficult that an actual _majority_ resolve the whole issue within fifteen seconds.^[2](#fn2x47)^ _Give_ me a _break._\n\nThis problem is by no means unique to AI. Physicists encounter plenty of nonphysicists with their own theories of physics, economists get to hear lots of amazing new theories of economics. If you’re an evolutionary biologist, anyone you meet can instantly solve any open problem in your field, usually by postulating group selection. Et cetera.\n\nMaier’s advice echoes the principle of the bottom line, that the effectiveness of our decisions is determined only by whatever evidence and processing we did in first arriving at our decisions—after you write the bottom line, it is too late to write more reasons above. If you make your decision very early on, it will, in fact, be based on very little thought, no matter how many amazing arguments you come up with afterward.\n\nAnd consider furthermore that we change our minds less often than we think: 24 people assigned an average 66% probability to the future choice thought more probable, but only 1 in 24 actually chose the option thought less probable. **Once you can guess what your answer will be, you have probably already decided.** If you can guess your answer half a second after hearing the question, then you have half a second in which to be intelligent. It’s not a lot of time.\n\nTraditional Rationality emphasizes _falsification_—the ability to _relinquish_ an initial opinion when confronted by clear evidence against it. But once an idea gets into your head, it will probably require way too much evidence to get it out again. Worse, we don’t always have the luxury of overwhelming evidence.\n\nI suspect that a more powerful (and more difficult) method is to _hold off on thinking of an answer_. To suspend, draw out, that tiny moment when we can’t yet guess what our answer will be; thus giving our intelligence a longer time in which to act.\n\nEven half a minute would be an improvement over half a second.\n\n^[1](#fn1x47-bk)^Robyn M. Dawes, _Rational Choice in An Uncertain World_, 1st ed., ed. Jerome Kagan (San Diego, CA: Harcourt Brace Jovanovich, 1988), 55–56.\n\n^[2](#fn2x47-bk)^See Yudkowsky, [“Artificial Intelligence as a Positive and Negative Factor in Global Risk](../Text/main-printch17.html#cite.0.Yudkowsky.2008).”"
          },
          "voteCount": 73
        },
        {
          "name": "Hindsight bias",
          "type": "post",
          "slug": "hindsight-bias",
          "_id": "fkM9XsNvXdYH6PPAx",
          "url": null,
          "title": "Hindsight bias",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Hindsight Bias"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_Hindsight bias_ is when people who know the answer vastly overestimate its _predictability_ or _obviousness,_ compared to the estimates of subjects who must guess without advance knowledge.  Hindsight bias is sometimes called the _I-knew-it-all-along effect_.\n\nFischhoff and Beyth (1975) presented students with historical accounts of unfamiliar incidents, such as a conflict between the Gurkhas and the British in 1814.  Given the account as background knowledge, five groups of students were asked what they would have predicted as the probability for each of four outcomes: British victory, Gurkha victory, stalemate with a peace settlement, or stalemate with no peace settlement.  Four experimental groups were respectively told that these four outcomes were the historical outcome.  The fifth, control group was not told any historical outcome.  In every case, a group told an outcome assigned substantially higher probability to that outcome, than did any other group or the control group.\n\nHindsight bias matters in legal cases, where a judge or jury must determine whether a defendant was legally negligent in failing to foresee a hazard (Sanchiro 2003). In an experiment based on an actual legal case, Kamin and Rachlinski (1995) asked two groups to estimate the probability of flood damage caused by blockage of a city-owned drawbridge. The control group was told only the background information known to the city when it decided not to hire a bridge watcher. The experimental group was given this information, plus the fact that a flood had actually occurred. Instructions stated the city was negligent if the foreseeable probability of flooding was greater than 10%. 76% of the control group concluded the flood was so unlikely that no precautions were necessary; 57% of the experimental group concluded the flood was so likely that failure to take precautions was legally negligent. A third experimental group was told the outcome andalso explicitly instructed to avoid hindsight bias, which made no difference: 56% concluded the city was legally negligent.\n\nViewing history through the lens of hindsight, we vastly underestimate the cost of effective safety precautions.  In 1986, the _Challenger_ exploded for reasons traced to an O-ring losing flexibility at low temperature.  There were warning signs of a problem with the O-rings.  But preventing the _Challenger_ disaster would have required, not attending to the problem with the O-rings, but attending to _every_ warning sign which seemed as severe as the O-ring problem, _without benefit of hindsight_.  It could have been done, but it would have required a _general policy_ much more expensive than just fixing the O-Rings.\n\nShortly after September 11th 2001, I thought to myself, _and now someone will turn up minor intelligence warnings of something-or-other, and then the hindsight will begin._  Yes, I'm sure they had some minor warnings of an al Qaeda plot, but they probably also had minor warnings of mafia activity, nuclear material for sale, and an invasion from Mars.\n\nBecause we don't see the cost of a general policy, we learn overly specific lessons.  After September 11th, the FAA prohibited box-cutters on airplanes—as if the problem had been the failure to take _this particular_ \"obvious\" precaution.  We don't learn the general lesson: _the cost of effective caution is very high because you must attend to problems that are not as obvious now as past problems seem in hindsight._\n\nThe test of a model is how much probability it assigns to the observed outcome.  Hindsight bias systematically distorts this test; we think our model assigned much more probability than it actually did.  Instructing the jury doesn't help.  You have to [write down your predictions in advance](http://www.overcomingbias.com/2007/08/conservation-of.html).  Or as Fischhoff (1982) put it:\n\n> When we attempt to understand past events, we implicitly test the hypotheses or rules we use both to interpret and to anticipate the world around us. If, in hindsight, we systematically underestimate the surprises that the past held and holds for us, we are subjecting those hypotheses to inordinately weak tests and, presumably, finding little reason to change them.\n\nPart of the sequence [_Mysterious Answers to Mysterious Questions_](http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions)\n\nNext post: \"[Hindsight Devalues Science](/lw/im/hindsight_devalues_science/)\"\n\nPrevious post: \"[Conservation of Expected Evidence](/lw/ii/conservation_of_expected_evidence/)\"\n\n* * *\n\nFischhoff, B. 1982. For those condemned to study the past: Heuristics and biases in hindsight. In Kahneman et. al. 1982: 332–351.\n\nFischhoff, B., and Beyth, R. 1975. I knew it would happen: Remembered probabilities of once-future things. Organizational Behavior and Human Performance, 13: 1-16.\n\nKamin, K. and Rachlinski, J. 1995. [Ex Post ≠ Ex Ante: Determining Liability in Hindsight](http://www.jstor.org/view/01477307/ap050075/05a00120/0). Law and Human Behavior, 19(1): 89-104.\n\nSanchiro, C. 2003. Finding Error. Mich. St. L. Rev. 1189."
          },
          "voteCount": 63
        },
        {
          "name": "The Trouble With \"Good\"",
          "type": "post",
          "slug": "the-trouble-with-good",
          "_id": "M2LWXsJxKS626QNEA",
          "url": null,
          "title": "The Trouble With \"Good\"",
          "author": "Scott Alexander",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Affect Heuristic"
            },
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Fuzzies"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [How An Algorithm Feels From Inside](https://www.lesswrong.com/posts/yA4gF5KrboK2m2Xu7/how-an-algorithm-feels-from-inside), [The Affect Heuristic](https://www.lesswrong.com/posts/Kow8xRzpfkoY7pa69/the-affect-heuristic), [The Power of Positivist Thinking](/lw/48/the_power_of_positivist_thinking/)\n\nI am a normative utilitarian and a descriptive emotivist: I believe utilitarianism is the correct way to resolve moral problems, but that the normal mental algorithms for resolving moral problems use emotivism.  \n  \nEmotivism, aka the yay/boo theory, is the belief that moral statements, however official they may sound, are merely personal opinions of preference or dislike. Thus, \"feeding the hungry is a moral duty\" corresponds to \"yay for feeding the hungry!\" and \"murdering kittens is wrong\" corresponds to \"boo for kitten murderers!\"  \n  \nEmotivism is a very nice theory of what people actually mean when they make moral statements. Billions of people around the world, even the non-religious, happily make moral statements every day without having any idea what they reduce to or feeling like they ought to reduce to anything.  \n  \nEmotivism also does a remarkably good job capturing the common meanings of the words \"good\" and \"bad\". An average person may have beliefs like \"pizza is good, but seafood is bad\", \"Israel is good, but Palestine is bad\", \"the book was good, but the movie was bad\", \"atheism is good, theism is bad\", \"evolution is good, creationism is bad\", and \"dogs are good, but cats are bad\". Some of these seem to be moral beliefs, others seem to be factual beliefs, and others seem to be personal preferences. But we are happy using the word \"good\" for all of them, and it doesn't feel like we're using the same word in several different ways, the way it does when we use \"right\" to mean both \"correct\" and \"opposite of left\". It feels like they're all just the same thing. The moral theory that captures that feeling is emotivism. Yay pizza, books, Israelis, atheists, dogs, and evolution! Boo seafood, Palestinians, movies, theists, creationism, and cats!\n\nRemember, evolution is a crazy tinker who recycles everything. So it would not be surprising to find that our morality is a quick hack on the same machinery that runs our decisions about which food to eat or which pet to adopt. To make an outrageous metaphor: our brains run a system rather like Less Wrong's karma. You're allergic to cats, so you down-vote \"cats\" a couple of points. You hear about a Palestinian committing a terrorist attack, so you down-vote \"Palestinians\" a few points. Richard Dawkins just said something especially witty, so you up-vote \"atheism\". High karma score means seek it, use it, acquire it, or endorse it. Low karma score means avoid it, ignore it, discard it, or condemn it.^1^  \n  \nRemember back during the presidential election, when a McCain supporter claimed that an Obama supporter attacked her and carved a \"B\" on her face with a knife? This was HUGE news. All of my Republican friends started emailing me  and saying \"Hey, did you hear about this, this proves we've been right all along!\" And all my Democratic friends were grumbling and saying how it was probably made up and how we should all just forget the whole thing.  \n  \nAnd then it turned out it WAS all made up, and the McCain supporter had faked the whole affair. And now all of my Democrat friends started emailing me and saying \"Hey, did you hear about this, it shows what those Republicans and McCain supporters are REALLY like!\" and so on, and the Republicans were trying to bury it as quickly as possible.  \n  \nThe overwhelmingly interesting thing I noticed here was that everyone seemed to accept - not explicitly, but implicitly very much - that an Obama supporter acting violently was in some sense evidence against Obama or justification for opposition to Obama; or, that a McCain supporter acting dishonestly was in some sense evidence against McCain or confirmation that Obama supporters were better people. To a Bayesian, this would be balderdash. But to an emotivist, where any bad feelings associated with Obama count against him, it sort of makes sense. All those people emailing me about this were saying: Look, here is something negative associated with Obama; downvote him!^2^  \n  \nSo this is one problem: the inputs to our mental karma system aren't always closely related to the real merit of a person/thing/idea.  \n  \nAnother problem: our interpretation of whether to upvote or downvote something depends on how many upvotes or downvotes it already has. Here on Less Wrong we call this an [information cascade](/lw/z/information_cascades/). In the mind, we call it an [Affective Death Spiral](https://www.lesswrong.com/posts/XrzQW69HpidzvBxGr/affective-death-spirals).  \n  \nAnother problem: we are tempted to assign everything about a concept the same score. Eliezer Yudkowsky currently has 2486 karma. How good is Eliezer at philosophy? Apparently somewhere around the level it would take to get 2486 karma. How much does he know about economics? Somewhere around level 2486 would be my guess. How well does he write? Probably well enough to get 2486 karma. Translated into mental terms, this looks like [the Halo Effect](https://www.lesswrong.com/posts/ACGeaAk6KButv2xwQ/the-halo-effect). Yes, we can pick apart our analyses in greater detail; having read Eliezer's posts, I know he's better at some things than others. But that 2486 number is going to cause anchoring-and-adjustment issues even so.  \n  \nBut the big problem, the world-breaking problem, is that sticking everything good and bad about something [into one big bin and making decisions](https://www.lesswrong.com/posts/Kow8xRzpfkoY7pa69/the-affect-heuristic) based on whether it's a net positive or a net negative is an unsubtle, leaky heuristic completely unsuitable for complicated problems.  \n  \nTake gun control. Are guns good or bad? My gut-level emotivist response is: bad. They're loud and scary and dangerous and they shoot people and often kill them. It is very tempting to say: guns are bad, therefore we should have fewer of them, therefore gun control. I'm not saying gun control is therefore wrong: reversed stupidity is not intelligence. I'm just saying that before you can rationally consider whether or not gun control is wrong, you need to get past this mode of thinking about the problem.  \n  \nIn the hopes of using theism less often, a bunch of Less Wrongers have agreed that the War on Drugs would make a good stock example of irrationality. So, why is the War on Drugs so popular? I think it's because drugs are obviously BAD. They addict people, break up their families, destroy their health, drive them into poverty, and eventually kill them. If we've got to have a category \"drugs\"^3^, and we've got to call it either \"good\" or \"bad\", then \"bad\" is clearly the way to go. And if drugs are bad, getting rid of them would be good! Right?  \n  \nSo how do we avoid all of these problems?  \n  \nI said at the very beginning that I think we should switch to solving moral problems through utilitarianism. But we can't do that directly. If we ask utilitarianism \"Are drugs good or bad?\" it returns: CATEGORY ERROR. Good for it.  \n  \nUtilitarianism can only be applied to states, actions, or decisions, and it can only return a comparative result. Want to know whether stopping or diverting the trolley in the Trolley Problem would be better? Utilitarianism can tell you. That's because it's a decision between two alternatives (alternate way of looking at it: two possible actions; or two possible states) and all you need to do is figure out which of the two is higher utility.  \n  \nWhen people say \"Utilitarianism says slavery is bad\" or \"Utilitarianism says murder is wrong\" - well, a utilitarian would endorse those statements over their opposites, but it takes a lot of interpretation first. What utilitarianism properly says is \"In this particular situation, the action of freeing the slaves leads to a higher utility state than not doing so\" and possibly \"and the same would be true of any broadly similar situation\".  \n  \nBut why in blue blazes can't we just go ahead and say \"slavery is bad\"? What could possibly go wrong?  \n  \nAsk an anarchist. Taxation of X% means you're forced to work for X% of the year without getting paid. Therefore, since slavery is \"being forced to work without pay\" taxation is slavery. Since slavery is bad, taxation is bad. Therefore government is bad and statists are no better than slavemasters.^4^  \n  \n(again, reversed stupidity is not intelligence. There are good arguments against taxation. But this is not one of them.)  \n  \nEmotivism is the native architecture of the human mind. No one can think like a utilitarian all the time. But when you are in an Irresolvable Debate, utilitarian thinking may become necessary to avoid dangling variable problems around the word \"good\" ([cf. Islam is a religion of peace](/lw/48/the_power_of_positivist_thinking/)). Problems that are insoluble at the emotivist level can be reduced, simplified, and resolved on the utilitarian level with enough effort.  \n  \nI've used the example before, and I'll use it again. Israel versus Palestine. One person can go on and on for months about all the reasons the Israelis are totally right and the Palestinians are completely in the wrong, and another person can go on just as long about how the Israelis are evil oppressors and the Palestinians just want freedom. And then if you ask them about an action, or a decision, or a state - they've never thought about it. They'll both answer something like \"I dunno, the two-state solution or something?\". And if they still disagree at this level, you can suddenly apply the full power of utilitarianism to the problem in a way that tugs sideways to all of their personal prejudices.  \n  \nIn general, any debate about whether something is \"good\" or \"bad\" is sketchy, and can be changed to a more useful form by converting the thing to an action and applying utilitarianism.\n\n**Footnotes:**  \n  \n**1:** It should be noted that this karma analogy can't explain our original perception of good and bad, only the system we use for combining, processing and utilizing it. My guess is that the original judgment of good or bad takes place through association with other previously determined good or bad things, down to the bottom level which are programmed into the organism (ie pain, hunger, death) with some input from the rational centers.  \n  \n**2:** More evidence: we tend to like the idea of \"good\" or \"bad\" being innate qualities of objects. Thus the alternative medicine practioner who tells you that real medicine is bad, because it uses scary pungent chemicals, which are unhealthy, and alternative medicine is good, because it uses roots and plants and flowers, which everyone likes. Or fantasy books, where the Golden Sword of Holy Light can only be wielded for good, and the Dark Sword of Demonic Shadow can only be wielded for evil.  \n  \n**3:** Of course, the battle has already been half-lost once you have a category \"drugs\". Eliezer once mentioned something about how considering {Adolf Hitler, Joe Stalin, John Smith} a natural category isn't going to do John Smith any good, no matter how nice a man he may be. In the category \"drugs\", which looks like {cocaine, heroin, LSD, marijuana}, LSD and marijuana get to play the role of John Smith.\n\n**4:** And, uh, [I'm sure Louis XVI would feel the same way](/lw/9b/help_help_im_being_oppressed/62n). Sorry. I couldn't think of a better example."
          },
          "voteCount": 100
        },
        {
          "name": "How to Not Lose an Argument",
          "type": "post",
          "slug": "how-to-not-lose-an-argument",
          "_id": "6yTShbTdtATxKonY5",
          "url": null,
          "title": "How to Not Lose an Argument",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Disagreement"
            },
            {
              "name": "Conversation (topic)"
            },
            {
              "name": "Tribalism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [Leave a Line of Retreat](http://www.overcomingbias.com/2008/02/leave-retreat.html)\n\n**Followup to:** [Talking Snakes: A Cautionary Tale](/lw/2d/talking_snakes_a_cautionary_tale/), [The Skeptic's Trilemma](/lw/2p/the_skeptics_trilemma/)\n\n_\"I argue very well. Ask any of my remaining friends. I can win an argument on any topic, against any opponent. People know this, and steer clear of me at parties. Often, as a sign of their great respect, they don't even invite me.\"_\n\n        --Dave Barry\n\nThe science of winning arguments is called Rhetoric, and it is one of the Dark Arts. Its study is forbidden to rationalists, and its tomes and treatises are kept under lock and key in a particularly dark corner of the Miskatonic University library. More than this it is not lawful to speak.  \n  \nBut I do want to talk about a very closely related skill: not losing arguments.  \n  \nRationalists probably find themselves in more arguments than the average person. And if we're doing it right, the truth is hopefully on our side and the argument is ours to lose. And far too often, we _do_ lose arguments, even when we're right. Sometimes it's because of biases or inferential distances or other things that can't be helped. But all too often it's because we're shooting ourselves in the foot.  \n  \nHow does one avoid shooting one's self in the foot? In rationalist language, the technique is called Leaving a Social Line of Retreat. In normal language, it's called being nice.  \n  \nFirst, what does it mean to win or lose an argument? There is an unspoken belief in some quarters that the point of an argument is to gain social status by utterly demolishing your opponent's position, thus proving yourself the better thinker. That can be fun sometimes, and if it's really all you want, go for it.  \n  \nBut the most important reason to argue with someone is to change his mind. If you want a world without fundamentalist religion, you're never going to get there just by making cutting and incisive critiques of fundamentalism that all your friends agree sound really smart. You've got to deconvert some actual fundamentalists. In the absence of changing someone's mind, you can at least get them to see your point of view. Getting fundamentalists to understand the [real reasons](http://www.overcomingbias.com/2008/10/traditional-cap.html) people find atheism attractive is a nice consolation prize.  \n  \nI make the anecdotal observation that a lot of smart people are very good at winning arguments in the first sense, and very bad at winning arguments in the second sense. Does that correspond to your experience?  \n  \nBack in 2008, Eliezer described how to [Leave a Line of Retreat](http://www.overcomingbias.com/2008/02/leave-retreat.html). If you believe morality is impossible without God, you have a strong disincentive to become an atheist. Even after you've realized which way the evidence points, you'll activate every possible defense mechanism for your religious beliefs. If all the defense mechanisms fail, you'll take God on utter faith or just [believe in belief](/lw/r/no_really_ive_deceived_myself/), rather than surrender to the unbearable position of an immoral universe.  \n  \nThe correct procedure for dealing with such a person, Eliezer suggests, isn't to show them yet another reason why God doesn't exist. They'll just reject it along with all the others. The correct procedure is to convince them, on a gut level, that morality is possible even in a godless universe. When disbelief in God is no longer so terrifying, people won't fight it quite so hard and may even deconvert themselves.  \n  \nBut there's another line of retreat to worry about, one I experienced firsthand in a very strange way. I had a dream once where God came down to Earth; I can't remember exactly why. In the borderlands between waking and sleep, I remember thinking: _I feel like a total moron_. Here I am, someone who goes to atheist groups and posts on atheist blogs and has told all his friends they should be atheists and so on, and now it turns out God exists. All of my religious friends whom I won all those arguments against are going to be secretly looking at me, trying as hard as they can to be nice and understanding, but secretly laughing about how I got my comeuppance. I can never show my face in public again. Wouldn't you feel the same?  \n  \nAnd then I woke up, and shook it off. I am an aspiring rationalist: if God existed, I would desire to believe that God existed. But I realized at that point the importance of the social line of retreat. The psychological resistance I felt to admitting God's existence, even after having seen Him descend to Earth, was immense. And, I realized, it was exactly the amount of resistance that every vocally religious person must experience towards God's _non_-existence.  \n  \nThere's not much we can do about this sort of high-grade long-term resistance. Either a person has enough of the [rationalist virtues](http://yudkowsky.net/rational/virtues) to overcome it, or he doesn't. But there is a less ingrained, more immediate form of social resistance generated with every heated discussion.  \n  \nLet's say you approach a theist (let's call him Theo) and say \"How can you, a grown man, still believe in something stupid like [talking snakes](/lw/2d/talking_snakes_a_cautionary_tale/) and magic sky kings? Don't you know you people are responsible for the Crusades and the Thirty Years' War and the Spanish Inquisition? You should be ashamed of yourself!\"  \n  \nThis suggests the following dichotomy in Theo's mind: **EITHER** God exists, **OR** I am an idiot who believes in stupid childish  things and am in some way partly responsible for millions of deaths and I should have lower status and this arrogant person who's just accosted me and whom I already hate should have higher status at my expense.  \n  \nUnless Theo has attained a level of rationality far beyond any of us, guess which side of that dichotomy he's going to choose? In fact, guess which side of that dichotomy he's now going to support with renewed vigor, even if he was only a lukewarm theist before? His social line of retreat has been completely closed off, and it's _your_ fault.  \n  \nHere the two definitions of \"winning an argument\" I suggested before come into conflict. If your goal is to absolutely demolish the other person's position, to make him feel awful and worthless - then you are also very unlikely to change his mind or win his understanding. And because our culture of debates and mock trials and real trials and flaming people on Usenet encourages the first type of \"winning an argument\", there's precious little genuine mind-changing going on.  \n  \nReally adjusting to the second type of argument, where you try to convince people, takes a lot more than just not insulting people outright^1^. You've got to completely rethink your entire strategy. For example, anyone used to the Standard Debates may already have a cached pattern of how they work. Activate the whole Standard Debate concept, and you activate a whole bunch of related thoughts like Atheists As The Enemy, Defending The Faith, and even in some cases (I've seen it happen) persecution of Christians by atheists in Communist Russia. To such a person, ceding an inch of ground in a Standard Debate may well be equivalent to saying all the Christians martyred by the Communists died in vain, or something similarly dreadful.  \n  \nSo try to show you're not just starting Standard Debate #4457. I remember once, during the middle of a discussion with a Christian, when I admitted I really didn't like Christopher Hitchens. Richard Dawkins, brilliant. Daniel Dennett, brilliant. But Christopher Hitchens always struck me as too black-and-white and just plain irritating. This one little revelation completely changed the entire tone of the conversation. I was no longer Angry Nonbeliever #116. I was no longer the living incarnation of All Things Atheist. I was just a person who happened to have a whole bunch of atheist ideas, along with a couple of ideas that weren't typical of atheists. I got the same sort of response by admitting I loved religious music. All of a sudden my friend was falling over himself to mention some scientific theory he found especially elegant in order to reciprocate^2^. I didn't end up deconverting him on the spot, but think he left with a much better appreciation of my position.  \n  \nAll of these techniques fall dangerously close to the Dark Arts, so let me be clear: I'm not suggesting you misrepresent yourself just to win arguments. I don't think misrepresenting yourself would even work; evolutionary psychology tells us humans are notoriously bad liars. Don't fake an appreciation for the other person's point of view, actually _develop_ an appreciation for the other person's point of view. Realize that [your points probably seem as absurd to others as their points seem to you](/lw/2d/talking_snakes_a_cautionary_tale/). Understand that [many false beliefs don't come from simple lying or stupidity](/lw/2p/the_skeptics_trilemma/), but from complex mixtures of truth and falsehood filtered by complex cognitive biases. Don't stop believing that you are right and they are wrong, unless the evidence points that way. But leave it at them being wrong, not them being wrong and stupid and evil.  \n  \nI think most people intuitively understand this. But considering how many smart people I see shooting their own foot off when they're trying to convince someone^3^, some of them clearly need a reminder.\n\n**Footnotes**\n\n**1:** An excellent collection of the deeper and most subtle forms of this practice of this sort can be found in Dale Carnegie's How to Win Friends and Influence People, one of the only self-help books I've read that was truly useful and not a regurgitation of cliches and applause lights. Carnegie's thesis is basically that being nice is the most powerful of the Dark Arts, and that a master of the Art of Niceness can use it to take over the world. It works better than you'd think.\n\n**2:** The following technique is definitely one of the Dark Arts, but I mention it because it reveals a lot about the way we think: when engaged in a really heated, angry debate, one where the insults are flying, suddenly stop and admit the other person is one hundred percent right and you're sorry for not realizing it earlier. Do it properly, and the other person will be flabbergasted, and feel deeply guilty at all the names and bad feelings they piled on top of you. Not only will you ruin their whole day, but for the rest of time, this person will secretly feel indebted to you, and you will be able to play with their mind in all sorts of little ways.\n\n**3:** Libertarians, you have a particular problem with this. If I wanted to know why I'm a Stalin-worshipper who has betrayed the Founding Fathers for personal gain and is controlled by his base emotions and wants to dominate others by force to hide his own worthlessness et cetera, I'd ask Ann Coulter. You're better than that. Come on. And then you wonder why people never vote for you."
          },
          "voteCount": 143
        },
        {
          "name": "You're Entitled to Arguments, But Not (That Particular) Proof",
          "type": "post",
          "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof",
          "_id": "vqbieD9PHG8RRJddu",
          "url": null,
          "title": "You're Entitled to Arguments, But Not (That Particular) Proof",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Motivated Reasoning"
            },
            {
              "name": "Evolution"
            },
            {
              "name": "Religion"
            },
            {
              "name": "Climate Change"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Followup to**:  [Logical Rudeness](/lw/1p1/logical_rudeness/)\n\n> \"Modern man is so committed to empirical knowledge, that he sets the standard for evidence higher than either side in his disputes can attain, thus suffering his disputes to be settled by philosophical arguments as to which party must be crushed under the burden of proof.\"  \n>         \\-\\- [Alan Crowe](http://www.overcomingbias.com/2008/09/beware-high-sta.html#comment-396465)\n\nThere's a story - in accordance with [Poe's Law](http://rationalwiki.com/wiki/Poe%27s_Law), I have no idea whether it's a joke or it actually happened - about a creationist who was trying to claim a \"gap\" in the fossil record, two species without an intermediate fossil having been discovered.  When an intermediate species _was_ discovered, the creationist responded, \"Aha!  Now there are _two_ gaps.\"\n\nSince I'm not a professional evolutionary biologist, I couldn't begin to rattle off all the ways that we know evolution is true; true facts tend to [leave traces](/lw/uw/entangled_truths_contagious_lies/) of themselves behind, and [evolution](http://wiki.lesswrong.com/wiki/Evolution) is the hugest fact in all of biology.  My specialty is the cognitive sciences, so I can tell you [of my own knowledge](/lw/la/truly_part_of_you/) that the human brain looks just like we'd expect it to look if it had evolved, and not at all like you'd think it would look if it'd been intelligently designed.  And I'm not really going to say much more on that subject.  As I once said to someone who questioned whether humans were really related to apes:  \"That question might have made sense when Darwin first came up with the hypothesis, but this is the twenty-first century.  We can _read the genes._  Human beings and chimpanzees have _95% shared genetic material._  It's _over._\"\n\nWell, it's over, unless you're crazy like a human (ironically, more evidence that the human brain was fashioned by [a sloppy and alien god](/lw/kr/an_alien_god/)).  If you're crazy like a human, you will engage in [motivated cognition](http://wiki.lesswrong.com/wiki/Motivated_cognition); and instead of focusing on the unthinkably huge heaps of evidence in favor of evolution, the innumerable signs by which the fact of evolution has left its heavy footprints on all of reality, the uncounted observations that discriminate between the world we'd expect to see if intelligent design ruled and the world we'd expect to see if evolution were true...\n\n...instead you search your mind, and you pick out one form of proof that you think evolutionary biologists _can't_ provide; and you demand, you insist upon that one form of proof; and when it is not provided, you take that as a refutation.\n\nYou say, \"Have you ever _seen_ an ape species evolving into a human species?\"  You insist on videotapes - on that _particular_ proof.\n\nAnd that _particular proof_ is one we couldn't possibly be expected to have on hand; it's a form of evidence we couldn't possibly be expected to be able to provide, _even given that evolution is true._\n\nYet it follows illogically that if a video tape would provide definite proof, then, likewise, the absence of a videotape must constitute definite disproof.  Or perhaps just render all other arguments void and turn the issue into a mere matter of personal opinion, with no one's opinion being better than anyone else's.\n\nSo far as I can tell, the position of human-caused global warming (anthropogenic global warming aka AGW) has the ball.  I get the impression there's a lot of evidence piled up, a lot of people trying and failing to poke holes, and so I have no reason to play contrarian here.  It's now heavily politicized science, which means that I take the assertions with a grain of skepticism and worry - well, to be honest I _don't_ spend a whole lot of time worrying about it, because (a) there are worse global catastrophic risks and (b) lots of other people are worrying about AGW already, so there are much better places to invest the next marginal minute of worry.\n\nBut if I pretend for a moment to live in the mainstream mental universe in which there is [nothing scarier to worry about](http://www.nickbostrom.com/existential/risks.html) than global warming, and a 6 °C (11 °F) rise in global temperatures by 2100 seems like a top issue for the care and feeding of humanity's future...\n\nThen I must shake a disapproving finger at anyone who claims the state of evidence on AGW is indefinite.\n\nSure, if we waited until 2100 to see how much global temperatures increased and how high the seas rose, we would have definite proof.  We would have definite proof in 2100, however, and that sounds just a little bit way the hell too late.  If there are cost-effective things we can do to mitigate global warming - and by this I don't mean ethanol-from-corn or cap-and-trade, more along the lines of standardizing on a liquid fluoride thorium reactor design and building 10,000 of them - if there's something we can do about AGW, we need to do it _now,_ not in a hundred years.\n\nWhen the hypothesis at hand makes _time valuable_ \\- when the proposition at hand, conditional on its being true, means there are certain things we should be doing NOW - then you've got to do your best to figure things out with the evidence that we have.  Sure, if we had annual data on global temperatures and CO2 going back to 100 million years ago, we would know more than we do right now.  But we don't have that time-series data - not because global-warming advocates destroyed it, or because they were neglectful in gathering it, but because they couldn't possibly be expected to provide it in the first place.  And so we've got to look among the observations we _can_ perform, to find those that discriminate between \"the way the world could be expected to look if AGW is true / a big problem\", and \"the way the world would be expected to look if AGW is false / a small problem\".  If, for example, we discover large deposits of [frozen methane clathrates](http://en.wikipedia.org/wiki/Arctic_methane_release) that are released with rising temperatures, this at least _seems_ like \"the sort of observation\" we might be making if we live in the sort of world where AGW is a big problem.  It's not a necessary connection, it's not sufficient on its own, it's something we _could_ potentially also observe in a world where AGW is _not_ a big problem - but unlike the perfect data we can never obtain, it's something we can actually find out, and in fact _have_ found out.\n\nYes, we've never actually experimented to observe the results over 50 years of artificially adding a large amount of carbon dioxide to the atmosphere.  But we know from physics that it's a greenhouse gas.  It's not a [privileged hypothesis](http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis) we're pulling out of nowhere.  It's not like saying \"You can't _prove_ there's no invisible pink unicorn in my garage!\"  AGW is, _ceteris paribus,_ what we should expect to happen if the other things we believe are true.  We don't have any experimental results on what will happen 50 years from now, and so you can't grant the proposition the special, super-strong status of something that has been [scientifically confirmed](/lw/in/scientific_evidence_legal_evidence_rational/) by a replicable experiment.  But as I point out in \"[Scientific Evidence, Legal Evidence, Rational Evidence](/lw/in/scientific_evidence_legal_evidence_rational/)\", if science couldn't say anything about that which has not already been observed, we couldn't ever make scientific _predictions_ by which the theories could be _confirmed._  Extrapolating from the science we _do_ know, global warming _should_ be occurring; you would need specific experimental evidence to _contradict_ that.\n\nWe are, I think, dealing with that old problem of motivated cognition.  As Gilovich says:  \"Conclusions a person does not want to believe are held to a higher standard than conclusions a person wants to believe.  In the former case, the person asks if the evidence _compels_ one to accept the conclusion, whereas in the latter case, the person asks instead if the evidence _allows_ one to accept the conclusion.\"  People [map the domain of belief onto the social domain of authority](/lw/mn/absolute_authority/), with a qualitative difference between absolute and nonabsolute demands:  If a teacher tells you certain things, and you have to believe them, and you have to recite them back on the test.  But when a student makes a suggestion in class, you don't have to go along with it - you're free to agree or disagree (it seems) and no one will punish you.\n\nAnd so the implicit emotional theory is that if something is not _proven_ \\- better yet, proven using a _particular_ piece of evidence that isn't available and that you're pretty sure is never going to become available - then you are allowed to disbelieve; it's like something a student says, not like something a teacher says.\n\nYou demand particular proof P; and if proof P is not available, then you're _allowed to disbelieve._\n\nAnd this is flatly wrong as probability theory.\n\nIf the hypothesis at hand is H, and we have access to pieces of evidence E1, E2, and E3, but we do _not_ have access to proof X one way or the other, then the rational probability estimate is the result of the [Bayesian](http://wiki.lesswrong.com/wiki/Bayes%27s_Theorem) [update](http://wiki.lesswrong.com/wiki/Updating) P(H|E1,E2,E3).  You do not get to say, \"Well, we don't know whether X or ~X, so I'm going to throw E1, E2, and E3 out the window until you tell me about X.\"  I cannot begin to describe how much that is not the way the laws of probability theory work.  You do not get to [screen off](/lw/lx/argument_screens_off_authority/) E1, E2, and E3 based on your _ignorance_ of X!\n\nNor do you get to ignore the arguments that influence the prior probability of H - the standard science by which, _ceteris paribus_ and without anything unknown at work, carbon dioxide is a greenhouse gas and _ought_ to make the Earth hotter.\n\nNor can you hold up the nonobservation of your particular proof X as a triumphant refutation.  If we _had_ time cameras and could look into the past, then indeed, the fact that no one had ever \"seen with their own eyes\" primates evolving into humans would refute the hypothesis.  But, _given_ that time cameras don't exist, then _assuming evolution to be true_ we don't expect anyone to have witnessed humans evolving from apes with our own eyes, for the laws of natural selection _require_ that this have happened far in the distant past.  And so, once you have updated on the fact that time cameras don't exist - computed P(_Evolution_|_~Camera_) \\- and the fact that time cameras don't exist hardly seems to refute the theory of evolution - then you obtain no further evidence by observing _~Video,_ i.e., P(_Evolution_|_~Video_,_~Camera_) = P(_Evolution_|_~Camera_).  In slogan-form, \"The absence of unobtainable proof is not even weak evidence of absence.\"  See appendix for details.\n\n(And while we're on the subject, yes, the laws of probability theory are laws, rather than suggestions.  It is like something the teacher tells you, okay?  If you're going to ignore the Bayesian update you logically have to perform when you see a new piece of evidence, you might as well ignore outright mathematical proofs.  I see no reason why it's any less epistemically sinful to ignore probabilities than to ignore certainties.)\n\nThrowing E1, E2 and E3 out the window, and ignoring the prior probability of H, because you haven't seen unobtainable proof x; or holding up the nonobservation of X as a triumphant refutation, when you couldn't reasonably expect to see X even given that the underlying theory is true; all this is more than just a formal probability-theoretic mistake.  It is _[logically rude](/lw/1p1/logical_rudeness/)._\n\nAfter all - in the absence of your unobtainable particular proof, there may be plenty of other arguments by which you can hope to figure out whether you live in a world where the hypothesis of interest is true, or alternatively false.  _It takes work to provide you with those arguments._  It takes work to provide you with extrapolations of existing knowledge to prior probabilities, and items of evidence with which to update those prior probabilities, to form a prediction about the unseen.  _Someone who does the work to provide those arguments is doing the best they can by you; throwing the arguments out the window is not just irrational, but logically rude._\n\nAnd I emphasize this, because it seems to me that the underlying metaphor of demanding particular proof is to say as if, \"You are supposed to provide me with a video of apes evolving into humans, I am entitled to see it with my own eyes, and it is your responsibility to make that happen; and if you do not provide me with that particular proof, you are deficient in your duties of argument, and I have no obligation to believe you.\"  And this is, in the first place, bad math as probability theory.  And it is, in the second place, an attitude of trying to be _defensible_ rather than _accurate,_ the attitude of someone who wants to be allowed to retain the beliefs they have, and not the attitude of someone who is [honestly curious](/lw/jz/the_meditation_on_curiosity/) and trying to figure out which possible world they live in, by whatever signs _are_ available.  But if these considerations do not move you, then even in terms of the original and flawed metaphor, you are in the wrong: you are _entitled to arguments, but not that particular proof._\n\nIgnoring someone's hard work to _provide_ you with the arguments you need - the extrapolations from existing knowledge to make predictions about events not yet observed, the items of evidence that are suggestive even if not definite and that fit some possible worlds better than others - and instead demanding proof they can't possibly give you, proof they couldn't be expected to provide _even if they were right_ \\- _that_ is logically rude.  It is invalid as probability theory, foolish on the face of it, and logically rude.\n\nAnd of course if you go so far as to _act smug_ about the absence of an unobtainable proof, or chide the other for their credulity, then you have crossed the line into outright ordinary rudeness as well.\n\nIt is likewise a madness of decision theory to hold off pending positive proof until it's too late to do anything; the whole point of decision theory is to choose under conditions of uncertainty, and that is not how the expected value of information is likely to work out.  Or in terms of plain common sense:  There are signs and portents, smoke alarms and hot doorknobs, by which you can hope to determine whether your house is on fire _before_ your face melts off your skull; and to delay leaving the house until _after_ your face melts off, because only this is the positive and particular proof that you demand, is decision-theoretical insanity.  It doesn't matter if you cloak your demand for that unobtainable proof under the heading of scientific procedure, saying, \"These are the proofs you could not obtain even if you were right, which I know you will not be able to obtain until the time for action has long passed, which surely any scientist would demand before confirming your proposition as a [scientific truth](/lw/in/scientific_evidence_legal_evidence_rational/).\"  It's still nuts.\n\n* * *\n\n_Since this post has already gotten long, I've moved some details of probability theory, the subtext on cryonics, the sub-subtext on molecular nanotechnology, and the sub-sub-subtext on Artificial Intelligence, into:_\n\n**[Demands for Particular Proof:  Appendices](/lw/1rv/demands_for_particular_proof_appendices/)**."
          },
          "voteCount": 68
        },
        {
          "name": "What's a Bias?",
          "type": "post",
          "slug": "what-s-a-bias",
          "_id": "jnZbHi873v9vcpGpZ",
          "url": null,
          "title": "What's a Bias?",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Heuristics & Biases"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The availability heuristic is a cognitive shortcut humans use to reach conclusions; and where this shortcut reliably causes inaccurate conclusions, we can say that an availability bias is at work. Scope insensitivity is another example of a _cognitive bias_.\n\n“Cognitive biases” are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by _the shape of our own mental machinery_. For example, our mental processes might be evolutionarily adapted to specifically believe some things that arent true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.\n\nOur brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a “(cognitive) bias.” Not to be confused with the colloquial “that person is biased,” which just means “that person has a skewed or prejudiced attitude toward something.”\n\nIn cognitive science, “biases” are distinguished from errors that arise from _cognitive content_, such as learned false beliefs. These we call “mistakes” rather than “biases,” and they are much easier to correct, once we’ve noticed them for ourselves. (Though the source of the mistake, or the source of the source of the mistake, may ultimately be some bias.)\n\n“Biases” are also distinguished from errors stemming from damage to an individual human brain, or from absorbed cultural mores; biases arise from machinery that is humanly universal.\n\nPlato wasn’t “biased” because he was ignorant of General Relativity—he had no way to gather that information, his ignorance did not arise from the shape of his mental machinery. But if Plato believed that philosophers would make better kings because he himself was a philosopher—and this belief, in turn, arose because of a universal adaptive political instinct for self-promotion, and not because Plato’s daddy told him that everyone has a moral duty to promote their own profession to governorship, or because Plato sniffed too much glue as a kid—then that was a bias, whether Plato was ever warned of it or not.\n\nWhile I am not averse (as you can see) to discussing definitions, I don’t want to suggest that the project of better wielding our own minds rests on a particular choice of terminology. If the term “cognitive bias” turns out to be unhelpful, we should just drop it.\n\nWe don’t start out with a moral duty to “reduce bias,” simply because biases are bad and evil and Just Not Done. This is the sort of thinking someone might end up with if they acquired a deontological duty of “rationality” by social osmosis, which leads to people trying to execute techniques without appreciating the reason for them. (Which is bad and evil and Just Not Done, according to _Surely You’re Joking, Mr. Feynman_, which I read as a kid.) A bias is an obstacle to our goal of obtaining truth, and thus _in our way_.\n\nWe are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, we're curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them “biases” or not."
          },
          "voteCount": 134
        },
        {
          "name": "Applause Lights",
          "type": "post",
          "slug": "applause-lights",
          "_id": "dLbkrPu5STNCBLRjr",
          "url": null,
          "title": "Applause Lights",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Tribalism"
            },
            {
              "name": "Social & Cultural Dynamics"
            },
            {
              "name": "Reversal Test"
            },
            {
              "name": "Applause Light"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "At the Singularity Summit 2007, one of the speakers called for democratic, multinational development of artificial intelligence. So I stepped up to the microphone and asked:\n\n> Suppose that a group of democratic republics form a consortium to develop AI, and there’s a lot of politicking during the process—some interest groups have unusually large influence, others get shafted—in other words, the result looks just like the products of modern democracies. Alternatively, suppose a group of rebel nerds develops an AI in their basement, and instructs the AI to poll everyone in the world—dropping cellphones to anyone who doesn’t have them—and do whatever the majority says. Which of these do you think is more “democratic,” and would you feel safe with either?\n\nI wanted to find out whether he believed in the pragmatic adequacy of the democratic political process, or if he believed in the moral rightness of voting. But the speaker replied:\n\n> The first scenario sounds like an editorial in *Reason* magazine, and the second sounds like a Hollywood movie plot.\n\nConfused, I asked:\n\n> Then what kind of democratic process *did* you have in mind?\n\nThe speaker replied:\n\n> Something like the Human Genome Project—that was an internationally sponsored research project.\n\nI asked:\n\n> How would different interest groups resolve their conflicts in a structure like the Human Genome Project?\n\nAnd the speaker said:\n\n> I don’t know.\n\nThis exchange puts me in mind of a [quote](http://content.time.com/time/magazine/article/0,9171,954853,00.html) from some dictator or other, who was asked if he had any intentions to move his pet state toward democracy:\n\n> We believe we are already within a democratic system. Some factors are still missing, like the expression of the people’s will.\n\nThe substance of a democracy is the specific mechanism that resolves policy conflicts. If all groups had the same preferred policies, there would be no need for democracy—we would automatically cooperate. The resolution process can be a direct majority vote, or an elected legislature, or even a voter-sensitive behavior of an artificial intelligence, but it has to be *something*. What does it *mean* to call for a “democratic” solution if you don’t have a conflict-resolution mechanism in mind?\n\nI think it means that you have said the word “democracy,” so the audience is supposed to cheer. It’s not so much a propositional statement or belief, as the equivalent of the “Applause” light that tells a studio audience when to clap.\n\nThis case is remarkable only in that I mistook the applause light for a policy suggestion, with subsequent embarrassment for all. Most applause lights are much more blatant, and can be detected by a simple reversal test. For example, suppose someone says:\n\n> We need to balance the risks and opportunities of AI.\n\nIf you reverse this statement, you get:\n\n> We shouldn’t balance the risks and opportunities of AI.\n\nSince the reversal sounds *ab*normal, the unreversed statement is probably normal, implying it does not convey new information.\n\nThere are plenty of legitimate reasons for uttering a sentence that would be uninformative in isolation. “We need to balance the risks and opportunities of AI” can introduce a discussion topic; it can emphasize the importance of a specific proposal for balancing; it can criticize an unbalanced proposal. Linking to a normal assertion can convey new information to a bounded rationalist—the link itself may not be obvious. But if *no* specifics follow, the sentence is probably an applause light.\n\nI am tempted to give a talk sometime that consists of *nothing but* applause lights, and see how long it takes for the audience to start laughing:\n\n> I am here to propose to you today that we need to balance the risks and opportunities of advanced artificial intelligence. We should avoid the risks and, insofar as it is possible, realize the opportunities. We should not needlessly confront entirely unnecessary dangers. To achieve these goals, we must plan wisely and rationally. We should not act in fear and panic, or give in to technophobia; but neither should we act in blind enthusiasm. We should respect the interests of all parties with a stake in the Singularity. We must try to ensure that the benefits of advanced technologies accrue to as many individuals as possible, rather than being restricted to a few. We must try to avoid, as much as possible, violent conflicts using these technologies; and we must prevent massive destructive capability from falling into the hands of individuals. We should think through these issues before, not after, it is too late to do anything about them . . ."
          },
          "voteCount": 199
        },
        {
          "name": "Explaining vs. Explaining Away",
          "type": "post",
          "slug": "explaining-vs-explaining-away",
          "_id": "cphoF8naigLhRf3tu",
          "url": null,
          "title": "Explaining vs. Explaining Away",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Causality"
            },
            {
              "name": "Reductionism"
            },
            {
              "name": "Physics"
            },
            {
              "name": "Emotions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "John Keats's [_Lamia_](http://en.wikisource.org/wiki/Lamia) (1819) surely deserves some kind of award for Most Famously Annoying Poetry:\n\n>                     ...Do not all charms fly  \n> At the mere touch of cold philosophy?  \n> There was an awful rainbow once in heaven:  \n> We know her woof, her texture; she is given  \n> In the dull catalogue of common things.  \n> Philosophy will clip an Angel's wings,  \n> Conquer all mysteries by rule and line,  \n> Empty the haunted air, and gnomed mine—  \n> Unweave a rainbow...\n\nMy usual reply ends with the phrase:  \"If we cannot learn to take joy in the merely real, our lives will be empty indeed.\"  I shall expand on that tomorrow.\n\nToday I have a different point in mind.  Let's just take the lines:\n\n> Empty the haunted air, and gnomed mine—  \n> Unweave a rainbow...\n\nApparently \"the mere touch of cold philosophy\", i.e., the truth, has destroyed:\n\n*   Haunts in the air\n*   Gnomes in the mine\n*   Rainbows\n\nWhich calls to mind a rather different bit of [verse](http://www.youtube.com/watch?v=Ect-kgxBb4M):\n\n> One of these things  \n> Is not like the others  \n> One of these things  \n> Doesn't belong\n\nThe air has been emptied of its haunts, and the mine de-gnomed—but the rainbow is still there!\n\nIn \"[Righting a Wrong Question](http://www.overcomingbias.com/2008/03/righting-a-wron.html)\", I wrote:\n\n> Tracing back the chain of causality, step by step, I discover that my belief that I'm wearing socks is fully explained by the fact that I'm wearing socks...  On the other hand, if I see a mirage of a lake in the desert, the correct causal explanation of my vision does not involve the fact of any actual lake in the desert.  In this case, my belief in the lake is not just _explained,_ but _explained away._\n\nThe rainbow was _explained._  The haunts in the air, and gnomes in the mine, were _explained away._\n\nI think this is the key distinction that anti-reductionists don't get about reductionism.\n\nYou can see this failure to get the distinction in the classic objection to reductionism:\n\n> If reductionism is correct, then even your belief in reductionism is just the mere result of the motion of molecules—why should I listen to anything you say?\n\nThe key word, in the above, is _mere;_ a word which implies that accepting reductionism would explain _away_ all the reasoning processes leading up to my acceptance of reductionism, the way that an optical illusion is explained _away_.\n\nBut you can explain how a cognitive process works without it being \"mere\"!  My belief that I'm wearing socks is a mere result of my visual cortex reconstructing nerve impulses sent from my retina which received photons reflected off my socks... which is to say, according to scientific reductionism, my belief that I'm wearing socks is a mere result of the fact that I'm wearing socks.\n\nWhat could be [going on in the anti-reductionists' minds](http://www.overcomingbias.com/2008/03/dissolving-the.html), such that they would put rainbows and belief-in-reductionism, in the same category as haunts and gnomes?\n\nSeveral things are going on simultaneously.  But for now let's focus on the basic idea introduced yesterday:  The [Mind Projection Fallacy](http://www.overcomingbias.com/2008/03/mind-projection.html) between a multi-level map and a mono-level territory.\n\n(I.e:  There's no way you can model a 747 quark-by-quark, so you've _got_ to use a multi-level map with explicit cognitive representations of wings, airflow, and so on.  This doesn't mean there's a multi-level territory.  The true laws of physics, to the best of our knowledge, are only over elementary particle fields.)\n\nI think that when physicists say \"There are no _fundamental_ rainbows,\" the anti-reductionists hear, \"There are no rainbows.\"\n\nIf you don't distinguish between the multi-level map and the mono-level territory, then when someone tries to explain to you that the rainbow is not a fundamental thing in physics, acceptance of this will _feel like_ erasing rainbows from your multi-level map, which _feels like_ erasing rainbows from the world.\n\nWhen Science says \"tigers are not _elementary_ particles, they are made of quarks\" the anti-reductionist hears this as the same sort of dismissal as \"we looked in your garage for a dragon, but there was just empty air\".\n\nWhat scientists did to rainbows, and what scientists did to gnomes, seemingly felt the same to Keats...\n\nIn support of this sub-thesis, I deliberately used several phrasings, in my discussion of Keats's poem, that were Mind Projection Fallacious.  If you didn't notice, this would seem to argue that such fallacies are customary enough to pass unremarked.\n\nFor example:\n\n> \"The air has been emptied of its haunts, and the mine de-gnomed—but the rainbow is still there!\"\n\nActually, Science emptied the _model of_ air of _belief in_ haunts, and emptied the _map of_ the mine of _representations of_ gnomes.  Science did not actually—as Keats's poem itself would have it—take real Angel's wings, and destroy them with a cold touch of truth.  In reality there _never were_ any haunts in the air, or gnomes in the mine.\n\nAnother example:\n\n> \"What scientists did to rainbows, and what scientists did to gnomes, seemingly felt the same to Keats.\"\n\nScientists didn't _do_ anything _to_ gnomes, only to \"gnomes\".  The quotation is not the referent.\n\nBut if you commit the Mind Projection Fallacy—and by default, our beliefs just feel like the way the world _is_—then at time T=0, the mines (apparently) contain gnomes; at time T=1 a scientist dances across the scene, and at time T=2 the mines (apparently) are empty.  Clearly, there used to be gnomes there, but the scientist killed them.\n\nBad scientist!  No poems for you, gnomekiller!\n\nWell, that's how it _feels,_ if you get emotionally attached to the gnomes, and then a scientist says there aren't any gnomes.  It takes a strong mind, a deep honesty, and a deliberate effort to say, at this point, \"That which can be destroyed by the truth should be,\" and \"The scientist hasn't taken the gnomes away, only taken my delusion away,\" and \"I never held just title to my belief in gnomes in the first place; I have not been deprived of anything I _rightfully_ owned,\" and \"If there are gnomes, I desire to believe there are gnomes; if there are no gnomes, I desire to believe there are no gnomes; let me not become attached to beliefs I may not want,\" and all the other things that rationalists are supposed to say on such occasions.\n\nBut with the rainbow it is not even necessary to go that far.  The rainbow is _still there!_"
          },
          "voteCount": 75
        },
        {
          "name": "Talking Snakes: A Cautionary Tale",
          "type": "post",
          "slug": "talking-snakes-a-cautionary-tale",
          "_id": "atcJqdhCxTZiJSxo2",
          "url": null,
          "title": "Talking Snakes: A Cautionary Tale",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Updated Beliefs (examples of)"
            },
            {
              "name": "Fallacies"
            },
            {
              "name": "Religion"
            },
            {
              "name": "Absurdity Heuristic"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "I particularly remember one scene from Bill Maher's \"[Religulous](http://www.religulousmovie.net/)\". I can't find the exact quote, but I will try to sum up his argument as best I remember.\n\n> Christians believe that sin is caused by a talking snake. They may have billions of believers, thousands of years of tradition behind them, and a vast literature of apologetics justifying their faith - but when all is said and done, they're adults who believe in a talking snake.\n\nI have read of the absurdity heuristic. I know that it is not carte blanche to go around rejecting beliefs that seem silly. But I was still sympathetic to the talking snake argument. After all...a _talking snake_?\n\nI changed my mind in a Cairo cafe, talking to a young Muslim woman. I let it slip during the conversation that I was an atheist, and she seemed genuinely curious why. You've all probably been in such a situation, and you probably know how hard it is to choose just one reason, but I'd been reading about Biblical contradictions at the time and I mentioned the myriad errors and atrocities and contradictions in all the Holy Books.  \n  \nHer response? \"Oh, thank goodness it's that. I was afraid you were one of those crazies who believed that monkeys transformed into humans.\"  \n  \nI admitted that [um, well, maybe I sorta kinda](http://www.overcomingbias.com/2008/09/say-it-loud.html) might in fact believe that.  \n  \nIt is hard for me to describe exactly the look of shock on her face, but I have no doubt that her horror was genuine. I may have been the first flesh-and-blood evolutionist she ever met. \"But...\" she looked at me as if I was an idiot. \"Monkeys don't change into humans. What on Earth makes you think monkeys can change into humans?\"  \n  \nI admitted that the whole process was rather complicated. I suggested that it wasn't exactly a Optimus Prime-style transformation so much as a gradual change over eons and eons. I recommended a few books on evolution that might explain it better than I could.  \n  \nShe said that she respected me as a person but that quite frankly I could save my breath because there was no way any book could possibly convince her that monkeys have human babies or whatever sort of balderdash I was preaching. She accused me and other evolution believers of being too willing to accept absurdities, motivated by our atheism and our fear of the self-esteem hit we'd take by accepting Allah was greater than ourselves.  \n  \nIt is not clear to me that this woman did anything differently than Bill Maher. Both heard statements that sounded so crazy as to not even merit further argument. Both recognized that there was a large group of people who found these statements plausible and had written extensive literature justifying them. Both decided that the statements were so absurd as to not merit examining that literature more closely. Both came up with reasons why they could discount the large number of believers because those believers must be biased.  \n  \nI post this as a cautionary tale as we [discuss the logic](/lw/26/rational_defense_of_irrational_beliefs/) or [illogic](/lw/1e/raising_the_sanity_waterline/) of theism. I propose taking from it the following lessons:  \n  \n\\- The [absurdity heuristic](http://www.overcomingbias.com/2007/09/absurdity-heuri.html) doesn't work very well.\n\n\\- Even on [things](http://www.overcomingbias.com/2008/12/we-agree-get-froze.html) [that](http://www.overcomingbias.com/2008/05/timeless-physic.html) [sound](http://www.overcomingbias.com/2008/06/the-quantum-phy.html) [really](http://www.overcomingbias.com/2008/05/many-worlds-one.html), [really](http://intelligence.org/) [absurd](/lw/20/the_apologist_and_the_revolutionary/14w#comments).\n\n\\- If a large number of intelligent people believe something, it deserves your attention. After you've studied it on its own terms, then you have a right to reject it. You could still be wrong, though.\n\n\\- Even if you can think of a good reason why people might be biased towards the silly idea, thus explaining it away, your good reason may still be false.\n\n\\- If someone cannot explain why something is not stupid to you over twenty minutes at a cafe, that doesn't mean it's stupid. It just means it's complicated, or they're not very good at explaining things.\n\n\\- There is no royal road.\n\n_(special note to those prone to [fundamental attribution errors](http://en.wikipedia.org/wiki/Fundamental_Attribution_Error): I do not accept theism. I think theism is wrong. I think it can be demonstrated to be wrong on logical grounds. I think the nonexistence of talking snakes is evidence against theism and can be worked into a general argument against theism. I just don't think it's as easy as saying \"talking snakes are silly, therefore theism is false.\" And I find it embarrassing when atheists say things like that, and then get called on it by intelligent religious people.)_"
          },
          "voteCount": 144
        },
        {
          "name": "Reversed Stupidity Is Not Intelligence",
          "type": "post",
          "slug": "reversed-stupidity-is-not-intelligence",
          "_id": "qNZM3EGoE5ZeMdCRt",
          "url": null,
          "title": "Reversed Stupidity Is Not Intelligence",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Epistemology"
            },
            {
              "name": "Principles"
            },
            {
              "name": "Reversed Stupidity Is Not Intelligence"
            },
            {
              "name": "Distinctions"
            },
            {
              "name": "Steelmanning"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> “. . . then our people on that time-line went to work with corrective action. Here.”\n> \n> He wiped the screen and then began punching combinations. Page after page appeared, bearing accounts of people who had claimed to have seen the mysterious disks, and each report was more fantastic than the last.\n> \n> “The standard smother-out technique,” Verkan Vall grinned. “I only heard a little talk about the ‘flying saucers,’ and all of that was in joke. In that order of culture, you can always discredit one true story by setting up ten others, palpably false, parallel to it.”\n> \n> —H. Beam Piper, _Police Operation_\n\nPiper had a point. Pers’nally, I don’t believe there are any poorly hidden aliens infesting these parts. But my disbelief has nothing to do with the awful embarrassing irrationality of flying saucer cults—at least, I hope not.\n\nYou and I believe that flying saucer cults arose in the total absence of any flying saucers. Cults can arise around almost any idea, thanks to human silliness. This silliness operates _orthogonally_ to alien intervention: We would expect to see flying saucer cults whether or not there were flying saucers. Even if there were poorly hidden aliens, it would not be any _less_ likely for flying saucer cults to arise. The conditional probability P(cults|aliens) isn’t less than P(cults|¬aliens), unless you suppose that poorly hidden aliens would deliberately suppress flying saucer cults.^[1](#fn1x19)^ By the Bayesian definition of evidence, the observation “flying saucer cults exist” is not evidence _against_ the existence of flying saucers. It’s not much evidence one way or the other.\n\nThis is an application of the general principle that, as Robert Pirsig puts it, “The world’s greatest fool may say the Sun is shining, but that doesn’t make it dark out.”^[2](#fn2x19)^\n\nIf you knew someone who was wrong 99.99% of the time on yes-or-no questions, you could obtain 99.99% accuracy just by reversing their answers. They would need to do all the work of obtaining good evidence entangled with reality, and processing that evidence coherently, just to _anticorrelate_ that reliably. They would have to be superintelligent to be that stupid.\n\nA car with a broken engine cannot drive backward at 200 mph, even if the engine is _really really broken._\n\nIf stupidity does not reliably anticorrelate with truth, how much less should human evil anticorrelate with truth? The converse of the halo effect is the horns effect: All perceived negative qualities correlate. If Stalin is evil, then everything he says should be false. You wouldn’t want to agree with _Stalin_, would you?\n\nStalin also believed that 2 + 2 = 4. Yet if you defend any statement made by Stalin, even “2 + 2 = 4,” people will see only that you are “agreeing with Stalin”; you must be on his side.\n\nCorollaries of this principle:\n\n*   To argue against an idea honestly, you should argue against the best arguments of the strongest advocates. Arguing against weaker advocates proves _nothing_, because even the strongest idea will attract weak advocates. If you want to argue against transhumanism or the intelligence explosion, you have to directly challenge the arguments of Nick Bostrom or Eliezer Yudkowsky post-2003. The least convenient path is the only valid one.^[3](#fn3x19)^\n*   Exhibiting sad, pathetic lunatics, driven to madness by their apprehension of an Idea, is no evidence against that Idea. Many New Agers have been made crazier by their personal apprehension of quantum mechanics.\n*   Someone once said, “Not all conservatives are stupid, but most stupid people are conservatives.” If you cannot place yourself in a state of mind where this statement, true or false, seems _completely irrelevant_ as a critique of conservatism, you are not ready to think rationally about politics.\n*   Ad hominem argument is not valid.\n*   You need to be able to argue against genocide without saying “Hitler wanted to exterminate the Jews.” If Hitler _hadn’t_ advocated genocide, would it thereby become okay?\n*   In Hansonian terms: Your instinctive willingness to believe something will change along with your willingness to _affiliate_ with people who are known for believing it—quite apart from whether the belief is actually _true._ Some people may be reluctant to believe that God does not exist, not because there is evidence that God _does_ exist, but rather because they are reluctant to affiliate with Richard Dawkins or those darned “strident” atheists who go around publicly saying “God does not exist.”\n*   If your current computer stops working, you can’t conclude that everything about the current system is wrong and that you need a new system without an AMD processor, an ATI video card, a Maxtor hard drive, or case fans—even though your current system has all these things and it doesn’t work. Maybe you just need a new power cord.\n*   If a hundred inventors fail to build flying machines using metal and wood and canvas, it doesn’t imply that what you really need is a flying machine of bone and flesh. If a thousand projects fail to build Artificial Intelligence using electricity-based computing, this doesn’t mean that electricity is the source of the problem. Until you understand the problem, hopeful reversals are exceedingly unlikely to hit the solution.^[4](#fn4x19)^\n\n^[1](#fn1x19-bk)^Read “P(cults|aliens)” as “the probability of UFO cults given that aliens have visited Earth,” and read “P(cults|¬aliens)” as “the probability of UFO cults given that aliens have not visited Earth.”\n\n^[2](#fn2x19-bk)^Robert M. Pirsig, _Zen and the Art of Motorcycle Maintenance: An Inquiry Into Values_, 1st ed. (New York: Morrow, 1974).\n\n^[3](#fn3x19-bk)^See Scott Alexander, “The Least Convenient Possible World,” _Less Wrong_ (blog), December 2, 2018, [http://lesswrong.com/lw/2k/the\\_least\\_convenient\\_possible\\_world/](http://lesswrong.com/lw/2k/the_least_convenient_possible_world/).\n\n^[4](#fn4x19-bk)^See also “Selling Nonapples.” [http://lesswrong.com/lw/vs/selling_nonapples](http://lesswrong.com/lw/vs/selling_nonapples)."
          },
          "voteCount": 99
        },
        {
          "name": "Cognitive Biases Potentially Affecting Judgment of Global Risks",
          "type": "post",
          "href": "https://intelligence.org/files/CognitiveBiases.pdf"
        },
        {
          "name": "Politics is the Mind-Killer",
          "type": "post",
          "slug": "politics-is-the-mind-killer",
          "_id": "9weLK2AJ9JEt2Tt8f",
          "url": null,
          "title": "Politics is the Mind-Killer",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Politics"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Tribalism"
            },
            {
              "name": "Social & Cultural Dynamics"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "People go funny in the head when talking about politics. The evolutionary reasons for this are so obvious as to be worth belaboring: In the ancestral environment, politics was a matter of life and death. And sex, and wealth, and allies, and reputation . . . When, today, you get into an argument about whether “we” ought to raise the minimum wage, you’re executing adaptations for an ancestral environment where being on the wrong side of the argument could get you killed. Being on the *right* side of the argument could let *you* kill your hated rival!\n\nIf you want to make a point about science, or rationality, then my advice is to not choose a domain from *contemporary* politics if you can possibly avoid it. If your point is inherently about politics, then talk about Louis XVI during the French Revolution. Politics is an important domain to which we should individually apply our rationality—but it’s a terrible domain in which to *learn* rationality, or discuss rationality, unless all the discussants are already rational.\n\nPolitics is an extension of war by other means. Arguments are soldiers. Once you know which side you’re on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it’s like stabbing your soldiers in the back—providing aid and comfort to the enemy. People who would be level-headed about evenhandedly weighing all sides of an issue in their professional life as scientists, can suddenly turn into slogan-chanting zombies when there’s a [Blue or Green](https://lesswrong.com/rationality/a-fable-of-science-and-politics) position on an issue.\n\nIn artificial intelligence, and particularly in the domain of nonmonotonic reasoning, there’s a standard problem: “All Quakers are pacifists. All Republicans are not pacifists. Nixon is a Quaker and a Republican. Is Nixon a pacifist?”\n\nWhat on Earth was the point of choosing this as an example? To rouse the political emotions of the readers and distract them from the main question? To make Republicans feel unwelcome in courses on artificial intelligence and discourage them from entering the field?[^1^](#fn1x14)\n\nWhy would anyone pick such a *distracting* example to illustrate nonmonotonic reasoning? Probably because the author just couldn’t resist getting in a good, solid dig at those hated Greens. It feels so *good* to get in a hearty punch, y’know, it’s like trying to resist a chocolate cookie.\n\nAs with chocolate cookies, not everything that feels pleasurable is good for you.\n\nI’m not saying that I think we should be apolitical, or even that we should adopt Wikipedia’s ideal of the Neutral Point of View. But try to resist getting in those good, solid digs if you can possibly avoid it. If your topic legitimately relates to attempts to ban evolution in school curricula, then go ahead and talk about it—but don’t blame it explicitly on the whole Republican Party; some of your readers may be Republicans, and they may feel that the problem is a few rogues, not the entire party. As with Wikipedia’s NPOV, it doesn’t matter whether (you think) the Republican Party really *is* at fault. It’s just better for the spiritual growth of the community to discuss the issue without invoking color politics.\n\n* * *\n\n[^1^](#fn1x14-bk)And no, I am not a Republican. Or a Democrat."
          },
          "voteCount": 153
        },
        {
          "name": "Newcomb's Problem and Regret of Rationality",
          "type": "post",
          "slug": "newcomb-s-problem-and-regret-of-rationality",
          "_id": "6ddcsdA2c2XpNpE5x",
          "url": null,
          "title": "Newcomb's Problem and Regret of Rationality",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Decision Theory"
            },
            {
              "name": "Newcomb's Problem"
            },
            {
              "name": "Pre-Commitment"
            },
            {
              "name": "One-Boxing"
            },
            {
              "name": "Conditional Consistency"
            },
            {
              "name": "Two-Boxing"
            },
            {
              "name": "Bayesianism"
            },
            {
              "name": "Something To Protect"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The following may well be the most controversial dilemma in the history of decision theory:\n\n> A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game.  In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.\n> \n> Box A is transparent and contains a thousand dollars.  \n> Box B is opaque, and contains either a million dollars, or nothing.\n> \n> You can take both boxes, or take only box B.\n> \n> And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.\n> \n> Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars.  (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)\n> \n> Before you make your choice, Omega has flown off and moved on to its next game.  Box B is already empty or already full.\n> \n> Omega drops two boxes on the ground in front of you and flies off.\n> \n> Do you take both boxes, or only box B?\n\nAnd the standard philosophical conversation runs thusly:\n\n> One-boxer:  \"I take only box B, of course.  I'd rather have a million than a thousand.\"\n> \n> Two-boxer:  \"Omega has already left.  Either box B is already full or already empty.  If box B is already empty, then taking both boxes nets me $1000, taking only box B nets me $0.  If box B is already full, then taking both boxes nets $1,001,000, taking only box B nets $1,000,000.  In either case I do better by taking both boxes, and worse by leaving a thousand dollars on the table - so I will be rational, and take both boxes.\"\n> \n> One-boxer:  \"If you're so rational, why ain'cha rich?\"\n> \n> Two-boxer:  \"It's not my fault Omega chooses to reward only people with irrational dispositions, but it's already too late for me to do anything about that.\"\n\nThere is a _large_ literature on the topic of Newcomblike problems - especially if you consider the Prisoner's Dilemma as a special case, which it is generally held to be.  \"Paradoxes of Rationality and Cooperation\" is an edited volume that includes Newcomb's original essay.  For those who read only online material, [this PhD thesis](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.724&rep=rep1&type=pdf) summarizes the major standard positions.\n\nI'm not going to go into the whole literature, but the dominant consensus in modern decision theory is that one should two-box, and Omega is just rewarding agents with irrational dispositions.  This dominant view goes by the name of \"causal decision theory\".\n\nAs you know, the primary [reason I'm blogging](/lw/jf/why_im_blooking/) is that I am an incredibly slow writer when I try to work in any other format.  So I'm not going to try to present my own analysis here.  Way too long a story, even by my standards.\n\nBut it is agreed even among causal decision theorists that if you have the power to precommit yourself to take one box, in Newcomb's Problem, then you should do so.  If you can precommit yourself before Omega examines you; then you are directly causing box B to be filled.\n\nNow in my field - which, in case you have forgotten, is self-modifying AI - this works out to saying that if you build an AI that two-boxes on Newcomb's Problem, it will self-modify to one-box on Newcomb's Problem, if the AI considers in advance that it might face such a situation.  Agents with free access to their own source code have access to a cheap method of precommitment.\n\nWhat if you expect that you might, in general, face a Newcomblike problem, without knowing the exact form of the problem?  Then you would have to modify yourself into a sort of agent whose disposition was such that it would generally receive high rewards on Newcomblike problems.\n\nBut what does an agent with a disposition generally-well-suited to Newcomblike problems look like?  Can this be formally specified?\n\nYes, but when I tried to write it up, I realized that I was starting to write a small book.  And it wasn't the most important book I had to write, so I shelved it.  My slow writing speed really is the bane of my existence.  The theory I worked out seems, to me, to have many nice properties besides being well-suited to Newcomblike problems.  It would make a nice PhD thesis, if I could get someone to accept it as my PhD thesis.  But that's pretty much what it would take to make me unshelve the project.  Otherwise I can't justify the time expenditure, not at the speed I currently write books.\n\nI say all this, because there's a common attitude that \"Verbal arguments for one-boxing are easy to come by, what's hard is developing a good decision theory that one-boxes\" - coherent math which one-boxes on Newcomb's Problem without producing absurd results elsewhere.  So I do understand that, and I did set out to develop such a theory, but my writing speed on big papers is so slow that I can't publish it.  Believe it or not, it's true.\n\nNonetheless, I would like to present some of my _motivations_ on Newcomb's Problem - the reasons I felt impelled to seek a new theory - because they illustrate my source-attitudes toward rationality.  Even if I can't present the theory that these motivations motivate...\n\nFirst, foremost, fundamentally, above all else:\n\nRational agents should WIN.\n\nDon't mistake me, and think that I'm talking about the Hollywood Rationality stereotype that rationalists should be selfish or shortsighted.  If your utility function has a term in it for others, then win their happiness.  If your utility function has a term in it for a million years hence, then win the eon.\n\nBut at any rate, _WIN_.  Don't lose reasonably, **_WIN_**.\n\nNow there are defenders of causal decision theory who argue that the two-boxers are doing their best to win, and cannot help it if they have been cursed by a Predictor who favors irrationalists.  I will talk about this defense in a moment.  But first, I want to draw a distinction between causal decision theorists who believe that two-boxers are genuinely doing their best to win; versus someone who thinks that two-boxing is the _reasonable_ or the _rational_ thing to do, but that the reasonable move just happens to predictably lose, in this case.  There are a _lot_ of people out there who think that rationality predictably loses on various problems - that, too, is part of the Hollywood Rationality stereotype, that Kirk is predictably superior to Spock.\n\nNext, let's turn to the charge that Omega favors irrationalists.  I can conceive of a superbeing who rewards only people born with a particular gene, _regardless of their choices._  I can conceive of a superbeing who rewards people whose brains inscribe the _particular algorithm_ of \"Describe your options in English and choose the last option when ordered alphabetically,\" but who does not reward anyone who chooses the same option for a different reason.  But Omega rewards people who choose to take only box B, _regardless of which algorithm they use to arrive at this decision,_ and this is why I don't buy the charge that Omega is rewarding the irrational.  Omega doesn't care whether or not you follow some particular ritual of cognition; Omega only cares about your predicted _decision_.\n\nWe can choose whatever reasoning algorithm we like, and will be rewarded or punished only according to that algorithm's choices, with no other dependency - Omega just cares where we go, not how we got there.\n\nIt is precisely the notion that Nature does not care about our _algorithm,_ which frees us up to pursue the winning Way - without attachment to any particular ritual of cognition, apart from our belief that it wins.  Every rule is up for grabs, _except_ the rule of winning.\n\nAs Miyamoto Musashi said - it's really worth repeating:\n\n> \"You can win with a long weapon, and yet you can also win with a short weapon.  In short, the Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size.\"\n\n(Another example:  It was [argued by McGee](/lw/na/trust_in_bayes/) that we must adopt bounded utility functions or be subject to \"Dutch books\" over infinite times.  But:  _The utility function is not up for grabs._  I love life [without limit or upper bound:](http://yudkowsky.net/singularity/simplified/)  There is no finite amount of life lived N where I would prefer a 80.0001% probability of living N years to an 0.0001% chance of living a googolplex years and an 80% chance of living forever.  This is a sufficient condition to imply that my utility function is unbounded.  So I just have to figure out how to optimize _for that morality._  You can't tell me, first, that above all I must conform to a particular ritual of cognition, and then that, if I conform to that ritual, I must change my morality to avoid being Dutch-booked.  Toss out the losing ritual; don't change the definition of winning.  That's like deciding to prefer $1000 to $1,000,000 so that Newcomb's Problem doesn't make your preferred ritual of cognition look bad.)\n\n\"But,\" says the causal decision theorist, \"to take only one box, you must somehow believe that your choice can affect whether box B is empty or full - and that's _unreasonable!_  Omega has already left!  It's physically impossible!\"\n\nUnreasonable?  I am a rationalist: what do I care about being unreasonable?  I don't have to conform to a particular ritual of cognition.  I don't have to take only box B _because I believe my choice affects the box, even though Omega has already left._  I can just... take only box B.\n\nI do have a proposed alternative ritual of cognition which computes this decision, which this margin is too small to contain; but I shouldn't need to show this to you.  The point is not to have an elegant theory of winning - the point is to win; elegance is a side effect.\n\nOr to look at it another way:  Rather than starting with a concept of what is the reasonable decision, and then asking whether \"reasonable\" agents leave with a lot of money, start by looking at the agents who leave with a lot of money, develop a theory of which agents tend to leave with the most money, and from this theory, try to figure out what is \"reasonable\".  \"Reasonable\" may just refer to decisions in conformance with our current ritual of cognition - what else would determine whether something seems \"reasonable\" or not?\n\nFrom James Joyce (no relation), _Foundations of Causal Decision Theory:_\n\n> Rachel has a perfectly good answer to the \"Why ain't you rich?\" question.  \"I am not rich,\" she will say, \"because I am not the kind of person the psychologist thinks will refuse the money.  I'm just not like you, Irene.  Given that I know that I am the type who takes the money, and given that the psychologist knows that I am this type, it was reasonable of me to think that the $1,000,000 was not in my account.  The $1,000 was the most I was going to get no matter what I did.  So the only reasonable thing for me to do was to take it.\"\n> \n> Irene may want to press the point here by asking, \"But don't you wish you were like me, Rachel?  Don't you wish that you were the refusing type?\"  There is a tendency to think that Rachel, a committed causal decision theorist, must answer this question in the negative, which seems obviously wrong (given that being like Irene would have made her rich).  This is not the case.  Rachel can and should admit that she does wish she were more like Irene.  \"It would have been better for me,\" she might concede, \"had I been the refusing type.\"  At this point Irene will exclaim, \"You've admitted it!  It wasn't so smart to take the money after all.\"  Unfortunately for Irene, her conclusion does not follow from Rachel's premise.  Rachel will patiently explain that wishing to be a refuser in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 _whatever type one is._  When Rachel wishes she was Irene's type she is wishing _for Irene's options,_ not sanctioning her choice.\n\nIt is, I would say, a general principle of rationality - indeed, part of how I _define_ rationality - that you never end up envying someone else's mere _choices._  You might envy someone their genes, if Omega rewards genes, or if the genes give you a generally happier disposition.  But Rachel, above, envies Irene her choice, and _only_ her choice, irrespective of what algorithm Irene used to make it.  Rachel wishes _just_ that she had a disposition to choose differently.\n\nYou shouldn't claim to be more rational than someone and simultaneously envy them their choice - _only_ their choice.  Just _do_ the act you envy.\n\nI keep trying to say that rationality is the winning-Way, but causal decision theorists insist that taking both boxes is what _really_ wins, because you _can't possibly do better_ by leaving $1000 on the table... even though the single-boxers leave the experiment with more money.  Be careful of this sort of argument, any time you find yourself defining the \"winner\" as someone other than the agent who is currently smiling from on top of a giant heap of utility.\n\nYes, there are various thought experiments in which some agents start out with an advantage - but if the task is to, say, decide whether to jump off a cliff, you want to be careful not to define cliff-refraining agents as having an unfair prior advantage over cliff-jumping agents, by virtue of their unfair refusal to jump off cliffs.  At this point you have covertly redefined \"winning\" as conformance to a particular ritual of cognition.  _Pay attention to the money!_\n\nOr here's another way of looking at it:  Faced with Newcomb's Problem, would you want to look really hard for a reason to believe that it was perfectly reasonable and rational to take only box B; because, if such a line of argument existed, you would take only box B and find it full of money?  Would you spend an extra hour thinking it through, if you were confident that, at the end of the hour, you would be able to convince yourself that box B was the rational choice?  This too is a rather odd position to be in.  Ordinarily, the work of rationality goes into figuring out which choice is the best - not finding a reason to believe that a particular choice is the best.\n\nMaybe it's too easy to say that you \"ought to\" two-box on Newcomb's Problem, that this is the \"reasonable\" thing to do, so long as the money isn't actually in front of you.  Maybe you're just numb to philosophical dilemmas, at this point.  What if your daughter had a 90% fatal disease, and box A contained a serum with a 20% chance of curing her, and box B might contain a serum with a 95% chance of curing her?  What if there was an asteroid rushing toward Earth, and box A contained an asteroid deflector that worked 10% of the time, and box B might contain an asteroid deflector that worked 100% of the time?\n\nWould you, at that point, find yourself _tempted to make an unreasonable choice?_\n\nIf the stake in box B was [something you _could not_ leave behind](/lw/nb/something_to_protect/)?  Something overwhelmingly more important to you than being reasonable?  If you absolutely _had to_ win - _really_ win, not just be defined as winning?\n\nWould you _wish with all your power_ that the \"reasonable\" decision was to take only box B?\n\nThen maybe it's time to update your definition of reasonableness.\n\nAlleged rationalists should not find themselves envying the mere decisions of alleged nonrationalists, because your decision can be whatever you like.  When you find yourself in a position like this, you shouldn't chide the other person for failing to conform to your concepts of reasonableness.  You should realize you got the Way wrong.\n\nSo, too, if you ever find yourself keeping separate track of the \"reasonable\" belief, versus the belief that seems likely to be actually _true._  Either you have misunderstood reasonableness, or your second intuition is just wrong.\n\nNow one can't simultaneously _define_ \"rationality\" as the winning Way, and _define_ \"rationality\" as Bayesian probability theory and decision theory.  But it is the argument that I am putting forth, and the moral of my advice to [Trust In Bayes](/lw/na/trust_in_bayes/), that the laws governing winning have indeed proven to be [math](/lw/mt/beautiful_probability/).  If it ever turns out that Bayes fails - receives systematically lower rewards on some problem, relative to a superior alternative, in virtue of its mere decisions - then Bayes has to go _out the window._ \"Rationality\" is just the label I use for my beliefs about the winning Way - the Way of the agent smiling from on top of the giant heap of utility.  _Currently,_ that label refers to Bayescraft.\n\nI realize that this is not a knockdown criticism of causal decision theory - that would take the actual book and/or PhD thesis - but I hope it illustrates some of my underlying attitude toward this notion of \"rationality\".\n\nYou shouldn't find yourself distinguishing the winning choice from the reasonable choice.  Nor should you find yourself distinguishing the reasonable belief from the belief that is most likely to be true.\n\nThat is why I use the word \"rational\" to denote my beliefs about accuracy and winning - _not_ to denote [verbal](/lw/go/why_truth_and/) reasoning, or strategies which yield [certain](/lw/mm/the_fallacy_of_gray/) success, or that which is [logically](/lw/k2/a_priori/) provable, or that which is [publicly demonstrable](/lw/in/scientific_evidence_legal_evidence_rational/), or that which is reasonable.\n\nAs Miyamoto Musashi said:\n\n> \"The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy's cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him.\""
          },
          "voteCount": 110
        }
      ]
    },
    {
      "title": "Top-75",
      "children": [
        {
          "name": "Semantic Stopsigns",
          "type": "post",
          "slug": "semantic-stopsigns",
          "_id": "FWMfQKG3RpZx6irjm",
          "url": null,
          "title": "Semantic Stopsigns",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Confirmation Bias"
            },
            {
              "name": "Paradoxes"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "*And the child asked:*\n\nQ: Where did this rock come from?\n\nA: I chipped it off the big boulder, at the center of the village.\n\nQ: Where did the boulder come from?\n\nA: It probably rolled off the huge mountain that towers over our village.\n\nQ: Where did the mountain come from?\n\nA: The same place as all stone: it is the bones of Ymir, the primordial giant.\n\nQ: Where did the primordial giant, Ymir, come from?\n\nA: From the great abyss, Ginnungagap.\n\nQ: Where did the great abyss, Ginnungagap, come from?\n\nA: Never ask that question.\n\nConsider the seeming paradox of the First Cause. Science has traced events back to the Big Bang, but why did the Big Bang happen? It’s all well and good to say that the zero of time begins at the Big Bang—that there is nothing before the Big Bang in the ordinary flow of minutes and hours. But saying this presumes our physical law, which itself appears highly structured; it calls out for explanation. Where did the physical laws come from? You could say that we’re all a computer simulation, but then the computer simulation is running on some other world’s laws of physics—where did *those* laws of physics come from?\n\nAt this point, some people say, “God!”\n\nWhat could possibly make anyone, even a highly religious person, think this even *helped* answer the paradox of the First Cause? Why wouldn’t you automatically ask, “Where did God come from?” Saying “God is uncaused” or “God created Himself” leaves us in exactly the same position as “Time began with the Big Bang.” We just ask why the whole metasystem exists in the first place, or why some events but not others are allowed to be uncaused.\n\nMy purpose here is not to discuss the seeming paradox of the First Cause, but to ask why anyone would think “God!” *could* resolve the paradox. Saying “God!” is a way of belonging to a tribe, which gives people a motive to say it as often as possible—some people even say it for questions like “Why did this hurricane strike New Orleans?” Even so, you’d hope people would notice that on the *particular* puzzle of the First Cause, saying “God!” doesn’t help. It doesn’t make the paradox seem any less paradoxical *even if true*. How could anyone *not* notice this?\n\nJonathan Wallace suggested that “God!” functions as a semantic stopsign—that it isn’t a propositional assertion, so much as a cognitive traffic signal: do not think past this point.^1^ Saying “God!” doesn’t so much resolve the paradox, as put up a cognitive traffic signal to halt the obvious continuation of the question-and-answer chain.\n\nOf course *you’d* never do that, being a good and proper atheist, right? But “God!” isn’t the *only* semantic stopsign, just the obvious first example.\n\nThe transhuman technologies—molecular nanotechnology, advanced biotech, genetech, artificial intelligence, et cetera—pose tough policy questions. What kind of role, if any, should a government take in supervising a parent’s choice of genes for their child? Could parents deliberately choose genes for schizophrenia? If enhancing a child’s intelligence is expensive, should governments help ensure access, to prevent the emergence of a cognitive elite? You can propose various institutions to answer these policy questions—for example, that private charities should provide financial aid for intelligence enhancement—but the obvious next question is, “Will this institution be effective?” If we rely on product liability lawsuits to prevent corporations from building harmful nanotech, will that really *work*?\n\nI know someone whose answer to every one of these questions is “Liberal democracy!” That’s it. That’s his answer. If you ask the obvious question of “How well have liberal democracies performed, historically, on problems this tricky?” or “What if liberal democracy does something stupid?” then you’re an autocrat, or libertopian, or otherwise a very very bad person. No one is allowed to question democracy.\n\nI once called this kind of thinking “the divine right of democracy.” But it is more precise to say that “Democracy!” functioned for him as a semantic stopsign. If anyone had said to him “Turn it over to the Coca-Cola corporation!” he would have asked the obvious next questions: “Why? What will the Coca-Cola corporation do about it? Why should we trust them? Have they done well in the past on equally tricky problems?”\n\nOr suppose that someone says, “Mexican-Americans are plotting to remove all the oxygen in Earth’s atmosphere.” You’d probably ask, “Why would they do *that*? Don’t Mexican-Americans have to breathe too? Do Mexican-Americans even function as a unified conspiracy?” If you don’t ask these obvious next questions when someone says, “Corporations are plotting to remove Earth’s oxygen,” then “Corporations!” functions for you as a semantic stopsign.\n\nBe careful here not to create a new generic counterargument against things you don’t like—“Oh, it’s just a stopsign!” No word is a stopsign of itself; the question is whether a word has that effect on a particular person. Having [strong emotions](https://lesswrong.com/rationality/feeling-rational) about something doesn’t qualify it as a stopsign. I’m not exactly fond of terrorists or fearful of private property; that doesn’t mean “Terrorists!” or “Capitalism!” are cognitive traffic signals unto me. (The word “intelligence” did once have that effect on me, though no longer.) What distinguishes a semantic stopsign is *failure to consider the obvious next question*.\n\n* * *\n\n^1^ See Wallace’s “God vs. God” ([http://www.spectacle.org/yearzero/godvgod.html](http://www.spectacle.org/yearzero/godvgod.html)) and “God as a Semantical Signpost” ([http://www.spectacle.org/1095/stop1.html](http://www.spectacle.org/1095/stop1.html))."
          },
          "voteCount": 114
        },
        {
          "name": "Dissolving the Question",
          "type": "post",
          "slug": "dissolving-the-question",
          "_id": "Mc6QcrsbH5NRXbCRX",
          "url": null,
          "title": "Dissolving the Question",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Introspection"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Philosophy"
            },
            {
              "name": "Dissolving the Question"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "\"If [a tree falls in the forest](/lw/np/disputing_definitions/), but no one hears it, does it make a sound?\"\n\nI didn't _answer_ that question.  I didn't pick a position, \"Yes!\" or \"No!\", and defend it.  Instead I went off and [deconstructed](/lw/no/how_an_algorithm_feels_from_inside/) the human algorithm for processing words, even going so far as to sketch an [illustration](/lw/nn/neural_categories/) of a neural network.  At the end, I hope, there was no question left—not even the feeling of a question.\n\nMany philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct:  If you give them a question, they try to answer it.\n\nLike, say, \"Do we have free will?\"\n\nThe dangerous instinct of philosophy is to marshal the arguments in favor, and marshal the arguments against, and weigh them up, and publish them in a prestigious journal of philosophy, and so finally conclude:  \"Yes, we must have free will,\" or \"No, we cannot possibly have free will.\"\n\nSome philosophers are wise enough to recall the warning that most philosophical disputes are really disputes over the meaning of a word, or confusions generated by [using different meanings for the same word in different places](/lw/oc/variable_question_fallacies/).  So they try to define very precisely what they mean by \"free will\", and then ask again, \"Do we have free will?  Yes or no?\"\n\nA philosopher wiser yet, may suspect that the confusion about \"free will\" shows the notion itself is flawed.  So they pursue the Traditional Rationalist course:  They argue that \"free will\" is inherently self-contradictory, or meaningless because it has no [testable consequences](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/).  And then they publish these devastating observations in a prestigious philosophy journal.\n\nBut _proving that_ you are confused may not make you feel any _less_ confused.  Proving that a question is meaningless may not help you any more than answering it.\n\nThe philosopher's instinct is to find the most defensible position, publish it, and move on.  But the \"naive\" view, the instinctive view, is a fact about human psychology.  You can prove that free will is impossible until the Sun goes cold, but this leaves an unexplained fact of cognitive science:  If free will doesn't exist, what goes on inside the head of a human being who thinks it does?  This is not a rhetorical question!\n\nIt is a fact about human psychology that people think they have free will.  Finding a more defensible _philosophical position_ doesn't change, or explain, that _psychological fact._  Philosophy may lead you to _reject_ the concept, but rejecting a concept is not the same as understanding the cognitive algorithms behind it.\n\nYou could look at the [Standard Dispute](/lw/np/disputing_definitions/) over \"If a tree falls in the forest, and no one hears it, does it make a sound?\", and you could do the Traditional Rationalist thing:  Observe that the two don't disagree on any point of [anticipated experience](/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/), and triumphantly declare the argument pointless.  That happens to be correct in this particular case; but, as _a question of cognitive science,_ why did the arguers make that mistake in the first place?\n\nThe key idea of the heuristics and biases program is that the _mistakes_ we make, often reveal far more about our underlying cognitive algorithms than our correct answers.  So (I asked myself, once upon a time) [what kind of mind design](/lw/nq/feel_the_meaning/) corresponds to the mistake of [arguing](/lw/np/disputing_definitions/) about trees falling in deserted forests?\n\nThe cognitive algorithms we use, _are_ [the way the world feels](/lw/no/how_an_algorithm_feels_from_inside/).  And these cognitive algorithms may not have a one-to-one correspondence with reality—not even macroscopic reality, to say nothing of the true quarks.  There can be things in the mind that cut skew to the world.\n\nFor example, there can be a [dangling unit](/lw/no/how_an_algorithm_feels_from_inside/) in the center of a [neural network](/lw/nn/neural_categories/), which does not correspond to any real thing, or any real property of any real thing, existent anywhere in the real world.  This dangling unit is often useful as a [shortcut in computation](/lw/o8/conditional_independence_and_naive_bayes/), which is why we have them.  (Metaphorically speaking.  Human neurobiology is surely far more [complex](/lw/o8/conditional_independence_and_naive_bayes/).)\n\nThis dangling unit _feels like_ an unresolved question, even after every answerable [query](/lw/nm/disguised_queries/) is answered.  No matter how much anyone proves to you that no difference of anticipated experience depends on the question, you're left wondering:  \"But does the falling tree _really_ make a sound, or not?\"\n\nBut once you understand _in detail_ how your brain generates the _feeling_ of the question—once you realize that your feeling of an unanswered question, corresponds to an illusory central unit wanting to know whether it should fire, even after all the edge units are clamped at known values—or better yet, you understand the technical workings of [Naive Bayes](/lw/o8/conditional_independence_and_naive_bayes/)—_then_ you're done.  Then there's no lingering feeling of confusion, no vague sense of dissatisfaction.\n\nIf there is _any_ lingering feeling of a remaining unanswered question, or of having been fast-talked into something, then this is a sign that you have not dissolved the question.  A [vague dissatisfaction](/lw/if/your_strength_as_a_rationalist/) should be as much warning as a shout.  _Really_ dissolving the question doesn't leave anything behind.\n\nA triumphant thundering refutation of free will, an absolutely unarguable proof that free will cannot exist, feels very _satisfying_—a [grand cheer](/lw/i6/professing_and_cheering/) for the [home team](/lw/mg/the_twoparty_swindle/).    And so you may not notice that—as a point of cognitive science—you do not have a full and satisfactory descriptive explanation of how each intuitive sensation arises, point by point.\n\nYou may not even want to admit your ignorance, of this point of cognitive science, because that would feel like a score against Your Team.  In the midst of smashing all foolish beliefs of free will, it would seem like a concession to the opposing side to concede that you've left anything unexplained.\n\nAnd so, perhaps, you'll come up with a [just-so evolutionary-psychological](/lw/mk/a_failed_justso_story/) argument that hunter-gatherers who believed in free will, were more likely to take a positive outlook on life, and so outreproduce other hunter-gatherers—to give one example of a completely bogus explanation.  If you say this, you are _arguing that_ the brain generates an illusion of free will—but you are not _explaining how._  You are trying to dismiss the opposition by deconstructing its motives—but in the story you tell, the illusion of free will is a brute fact.  You have not taken the illusion apart to see the wheels and gears.\n\nImagine that in the Standard Dispute about a tree falling in a deserted forest, you first prove that no difference of anticipation exists, and then go on to hypothesize, \"But perhaps people who said that arguments were meaningless were viewed as having conceded, and so lost social status, so now we have an instinct to argue about the meanings of words.\"  That's _arguing that_ or _explaining why_ a confusion exists.  Now look at the neural network structure in [Feel the Meaning](/lw/nq/feel_the_meaning/).  That's _explaining how_, disassembling the confusion into smaller pieces which are not themselves confusing.  See the difference?\n\nComing up with good hypotheses about cognitive algorithms (or even hypotheses that hold together for half a second) is a good deal harder than just refuting a philosophical confusion.  Indeed, it is an entirely different art.  Bear this in mind, and you should feel less embarrassed to say, \"I know that what you say can't possibly be true, and I can prove it.  But I cannot write out a flowchart which shows how your brain makes the mistake, so I'm not done yet, and will continue investigating.\"\n\nI say all this, because it sometimes seems to me that at least 20% of the real-world effectiveness of a skilled rationalist comes from [not stopping too early](/lw/jz/the_meditation_on_curiosity/).  If you keep asking questions, you'll get to your destination eventually.  If you decide too early that you've found an answer, you won't.\n\nThe challenge, above all, is to notice when you are confused—even if it just feels like a little tiny bit of confusion—and even if there's someone standing across from you, _insisting_ that humans have free will, and _smirking_ at you, and the fact that you don't know _exactly_ how the cognitive algorithms work, has _nothing to do_ with the searing folly of their position...\n\nBut when you can lay out the cognitive algorithm in sufficient detail that you can walk through the thought process, step by step, and describe how each intuitive perception arises—decompose the confusion into smaller pieces not themselves confusing—_then_ you're done.\n\nSo be warned that you may _believe_ you're done, when all you have is a mere triumphant [refutation of a mistake](/lw/lw/reversed_stupidity_is_not_intelligence/).\n\nBut when you're _really_ done, you'll _know_ you're done.[ ](/lw/gr/the_modesty_argument/)  Dissolving the question is an unmistakable feeling—once you experience it, and, having experienced it, resolve not to be fooled again.  [Those who dream do not know they dream, but when you wake you know you are awake.](/lw/gr/the_modesty_argument/)\n\nWhich is to say:  When you're done, you'll know you're done, but unfortunately the reverse implication does not hold.\n\nSo here's your homework problem:  What kind of cognitive algorithm, as felt from the inside, would generate the observed debate about \"free will\"?\n\nYour assignment is not to argue about whether people have free will, or not.\n\nYour assignment is not to argue that free will is compatible with determinism, or not.\n\nYour assignment is not to argue that the question is ill-posed, or that the concept is self-contradictory, or that it has no testable consequences.\n\nYou are not asked to invent an evolutionary explanation of how people who believed in free will would have reproduced; nor an account of how the concept of free will seems suspiciously congruent with bias X.  Such are mere attempts to _explain why_ people believe in \"free will\", not _explain how._\n\nYour homework assignment is to write a stack trace of the internal algorithms of the human mind as they produce the intuitions that power the whole damn philosophical argument.\n\nThis is one of the first real challenges I tried as an aspiring rationalist, once upon a time.  One of the easier conundrums, relatively speaking.  May it serve you likewise."
          },
          "voteCount": 85
        },
        {
          "name": "Learned Blankness",
          "type": "post",
          "slug": "learned-blankness",
          "_id": "puhPJimawPuNZ5wAR",
          "url": null,
          "title": "Learned Blankness",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Identity"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Motivations"
            },
            {
              "name": "Cached Thoughts"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Notice your learned blankness",
                "anchor": "Notice_your_learned_blankness",
                "level": 1
              },
              {
                "title": "Reduce learned blankness",
                "anchor": "Reduce_learned_blankness",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "188 comments"
              }
            ],
            "headingsCount": 4
          },
          "contents": {
            "markdown": "Related to: [Semantic stopsigns](/lw/it/semantic_stopsigns/), [Truly part of you](/lw/la/truly_part_of_you/).\n\nOne day, the dishwasher broke. I asked Steve Rayhawk to look at it because he’s “good with mechanical things”.\n\n“The drain is clogged,” he said.\n\n“How do you know?” I asked.\n\nHe pointed at a pool of backed up water. “Because the water is backed up.”\n\nWe cleared the clog and the dishwasher started working.\n\nI felt silly, because I, too, could have reasoned that out.  The water wasn’t draining -- therefore, perhaps the drain was clogged.  Basic rationality in action.\\[1\\]\n\nBut before giving it even _ten seconds’_ thought, I’d classified the problem as a “mechanical thing”.  And I’d remembered I “didn’t know how mechanical things worked” (a [cached thought](/lw/k5/cached_thoughts/)).  And then -- prompted by my cached belief that there was a magical “way mechanical things work” that some knew and I didn’t -- I stopped trying to think at all.  \n\n“Mechanical things” was for me a mental [stopsign](/lw/it/semantic_stopsigns/) \\-\\- a blank domain that stayed blank, because I never asked the obvious next questions (questions like “does the dishwasher look unusual in any way?  Why is there water at the bottom?”).\n\nWhen I tutored math, new students acted as though the laws of exponents (or whatever we were learning) had fallen from the sky on stone tablets.  They clung rigidly to the handed-down procedures.  It didn’t occur to them to try to understand, or to improvise.  The students treated math the way I treated broken dishwashers.\n\nMartin Seligman coined the term \"[learned helplessness](http://en.wikipedia.org/wiki/Learned_helplessness)\" to describe a condition in which someone has learned to behave as though they were helpless. I think we need a term for learned helplessness about thinking (in a particular domain).  I’ll call this “learned blankness”\\[2\\].  Folks who fall prey to learned blankness may still take actions -- sometimes my students practiced the procedures again and again, hired a tutor, etc.  But they do so as though carrying out rituals to an unknown god -- parts of them may be trying, but their “understand X” center has given up.\n\nTo avoid misunderstanding: calling a plumber, and realizing he knows more than you do, can be good.  The thing to avoid is mentally walling off your own impressions; keeping parts of your map blank, because you imagine either that the domain itself is [chaotic](/lw/wb/chaotic_inversion/), or that one needs some [special skillset](/lw/qs/einsteins_superpowers/) to reason about \\*that\\*.\n\nNotice your learned blankness\n-----------------------------\n\nLearned blankness is common.  My guess is that most of us treat _most_ of our environment as blank givens inaccessible to reason\\[3\\]. To spot it in yourself, try comparing yourself to the following examples:\n\n1\\.  Sandra runs helpless to her roommate when her computer breaks -- she isn’t “good with computers”.  Her roommate, by contrast, clicks on one thing and then another, doing Google searches and puzzling it out.\\[4\\]\n\n2\\.  Most scientists know the scientific method is good (and that e.g. p-values of 0.05 are good).  But many not only [don’t understand](/lw/gv/outside_the_laboratory/) why the scientific method (or these p-values) are good -- they don’t understand that it’s the sort of thing one _could_ understand.  \n\n3\\.  Many respond to questions about consciousness, morality, or [God](/lw/i8/religions_claim_to_be_nondisprovable/) by expecting that some other, [special](/lw/iu/mysterious_answers_to_mysterious_questions/) kind of reasoning is needed, and, thus, walling off and distrusting their own impressions.  \n\n4\\.  Fred finds he has an intuition about how serious nano risks are.  His intuition is a blank for him; something he can act on or ignore, but not examine.  It doesn’t occur to him that he could examine the causes of his intuition\\[5\\], or could examine the accuracy rate of similar intuitions.\n\n5\\.  I find it hard to fully try to write fiction -- though a drink of alcohol helps.  The trouble is that since I’m unskilled at fiction-writing, and since I find it painful to [notice](/lw/21b/ugh_fields/) my un-skill, [most of my mind](/lw/2q6/compartmentalization_in_epistemic_and/) prefers to either not write at all, or to write half-heartedly, picking at the page without \\*really\\* trying.  Similarly, many pure math specialists avoid seriously trying their hand at philosophy, social science, or other “messy” areas.\n\n6\\.  Bob feels a vague desire to \"win\" at life, and a vague dissatisfaction with his current trajectory.  But he's never tried to [write down what he means by \"win\"](/lw/25d/too_busy_to_think_about_life/), or what he needs to change to achieve it.  He doesn't even realize that he _could_.\n\n7\\.  Sandra just doesn’t think about much of anything.  She drives to work in a car that works by magic, sits down in her cubicle at a company that makes profits by magic, and thinks through her actual coding work.  Then she orders some lunch that she magically likes, chats with coworkers via magically habitual chatting-patterns, does another four hours’ work, and drives home to a relationship that is magically succeeding or failing.\n\nI’m not saying we should constantly re-examine everything. Directed attention, and a focus on your day’s work, is useful. But the “learned blankness” I’m discussing is not goal-oriented.  Learned blankness means not just choosing to ignore a domain, but viewing that domain as inaccessible; it means being [alienated](/lw/2q6/compartmentalization_in_epistemic_and/) from the parts of your mind that could otherwise understand the thing.\n\nAnalogously, there are often good reasons not to e.g. seek a new job, skillset, or romantic partner... but one usually shouldn’t be in the depression-like state of learned helplessness about doing so.\n\nReduce learned blankness\n------------------------\n\nThere are many reasons folks feel helpless about understanding a given topic, including:\n\n*   A.  Simple habit. You aren’t used to thinking about it; and so you just automatically don’t.\n\n*   B.  Desire to avoid initial blunders that will force you to emotionally confront potential incompetence (as with my fear of writing fiction);\n\n*   C.  Avoidance of social conflict, or of status-claims; if your boss/spouse/whoever will be upset by your disagreement, it may be more comfortable to “not understand” the domain.\n\nSo, if you’d like to reduce your learned blankness, try to notice areas you care about, that you’ve been treating as blank defaults.  Then, seed some thoughts in that area: set a ten minute timer, and write as many questions as you can about that topic before it beeps.  Better yet: hang out with some people for whom the area _isn't_ blank.  [Do some mundane tasks that are new to you](/lw/58g/levels_of_action/), so that more of your world is filled in.  Ask [what subskills](/lw/58m/build_small_skills_in_the_right_order/) can give you stepping-stones.\n\nIf fears such as (B) and (C) pop up, try asking “I wonder what it would take to \\[hit my goals\\]?”.  Like: “I wonder what it would take to feel comfortable dancing?” or “I wonder what it would take write fiction without fear?”.  \n\nYou don’t even have to try answering the question; if it’s a topic you’ve feared, just asking it will open up space in your mind. Then, look up the answers on Google or Wikipedia or How.com and experience the pleasure of gaining competence.\n\n* * *\n\n\\[1\\] Richard Feynman, as a kid, surprised people because he could “[fix radios by thinking](http://books.google.com/books?id=7papZR4oVssC&pg=PA20&lpg=PA20&dq=fix+radios+thinking&source=bl&ots=esNY8elP_U&sig=m7miPOIM83Ep_fdVvXBJze3aFyk&hl=en&ei=koesTYSWJIfliAKpiM3vDA&sa=X&oi=book_result&ct=result&resnum=7&ved=0CDcQ6AEwBg#v=onepage&q=fix%20radios%20thinking&f=false)”; apparently it's common to not-notice that reasoning works on machines.\n\n\\[2\\] Thanks to Steve Rayhawk for suggesting this term.  Also, thanks to Lukeprog for helping me write this post.\n\n\\[3\\] [Eliezer’s Harry Potter](http://www.fanfiction.net/s/5782108/1/Harry_Potter_and_the_Methods_of_Rationality) [suggests](http://www.fanfiction.net/s/5782108/69/Harry_Potter_and_the_Methods_of_Rationality) that \\*not\\* having learned blankness be pervasive -- not having your world be tiny tunnels of thought, surrounded by large swaths of blankness that you leave alone -- is what it takes to be a “hero”.  To quote:\n\n> \"Ah...\" Harry said. His fork and knife nervously sawed at a piece of steak, cutting it into tinier and tinier pieces. \"I think a lot of people can do things when the world channels them into it... like people are expecting you to do it, or it only uses skills you already know, or there's an authority watching to catch your mistakes and make sure you do your part. But problems like that are probably already being solved, you know, and then there's no need for heroes. So I think the people we call 'heroes' are rare because they've got to make everything up as they go along, and most people aren't comfortable with that.”\n\n\\[4\\] Thanks to Zack Davis for noting that the “good with computers” trait seems to be substantially about the willingness to play around and figure things out.  If you’d like to reduce the amount of cached blankness in your life, and you’re not already good with computers, acquiring the “good with computers” trait in Zack’s sense is an easy place to start.\n\n\\[5\\] One way to get at the causes of an intuition is to imagine alternate scenarios and see how your intuition changes.  Fred might ask himself: \"Suppose nanotech was developed via a Manhattan project.  How much doom would I expect then?\" or \"Suppose John (who I learned all this from) changed his mind about doom probabilities.  Would that shift my views?\"."
          },
          "voteCount": 171
        },
        {
          "name": "Reason as memetic immune disorder",
          "type": "post",
          "slug": "reason-as-memetic-immune-disorder",
          "_id": "aHaqgTNnFzD7NGLMx",
          "url": null,
          "title": "Reason as memetic immune disorder",
          "author": "PhilGoetz",
          "question": false,
          "tags": [
            {
              "name": "Memetic Immune System"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Pitfalls of Rationality"
            },
            {
              "name": "Cultural knowledge"
            },
            {
              "name": "Valley of Bad Rationality"
            },
            {
              "name": "Memetics"
            },
            {
              "name": "Compartmentalization"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "A prophet is without dishonor in his hometown",
                "anchor": "A_prophet_is_without_dishonor_in_his_hometown",
                "level": 1
              },
              {
                "title": "Cultural immunity",
                "anchor": "Cultural_immunity",
                "level": 1
              },
              {
                "title": "Reason as immune suppression",
                "anchor": "Reason_as_immune_suppression",
                "level": 1
              },
              {
                "title": "The vaccines: Updating and emotions",
                "anchor": "The_vaccines__Updating_and_emotions",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "181 comments"
              }
            ],
            "headingsCount": 6
          },
          "contents": {
            "markdown": "#### A prophet is without dishonor in his hometown  \n\nI'm reading the book \"[The Year of Living Biblically](http://www.ajjacobs.com/books/yolb.asp),\" by A.J. Jacobs.  He tried to follow all of the commandments in the Bible (Old and New Testaments) for one year.  He quickly found that\n\n*   a lot of the rules in the Bible are impossible, illegal, or embarassing to follow nowadays; like wearing tassels, tying your money to yourself, stoning adulterers, not eating fruit from a tree less than 5 years old, and not touching anything that a menstruating woman has touched; and\n*   this didn't seem to bother more than a handful of the one-third to one-half of Americans who claim the Bible is the word of God.\n\nYou may have noticed that people who convert to religion after the age of 20 or so are generally more zealous than people who grew up with the same religion.  People who grow up with a religion learn how to cope with its more inconvenient parts by partitioning them off, rationalizing them away, or forgetting about them.  Religious communities actually protect their members from religion in one sense - they develop an unspoken consensus on which parts of their religion members can legitimately ignore.  New converts sometimes try to actually do what their religion tells them to do.\n\nI remember many times growing up when missionaries described the crazy things their new converts in remote areas did on reading the Bible for the first time - they refused to be taught by female missionaries; they insisted on following Old Testament commandments; they decided that everyone in the village had to confess all of their sins against everyone else in the village; they prayed to God and assumed He would do what they asked; they believed the Christian God would cure their diseases.  We would always laugh a little at the naivete of these new converts; I could barely hear the tiny voice in my head saying _but they're just believing that the Bible means what it says..._\n\nHow do we explain the blindness of people to a religion they grew up with?\n\n#### Cultural immunity  \n\nEurope has lived with Christianity for nearly 2000 years.  European culture has co-evolved with Christianity.  Culturally, memetically, it's developed a tolerance for Christianity.  These new Christian converts, in Uganda, Papua New Guinea, and other remote parts of the world, were being exposed to Christian memes for the first time, and had no immunity to them.\n\nThe history of religions sometimes resembles the history of viruses.  Judaism and Islam were both highly virulent when they first broke out, driving the first generations of their people to conquer (Islam) or just slaughter (Judaism) everyone around them for the sin of not being them.  They both grew more sedate over time.  (Christianity was pacifist at the start, as it arose in a conquered people.  When the Romans adopted it, it didn't make them any more militaristic than they already were.)\n\nThe mechanism isn't the same as for diseases, which can't be too virulent or they kill their hosts.  Religions don't generally kill their hosts.  I suspect that, over time, individual selection favors those who are less zealous.  The point is that a culture develops antibodies for the particular religions it co-exists with - attitudes and practices that make them less virulent.\n\nI have a theory that \"radical Islam\" is not native Islam, but Westernized Islam.  Over half of 75 Muslim terrorists studied by [Bergen & Pandey 2005 in the New York Times](http://www.nytimes.com/2005/06/14/opinion/14bergen.html) had gone to a Western college.  (Only 9% had attended madrassas.)  A very small percentage of all Muslims have received a Western college education.   When someone lives all their life in a Muslim country, they're not likely to be hit with the urge to travel abroad and blow something up.  But when someone from an Islamic nation goes to Europe for college, and comes back with Enlightenment ideas about reason and seeking logical closure over beliefs, and applies them to the Koran, _then_ you have troubles.  They have lost their cultural immunity.\n\nI'm also reminded of a talk I attended by one of the Dalai Lama's assistants.  This was not slick, Westernized Buddhism; this was saffron-robed fresh-off-the-plane-from-Tibet Buddhism.  He spoke about his beliefs, and then took questions.  People began asking him about some of the implications of his belief that life, love, feelings, and the universe as a whole are inherently bad and undesirable.  He had great difficulty comprehending the questions - not because of his English, I think; but because the notion of taking a belief expressed in one context, and applying it in another, seemed completely new to him.  To him, knowledge came in units; each unit of knowledge was a story with a conclusion and a specific application.  (No wonder they think understanding Buddhism takes decades.)  He seemed not to have the idea that these units could interact; that you could take an idea from one setting, and explore its implications in completely different settings.  This may have been an extreme form of cultural immunity.\n\nWe think of Buddhism as a peaceful, caring religion.  A religion that teaches that striving and status are useless is probably going to be more peaceful than one that teaches that the whole world must be brought under its dominion; and religions that lack the power of the state (e.g., the early Christians) are usually gentler than those with the power of life and death.  But much of Buddhism's kind public face may be due to cultural norms that prevent Buddhists from connecting all of their dots.  Today, we worry about Islamic terrorists.  A hundred years from now, we'll worry about Buddhist physicists.\n\n#### Reason as immune suppression\n\nThe reason I bring this up is that intelligent people sometimes do things more stupid than stupid people are capable of.  There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes.  The trouble is that these antibodies are not logical.  On the contrary; these antibodies are often highly _illogical_.  They are the blind spots that let us live with a dangerous meme without being impelled to action by it.  The dangerous effects of these memes are most obvious with religion; but I think there is an element of this in many social norms.  We have a powerful cultural norm in America that says that all people are equal (whatever that means); originally, this powerful and ambiguous belief was counterbalanced by a set of blind spots so large that this belief did not even impel us to free slaves or let women or non-property-owners vote.  We have another cultural norm that says that hard work reliably and exclusively leads to success; and another set of blind spots that prevent this belief from turning us all into Objectivists.\n\nA little reason can be a dangerous thing.  The landscape of rationality is not smooth; there is no guarantee that removing one false belief will improve your reasoning instead of degrading it.  Sometimes, reason lets us see the dangerous aspects of our memes, but not the blind spots that protect us from them.  Sometimes, it lets us see the blind spots, but not the dangerous memes.  Either of these ways, reason can lead an individual to be unbalanced, no longer adapted to their memetic environment, and free to follow previously-dormant memes through to their logical conclusions.    (To paraphrase Steve Weinberg, \"For a smart person to do something truly stupid, they need a theory.\"  Actually, I could have quoted him directly - \"stupid\" is just a lighter shade of \"evil\".  Communism and fascism both begin by exercising complete control over the memetic environment, in order to create a new man stripped of cultural immunity, who will do whatever they tell him to.)\n\n#### The vaccines: Updating and emotions  \n\nHow can you tell when you have removed one set of blind spots from your reasoning without removing its counterbalances?  One heuristic to counter this loss of immunity, is to be very careful when you find yourself deviating from everyone around you.  I deviate from those around me all the time, so I admit I haven't found this heuristic to be very helpful.\n\nAnother heuristic is to listen to your feelings.  If your conclusions seem repulsive to you, you may have stripped yourself of cognitive immunity to something dangerous."
          },
          "voteCount": 344
        },
        {
          "name": "Making your explicit reasoning trustworthy",
          "type": "post",
          "slug": "making-your-explicit-reasoning-trustworthy",
          "_id": "m5AH78nscsGjMbBwv",
          "url": null,
          "title": "Making your explicit reasoning trustworthy ",
          "author": "AnnaSalamon",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Pitfalls of Rationality"
            },
            {
              "name": "Valley of Bad Rationality"
            },
            {
              "name": "Explicit Reasoning"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Explicit reasoning is often nuts",
                "anchor": "Explicit_reasoning_is_often_nuts",
                "level": 1
              },
              {
                "title": "Reasoning can be made less risky",
                "anchor": "Reasoning_can_be_made_less_risky",
                "level": 2
              },
              {
                "title": "Skills for safer reasoning",
                "anchor": "Skills_for_safer_reasoning",
                "level": 1
              },
              {
                "title": "Why it matters (again)",
                "anchor": "Why_it_matters__again_",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "96 comments"
              }
            ],
            "headingsCount": 6
          },
          "contents": {
            "markdown": "Or: “I don’t want to think about _that_! I might be left with _mistaken beliefs_!”\n\nRelated to: [Rationality as memetic immune disorder](/lw/18b/reason_as_memetic_immune_disorder/); [Incremental progress and the valley](/lw/7k/incremental_progress_and_the_valley/); [Egan's Law](http://wiki.lesswrong.com/wiki/Egan's_law).\n\n_tl;dr: Many of us hesitate to trust explicit reasoning because... we haven’t built the skills that make such reasoning trustworthy. Some simple strategies can help._\n\nMost of us are afraid to think fully about certain subjects.\n\nSometimes, we avert our eyes for fear of unpleasant conclusions. (“What if it’s my fault? What if I’m not good enough?”)\n\nBut other times, oddly enough, we avert our eyes for fear of _inaccurate_ conclusions.\\[1\\] People fear questioning their religion, lest they disbelieve and become damned. People fear questioning their “don't walk alone at night” safety strategy, lest they venture into danger. And I find I hesitate when pondering [Pascal’s wager](http://www.overcomingbias.com/2008/08/where-does-pasc.html), [infinite ethics](http://www.nickbostrom.com/ethics/infinite.pdf), the [Simulation argument](http://www.simulation-argument.com/simulation.html), and whether I’m a [Boltzmann brain](/lw/17d/forcing_anthropics_boltzmann_brains/)... because I’m afraid of losing my bearings, and believing mistaken things.\n\n[Ostrich Theory](http://2.bp.blogspot.com/_vWrx43Kdj4U/S8VVCyMj4DI/AAAAAAAAAH0/Tffuc96arP4/s1600/ostrich_head_in_ground_full.jpg), one might call it. Or I’m Already Right theory. The theory that we’re more likely to act sensibly if we _don’t_ think further, than if we _do_. Sometimes Ostrich Theories are unconsciously held; one just wordlessly backs away from certain thoughts. Other times full or partial Ostrich Theories are put forth explicitly, as in Phil Goetz’s [post](/lw/18b/reason_as_memetic_immune_disorder/), [this LW comment](/lw/2l6/taking_ideas_seriously/2fgm?c=1), discussions of Tetlock's \"[foxes vs hedgehogs](http://www.overcomingbias.com/2006/11/foxes_vs_hedgho.html)\" research, enjoinders to use \"[outside](http://wiki.lesswrong.com/wiki/Outside_view) [views](http://www.overcomingbias.com/2008/06/singularity-out.html)\", enjoinders not to second-guess expert systems, and [cautions for Christians against “clever arguments”](http://www.pbc.org/files/messages/5632/0162.html).\n\n**Explicit reasoning is often nuts**\n------------------------------------\n\nOstrich Theories sound implausible: why would _not_ thinking through an issue make our actions _better_? And yet examples abound of folks whose theories and theorizing (as contrasted with their habits, wordless intuitions, and unarticulated responses to social pressures or their own emotions) made significant chunks of their actions worse. Examples include, among many others:\n\n*   Most early Communists;\n*   [Ted Kaczynski](http://en.wikipedia.org/wiki/Ted_Kaczynski) (The Unabomber; an IQ 160 math PhD who wrote an interesting treatise about the human impacts of technology, and also murdered innocent people while accomplishing nothing);\n*   [Mitchell Heisman](http://www.huffingtonpost.com/2010/09/24/mitchell-heisman-suicide_n_738121.html);\n*   Folks who go to great lengths to keep kosher;\n*   Friends of mine who’ve gone to great lengths to be meticulously denotationally honest, including refusing jobs that required a government loyalty oath, and refusing to click on user agreements for videogames; and\n*   Many who’ve gone to war for the sake of religion, national identity, or many different far-mode ideals.\n\nIn fact, the examples of religion and war suggest that the trouble with, say, Kaczynski wasn’t that his beliefs were unusually crazy. The trouble was that his beliefs were an ordinary amount of crazy, and he was unusually prone to acting on his beliefs. If the average person started to actually act on their nominal, verbal, explicit beliefs, they, too, would in many cases look plumb nuts. For example, a Christian might give away all their possessions, rejoice at the death of their children in circumstances where they seem likely to have gone to heaven, and generally treat their chances of Heaven vs Hell as their top priority. Someone else might risk their life-savings betting on an [election outcome](http://www.intrade.com/) or business about which they were [“99% confident”](http://en.wikipedia.org/wiki/Overconfidence_effect).\n\nThat is: many peoples’ abstract reasoning is not up to the task of day to day decision-making. This doesn't impair folks' actions all that much, because peoples' abstract reasoning has little bearing on our actual actions. Mostly we [just find ourselves doing things](/lw/2p5/humans_are_not_automatically_strategic/) (out of habit, emotional inclination, or social copying) and [make up the reasons post-hoc](http://faculty.virginia.edu/haidtlab/articles/haidt.emotionaldog.manuscript.pdf). But when we do try to choose actions from theory, the results are far from reliably helpful -- and so many folks' early steps toward rationality go unrewarded.\n\nWe are left with two linked barriers to rationality: (1) nutty abstract reasoning; and (2) fears of reasoned nuttiness, and other failures to believe that thinking things through is actually helpful.\\[2\\]\n\n**Reasoning can be made less risky**\n\nMuch of this nuttiness is unnecessary. There are learnable skills that can both make our abstract reasoning more trustworthy and also make it easier for us to trust it.\n\nHere's the basic idea:\n\nIf you know the limitations of a pattern of reasoning, learning better what it says won’t hurt you. It’s like having a friend who’s often wrong. If you don’t know your friend’s limitations, his advice might harm you. But once you do know, you don’t have to gag him; you can listen to what he says, and then take it with a grain of salt.\\[3\\]\n\nReasoning is the meta-tool that lets us figure out what methods of inference are trustworthy where. Reason lets us look over the track records of our own explicit theorizing, outside experts' views, our near-mode intuitions, etc. and figure out which is how trustworthy in a given situation.\n\nIf we learn to use this meta-tool, we can walk into rationality without fear.\n\nSkills for safer reasoning\n--------------------------\n\n_1\\. Recognize implicit knowledge._\n\nRecognize when your habits, or outside customs, are likely to work better than your reasoned-from-scratch best guesses. Notice how different groups act and what results they get. Take pains to stay aware of your own anticipations, especially in cases where you have [explicit verbal models that might block your anticipations from view](/lw/dl/verbal_overshadowing_and_the_art_of_rationality/). And, by studying track records, get a sense of which prediction methods are trustworthy where.\n\nUse track records; don't assume that just because folks' _justifications_ are incoherent, the _actions they are justifying_ are foolish. But also don't assume that tradition is better than your models. Be empirical.\n\n_2\\. Plan for errors in your best-guess models._\n\nWe tend to be [overconfident](http://en.wikipedia.org/wiki/Overconfidence_effect) in our own beliefs, to [overestimate the probability of conjunctions](http://en.wikipedia.org/wiki/Conjunction_fallacy) (such as multi-part reasoning chains), and to [search preferentially for evidence that we’re right](http://en.wikipedia.org/wiki/Confirmation_bias). Put these facts together, and theories folks are \"almost certain\" of turn out to be wrong pretty often. Therefore:\n\n*   Make predictions from as many angles as possible, to build redundancy. Use multiple theoretical frameworks, multiple datasets, multiple experts, multiple disciplines.\n*   When some lines of argument point one way and some another, don't give up or take a vote. Instead, notice that you're confused, and (while guarding against confirmation bias!) seek follow-up information.\n*   Use your memories of past error to bring up honest curiosity and fear of error. Then, really search for evidence that you’re wrong, the same way you'd search if your life were being bet on someone _else's_ theory.\n*   Build safeguards, alternatives, and repurposable resources into your plans.\n\n_3\\. Beware rapid belief changes._\n\nSome people find their beliefs changing rapidly back and forth, based for example on the particular lines of argument they're currently pondering, or the beliefs of those they've recently read or talked to. Such fluctuations are generally bad news for both the accuracy of your beliefs and the usefulness of your actions. If this is your situation:\n\n*   Remember that accurate beliefs come from an even, long-term collection of all the available evidence, with no extra weight for arguments presently in front of one. Thus, they shouldn't fluctuate dramatically back and forth; you should never be able to predict which way your future probabilities will move.\n*   If you _can_ predict what you'll believe a few years from now, consider believing that already.\n*   Remember that if reading X-ist books will predictably move your beliefs toward X, and you know there are X-ist books out there, you should move your beliefs toward X already. Remember the [Conservation of Expected Evidence](/lw/ii/conservation_of_expected_evidence/) more generally.\n*   Consider what emotions are driving the rapid fluctuations. If you’re uncomfortable ever disagreeing with your interlocutors, build comfort with disagreement. If you're uncomfortable not knowing, so that you find yourself grasping for one framework after another, build your tolerance for ambiguity, complexity, and unknowns.\n\n_4\\. Update your near-mode anticipations, not just your far-mode beliefs._\n\nSometimes your far-mode is smart and you near-mode is stupid. For example, Yvain's rationalist [knows abstractly that there aren’t ghosts, but nevertheless fears them](/lw/1l/the_mystery_of_the_haunted_rationalist/). Other times, though, your near-mode is smart and your far-mode is stupid. You might “believe” in an afterlife but retain a concrete, near-mode fear of death. You might advocate Communism but have a sinking [feeling](http://www.sciencemag.org/cgi/content/abstract/275/5304/1293) in your stomach as you conduct your tour of Stalin’s Russia.\n\nThus: trust abstract reasoning or concrete anticipations in different situations, according to their strengths. But, whichever one you bet your actions on, keep the other one in view. Ask it what it expects and why it expects it. Show it why you disagree ([visualizing your evidence concretely](/lw/1r/striving_to_accept/), if you’re trying to talk to your wordless anticipations), and see if it finds your evidence convincing. Try to grow [all](/lw/2q6/compartmentalization_in_epistemic_and/) your cognitive subsystems, so as to form a whole mind.\n\n_5\\. Use raw motivation, emotion, and behavior to determine at least part of your priorities._\n\nOne of the commonest routes to theory-driven nuttiness is to take a [“goal”](/lw/kx/fake_selfishness/) that isn’t your goal. Thus, folks claim to care “above all else” about their selfish well-being, the abolition of suffering, an objective Morality discoverable by superintelligence, or average utilitarian happiness-sums. They then find themselves either [without motivation](http://wiki.lesswrong.com/wiki/Akrasia) to pursue “their goals”, or else pulled into chains of actions that they dread and do not want.\n\nConcrete local motivations are often embarrassing. For example, I find myself concretely motivated to “win” arguments, even though I'd think better of myself if I was driven by curiosity. But, like near-mode beliefs, concrete local motivations can act as a safeguard and an anchor. For example, if you become abstractly confused about meta-ethics, you'll [still have](/lw/sk/changing_your_metaethics/) a concrete desire to pull babies off train tracks. And so dialoguing with your near-mode wants and motives, like your near-mode anticipations, can help build a robust, trust-worthy mind.\n\n**Why it matters (again)**\n--------------------------\n\nSafety skills such as the above are worth learning for three reasons.\n\n1.  They help us avoid nutty actions.\n2.  They help us reason unhesitatingly, instead of flinching away out of fear.\n3.  They help us build a rationality for the whole mind, with the strengths of near-mode as well as of abstract reasoning.\n\n* * *\n\n\\[1\\] These are not the only reasons people fear thinking. At minimum, there is also:\n\n*   Fear of social censure for the new beliefs (e.g., for changing your politics, or failing to believe your friend was justified in his divorce);\n*   Fear that part of you will use those new beliefs to justify actions that you as a whole do not want (e.g., you may fear to read a study about upsides of nicotine, lest you use it as a rationalization to start smoking again; you may similarly fear to read a study about how easily you can save African lives, lest it ends up prompting you to donate money).\n\n\\[2\\] Many points in this article, and especially in the \"explicit reasoning is often nuts\" section, are stolen from Michael Vassar. Give him the credit, and me the blame and the upvotes.\n\n\\[3\\] [Carl](/user/CarlShulman) points out that Eliezer points out that [studies show we can't](/lw/k4/do_we_believe_everything_were_told/). But it seems like explicitly modeling when your friend is and isn't accurate, and when explicit models have and haven't led you to good actions, should at least help."
          },
          "voteCount": 93
        },
        {
          "name": "Confidence levels inside and outside an argument",
          "type": "post",
          "slug": "confidence-levels-inside-and-outside-an-argument",
          "_id": "GrtbTAPfkJa4D6jjH",
          "url": null,
          "title": "Confidence levels inside and outside an argument",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Probability & Statistics"
            },
            {
              "name": "Forecasting & Prediction"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Inside, Outside View"
            },
            {
              "name": "Distinctions"
            },
            {
              "name": "Explicit Reasoning"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [Infinite Certainty](/lw/mo/infinite_certainty/)\n\nSuppose the people at [FiveThirtyEight](http://fivethirtyeight.blogs.nytimes.com/) have created a model to predict the results of an important election. After crunching poll data, area demographics, and all the usual things one crunches in such a situation, their model returns a greater than 999,999,999 in a billion chance that the incumbent wins the election. Suppose further that the results of this model are your only data and you know nothing else about the election. What is your confidence level that the incumbent wins the election?  \n  \nMine would be significantly less than 999,999,999 in a billion.\n\nWhen an argument gives a probability of 999,999,999 in a billion for an event, then probably the majority of the probability of the event is no longer in \"But that still leaves a one in a billion chance, right?\". The majority of the probability is in \"That argument is flawed\". Even if you have no particular reason to believe the argument is flawed, the background chance of an argument being flawed is still greater than one in a billion.\n\n  \nMore than one in a billion times a political scientist writes a model, ey will get completely confused and write something with no relation to reality. More than one in a billion times a programmer writes a program to crunch political statistics, there will be a bug that completely invalidates the results. More than one in a billion times a staffer at a website publishes the results of a political calculation online, ey will accidentally switch which candidate goes with which chance of winning.  \n  \nSo one must distinguish between levels of confidence internal and external to a specific model or argument. Here the model's internal level of confidence is 999,999,999/billion. But my external level of confidence should be lower, even if the model is my only evidence, by an amount proportional to my trust in the model.\n\n  \n  \n**Is That Really True?**\n\nOne might be tempted to respond \"But there's an equal chance that the false model is too high, versus that it is too low.\" Maybe there was a bug in the computer program, but it prevented it from giving the incumbent's real chances of 999,999,999,999 out of a _trillion_.  \n  \nThe prior probability of a candidate winning an election is 50%^1^. We need information to push us away from this probability in either direction. To push significantly away from this probability, we need strong information. Any weakness in the information weakens its ability to push away from the prior. If there's a flaw in FiveThirtyEight's model, that takes us away from their probability of 999,999,999 in of a billion, and back closer to the prior probability of 50%  \n  \nWe can confirm this with a quick sanity check. Suppose we know nothing about the election (ie we still think it's 50-50) until an insane person reports a hallucination that an angel has declared the incumbent to have a 999,999,999/billion chance. We would not be tempted to accept this figure on the grounds that it is equally likely to be too high as too low.  \n  \nA second objection covers situations such as a lottery. I would like to say the chance that Bob wins a lottery with one billion players is 1/1 billion. Do I have to adjust this upward to cover the possibility that my model for how lotteries work is somehow flawed? No. Even if I am misunderstanding the lottery, I have not departed from my prior. Here, new information really does have an equal chance of going against Bob as of going in his favor. For example, the lottery may be fixed (meaning my original model of how to determine lottery winners is fatally flawed), but there is no greater reason to believe it is fixed in favor of Bob than anyone else.^2^  \n  \n**Spotted in the Wild**  \n  \nThe recent Pascal's Mugging thread spawned a discussion of the Large Hadron Collider destroying the universe, which also got continued on an older LHC thread from a few years ago. Everyone involved agreed the chances of the LHC destroying the world were less than one in a million, but several people gave extraordinarily low chances based on cosmic ray collisions. The argument was that since cosmic rays have been performing particle collisions similar to the LHC's zillions of times per year, the chance that the LHC will destroy the world is either literally zero, or else a number related to the probability that there's some chance of a cosmic ray destroying the world so miniscule that it hasn't gotten actualized in zillions of cosmic ray collisions. Of the commenters mentioning this argument, one gave a probability of 1/3*10^22, another suggested 1/10^25, both of which may be good numbers for the internal confidence of this argument.  \n  \nBut the connection between this argument and the general LHC argument flows through statements like \"collisions produced by cosmic rays will be exactly like those produced by the LHC\", \"our understanding of the properties of cosmic rays is largely correct\", and \"I'm not high on drugs right now, staring at a package of M&Ms and mistaking it for a really intelligent argument that bears on the LHC question\", all of which are probably more likely than 1/10^20. So instead of saying \"the probability of an LHC apocalypse is now 1/10^20\", say \"I have an argument that has an internal probability of an LHC apocalypse as 1/10^20, which lowers my probability a bit depending on how much I trust that argument\".  \n  \nIn fact, the argument has a potential flaw: according to Giddings and Mangano, the physicists officially tasked with investigating LHC risks, black holes from cosmic rays [might have enough momentum](http://arxiv.org/ftp/arxiv/papers/0912/0912.5480.pdf) to fly through Earth without harming it, and black holes from the LHC might not^3^. This was predictable: this was a simple argument in a complex area trying to prove a negative, and it would have been presumptous to believe with greater than 99% probability that it was flawless. If you can only give 99% probability to the argument being sound, then it can only reduce your probability in the conclusion by a factor of a hundred, not a factor of 10^20.  \n  \nBut it's hard for me to be properly outraged about this, since the LHC did not destroy the world. A better example might be the following, taken from an online [discussion of creationism](http://www.sciforums.com/Scientific-Reasons-for-God-t-44465.html)^4^ and apparently based off of something by Fred Hoyle:\n\n> In order for a single cell to live, all of the parts of the cell must be assembled before life starts. This involves 60,000 proteins that are assembled in roughly 100 different combinations. The probability that these complex groupings of proteins could have happened just by chance is extremely small. It is about 1 chance in 10 to the 4,478,296 power. The probability of a living cell being assembled just by chance is so small, that you may as well consider it to be impossible. This means that the probability that the living cell is created by an intelligent creator, that designed it, is extremely large. The probability that God created the living cell is 10 to the 4,478,296 power to 1.\n\nNote that someone just gave a confidence level of 10^4478296 to one and was wrong. This is the sort of thing that should _never ever happen_. This is possibly the _most wrong anyone has ever been_.  \n  \nIt is hard to say in words exactly how wrong this is. Saying \"This person would be willing to bet the entire world GDP for a thousand years if evolution were true against a one in one million chance of receiving a single penny if creationism were true\" doesn't even begin to cover it: a mere 1/10^25 would suffice there. Saying \"This person believes he could make one statement about an issue as difficult as the origin of cellular life per Planck interval, every Planck interval from the Big Bang to the present day, and not be wrong even once\" only brings us to 1/10^61 or so. If the chance of getting [Ganser's Syndrome](http://en.wikipedia.org/wiki/Ganser%27s_syndrome), the extraordinarily rare psychiatric condition that manifests in a compulsion to say false statements, is one in a hundred million, and the world's top hundred thousand biologists all agree that evolution is true, then this person should preferentially believe it is more likely that all hundred thousand have simultaneously come down with Ganser's Syndrome than that they are doing good biology^5^  \n  \nThis creationist's flaw wasn't mathematical; the math probably does return that number. The flaw was confusing the internal probability (that complex life would form completely at random in a way that can be represented with this particular algorithm) with the external probability (that life could form without God). He should have added a term representing the chance that his knockdown argument just didn't apply.  \n  \nFinally, consider the question of whether you can assign 100% certainty to a mathematical theorem for which a proof exists. Eliezer [has already examined this issue](/lw/mo/infinite_certainty/ ) and come out against it (citing as an example [this story of Peter de Blanc's](http://www.spaceandgames.com/?p=27)). In fact, this is just the specific case of differentiating internal versus external probability when internal probability is equal to 100%. Now your probability that the theorem is false is entirely based on the probability that you've made some mistake.  \n  \nThe many [mathematical proofs](http://mathoverflow.net/questions/35468) [that were later overturned](http://en.wikipedia.org/wiki/List_of_published_incomplete_proofs ) provide practical justification for this mindset.  \n  \nThis is not a fully general argument against giving very high levels of confidence: very complex situations and situations with many exclusive possible outcomes (like the lottery example) may still make it to the 1/10^20 level, albeit probably not the 1/10^4478296. But in other sorts of cases, giving a very high level of confidence requires a check that you're not confusing the probability inside one argument with the probability of the question as a whole.\n\n**Footnotes**\n\n**1.** Although technically we know we're talking about an incumbent, who typically has a much higher chance, around 90% in Congress.  \n  \n**2.** A particularly devious objection might be \"What if the lottery commissioner, in a fit of political correctness, decides that \"everyone is a winner\" and splits the jackpot a billion ways? If this would satisfy your criteria for \"winning the lottery\", then this mere possibility should indeed move your probability upward. In fact, since there is probably greater than a one in one billion chance of this happening, the majority of your probability for Bob winning the lottery should concentrate here!  \n  \n**3.** Giddings and Mangano then go on to re-prove the original \"won't cause an apocalypse\" argument using a more complicated method involving white dwarf stars.  \n  \n**4.** While searching creationist websites for the half-remembered argument I was looking for, I [found](http://www.christiananswers.net/q-eden/origin-of-life-ref.html) what may be my new favorite quote: \"Mathematicians generally agree that, statistically, any odds beyond 1 in 10 to the 50th have a zero probability of ever happening.\"   \n  \n**5.** I'm a little worried that five years from now I'll see this quoted on some creationist website as an actual argument."
          },
          "voteCount": 174
        },
        {
          "name": "Truly Part Of You",
          "type": "post",
          "slug": "truly-part-of-you",
          "_id": "fg9fXrHpeaDD6pEPL",
          "url": null,
          "title": "Truly Part Of You",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "AI"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Techniques"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "A classic paper by Drew McDermott, “Artificial Intelligence Meets Natural Stupidity,” criticized AI programs that would try to represent notions like *happiness is a state of mind* using a semantic network:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1586123740/Screen_Shot_2020-04-05_at_2.55.20_PM_gfaghl.png)\n\nAnd of course there’s nothing *inside* the HAPPINESS node; it’s just a naked LISP token with a suggestive English name.\n\nSo, McDermott says, “A good test for the disciplined programmer is to try using gensyms in key places and see if he still admires his system. For example, if STATE-OF-MIND is renamed G1073. . .” then we would have IS-A(HAPPINESS, G1073) “which looks much more dubious.”\n\nOr as I would slightly rephrase the idea: If you substituted randomized symbols for *all* the suggestive English names, you would be completely unable to figure out what G1071(G1072, G1073) meant. Was the AI program meant to represent hamburgers? Apples? Happiness? Who knows? *If you delete the suggestive English names, they* *don’t grow back.*\n\nSuppose a physicist tells you that “Light is waves,” and you *believe* the physicist. You now have a little network in your head that says:\n\nIS-A(LIGHT, WAVES)\n\nAs McDermott says, “The whole problem is getting the hearer to notice what it has been told. Not ‘understand,’ but ‘notice.’ ” Suppose that instead the physicist told you, “Light is made of little curvy things.”^1^ Would you *notice* any difference of anticipated experience?\n\nHow can you realize that you shouldn’t trust your seeming knowledge that “light is waves”? One test you could apply is asking, “Could I *regenerate* his knowledge if it were somehow deleted from my mind?”\n\nThis is similar in spirit to scrambling the names of suggestively named lisp tokens in your AI program, and seeing if someone else can figure out what they allegedly “refer” to. It’s also similar in spirit to observing that an [Artificial Arithmetician](https://lesswrong.com/rationality/artificial-addition) programmed to record and play back\n\nPlus-Of(Seven, Six) = Thirteen\n\ncan’t regenerate the knowledge if you delete it from memory, until another human re-enters it in the database. Just as if you forgot that “light is waves,” you couldn’t get back the knowledge except the same way you got the knowledge to begin with—by asking a physicist. You couldn’t generate the knowledge for yourself, the way that physicists originally generated it.\n\nThe same experiences that lead us to formulate a belief, connect that belief to other knowledge and sensory input and motor output. If you see a beaver chewing a log, then you know what this thing-that-chews-through-logs looks like, and you will be able to recognize it on future occasions whether it is called a “beaver” or not. But if you acquire your beliefs about beavers by someone else telling you facts about “beavers,” you may not be able to recognize a beaver when you see one.\n\nThis is the terrible danger of trying to *tell* an artificial intelligence facts that it could not learn for itself. It is also the terrible danger of trying to *tell* someone about physics that they cannot verify for themselves. For what physicists mean by “wave” is not “little squiggly thing” but a purely mathematical concept.\n\nAs Donald Davidson observes, if you believe that “beavers” live in deserts, are pure white in color, and weigh 300 pounds when adult, then you do not have any beliefs *about* beavers, true or false. Your belief about “beavers” is not right enough to be wrong.^2^ If you don’t have enough experience to regenerate beliefs when they are deleted, then do you have enough experience to connect that belief to anything at all? Wittgenstein: “A wheel that can be turned though nothing else moves with it, is not part of the mechanism.”\n\nAlmost as soon as I started reading about AI—even before I read McDermott—I realized it would be *a really good idea* to always ask myself: “How would I regenerate this knowledge if it were deleted from my mind?”\n\nThe deeper the deletion, the stricter the test. If all proofs of the Pythagorean Theorem were deleted from my mind, could I re-prove it? I think so. If all knowledge of the Pythagorean Theorem were deleted from my mind, would I notice the Pythagorean Theorem to re-prove? That’s harder to boast, without putting it to the test; but if you handed me a right triangle with sides of length 3 and 4, and told me that the length of the hypotenuse was calculable, I think I would be able to calculate it, if I still knew all the rest of my math.\n\nWhat about the notion of *mathematical proof*? If no one had ever told it to me, would I be able to reinvent *that* on the basis of other beliefs I possess? There was a time when humanity did not have such a concept. Someone must have invented it. What was it that they noticed? Would I notice if I saw something equally novel and equally important? Would I be able to think that far outside the box?\n\nHow much of your knowledge could you regenerate? From how deep a deletion? It’s not just a test to cast out insufficiently connected beliefs. It’s a way of absorbing *a fountain of knowledge, not just one fact*.\n\nA shepherd builds a counting system that works by throwing a pebble into a bucket whenever a sheep leaves the fold, and taking a pebble out whenever a sheep returns. If you, the apprentice, do not understand this system—if it is magic that works for no apparent reason—then you will not know what to do if you accidentally drop an extra pebble into the bucket. That which you cannot make yourself, you cannot *remake* when the situation calls for it. You cannot go back to the source, tweak one of the parameter settings, and regenerate the output, without the source. If “two plus four equals six” is a brute fact unto you, and then one of the elements changes to “five,” how are you to know that “two plus five equals seven” when you were simply *told* that “two plus four equals six”?\n\nIf you see a small plant that drops a seed whenever a bird passes it, it will not occur to you that you can use this plant to partially automate the sheep-counter. Though you learned something that the original maker would use to improve on their invention, you can’t go back to the source and re-create it.\n\nWhen you contain the source of a thought, that thought can change along with you as you acquire new knowledge and new skills. When you contain the source of a thought, it becomes truly a part of you and grows along with you.\n\nStrive to make yourself the source of every thought worth thinking. If the thought originally came from outside, make sure it comes from inside as well. Continually ask yourself: “How would I regenerate the thought if it were deleted?” When you have an answer, imagine *that* knowledge being deleted as well. And when you find a fountain, see what else it can pour.\n\n* * *\n\n^1^ Not true, by the way.\n\n^2^ Richard Rorty, “Out of the Matrix: How the Late Philosopher Donald Davidson Showed That Reality Can’t Be an Illusion,” *The Boston Globe*, 2003, [http://archive.boston.com/news/globe/ideas/articles/2003/10/05/out_ of_ the_ matrix/](http://archive.boston.com/news/globe/ideas/articles/2003/10/05/out_of_the_matrix/)."
          },
          "voteCount": 113
        },
        {
          "name": "The 5-Second Level",
          "type": "post",
          "slug": "the-5-second-level",
          "_id": "JcpzFpPBSmzuksmWM",
          "url": null,
          "title": "The 5-Second Level",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Noticing"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "To develop methods of teaching rationality skills, you need to learn to focus on mental events that occur in 5 seconds or less.  Most of what you want to teach is directly on this level; the rest consists of chaining together skills on this level.\n\nAs our first example, let's take the vital rationalist skill, \"Be specific.\"\n\nEven with people who've had moderate amounts of exposure to Less Wrong, a fair amount of my helping them think effectively often consists of my saying, \"Can you give me a specific example of that?\" or \"Can you be more concrete?\"\n\nA couple of formative childhood readings that taught me to be specific:\n\n> \"What is meant by the word _red?_\"  \n> \"It's a color.\"  \n> \"What's a color?\"  \n> \"Why, it's a quality things have.\"  \n> \"What's a _quality?_\"  \n> \"Say, what are you trying to do, anyway?\"\n> \n> You have pushed him into the clouds.  If, on the other hand, we habitually go _down_ the abstraction ladder to _lower_ levels of abstraction when we are asked the meaning of a word, we are less likely to get lost in verbal mazes; we will tend to \"have our feet on the ground\" and know what we are talking about.  This habit displays itself in an answer such as this:\n> \n> \"What is meant by the word _red?_\"  \n> \"Well, the next time you see some cars stopped at an intersection, look at the traffic light facing them.  Also, you might go to the fire department and see how their trucks are painted.\"\n> \n> \\-\\- S. I. Hayakawa, [_Language in Thought and Action_](http://books.google.com/books?id=0H1p2sMdyXEC&pg=PA88&lpg=PA88&dq=%22it%27s+a+quality+things+have%22&source=bl&ots=e-xeTes-ey&sig=CRIaG7PXpMZoC6hjqIf7ZqM1tzM&hl=en&ei=KM3ETc7QJZOWsAP56-mmCA&sa=X&oi=book_result&ct=result&resnum=1&ved=0CBYQ6AEwAA#v=onepage&q=%22it%27s%20a%20quality%20things%20have%22&f=false)\n\nand:\n\n> \"Beware, demon!\" he intoned hollowly.  \"I am not without defenses.\"  \n> \"Oh yeah?  Name three.\"\n> \n> \\-\\- Robert Asprin, _Another Fine Myth_\n\nAnd now, no sooner does someone tell me that they want to \"facilitate communications between managers and employees\" than I say, \"Can you give me a concrete example of how you would do that?\"  Hayakawa taught me to distinguish the concrete and the abstract; and from that small passage in Asprin, I picked up the dreadful personal habit of calling people's bluffs, often using the specific phrase, \"Name three.\"\n\nBut the real subject of today's lesson is how to see skills like this on the 5-second level.  And now that we have a _specific example_ in hand, we can proceed to try to zoom in on the level of cognitive events that happen in 5 seconds or less.\n\nOver-abstraction happens because it's _easy_ to be abstract.  It's _easier_ to say \"red is a color\" than to pause your thoughts for long enough to come up with the example of a stop sign.  Abstraction is a path of least resistance, a form of mental laziness.\n\nSo the first thing that needs to happen on a timescale of 5 seconds is _perceptual recognition_ of highly abstract statements unaccompanied by concrete examples, accompanied by an _automatic aversion_, an ick reaction - this is the trigger which invokes the skill.\n\nThen, you have _actionable stored procedures_ that associate to the trigger.  And \"come up with a concrete example\" is not a 5-second-level skill, not an actionable procedure, it doesn't [transform the problem into a task](/lw/1ai/the_first_step_is_to_admit_that_you_have_a_problem/).  An actionable mental procedure that could be learned, stored, and associated with the trigger would be \"Search for a memory that instantiates the abstract statement\", or \"Try to come up with hypothetical examples, and then discard the lousy examples your imagination keeps suggesting, until you finally have a good example that really shows what you were originally trying to say\", or \"Ask why you were making the abstract statement in the first place, and recall the original mental causes of your making that statement to see if they suggest something more concrete.\"\n\nOr to be more specific on the last mental procedure:  Why were you _trying_ to describe redness to someone?  Did they just run a red traffic light?\n\n(And then what kind of exercise can you run someone through, which will get them to distinguish red traffic lights from green traffic lights?  What could teach someone to distinguish red from green?)\n\nWhen you ask how to teach a rationality skill, don't ask \"How can I teach people to be more specific?\"  Ask, \"What sort of exercise will lead people through the part of the skill where they perceptually recognize a statement as overly abstract?\"  Ask, \"What exercise teaches people to think about why they made the abstract statement in the first place?\"  Ask, \"What exercise could cause people to form, store, and associate with a trigger, a procedure for going through hypothetical examples until a good one or at least adequate one is invented?\"\n\nComing up with good ways to teach mental skills requires thinking on the 5-second level, because until you've reached that level of introspective concreteness, that fineness of granularity, you can't recognize the elements you're trying to teach; you can't recognize the patterns of thought you're trying to build inside a mind.\n\nTo come up with a 5-second description of a rationality skill, I would suggest zooming in on a concrete case of a real or hypothetical person who (a) fails in a typical fashion and (b) successfully applies the skill.  Break down their _internal experience_ into the smallest granules you can manage:  perceptual classifications, contexts that evoke emotions, fleeting choices made too quick for verbal consideration.  And then generalize what they're doing while _staying on the 5-second level_.\n\nStart with the concrete example of the person who starts to say \"Red is a color\" and cuts themselves off and says \"Red is what that stop sign and that fire engine have in common.\"  What did they do on the 5-second level?\n\n1.  Perceptually recognize a statement they made as overly abstract.\n2.  Feel the need for an accompanying concrete example.\n3.  Be sufficiently averse to the lack of such an example to avoid the path of least resistance where they just let themselves be lazy and abstract.\n4.  Associate to and activate a stored, actionable, procedural skill, e.g:  \n    4a.  Try to remember a memory which matches that abstract thing you just said.  \n    4b.  Try to invent a specific hypothetical scenario which matches that abstract thing you just said.  \n    4c.  Ask why you said the abstract thing in the first place and see if that suggests anything.\n\n_and_\n\n*   Before even 1:  They recognize that the notion of \"concrete\" means things like folding chairs, events like a young woman buying a vanilla ice cream, and the number 17, i.e. _specific enough to be visualized;_ and they know \"red is a color\" is _not_ specific enough to be satisfying.  They perceptually recognize (this is what Hayakawa was trying to teach) the cardinal directions \"more abstract\" and \"less abstract\" as they apply within the landscape of the mind.\n\nIf you are thinking on this level of granularity, then you're much more likely to come up with a good method for teaching the skill \"be specific\", because you'll know that whatever exercise you come up with, it ought to cause people's minds to go through events 1-4, and provide examples or feedback to train perception 0.\n\nNext example of thinking on the 5-second scale:  I previously asked some people (especially from the New York LW community) the question \"What makes rationalists fun to be around?\", i.e., why is it that once you try out being in a rationalist community you can't bear the thought of going back?  One of the primary qualities cited was \"Being non-judgmental.\"  Two different people came up with that exact phrase, but it struck me as being not _precisely_ the right description - rationalists go around judging and estimating and weighing things all the time.  (Noticing small discordances in an _important_ description, and reacting by trying to find an exact description, is another one of those 5-second skills.)  So I pondered, trying to come up with a _more specific image_ of _exactly what it was we weren't doing_, i.e. Being Specific, and after further visualization it occurred to me that a better description might be something like this:  If you are a fellow member of my rationalist community and you come up with a proposal that I disagree with - like \"We should all practice lying, so that we feel less pressure to believe things that sound good to endorse out loud\" - then I may argue with the proposal on consequentialist grounds.  I may _judge._  But I won't start saying in immense indignation what a terrible person you must be for suggesting it.\n\nNow I could try to verbally define exactly what it is we don't do, but this would fail to approach the 5-second level, and probably _also_ fail to get at the real quality that's important to rationalist communities.  That would merely be another attempt to legislate what people are or aren't allowed to say, and that would make things _less_ fun.  There'd be a new accusation to worry about if you said the wrong thing - \"Hey!  Good rationalists don't do that!\" followed by a debate that wouldn't be experienced as pleasant for anyone involved.\n\nIn this case I think it's actually _easier_ to define the thing-we-avoid on the 5-second level.  Person A says something that Person B disagrees with, and now in Person B's mind there's an option to go in the direction of a certain poisonous pleasure, an opportunity to experience an emotional burst of righteous indignation and a feeling of superiority, a chance to castigate the other person.  On the 5-second level, Person B rejects this temptation, and instead invokes the procedure of (a) pausing to reflect and then (b) talking about the consequences of A's proposed policy in a tone that might perhaps be _worried_ (for the way of rationality is not to refuse all emotion) but nonetheless is not filled with _righteous outrage and indignation which demands that all others share that indignation or be likewise castigated._\n\n(Which in practice, makes a really huge difference in how much rationalists can relax when they are around fellow rationalists.  It's the difference between having to carefully tiptoe through a minefield and being free to run and dance, knowing that even if you make a mistake, it won't socially kill you.  You're even allowed to say \"Oops\" and change your mind, if you want to backtrack (but that's a whole 'nother topic of 5-second skills)...)\n\nThe point of _5-second-level_ analysis is that to teach the _procedural habit_, you don't go into the evolutionary psychology of politics or the game theory of punishing non-punishers (by which the indignant demand that others agree with their indignation), which is unfortunately how I tended to write back when I was writing the original Less Wrong sequences.  Rather you try to come up with exercises which, if people go through them, causes them to experience the 5-second events - to feel the temptation to indignation, and to make the choice otherwise, and to associate alternative procedural patterns such as pausing, reflecting, and asking \"What is the evidence?\" or \"What are the consequences?\"\n\nWhat _would_ be an exercise which develops that habit?  I don't know, although it's worth noting that a lot of traditional rationalists not associated with LW also have this skill, and that it seems fairly learnable by osmosis from watching other people in the community not be indignant.  One method that seems worth testing would be to expose people to assertions that seem like obvious temptations to indignation, and get them to talk about evidence or consequences instead.  Say, you propose that eating one-month-old human babies ought to be legal, because one-month-old human babies aren't as intelligent as pigs, and we eat pigs.  Or you could start talking about feminism, in which case you can say pretty much anything and it's bound to offend someone.  (Did that last sentence offend you?  _Pause and reflect!_)  The point being, not to persuade anyone of anything, but to get them to introspectively recognize the moment of that choice between indignation and not-indignation, and walk them through an alternative response, so they store and associate that procedural skill.  The exercise might fail if the context of a school-exercise meant that the indignation never got started - if the temptation/choice were never experienced.  But we could _try_ that teaching method, at any rate.\n\n(There's this 5-second skill where you respond to mental uncertainty about whether or not something will work, by imagining _testing_ it; and if it looks like you can just go test something, then the thought occurs to you to just go test it.  To teach this skill, we might try showing people a list of hypotheses and asking them to _quickly_ say on a scale of 1-10 how easy they look to test, because we're trying to teach people a procedural habit of _perceptually_ considering the testableness of ideas.  You wouldn't give people lots of time to think, because then that teaches a procedure of _going through complex arguments about testability,_ which you _wouldn't_ use routinely in real life and would end up associating primarily to a school-context where a defensible verbal argument is expected.)\n\nI should mention, at this point, that learning to see the 5-second level draws heavily on the introspective skill of visualizing mental events in specific detail, and maintaining that introspective image in your mind's eye for long enough to reflect on it and analyze it.  This may take practice, so if you find that you can't do it right away, instinctively react by feeling that you need more practice to get to the lovely reward, instead of instinctively giving up.\n\nHas everyone learned from these examples a perceptual recognition of what the \"5-second level\" looks like?  Of course you have!  You've even installed a mental habit that when you or somebody else comes up with a supposedly 5-second-level description, you automatically inspect each part of the description to see if it contains any block units like \"Be specific\" which are actually high-level chunks.\n\nNow, as your exercise for learning the skill of \"Resolving cognitive events to the 5-second level\", take a rationalist skill you think is important (or pick a random LW post from [How To Actually Change Your Mind](http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind)); come up with a concrete example of that skill being used successfully; decompose that usage to a 5-second-level description of perceptual classifications and emotion-evoking contexts and associative triggers to actionable procedures etcetera; check your description to make sure that each part of it can be visualized as a concrete mental process and that there are no non-actionable abstract chunks; come up with a teaching exercise which seems like it ought to cause those sub-5-second events to occur in people's minds; and then post your analysis and proposed exercise in the comments.  Hope to hear from you soon!"
          },
          "voteCount": 167
        },
        {
          "name": "Ureshiku Naritai",
          "type": "post",
          "slug": "ureshiku-naritai",
          "_id": "xnPFYBuaGhpq869mY",
          "url": null,
          "title": "Ureshiku Naritai",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Emotions"
            },
            {
              "name": "Practical"
            },
            {
              "name": "Well-being"
            },
            {
              "name": "World Optimization"
            },
            {
              "name": "Happiness"
            },
            {
              "name": "Reset (technique)"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "This is a supplement to the [luminosity sequence](/lw/1xh/living_luminously/).  In [this comment](/lw/1za/the_spotlight/1t8q), I mentioned that I have raised my happiness set point (among other things), and this declaration was met with some interest.  Some of the details are lost to memory, but below, I reconstruct for your analysis what I can of the process.  It contains _lots of gooey self-disclosure_; skip if that's not your thing.\n\nIn summary: I decided that I had to and wanted to become happier; I re-labeled my moods and approached their management accordingly; and I consistently treated my mood maintenance and its support behaviors (including discovering new techniques) as immensely important.  The steps in more detail:\n\n1.  _I came to understand the **necessity** of becoming happier._  Being unhappy was not just unpleasant.  It was _dangerous_: I had a history of suicidal ideation.  This hadn't resulted in actual attempts at killing myself, largely because I attached hopes for improvement to concrete external milestones (various academic progressions) and therefore imagined myself a magical healing when I got the next diploma (the next one, the next one.)  Once I noticed I was doing that, it was unsustainable.  If I wanted to live, I had to find a safe emotional place on which to stand.  It had to be my top priority.  This required several sub-projects:\n\n*   I had to eliminate the baggage that told me it was _appropriate_ or _accurate_ to feel bad most of the time.  I endorse my ability to react emotionally to my environment; but this should be acute, not chronic.  Reacting emotionally is about feeling worse when things _get_ worse, not feeling bad when things are bad for months or years on end.  (Especially not when feeling bad reduces the ability to make things less bad.)  Further, having a lower set point did not affect my emotional _range_ except to shrink it; it reduced the possible impact of real grief, and wasn't compatible with the \"react emotionally\" plan.  The low set point also compromised my ability to react emotionally to _positive_ input, because it was attached to a systematic discounting of such positivity.\n*   I had to eliminate the baggage that told me it was not possible to _cognitively_ change my mood.  Moods correspond to thoughts, and while it can be hard to _avoid_ thinking about things, I can decide _to_ think about whatever I want.  A decade of assorted antidepressants had wreaked no discernible change on my affect, which constituted strong evidence that chemicals were not my problem.  And it was easy to see that my mood varied on a small scale with things under my complete or partial control, like sleep, diet, and activity.  It did not seem outrageous that long-term, large-scale interventions could have similar effects on my overall mood.\n*   I had to decide, and act on the decision, that my happiness was important and worth my time and attention.  I had to pay attention, and note what helped and what hurt.  I had to put increasing the helping factors and decreasing the hurting factors at the top of my list whenever it was remotely feasible, and relax my standards around \"remote feasibility\" to prevent self-sabotage.  And I had to commit to abandoning counterproductive projects or interactions, at least until I'd developed the stability to deal with the emotions they generated without suffering permanent setbacks.\n\n2.  _I **re-labeled** my moods, so that identifying them in the moment prompted the right actions._  When a given point on the unhappy-happy spectrum - let's call it \"2\" on a scale of 1 to 10 - was labeled \"normal\" or \"set point\", then when I was feeling \"2\", I didn't assume that meant anything; that was the default state.  That left me feeling \"2\" a lot of the time, and when things went wrong, I dipped lower, and I waited for things outside of myself to go right before I went higher.  The problem was that \"2\" was not a good place to be spending most of my time.\n\n*   I had to label the old set-point as subnormal, a _problem state_ that generated a need for immediate action from me to _fix_ it.  It was like telling myself that, unbeknownst to me, my left foot was in constant pain and needed medicine at once: kind of hard to swallow, given that my left foot always felt pretty much the same unless I'd just stubbed a toe or received a massage.  But eventually, I attached _urgency_ to the old set point.  It was not _just_ how things were normally; it was a sign that something was _wrong._\n*   I had to make sure that I had many accessible, cheap excuses to cheer up, so I didn't ever fall into the trap of \"just this once\" leaving myself at a \"2\" state instead of acting.  I designated a favorite pair of socks and wore them whenever I woke up on the wrong side of the bed; I took up the habit of saving every picture of a cute animal I found on the Internet so I could leaf through the collection whenever I wanted; I threw myself into developing the skill of making friends _on purpose_ so I'd have lots and if I happened to log onto my IM client, someone would be there who would talk to me; I became very acquisitive of inexpensive goods like music and interesting websites.  When one of these interventions failed to work, I forced myself to try something else, rather than falling into the self-talk disaster of \"well, that didn't help; I guess something must really be wrong and I should feel like this until it goes away by itself.\"  I also harnessed my tendency to feel better after a night's sleep - if I felt suboptimal close to bedtime, I'd turn in early and reasonably expect to wake up improved.\n*   I stopped tolerating the minor injuries to my affect that I identified as most consistent and, therefore, most likely to contribute to my poor set point.  For instance, I noticed that I always slept better when I didn't go to bed expecting to awaken to the sound of an alarm, so I aggressively rearranged my schedule to give me morning leeway, and found alarm software that would wake me more gently when an early start was absolutely necessary.  I identified people with whom interaction was frustrating and draining, and I limited interaction with them both by reducing opportunities to start, and by dropping my standards for abandoning the exchange midway through so I could leave before things got very bad.  I practiced, in general, \"writing things off\" and rehearsed internal monologues about how I no longer needed to worry about \\[thing X\\].  (\"I cannot control the speed of the bus.  I caught it, and it will get there when it gets there.  There is no point in further fretting about being late until I'm moving under my own power again - so I'll stop.  To manage my strong, intrusive desire to be on time, I will start thinking about how to choose an efficient path to walk once I get off the bus.\")\n*   I labeled my new desired set point - a safe spot on the spectrum, call it \"5\", which was ambitious yet felt attainable - as \"normal\".  When asked how I was in this state, I consciously chose to say that I was \"fine\" or \"okay\" instead of something more enthusiastic, like \"great\", that I might have said before - the energy I felt at \"5\" was no longer to be considered _extra_.  Similarly, these were not suitable occasions to do displeasing things.  I didn't have happiness to burn at \"5\" - I waited until I was even better before I relaxed my emotional avarice.  Instead, \"5\" was a good place from which to undertake more expensive entertainments that offered net improvement.  (More difficult than choosing a specific pair of socks to wear is starting a D&D game, or walking around and exploring a new location, or working on a piece of artwork or fiction; the lag time and effort makes them poor \"cheer up\" activities, but excellent ways to get from \"5\" to \"6\" or \"7\".)\n*   I made a point of noting non-sadness deficiencies in my status like boredom, hunger, tiredness, or annoyance.  These weren't directly related to the set point I was trying to affect, but they could exacerbate a bad influence or limit the power of a good one.  Additionally, at the level of luminosity I then had to work with, they could also mask moods that were actually sadness, in much the same way that sometimes one can feel hungry when in fact just thirsty.\n\n3.  _I treated my own mood as **manageable**._ Thinking of it as a thing that attacked me with no rhyme or reason - treating a bout of depression like a cold - didn't just cost me the opportunity to fight it, but also made the entire situation seem more out-of-control and hopeless.  I was wary of learned helplessness; I decided that it would be best to interpret my historically static set point as an indication that I hadn't hit on the right techniques yet, not as an indication that it was inviolable and everlasting.  Additionally, the fact that I didn't know how to fix it yet meant that if it was going to be my top priority, I had to treat the value of information as very high; it was worth experimenting, and I didn't have to wait for surety before I gave something a shot.\n\n*   Even if I determined that my mood reacted to my environment in some way, that only removed my power over it one step: I could control my environment to a considerable degree, and with a strong enough reason to do so, I committed to enacting that power.  (This sometimes has had unexpected and dramatic consequences.  For example, once I determined that grad school was no longer compatible with my happiness, I dropped out as soon as I had something promising to switch to - mid semester - and moved across the country.  To excellent effect, I might add.)\n*   Even if I have a lot on my plate, being happier will help me do it.  It's like sleep: it's easy to keep staying up and staying up, because sleep just seems so _unproductive_, and you can get _some_ work done however tired you are.  But over the long term, getting to sleep at a sane hour every day will let you accomplish more; and so with maintaining a good affect consistently.  Mood maintenance is typically not the most _immediately productive_ thing I could be doing, but treating it as my top priority save in dire emergency has let me be more effective than I was before.\n*   I had to be willing to expend resources on my project.  This involved working around some neuroses, like my unwillingness to spend money, and overcoming some background reluctance to try new things.  Also, I had to allow myself to be somewhat subject to my whims.  I still don't know what makes the mood to, say, do artwork strike me, but when it strikes, I have to do art or lose the inclination.  Efficacious inclinations to do fun things are precious to me, and so whenever possible, I don't restrain them - even though this costs time and occludes other activities."
          },
          "voteCount": 171
        },
        {
          "name": "Guilt: Another Gift Nobody Wants",
          "type": "post",
          "slug": "guilt-another-gift-nobody-wants",
          "_id": "CZnBQtvDw33rmWpBD",
          "url": null,
          "title": "Guilt: Another Gift Nobody Wants",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Emotions"
            },
            {
              "name": "Signaling"
            },
            {
              "name": "Evolutionary Psychology"
            },
            {
              "name": "Guilt & Shame"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Evolutionary psychology has made impressive progress in understanding the origins of morality. Along with the [many posts](http://wiki.lesswrong.com/wiki/Evolutionary_psychology) about these origins on Less Wrong I recommend Robert Wright's _The Moral Animal_ for an excellent introduction to the subject.  \n  \nGuilt does not naturally fall out of these explanations. One can imagine a mind design that although often behaving morally for the same reasons we do, sometimes decides a selfish approach is best and pursues that approach without compunction. In fact, this design would have advantages; it would remove a potentially crippling psychological burden, prevent loss of status from admission of wrongdoing, and allow more rational calculation of when moral actions are or are not advantageous. So why guilt?  \n  \nIn one of the few existing writings I could find on the subject, Tooby and Cosmides [theorize that](http://www.psych.ucsb.edu/research/cep/emotion.html) \"guilt functions as an emotion mode specialized for recalibration of regulatory variables that control trade-offs in welfare between self and other.\"  \n  \nIf I understand their meaning, they are saying that when an action results in a bad outcome, guilt is a byproduct of updating your mental processes so that it doesn't happen again. In their example, if you don't share food with your sister, and your sister starves and becomes sick, your brain gives you a strong burst of negative emotion around the event so that you reconsider your decision not to share. It is generally a bad idea to disagree with Tooby and Cosmides, but this explanation doesn't satisfy me for several reasons.  \n  \nFirst, guilt is just as associated with good outcomes as bad outcomes. If I kill my brother so I can inherit the throne, then even if everything goes according to plan and I become king, I may still feel guilt. But why should I recalibrate here? My original assumptions - that fratricide would be easy and useful - were entirely correct. But I am still likely to feel bad about it. In fact, some criminals report feeling \"relieved\" when caught, as if a negative outcome decreased their feelings of guilt instead of exacerbating them.  \n  \nSecond, guilt is not only an emotion, but an entire complex of behaviors. Our modern word self-flagellation comes from the old practice of literally whipping one's self out of feelings of guilt or unworthiness. We may not literally self-flagellate anymore, but when I feel guilty I am less likely to do activities I enjoy and more likely to deliberately make myself miserable.  \n  \nThird, although guilt can be very private it has an undeniable social aspect. People have messaged me at 3 AM in the morning just to tell me how guilty they feel about something they did to someone I've never met; this sort of outpouring of emotion can even be therapeutic. The aforementioned self-flagellators would parade around town in their sackcloth and ashes, just in case anyone didn't know how guilty they felt. And we expect guilt in certain situations: a criminal who feels guilty about what ey has done may get a shorter sentence.  \n  \nFourth, guilt sometimes occurs even when a person has done nothing wrong. People who through no fault of their own are associated with disasters can nevertheless report \"survivor's guilt\" and feel like events were partly their fault. If this is a tool for recalibrating choices, it is a very bad one. This is not a knockdown argument - a lot of mental adaptations are very bad at what they do - but it should at least raise suspicion that there is another part to the puzzle besides recalibration.\n\n  \n**THE PARABLE OF THE LAWYER**  \n  \nSuppose you need a lawyer for some important and very lucrative legal case. And suppose by a freak legislative oversight, your state has no laws against legal malpractice and unethical lawyers can get off scot-free. You are going to want to invest a lot of effort into evaluating the morals of the many lawyers anxious to take your case.  \n  \nOne lawyer you meet, Mr. Dewey, has an unusual appearance. A small angel, about the size of a rat, sits on his right shoulder holding an electric cattle prod. This is remarkable, and so you remark upon it.  \n  \nMr. Dewey scowls. \"That angel has been sitting there for as long as I can remember,\" he tells you. \"Every time I do something wrong, she pokes me with her prod. If it's a minor sin like profanity, maybe she'll only poke me once or twice, but if I lie or swindle, she'll turn the power up on max and keep shocking me for days. It's a miserable, miserable existence, and I'm constantly scared to death I'll slip up and make her angry, but I can't figure out how to get rid of her.\"  \n  \nYou express some skepticism about this story, so Mr. Dewey offers to demonstrate. He says a mild curse word, and sure enough, the angel pokes him with the cattle prod, giving him a mild electric shock.  \n  \nSuddenly, Mr. Dewey is a very attractive candidate for your lucrative case. You can be assured that he won't swindle you, because whatever gains he might take from the swindle are less attractive than the punishment he would get from the angel afterwards.  \n  \nSurgeon Paul Brand considered pain so useful to the body's functioning that he [called it](http://www.amazon.com/Pain-Nobody-Paul-W-Brand/dp/0310616786) \"the gift nobody wants\". Mr. Dewey's angel is also such a gift, even though he might not appreciate it: clients worried about ethical issues will bring their patronage to his law firm, giving him a major advantage over the competition.  \n  \nWhereas normally we must trust a lawyer's altruism if we expect em not to con us, in Mr. Dewey's case we need only trust him to pursue his own self-interest. This, then, is the role of guilt: it provides assurance to others that we will be punished for our misdeeds even if there is no external authority to punish us, avoiding Parfitian hitchhiker  dilemmas and ensuring fair play. The assurance of punishment ensures fair play and makes mutually beneficial transactions possible.  \n  \n**FAKEABLE AND UNFAKEABLE SIGNALS  \n**  \nThe big difference between Mr. Dewey and ourselves is that where Mr. Dewey has unquestionable evidence of his commitment to self punishment in the form of a very visible angel on his shoulder, for the rest of us guilt is a private mental affair and can be faked. It would seem to be a winning strategy, then, to claim a tendency to guilt while not really having one.  \n  \nMs. Wolfram is Mr. Dewey's main competitor, and is outraged at her rival's business success. In an attempt to even the scales, she buys a plastic angel figure from the local church and glues it to her shoulder. \"Look!\" she tells clients. \"I, too, suffer pain when I commit misdeeds!\" Her business shoots up to the same high levels as Mr. Dewey's.  \n  \nOne day, the news comes that Mr. Dewey was spotted whipping himself in the town square. When asked why, he explained that in a moment of weakness, he had overcharged a customer. His angel, who had lost its cattle prod, was mind-controlling him into the self-flagellation in place of its more usual punishment.  \n  \nThis provides an impressive bar for Ms. Wolfram to live up to. Sure, she could just whip herself like Mr. Dewey is doing. But it wouldn't be worth it - she just doesn't like the money enough that she would whip herself after every swindle just to drum up business. If she's going to have to whip herself to fake remorse whenever she commits wrongdoing,  her best policy really _is_ to genuinely stop swindling people.  \n  \nMr. Dewey has found an unfakeable signal. Even though whipping himself in public is one of the most unpleasant things he could do, in this case it is good business practice. It once again differentiates him from Ms. Wolfram and restores his status as the city's most desirable attorney.  \n  \nIn evolutionary terms, guilt becomes more credible the more it requires publicly visible behavior that no reasonable cheat would want to fake. Hurting oneself, avoiding pleasurable activities, lowering your own status, and withdrawing from social activities are all evolutionary costly and therefore good ways to prove you are experiencing guilt; the usual vocal, postural, and facial cues of being miserable are also useful.  \n  \nThere's no reason people should evolve an all-consuming sense of guilt. If an opportunity comes along where the benefits of cheating are greater than the social costs, an organism should still take it. Therefore, guilt has to be unpleasant but not infinitely unpleasant. A person who committed suicide in response to even the slightest moral infraction would be trustworthy, but they'd miss out if an excellent opportunity to win major gains for cheating happened to fall into their lap.  \n  \nThe conspicuous experience of guilt is an evolutionarily advantageous way of assuring potential trading partners that you will be punished for defection. The behaviors associated with guilt are costly signals that help differentiate false claims of guilt from the real thing and add to public verifiability of the punishment involved.  \n  \n**UNDESERVED GUILT**  \n  \nIf you kill your brother in order to inherit the throne, you probably deserve whatever guilt you feel. But in the phenomenon of \"survivor's guilt\", people feel guilt for events that weren't even remotely their fault. Maybe you go hiking with your brother, and through no fault of your own he trips and falls down a crevasse and dies, and now you feel guilty. Why?  \n  \nHunter-gatherer societies were more violent than our own; statistics differ but by [some estimates](http://www.economist.com/node/10278703) around 30% of hunter-gatherer males died of homicide. Even as late as the Bronze Age, Biblical figures who killed their brothers comprise a rather impressive list including Cain, Solomon, Ammon, Abimelech, and Jehoram; Jacob's sons merely attempted to do so. So the priors for suspicious death must have been very different in the olden days.  \n  \nFurther, in such a crime-ridden culture, there may have been more incentives to blame an enemy for a death, even if that enemy was not responsible. A person whose brother has accidentally died on a hiking trip with no witnesses would be very targetable.  \n  \nAnd even in less drastic situations than blaming survivors for a death, there may be other possible threats to reputation. If there is only one survivor of a battle, he may be suspected of cowardice; if there is only one survivor of a disaster, she may be suspected of running away without helping others.  \n  \nTherefore, it would be advantageous to have a method of proving your innocence. Suppose that you would gain benefits X from killing your brother and covering it up, but that you would suffer losses Y if you were suspected of the crime and punished. A precommitment to a policy of experiencing a level of guilt between X and Y provides a tool for proving your innocence. It would no longer be in your self-interest to kill your brother, because you will suffer so much guilt that you won't be able to enjoy the benefits of your crime; your would-be accusers realize this and admit your innocence, saving you from the still worse outcome Y.  \n  \nIn this case, guilt would be an entirely adaptive response to a disaster with which you were associated, even if your own actions were beyond reproach. A level of unhappiness worse than any benefits you could get by profiting the tragedy, but less than any punishment you might receive if you were suspected of profiting from the tragedy, would be helpful in clearing your name of any wrongdoing.\n\n(The proposed mechanism is almost identical to one cited in Thornhill and Palmer's controversial and unpleasant evolutionary [account of post-traumatic stress after rape](http://books.google.com/books?id=xH6v-nB6EegC&lpg=PA96&ots=Q2FNixrDci&dq=thornhill%20psychological%20adaptation%20women&pg=PA86#v=onepage&q=thornhill%20psychological%20adaptation%20women&f=false).)  \n  \nThis theory makes some testable predictions, which as far as I know have not been tested:\n\n\\- People should feel guiltier about events for which reasonable suspicion might exist that they played a part; for example, if your brother slipped and fell while you were hiking alone with him rather than in a large group with many witnesses.  \n\\- People should feel guiltier about events for which they might profit; for example, if you stood to inherit money from your brother, or never liked him much anyway.  \n\\- People may be suspicious of people who come out of a disaster feeling no survivor's guilt.\n\n**CONCLUSION**\n\nGuilt, like pain, is \"a gift nobody wants\". Because people with guilt are known to punish themselves for moral wrongdoing, their social group considers them more trustworthy and they gain the advantages of trade and cooperation. In order to prove that their guilt is real rather than feigned, they use costly signals like deliberate self-harm and self-denial to display their punishment publicly  \n  \nWhen one has done nothing wrong, it can sometimes be advantageous to paradoxically display guilt in order to prove one's lack of wrongdoing. These costly signals demonstrate that it is not in one's self-interest to lie about these matters, while still being less costly than the punishment for defection.  \n  \nAlthough this could theoretically be mediated by the behavioral strategies of a sufficiently intelligent and Machiavellian unconscious mind, it fits within the framework of evolutionary psychology and can also be interpreted in evolutionary terms."
          },
          "voteCount": 80
        },
        {
          "name": "How to Beat Procrastination",
          "type": "post",
          "slug": "how-to-beat-procrastination",
          "_id": "RWo4LwFzpHNQCTcYt",
          "url": null,
          "title": "How to Beat Procrastination",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Practical"
            },
            {
              "name": "Akrasia"
            },
            {
              "name": "Procrastination"
            },
            {
              "name": "Self Improvement"
            },
            {
              "name": "Productivity"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Part of the sequence: [The Science of Winning at Life](http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life)\n\n> My own behavior baffles me. I find myself doing what I hate, and not doing what I really want to do!\n\n\\- Saint Paul (Romans 7:15)\n\nOnce you're trained in [BayesCraft](http://commonsenseatheism.com/?p=12147), it may be tempting to tackle classic problems \"from scratch\" with your new Rationality Powers. But often, it's more effective to [do a bit of scholarship](/lw/3m3/the_neglected_virtue_of_scholarship/) first and at least *start* from [the state of our scientific knowledge](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/) on the subject.\n\nToday, I want to tackle **procrastination** by summarizing what we know about it, and how to overcome it.\n\nLet me begin with three character vignettes...\n\nEddie attended the sales seminar, read all the books, and repeated the self-affirmations in the mirror this morning. But he has yet to make his first sale. Rejection after rejection has demoralized him. He organizes his desk, surfs the internet, and puts off his cold calls until potential clients are leaving for the day.\n\nThree blocks away, Valerie stares at a blank document in Microsoft Word. Her essay assignment on municipal politics, due tomorrow, is mind-numbingly dull. She decides she needs a break, texts some friends, watches a show, and finds herself even less motivated to write the paper than before. At 10pm she dives in, but the result reflects the time she put into it: it's terrible.\n\nIn the next apartment down, Tom is ahead of the game. He got his visa, bought his plane tickets, and booked time off for his vacation to the Dominican Republic. He still needs to reserve a hotel room, but that can be done anytime. Tom keeps pushing the task forward a week as he has more urgent things to do, and then forgets about it altogether. As he's packing, he remembers to book the room, but by now there are none left by the beach. When he arrives, he finds his room is 10 blocks from the beach and decorated with dead mosquitos.\n\nEddie, Valerie, and Tom are all procrastinators, but in different ways.^1^\n\nEddie's problem is *low expectancy*. By now, he expects only failure. Eddie has low expectancy of success from making his next round of cold calls. Results from 39 procrastination studies show that low expectancy is a major cause of procrastination.^2^ You doubt your ability to follow through with the diet. You don't expect to get the job. You really should be going out and meeting girls and learning to flirt better, but you expect only rejection now, so you procrastinate. You have [learned to be helpless](http://en.wikipedia.org/wiki/Learned_helplessness).\n\nValerie's problem is that her task has *low value* for her. We all put off what we *dislike*.^3^ It's easy to meet up with your friends for drinks or start playing a videogame; not so easy to start doing your taxes. This point may be obvious, but it's nice to see it confirmed in over a dozen scientific studies. We put off things we don't like to do.\n\nBut the *strongest* predictor of procrastination is Tom's problem: *impulsiveness*. It would have been easy for Tom to book the hotel in advance, but he kept getting distracted by more urgent or interesting things, and didn't remember to book the hotel until the last minute, which left him with a poor selection of rooms. Dozens of studies have shown that procrastination is closely tied to impulsiveness.^4^\n\nImpulsiveness fits into a broader component of procrastination: *time*. An event's impact on our decisions decreases as its temporal distance from us increases.^5^ We are less motivated by [delayed rewards](/lw/6c/akrasia_hyperbolic_discounting_and_picoeconomics/) than by immediate rewards, and the more impulsive you are, the more your motivation is affected by such delays.\n\nExpectancy, value, delay, and impulsiveness are the four major components of procrastination. [Piers Steel](http://haskayne.ucalgary.ca/profiles/piers-steel), a leading researcher on procrastination, [explains](http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20):\n\n> Decrease the certainty or the size of a task's reward - its expectancy or its value - and you are unlikely to pursue its completion with any vigor. Increase the delay for the task's reward and our susceptibility to delay - impulsiveness - and motivation also dips.\n\nThe Procrastination Equation\n\nThis leaves us with \"the procrastination equation\":\n\n![](https://web.archive.org/web/20180726091558im_/http://commonsenseatheism.com/wp-content/uploads/2011/01/procrastination-equation.png)\n\nThough [we are always learning more](http://http-server.carleton.ca/~tpychyl/prg/research/research_complete_biblio.html), the procrastination equation accounts for every major finding on procrastination, and draws upon our best current theories of motivation.^6^\n\nIncrease the size of a task's reward (including both the pleasantness of doing the task and the value of its after-effects), and your motivation goes up. Increase the perceived *odds* of getting the reward, and your motivation also goes up.\n\nYou might have noticed that this part of the equation is one of the basic equations of the [expected utility theory](http://en.wikipedia.org/wiki/Expected_utility_hypothesis) at the heart of economics. But one of the major criticisms of standard economic theory was that it did not account for time. For example, in 1991 George Akerlof [pointed out](http://commonsenseatheism.com/wp-content/uploads/2011/01/Akerlof-Procrastination-and-Obedience.pdf) that we irrationally find *present* costs more salient than *future* costs. This led to the flowering of [behavioral economics](http://en.wikipedia.org/wiki/Behavioral_economics), which integrates time (among other things).\n\nHence the denominator, which covers the effect of *time* on our motivation to do a task. The longer the delay before we reap a task's reward, the less motivated we are to do it. And the negative effect of this delay on our motivation is *amplified* by our level of impulsiveness. For highly impulsive people, delays do even *greater* damage to their motivation.\n\nThe Procrastination Equation in Action\n\nAs an example, consider the college student who must write a term paper.^7^ Unfortunately for her, colleges have created a perfect storm of procrastination components. First, though the value of the paper for her *grades* may be high, the more immediate value is *very low*, assuming she dreads writing papers as much as most college students do.^8^ Moreover, her *expectancy* is probably low. Measuring performance is hard, and any essay re-marked by another professor may get a very different grade: a B+ essay will get an A+ if she's lucky, or a C+ if she's unlucky.^9^ There is also a large *delay*, since the paper is due at the end of the semester. If our college student has an impulsive personality, the negative effect of this delay on her motivation to write the paper is greatly amplified. Writing a term paper is grueling (low value), the results are uncertain (low expectancy), and the deadline is far away (high delay).\n\nBut there's more. College dorms, and college campuses in general, might be the most distracting places on earth. There are *always* pleasures to be had (campus clubs, parties, relationships, games, events, alcohol) that are reliable, immediate, and intense. No wonder that the task of writing a term paper can't compete. These potent distractions amplify the negative effect of the delay in the task's reward and the negative effect of the student's level of impulsiveness. \n\nHow to Beat Procrastination\n\nAlthough much is known about the neurobiology behind procrastination, I won't cover that subject here.^10^ Instead, let's jump right to the *solutions* to our procrastination problem.\n\nOnce you know the procrastination equation, our general strategy is obvious. Since there is usually little you can do about the *delay* of a task's reward, we'll focus on the three terms of the procrastination equation over which we have some control. To beat procrastination, we need to:\n\n1.  Increase your *expectancy* of success.\n2.  Increase the task's *value* (make it more pleasant and rewarding).\n3.  Decrease your *impulsiveness*.\n\nYou might think these things are out of your control, but researchers have found several useful methods for achieving each of them.\n\nMost of the advice below is taken from the best book on procrastination available, Piers Steel's [*The Procrastination Equation*](http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20), which explains these methods *and others* in more detail.\n\nOptimizing Optimism\n\nIf you don't think you can succeed, you'll have little motivation to do the task that needs doing. You've probably heard the advice to \"Be positive!\" But how? So far, researchers have identified three major techniques for increasing optimism: Success Spirals, Vicarious Victory, and Mental Contrasting.\n\n*Success Spirals*\n\nOne way to build your optimism for success is to make use of *success spirals*.^11^ When you achieve one challenging goal after another, your obviously gain confidence in your ability to succeed. So: give yourself a series of meaningful, challenging but achievable goals, and then achieve them! Set yourself up for success by doing things you know you can succeed at, again and again, to keep your confidence high.\n\nSteel recommends that for starters, \"it is often best to have process or learning goals rather than product or outcome goals. That is, the goals are acquiring or refining new skills or steps (the process) rather than winning or getting the highest score (the product).\"^12^\n\nWilderness classes and adventure education (rafting, rock-climbing, camping, etc.) are *excellent* for this kind of thing.^13^ Learn a new skill, be it cooking or karate. Volunteer for more responsibilities at work or in your community. Push a favorite hobby to the next level. The key is to achieve one goal after another and pay attention to your successes.^14^ Your brain will reward you with increased *expectancy* for success, and therefore a better ability to beat procrastination.\n\n*Vicarious Victory*\n\nPessimism and optimism are both contagious.^15^ Wherever you are, you probably have access to community groups that are great for fostering positivity: [Toastmasters](http://www.toastmasters.org/), [Rotary](http://www.rotary.org/), [Elks](http://www.elks.org/), [Shriners](http://www.beashrinernow.com/), and other local groups. I recommend you visit 5-10 such groups in your area and join the best one.\n\nYou can also boost your optimism by [watching inspirational movies](http://rogerebert.suntimes.com/apps/pbcs.dll/article?AID=/20060616/COMMENTARY/60616003), [reading inspirational biographies](http://www.biographyonline.net/people/inspirational/index.html), and [listening to motivational speakers](http://www.motivational-well-being.com/motivational-speakers.html).\n\n*Mental Contrasting*\n\nMany popular self-help books encourage *creative visualization*, the practice of regularly and vividly imagining what you want to achieve: a car, a career, an achievement. Surprisingly, research shows this method can actually *drain* your motivation.^16^\n\nUnless, that is, you add a second *crucial* step: *mental contrasting*. After imagining what you want to achieve, mentally contrast that with where you are now. Visualize your old, rusty car and your small paycheck. This presents your current situation as an obstacle to be overcome to achieve your dreams, and jumpstarts planning and effort.^17^\n\n*Guarding Against Too Much Optimism*\n\nFinally, I should note that *too much* optimism can also be a problem,^18^ though this is less common. For example, too much optimism about [how long a task will take](/lw/jg/planning_fallacy/) may cause you to put it off until the last minute, which turns out to be too late. Something like Rhonda Byrne's [*The Secret*](http://www.amazon.com/Secret-Rhonda-Byrne/dp/1582701709/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) may be *too* optimistic.\n\nHow can you guard against too much optimism? Plan for the worst but hope for the best.^19^ Pay attention to how you procrastinate, make backup plans for failure, but then use the methods in this article to succeed as much as possible.\n\nIncreasing Value\n\nIt's hard to be motivated to do something that doesn't have much value to us - or worse, is downright *unpleasant*. The good news is that value is to some degree *constructed* and *relative*. The malleability of value is a well-studied area called *psychophysics*,^20^ and researchers have some advice for how we can inject value into necessary tasks.\n\n*Flow*\n\nIf the task you're avoiding is *boring*, try to make it more difficult, right up to the point where the difficulty level matches your current skill, and you achieve \"flow.\"^21^ This is what the state troopers of *Super Troopers* did: they devised strange games and challenges to make their boring job passable. [Myrtle Young](http://www.youtube.com/watch?v=EY3Lw_-bj5U) made her boring job at a potato chip factory more interesting and challenging by looking for potato chips that resembled celebrities and pulling them off the conveyor belts.\n\n*Meaning*\n\nIt also helps to make sure tasks are connected to something you care about for its own sake,^22^ at least through a chain: you read the book so you can pass the test so you can get the grade so you can get the job you want and have a fulfilling career. Breaking the chain leaves a task feeling meaningless.\n\n*Energy*\n\nObviously, tasks are harder when you don't have much energy.^23^ Tackle tasks when you are most alert. This depends on your circadian rhythm,^24^ but most people have the most energy during a period starting a few hours after they wake up and lasting 4 hours.^25^ Also, make sure to get enough sleep and exercise regularly.^26^\n\nOther things that have worked for many people are:\n\n*   Drink lots of water.\n*   Stop eating anything that contains wheat and other grains.\n*   Use drugs (especially [modafinil](http://www.gwern.net/Modafinil)) as necessary.\n*   Do short but intense exercise once a week.\n*   When tired, splash cold water on your face or take a shower or do jumping jacks or go running.\n*   Listen to music that picks up your mood.\n*   De-clutter your life, because clutter is cognitively exhausting for your brain to process all day long.\n\n*Rewards*\n\nOne obvious way to inject more value into a task is to *reward yourself for completing it*.^27^\n\nAlso, mix bitter medicine with sweet honey. Pair a long-term interest with a short-term pleasure.^28^ Find a workout partner whose company you enjoy. Treat yourself to a specialty coffee when doing your taxes. I bribe myself with [Pinkberry frozen yogurt](http://www.pinkberry.com/) to do things I hate doing.\n\n*Passion*\n\nOf course, the most *powerful* way to increase the value of a task is to focus on doing what you love wherever possible. It doesn't take much extra motivation for me to [research meta-ethics](/lw/43v/the_urgent_metaethics_of_friendly_artificial/) or write [summaries of scientific self-help](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/): that is what I love to do. Some people who love playing video games have made [careers](http://en.wikipedia.org/wiki/Gold_farming) out of it. To figure out which career might be full of tasks that you love to do, taking a [RIASEC](http://www.bigjobportal.com/riasec/) personality test might help. In the USA, [O*NET](http://www.onetonline.org/) can help you find jobs that are in-demand and fit your personality.\n\nHandling Impulsiveness\n\nImpulsiveness is, on average, the biggest factor in procrastination. Here are two of Steel's (2010a) methods for dealing with impulsiveness.\n\n*Commit Now*\n\n[Ulysses](http://en.wikipedia.org/wiki/Odysseus) did not make it past the beautiful singing [Sirens](http://en.wikipedia.org/wiki/Sirens) with *willpower*. Rather, he knew his weaknesses and so he committed in advance to sail past them: he literally tied himself to his ship's mast. Several forms of *precommitment* are useful in handling impulsiveness.^29^\n\nOne method is to \"throw away the key\": Close off tempting alternatives. Many people see a productivity boost when they decide not to allow a TV in their home; I haven't owned one in years. But now, TV and more is available on the internet. To block that, you might need a tool like [RescueTime](http://www.rescuetime.com/). Or, unplug your router when you've got work to do.\n\nAnother method is to make failure *really painful*. The website [stickK](http://www.stickk.com/) lets you set aside money you will *lose* if you don't meet your goal, and ensures that you have an outside referee to decide whether your met your goal or not. To \"up the ante,\" set things up so that your money will go to an organization you *hate* if you fail. And have your chosen referee agree to post the details of your donation to Facebook if you don't meet your goal.\n\n*Set Goals*\n\nHundreds of books stress SMART goals: goals that are Specific, Measurable, Attainable, Realistic, and Time-Anchored.^30^ Is this recommendation backed by good research? Not quite. First, notice that Attainable is redundant with Realistic, and Specific is Redundant with Measurable and Time-Anchored. Second, important concepts are *missing*. Above, we emphasized the importance of goals that are challenging (and thus, lead to \"flow\") and meaningful (connected to things you desire for their own sake).\n\nIt's also important to break up goals into lots of smaller subgoals which, by themselves, are easier to achieve and have more immediate deadlines. Typically, *daily* goals are frequent enough, but it can also help to set an immediate goal to break you through the \"getting started\" threshold. Your first goal can be \"Write the email to the producer,\" and your next goal can be the daily goal. Once that first, 5-minute task has been completed, you'll probably already be on your way to the larger daily goal, even if it takes 30 minutes or 2 hours.^31^\n\nAlso: Are your goals measuring inputs or outputs? Is your goal to *spend 30 minutes on X* or is it to *produce final product X*? Try it different ways for different tasks, and see what works for you.\n\nBecause we are creatures of habit, it helps to get into a routine.^32^ For example: Exercise at the same time, every day.\n\nConclusion\n\nSo there you have it. To beat procrastination, you need to increase your motivation to do each task on which you are tempted to procrastinate. To do that, you can (1) optimize your optimism for success on the task, (2) make the task more pleasant, and (3) take steps to overcome your impulsiveness. And to do each of *those* things, use the specific methods explained above (set goals, pre-commit, make use of success spirals, etc.).\n\nA warning: Don't try to be perfect. Don't try to completely *eliminate* procrastination. Be real. Overregulation will make you unhappy. You'll have to find a balance.\n\nBut now you have the tools you need. Identify which parts of the procrastination equation need the most work in your situation, and figure out which methods for dealing with that part of the problem work best for you. Then, go out there and [make yourself stronger](/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/), [score that job](/lw/43m/optimal_employment/), and [help save the world](/lw/373/how_to_save_the_world/)!\n\n(And, read [*The Procrastination Equation*](http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) if you want more detail than I included here.)\n\nNext post: [My Algorithm for Beating Procrastination](/lw/9wr/my_algorithm_for_beating_procrastination/)\n\nPrevious post: [Scientific Self-Help: The State of Our Knowledge](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/)\n\nNotes\n\n^1^ These are the fictional characters used to illustrate the procrastination equation in Steel (2010a).\n\n^2^ Expectancy corresponds most closely to the commonly measured trait of \"self efficacy.\" The relatively strong correlation between low self-efficacy and procrastination (across 39 studies) is shown table 3 of Steel (2007).\n\n^3^ In [a recent post](/lw/3kv/working_hurts_less_than_procrastinating_we_fear/), Eliezer Yudkowsky claimed that \"on a moment-to-moment basis, being in the middle of doing the work is usually less painful than being in the middle of procrastinating.\" Thus, \"when you procrastinate, you're probably not procrastinating because of the pain of working.\" That might be true for Eliezer in particular, but studies on procrastination suggest it's not true for most people. The pain of doing a task *is* a major factor contributing to procrastination. This is known as the problem of *task aversiveness* (Brown 1991; Burka & Yuen 1983; Ellis & Knauss 1977), also known as the problem of *task appeal* (Harris & Sutton, 1983) or as the *dysphoric affect* (Milgram, Sroloff, & Rosenbaum, 1988). For an overview of additional literature demonstrating this point, see page 75 of Steel (2007).\n\n^4^ For an overview of the correlation between impulsiveness and procrastination, see pages 76-79 and 81 of Steel (2007).\n\n^5^ This is recognized as one of the psychological laws of learning (Schwawrtz, 1989), and plays a role in the dominant economic role of discounted utility (Loewenstein & Elster, 1992). In particular, see the work on *temporal construal theory* (Trope & Liberman, 2003).\n\n^6^ The procrastination equation is called *temporal motivational theory* (TMT). See Steel (2007) on how TMT accounts for every major finding on procrastination. See Steel & Konig (2006) on how TMT draws upon and integrates our best psychological theories of motivation. There are [other theories of procrastination](http://webapps2.ucalgary.ca/~steel/procrastination-theories/) \\- the most popular may be the decisional-avoidant-arousal theory proposed by Ferrari (1992). But a recent meta-analysis shows that TMT is more consistent with the data (Steel, 2010b). An **important note** is that the full version of TMT places a constant in the denominator to prevent the denominator from skyrocketing into infinity as delay approaches 0. Also, 'impulsiveness' here is a substitute for 'susceptibility to delay,' something which may vary by task, whereas 'impulsiveness' sounds like a stable character trait that might not help to explain having different motivations to perform different tasks.\n\n^7^ This example taken from Steel (2010a). Academic procrastination is the most-studied kind of procrastination (McCown & Roberts, 1994).\n\n^8^ Even George Orwell hated writing. He [wrote](http://orwell.ru/library/essays/wiw/english/e_wiw): \"Writing a book is a horrible, exhausting struggle, like a long bout of some painful illness.\"\n\n^9^ See Cannings et al. (2005) and Newstead (2002).\n\n^10^ Read chapter 3 of Steel (2010a).\n\n^11^ In business academia, success spirals are known as \"efficacy-performance spirals\" or \"efficacy-performance deviation amplifying loops\". See Lindsley et al. (1995).\n\n^12^ See Steel (2010a), note 9 in chapter 7.\n\n^13^ See Hans (2000), Feldman & Matjasko (2005), and World Organization of the Scout Movement (1998).\n\n^14^ Zimmerman (2002).\n\n^15^ Aarts et al. (2008), Armitage & Conner (2001), Rivs & Sheeran (2003), van Knippenberg et al. (2004).\n\n^16^ Levin & Spei (2004), Rhue & Lynn (1987), Schneider (2001), Waldo & Merritt (2000).\n\n^17^ Achtziger et al. (2008), Oettingen et al. (2005), Oettingen & Thorpe (2006), Kavanagh et al. (2005), Pham & Taylor (1999).\n\n^18^ Sigall et al. (2000).\n\n^19^ Aspinwall (2005).\n\n^20^ A good overview is Weber (2003).\n\n^21^ Csikszentmihalyi (1990).\n\n^22^ Miller & Brickman (2004), Schraw & Lehman (2001), Wolters (2003).\n\n^23^ Steel (2007), Gropel & Steel (2008).\n\n^24^ Furnham (2002).\n\n^25^ Klein (2009).\n\n^26^ Oaten & Cheng (2006).\n\n^27 ^Bandura (1976), Febbraro & Clum (1998), Ferrari & Emmons (1995). This is known as *learned industriousness*, *impulse pairing* or *impulse fusion*. See Eisenberger (1992), Renninger (2000), Stromer et al. (2000).\n\n^28^ Ainslie (1992).\n\n^29^ Ariely & Wertenbroch (2002) and Schelling (1992).\n\n^30^ Locke & Latham (2002).\n\n^31^ Gropel & Steel (2008), Steel (2010a).\n\n^32^ Diefendorff et al. (2006), Gollwitzer (1996), Silver (1974).\n\nReferences\n\nAarts, Dijksterhuis, & Dik (2008). [Goal contagion: Inferring goals from others' actions - and what it leads to](http://commonsenseatheism.com/wp-content/uploads/2011/02/Aarts-Goal-contagion-Inferring-goals-from-others-actions.pdf). In Shah & Gardner (Eds.), *Handbook of motivation* (pp. 265-280). New York: Guilford Press.\n\nAchtziger, Fehr, Oettingen, Gollwitzer, & Rockstroh (2008). Strategies of intention formation are reflected in continuous MEG activity. *Social Neuroscience, 4(1)*, 1-17.\n\nAinslie (1992). [*Picoeconomics: The strategic interaction of successive motivational states within the person*](http://www.amazon.com/Picoeconomics-Interaction-Successive-Motivational-Rationality/dp/0521158702/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). New York: Cambridge University Press.\n\nAriely & Wertenbroch (2002). [Procrastination, deadlines, and performance: Self-control by precommitment](http://commonsenseatheism.com/wp-content/uploads/2011/02/Ariely-Procrastination-deadlines-and-performance.pdf). *Psychological Science, 13(3)*: 219-224.\n\nArmitage & Conner (2001). [Efficacy of the theory of planned behavior: A meta-analytic review](http://commonsenseatheism.com/wp-content/uploads/2011/02/Armitage-Efficacy-of-the-theory-of-planned-behavior.pdf). *British Journal of Social Psychology, 40(4)*: 471-499.\n\nAspinwall (2005). [The psychology of future-oriented thinking: From achievement to proactive coping, adaptation, and aging](http://commonsenseatheism.com/wp-content/uploads/2011/02/Aspinwall-The-psychology-of-future-oriented-thinking.pdf). *Motivation and Emotion, 29(4)*: 203-235.\n\nBandura (1976). [Self-reinforcement: Theoretical and methodological considerations](http://commonsenseatheism.com/wp-content/uploads/2011/02/Bandura-Self-reinforcement-Theoretical-and-methodological-considerations.pdf). *Behaviorism, 4(2)*: 135-155.\n\nBrown (1991). Helping students confront and deal with stress and procrastination. *Journal of College Student Psychotherapy, 6*: 87-102.\n\nBurka & Yuen (1983). [*Procrastination: Why you do it, what to do about it*](http://www.amazon.com/Procrastination-Why-You-What-About/dp/0738211702/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). Reading, MA: Addison-Wesley.\n\nCannings, Hawthorne, Hood, & Houston (2005). [Putting double marking to the test: a framework to assess if it is worth the trouble](http://commonsenseatheism.com/wp-content/uploads/2011/02/Cannings-Putting-double-marking-to-the-test-a-framework-to-assess-if-it-is-worth-the-trouble.pdf). *Medical Education, 39(3):* 299-308.\n\nCsikszentmihalyi (1990). [*Flow: The psychology of optimal experience*](http://www.amazon.com/Flow-Psychology-Optimal-Experience-P-S/dp/0061339202/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). New York: Harper & Row.\n\nDiefendorff, Richard, & Gosserand (2006). [Examination of situational and attitudinal moderators of the hesitation and performance relation](http://commonsenseatheism.com/wp-content/uploads/2011/02/Diefendorff-Examination-of-situational-and-attitudinal-moderators-of-the-hesitation-and-performance-relation.pdf). *Personnel Psychology, 59*: 365-393.\n\nEisenberger (1992). [Learned industriousness](http://commonsenseatheism.com/wp-content/uploads/2011/02/Eisenberger-Learned-industriousness.pdf). *Psychological Review, 99*: 248-267.\n\nEllis & Knauss (1977). *Overcoming procrastination*. New York: Signet Books.\n\nFebbraro & Clum (1998). [Meta-analytic investigation of the effectiveness of self-regulatory components in the treatment of adult problem behaviors](http://commonsenseatheism.com/wp-content/uploads/2011/02/Febbraro-Meta-analytic-investigation-of-the-effectiveness-of-self-regulatory-components-in-the-treatment-of-adult-problem-behaviors.pdf). *Clinical Psychology Review, 18(2)*: 143-161.\n\nFeldman & Matjasko (2005). [The role of school-based extracurricular activities in adolescent development: A comprehensive review and future directions](http://commonsenseatheism.com/wp-content/uploads/2011/02/Feldman-The-role-of-school-based-extracurricular-activities-in-adolescent-development.pdf). *Review of Educational Research, 75(2)*, 159-210.\n\nFerrari (1992). [Psychometric validation of two procrastination inventoriesfor adults: Arousal and avoidance measures](http://commonsenseatheism.com/wp-content/uploads/2011/02/Ferrari-Psychometric-validation-of-two-procrastination-inventoriesfor-adults.pdf). *Journal of Psychopathology and Behavioral Assessment, 14(2):* 97–110.\n\nFerrari & Emmons (1995). Methods of procrastination and their relation to self-control and self-reinforcement: An exploratory study. *Journal of Social Behavior & Personality, 10(1)*: 135-142.\n\nFurnham (2002). [*Personality at work: The role of individual differences in the workplace*](http://www.amazon.com/Personality-Work-Individual-Differences-Workplace/dp/0415106486/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). New York: Routledge.\n\nGollwitzer (1996). The volitional benefits from planning. In Gollwitzer & Bargh (Eds.), *The psychology of action: Linking cognition and motivation to behavior* (pp. 287-312). New York: Guilford Press.\n\nGropel & Steel (2008). [A mega-trial investigation of goal setting, interest enhancement, and energy on procrastination](http://commonsenseatheism.com/wp-content/uploads/2011/02/Gropel-Steel-A-mega-trial-investigation-of-goal-setting-interest-enhancement-and-energy-on-procrastination.pdf). *Personality and Individual Differences, 45*: 406-411.\n\nHans (2000). [A meta-analysis of the effects of adventure programming on locus of control](http://commonsenseatheism.com/wp-content/uploads/2011/02/Hans-A-meta-analysis-of-the-effects-of-adventure-programming-on-locus-of-control.pdf). *Journal of Contemporary Psychotherapy, 30(1)*: 33-60.\n\nHarris & Sutton (1983). [Task procrastination in organizations: A framework for research](http://commonsenseatheism.com/wp-content/uploads/2011/02/Harris-Sutton-Task-procrastination-in-organizations-A-framework-for-research.pdf). *Human Relations, 36*: 987-995.\n\nKavanagh, Andrade, & May (2005). [Imaginary relish and exquisite torture: The elaborated intrusion theory of desire](http://commonsenseatheism.com/wp-content/uploads/2011/02/Kavanagh-Imaginary-relish-and-exquisite-torture.pdf). *Psychological Review, 112(2)*, 446-467.\n\nKlein (2009). [*The secret pulse of time: Making sense of life's scarcest commodity*](http://www.amazon.com/Secret-Pulse-Time-Scarcest-Commodity/dp/0738212563/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). Cambridge, MA: Da Capo Lifelong Books.\n\nLevin & Spei (2004). [Relationship of purported measures of pathological and nonpathological dissociation to self-reported psychological distress and fantasy immersion](http://commonsenseatheism.com/wp-content/uploads/2011/02/Levin-Spei-Relationship-of-purported-measures-of-pathological-and-nonpathological-dissociation-to-self-reported-psychological-distress-and-fantasy-immersion.pdf). *Assessment, 11(2)*: 160-168.\n\nLindsley, Brass, & Thomas (1995). [Efficacy-performance spirals: A multilevel perspective](http://commonsenseatheism.com/wp-content/uploads/2011/02/Lindsley-Efficacy-performance-spirals-A-multilevel-perspective.pdf). *Academy of Management Review, 20(3)*: 645-678.\n\nLocke & Latham (2002). [Building a practically useful theory of goal setting and task motivation: A 35-year odyssey](http://commonsenseatheism.com/wp-content/uploads/2011/02/Locke-Latham-Building-a-practically-useful-theory-of-goal-setting-and-task-motivation.pdf). *American Psychologist, 57(9)*: 705-717.\n\nLoewenstein & Elster (1992). The fall and rise of psychological explanations in the economics of intertemporal choice. In Loewenstein & Elster (Eds.), *Choice over time* (pp. 3-34). New York: Russell Sage Foundation.\n\nMcCown & Roberts (1994). A study of academic and work-related dysfunctioning relevant to the college version of an indirect measure of impulsive behavior. Integra Technical Paper 94-28, Radnor, PA: Integra, Inc.\n\nMilgram, Sroloff, & Rosenbaum (1988). [The procrastination of everyday life](http://commonsenseatheism.com/wp-content/uploads/2011/02/Milgram-The-procrastination-of-everyday-life.pdf). *Journal of Research in Personality, 22*: 197-212.\n\nMiller & Brickman (2004). [A model of future-oriented motivation and self-regulation](http://commonsenseatheism.com/wp-content/uploads/2011/02/Miller-Brickman-A-model-of-future-oriented-motivation-and-self-regulation.pdf). *Educational Psychology Review, 16(1)*: 9-33.\n\nNewstead (2002). [Examining the examiners: Why are we so bad at assessing students?](http://commonsenseatheism.com/wp-content/uploads/2011/02/Newstead-Examining-the-examiners.pdf) *Psychology Learning and Teaching, 2(2):* 70-75.\n\nOaten & Cheng (2006). [Longitudinal gains in self-regulation from regular physical exercise](http://commonsenseatheism.com/wp-content/uploads/2011/02/Oaten-Longitudinal-gains-in-self-regulation-from-regular-physical-exercise.pdf). *British Journal of Health Psychology, 11(4)*: 713-733.\n\nOettingen, Mayer, Thorpe, Janatzke, & Lorenz (2005). [Turning fantasies about positive and negative futures into self-improvement goals](http://commonsenseatheism.com/wp-content/uploads/2011/02/Oettingen-Turning-fantasies-about-positive-and-negative-futures-into-self-improvement-goals.pdf). *Motivation and Emotion, 29(4)*: 236-266.\n\nOettingen & Thorpe (2006). Fantasy realization and the bridging of time. In Sanna & Chang (Eds.), *Judgments over time: The interplay of thoughts, feelings, and behaviors* (pp. 120-143). Oxford: Oxford University Press.\n\nPham & Taylor (1999). [From thought to action: Effects of process- versus outcome-based mental simulations on performance](http://commonsenseatheism.com/wp-content/uploads/2011/02/Pham-Taylor-From-Thought-to-Action.pdf). *Personality and Social Psychology Bulletin, 25*: 250-260.\n\nRenninger (2000). Individual interest and its implications for understanding intrinsic motivation. In Sansone & Harackiewicz (Eds.), *Inntrinsic and extrinsic motivation: The search for optimal motivation and performance* (pp. 373-404). San Diego, CA: Academic Press.\n\nRhue & Lynn (1987). Fantasy proneness: The ability to hallucinate \"as real as real.\" *British Journal of Experimental and Clinical Hypnosis, 4*: 173-180.\n\nRivis & Sheeran (2003). [Descriptive norms as an additional predictor in the theory of planned behaviour: A meta-analysis](http://commonsenseatheism.com/wp-content/uploads/2011/02/Rivis-Sheeran-Descriptive-norms-as-an-additional-predictor-in-the-theory-of-planned-behaviour.pdf). *Current Psychology, 22(3)*, 218-233.\n\nSchelling (1992). Self-command: A new discipline. In Loewenstein & Elster (Eds.), *Choice over time* (pp. 167-176). New York: Russell Sage Foundation.\n\nSchneider (2001). [In search of realistic optimism: Meaning, knowledge, and warm fuzziness](http://commonsenseatheism.com/wp-content/uploads/2011/02/Schneider-In-search-of-realistic-optimism.pdf). *American Psychologist, 56(3)*: 250-263.\n\nSchraw & Lehman (2001). [Situational interest: A review of the literature and directions for future research](http://commonsenseatheism.com/wp-content/uploads/2011/02/Schraw-Situational-interest-A-review-of-the-literature-and-directions-for-future-research.pdf). *Educational Psychology Review, 13(1)*: 23-52.\n\nSchwartz (1989). [*Psychology of learning and behavior*](http://www.amazon.com/Psychology-Learning-Behavior-Steven-Robbins/dp/0393975916/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) (3rd ed.). New York: Norton.\n\nSigall, Kruglanski, & Fyock (2000). Wishful thinking and procrastination. *Journal of Social Behavior & Personality, 15(5)*: 283-296.\n\nSilver (1974). Procrastination. *Centerpoint, 1(1)*: 49-54.\n\nSteel (2007). [The nature of procrastination](http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-The-Nature-of-Procrastination.pdf). *Psychological Bulletin, 133(1)*: 65-94.\n\nSteel (2010a). [*The Procrastination Equation*](http://www.amazon.com/Procrastination-Equation-Putting-Things-Getting/dp/0061703613/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). New York: Harper.\n\nSteel (2010b). Arousal, avoidant and decisional procrastinators: Do they exist? *Personality and Individual Differences, 48*: 926-934.\n\nSteel & Konig (2006). [Integrating theories of motivation](http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-Konig-Integrating-Theories-of-Motivation.pdf). *Academy of Management Review, 31(4)*: 889-913. \n\nTrope & Liberman (2003). [Temporal construal](http://commonsenseatheism.com/wp-content/uploads/2011/02/Trope-Liberman-Temporal-construal.pdf). *Psychological Review, 110*: 403-421.\n\nvan Knippenberg, van Nippenberg, De Cremer, & Hegg (2004). [Leadership, self, and identity: A review and research agenda](http://commonsenseatheism.com/wp-content/uploads/2011/02/Van-Knippenberg-Leadership-self-and-identity-A-review-and-research-agenda.pdf). *The Leadership Quarterly, 15(6)*, 825-856.\n\nWaldo & Merritt (2000). [Fantasy proneness, dissociation, and DSM-IV axis II symptomatology](http://commonsenseatheism.com/wp-content/uploads/2011/02/Waldo-Fantasy-proneness-dissociation-and-DSM-IV-axis-II-symptomatology.pdf). *Journal of Abnormal Psychology, 109(3)*: 555-558.\n\nWeber (2003). Perception matters: Psychophysics for economists. In Brocas & Carrillo (Eds.), *The Psychology of Economic Decisions (Vol. II)*. New York: Oxford University Press.\n\nWolters (2003). [Understanding procrastination from a self-regulated learning perspective](http://commonsenseatheism.com/wp-content/uploads/2011/02/Wolters-Understanding-procrastination-from-a-self-regulated-learning-perspective.pdf). *Journal of Educational Psychology, 95(1)*: 179-187.\n\nWorld Organization of the Scout Movement (1998). *Scouting: An educational system*. Geneva, Switzerland: World Scout Bureau.\n\nZimmerman (2002). [Becoming a self-regulated learner: An overview](http://commonsenseatheism.com/wp-content/uploads/2011/02/Zimmerman-Becoming-a-self-regulated-learner.pdf). *Theory into Practice, 41(2)*: 64-70."
          },
          "voteCount": 222
        },
        {
          "name": "Evaporative Cooling of Group Beliefs",
          "type": "post",
          "slug": "evaporative-cooling-of-group-beliefs",
          "_id": "ZQG9cwKbct2LtmL3p",
          "url": null,
          "title": "Evaporative Cooling of Group Beliefs",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Groupthink"
            },
            {
              "name": "Social & Cultural Dynamics"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Early studiers of cults were surprised to discover than when cults receive a major shock—a prophecy fails to come true, a moral flaw of the founder is revealed—they often come back stronger than before, with increased belief and fanaticism. The Jehovah’s Witnesses placed Armageddon in 1975, based on Biblical calculations; 1975 has come and passed. The Unarian cult, still going strong today, survived the nonappearance of an intergalactic spacefleet on September 27, 1975.\n\nWhy would a group belief become _stronger_ after encountering crushing counterevidence?\n\nThe conventional interpretation of this phenomenon is based on cognitive dissonance. When people have taken “irrevocable” actions in the service of a belief—given away all their property in anticipation of the saucers landing—they cannot possibly admit they were mistaken. The challenge to their belief presents an immense cognitive dissonance; they must find reinforcing thoughts to counter the shock, and so become more fanatical. In this interpretation, the increased group fanaticism is the result of increased individual fanaticism.\n\nI was looking at a Java applet which demonstrates the use of evaporative cooling to form a Bose-Einstein condensate, when it occurred to me that another force entirely might operate to increase fanaticism. Evaporative cooling sets up a potential energy barrier around a collection of hot atoms. Thermal energy is essentially statistical in nature—not all atoms are moving at the exact same speed. The kinetic energy of any given atom varies as the atoms collide with each other. If you set up a potential energy barrier that’s just a little higher than the average thermal energy, the workings of chance will give an occasional atom a kinetic energy high enough to escape the trap. When an unusually fast atom escapes, it takes with it an unusually large amount of kinetic energy, and the average energy decreases. The group becomes substantially cooler than the potential energy barrier around it.\n\nIn Festinger, Riecken, and Schachter’s classic _When Prophecy Fails_, one of the cult members walked out the door immediately after the flying saucer failed to land. Who gets fed up and leaves _first_? An _average_ cult member? Or a relatively skeptical member, who previously might have been acting as a voice of moderation, a brake on the more fanatic members?\n\nAfter the members with the highest kinetic energy escape, the remaining discussions will be between the extreme fanatics on one end and the slightly less extreme fanatics on the other end, with the group consensus somewhere in the “middle.”\n\nAnd what would be the analogy to collapsing to form a Bose-Einstein condensate? Well, there’s no real need to stretch the analogy that far. But you may recall that I used a fission chain reaction analogy for the affective death spiral; when a group ejects all its voices of moderation, then all the people encouraging each other, and suppressing dissents, may internally increase in average fanaticism.^[1](#fn1x57)^\n\nWhen [Ayn Rand](https://www.lesswrong.com/posts/96TBXaHwLbFyeAxrg/guardians-of-ayn-rand)’s long-running affair with Nathaniel Branden was revealed to the Objectivist membership, a substantial fraction of the Objectivist membership broke off and followed Branden into espousing an “open system” of Objectivism not bound so tightly to Ayn Rand. Who stayed with Ayn Rand even after the scandal broke? The ones who _really, really_ believed in her—and perhaps some of the undecideds, who, after the voices of moderation left, heard arguments from only one side. This may account for how the Ayn Rand Institute is (reportedly) more fanatical after the breakup than the original core group of Objectivists under Branden and Rand.\n\nA few years back, I was on a transhumanist mailing list where a small group espousing “social democratic transhumanism” vitriolically insulted every libertarian on the list. Most libertarians left the mailing list; most of the others gave up on posting. As a result, the remaining group shifted substantially to the left. Was this deliberate? Probably not, because I don’t think the perpetrators knew that much psychology.^[2](#fn2x57)^ At most, they might have thought to make themselves “bigger fish in a smaller pond.”\n\nThis is one reason why it’s important to be prejudiced in favor of tolerating dissent. Wait until substantially _after_ it seems to you justified in ejecting a member from the group, before actually ejecting. If you get rid of the old outliers, the group position will shift, and someone else will become the oddball. If you eject them too, you’re well on the way to becoming a Bose-Einstein condensate and, er, exploding.\n\nThe flip side: Thomas Kuhn believed that a science has to become a “paradigm,” with a shared technical language that excludes outsiders, before it can get any real work done. In the formative stages of a science, according to Kuhn, the adherents go to great pains to make their work comprehensible to outside academics. But (according to Kuhn) a science can only make real progress as a technical discipline once it abandons the requirement of outside accessibility, and scientists working in the paradigm assume familiarity with large cores of technical material in their communications. This sounds cynical, relative to what is usually said about public understanding of science, but I can definitely see a core of truth here.^[3](#fn3x57)^\n\n^[1](#fn1x57-bk)^No thermodynamic analogy here, unless someone develops a nuclear weapon that explodes when it gets cold.\n\n^[2](#fn2x57-bk)^For that matter, I can’t recall seeing the evaporative cooling analogy elsewhere, though that doesn’t mean it hasn’t been noted before.\n\n^[3](#fn3x57-bk)^My own theory of Internet moderation is that you have to be willing to exclude trolls and spam to get a conversation going. You must even be willing to exclude kindly but technically uninformed folks from technical mailing lists if you want to get any work done. A genuinely open conversation on the Internet degenerates fast.\n\nIt’s the _articulate_ trolls that you should be wary of ejecting, on this theory—they serve the hidden function of legitimizing less extreme disagreements. But you should not have so many articulate trolls that they begin arguing with each other, or begin to dominate conversations. If you have one person around who is the famous Guy Who Disagrees With Everything, anyone with a more reasonable, more moderate disagreement won’t look like the sole nail sticking out. This theory of Internet moderation may not have served me too well in practice, so take it with a grain of salt."
          },
          "voteCount": 98
        },
        {
          "name": "Occam's Razor",
          "type": "post",
          "slug": "occam-s-razor",
          "_id": "f4txACqDWithRi7hs",
          "url": null,
          "title": "Occam's Razor",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Occam's Razor"
            },
            {
              "name": "Principles"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The more complex an explanation is, the more evidence you need just to find it in belief-space. (In Traditional Rationality this is often phrased misleadingly, as “The more complex a proposition is, the more evidence is required to argue for it.”) How can we measure the complexity of an explanation? How can we determine how much evidence is required?\n\nOccam’s Razor is often phrased as “The simplest explanation that fits the facts.” Robert Heinlein replied that the simplest explanation is “The lady down the street is a witch; she did it.”\n\nOne observes that the length of an English sentence is not a good way to measure “complexity.” And “fitting” the facts by merely *failing to prohibit* them is insufficient.\n\nWhy, exactly, is the length of an English sentence a poor measure of complexity? Because when you speak a sentence aloud, you are using *labels* for concepts that the listener shares—the receiver has already stored the complexity in them. Suppose we abbreviated Heinlein’s whole sentence as “Tldtsiawsdi!” so that the entire explanation can be conveyed in one word; better yet, we’ll give it a short arbitrary label like “Fnord!” Does this reduce the complexity? No, because you have to tell the listener in advance that “Tldtsiawsdi!” stands for “The lady down the street is a witch; she did it.” “Witch,” itself, is a label for some extraordinary assertions—just because we all know what it means doesn’t mean the concept is simple.\n\nAn enormous bolt of electricity comes out of the sky and hits something, and the Norse tribesfolk say, “Maybe a really powerful agent was angry and threw a lightning bolt.” The human brain is the most complex artifact in the known universe. If *anger* seems simple, it’s because we don’t see all the neural circuitry that’s implementing the emotion. (Imagine trying to explain why *Saturday Night Live* is funny, to an alien species with no sense of humor. But don’t feel superior; you yourself have no sense of fnord.) The complexity of anger, and indeed the complexity of intelligence, was glossed over by the humans who hypothesized Thor the thunder-agent.\n\n*To a human,* Maxwell’s equations take much longer to explain than Thor. Humans don’t have a built-in vocabulary for calculus the way we have a built-in vocabulary for anger. You’ve got to explain your language, and the language behind the language, and the very concept of mathematics, before you can start on electricity.\n\nAnd yet it seems that there should be some sense in which Maxwell’s equations are *simpler* than a human brain, or Thor the thunder-agent.\n\nThere is. It’s *enormously* easier (as it turns out) to write a computer program that simulates Maxwell’s equations, compared to a computer program that simulates an intelligent emotional mind like Thor.\n\nThe formalism of Solomonoff induction measures the “complexity of a description” by the length of the shortest computer program which produces that description as an output. To talk about the “shortest computer program” that does something, you need to specify a space of computer programs, which requires a language and interpreter. Solomonoff induction uses Turing machines, or rather, bitstrings that specify Turing machines. What if you don’t like Turing machines? Then there’s only a constant complexity penalty to design your own universal Turing machine that interprets whatever code you give it in whatever programming language you like. Different inductive formalisms are penalized by a worst-case constant factor relative to each other, corresponding to the size of a universal interpreter for that formalism.\n\nIn the better (in my humble opinion) versions of Solomonoff induction, the computer program does not produce a deterministic prediction, but assigns probabilities to strings. For example, we could write a program to explain a fair coin by writing a program that assigns equal probabilities to all 2^N^ strings of length N. This is Solomonoff induction’s approach to *fitting* the observed data. The higher the probability a program assigns to the observed data, the better that program *fits* the data. And probabilities must sum to 1, so for a program to better “fit” one possibility, it must steal probability mass from some other possibility which will then “fit” much more poorly. There is no superfair coin that assigns 100% probability to heads and 100% probability to tails.\n\nHow do we trade off the fit to the data, against the complexity of the program? If you ignore complexity penalties, and think *only* about fit, then you will always prefer programs that claim to deterministically predict the data, assign it 100% probability. If the coin shows HTTHHT, then the program that claims that the coin was fixed to show HTTHHT fits the observed data 64 times better than the program which claims the coin is fair. Conversely, if you ignore fit, and consider *only* complexity, then the “fair coin” hypothesis will always seem simpler than any other hypothesis. Even if the coin turns up HTHHTHHHTHHHHTHHHHHT  . . .\n\nIndeed, the fair coin *is* simpler and it fits this data exactly as well as it fits any other string of 20 coinflips—no more, no less—but we see another hypothesis, seeming not too complicated, that fits the data much better.\n\nIf you let a program store one more binary bit of information, it will be able to cut down a space of possibilities by half, and hence assign twice as much probability to all the points in the remaining space. This suggests that one bit of program complexity should cost *at least* a “factor of two gain” in the fit. If you try to design a computer program that explicitly stores an outcome like HTTHHT, the six bits that you lose in complexity must destroy all plausibility gained by a 64-fold improvement in fit. Otherwise, you will sooner or later decide that all fair coins are fixed.\n\nUnless your program is being smart, and *compressing* the data, it should do no good just to move one bit from the data into the program description.\n\nThe way Solomonoff induction works to predict sequences is that you sum up over all allowed computer programs—if every program is allowed, Solomonoff induction becomes uncomputable—with each program having a prior probability of 1/2 to the power of its code length in bits, and each program is further weighted by its fit to all data observed so far. This gives you a weighted mixture of experts that can predict future bits.\n\nThe Minimum Message Length formalism is nearly equivalent to Solomonoff induction. You send a string describing a code, and then you send a string describing the data in that code. Whichever explanation leads to the shortest *total* message is the best. If you think of the set of allowable codes as a space of computer programs, and the code description language as a universal machine, then Minimum Message Length is nearly equivalent to Solomonoff induction.[^1^](#fn1x26)\n\nThis lets us see clearly the problem with using “The lady down the street is a witch; she did it” to explain the pattern in the sequence 0101010101. If you’re sending a message to a friend, trying to describe the sequence you observed, you would have to say: “The lady down the street is a witch; she made the sequence come out 0101010101.” Your accusation of witchcraft wouldn’t let you *shorten* the rest of the message; you would still have to describe, in full detail, the data which her witchery caused.\n\nWitchcraft may fit our observations in the sense of qualitatively *permitting* them; but this is because witchcraft permits *everything* , like saying “Phlogiston!” So, even after you say “witch,” you still have to describe all the observed data in full detail. You have not *compressed the total length of the message describing your observations* by transmitting the message about witchcraft; you have simply added a useless prologue, increasing the total length.\n\nThe real sneakiness was concealed in the word “it” of “A witch did it.” A witch did *what*?\n\nOf course, thanks to [hindsight bias](https://lesswrong.com/lw/il/hindsight_bias/) and [anchoring](https://www.lesswrong.com/rationality/anchoring-and-adjustment) and [fake explanations](https://www.lesswrong.com/rationality/fake-explanations) and [fake causality](https://www.lesswrong.com/rationality/fake-causality) and [positive bias](https://www.lesswrong.com/rationality/positive-bias-look-into-the-dark) and [motivated cognition](https://www.lesswrong.com/rationality/knowing-about-biases-can-hurt-people), it may seem all too obvious that if a woman is a witch, of *course* she would make the coin come up 0101010101. But I’ll get to that soon enough. . .\n\n* * *\n\n^1^ Nearly, because it chooses the *shortest* program, rather than summing up over all programs."
          },
          "voteCount": 79
        },
        {
          "name": "Anchoring and Adjustment",
          "type": "post",
          "slug": "anchoring-and-adjustment",
          "_id": "bMkCEZoBNhgRBtzoj",
          "url": null,
          "title": "Anchoring and Adjustment",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Priming"
            },
            {
              "name": "Anchoring"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Suppose I spin a Wheel of Fortune device as you watch, and it comes up pointing to 65. Then I ask: Do you think the percentage of countries in the United Nations that are in Africa is above or below this number? What do you think is the percentage of UN countries that are in Africa? Take a moment to consider these two questions yourself, if you like, and please don’t Google.\n\nAlso, try to guess, within _five seconds_, the value of the following arithmetical expression. Five seconds. Ready? Set . . . _Go!_\n\n**1 × 2 × 3 × 4 × 5 × 6 × 7 × 8**\n\nTversky and Kahneman recorded the estimates of subjects who saw the Wheel of Fortune showing various numbers.^[1](#fn1x38)^ The median estimate of subjects who saw the wheel show 65 was 45%; the median estimate of subjects who saw 10 was 25%.\n\nThe current theory for this and similar experiments is that subjects take the initial, uninformative number as their starting point or _anchor_; and then they _adjust_ upward or downward from their starting estimate until they reach an answer that “sounds plausible”; and then they stop adjusting. This typically results in under-adjustment from the anchor—more distant numbers could also be “plausible,” but one stops at the first satisfying-sounding answer.\n\nSimilarly, students shown “1 × 2 × 3 × 4 × 5 × 6 × 7 × 8” made a median estimate of 512, while students shown “8 × 7 × 6 × 5 × 4 × 3 × 2 × 1” made a median estimate of 2,250. The motivating hypothesis was that students would try to multiply (or guess-combine) the first few factors of the product, then adjust upward. In both cases the adjustments were insufficient, relative to the true value of 40,320; but the first set of guesses were much more insufficient because they started from a lower anchor.\n\nTversky and Kahneman report that offering payoffs for accuracy did not reduce the anchoring effect.\n\nStrack and Mussweiler asked for the year Einstein first visited the United States.^[2](#fn2x38)^ Completely implausible anchors, such as 1215 or 1992, produced anchoring effects just as large as more plausible anchors such as 1905 or 1939.\n\nThere are obvious applications in, say, salary negotiations, or buying a car. I won’t suggest that you exploit it, but watch out for exploiters.\n\nAnd watch yourself thinking, and try to notice when you are _adjusting_ a figure in search of an estimate.\n\nDebiasing manipulations for anchoring have generally proved not very effective. I would suggest these two: First, if the initial guess sounds implausible, try to throw it away entirely and come up with a new estimate, rather than sliding from the anchor. But this in itself may not be sufficient—subjects instructed to avoid anchoring still seem to do so.^[3](#fn3x38)^ So, second, even if you are trying the first method, try also to think of an anchor in the opposite direction—an anchor that is clearly too small or too large, instead of too large or too small—and dwell on it briefly.\n\n^[1](#fn1x38-bk)^Amos Tversky and Daniel Kahneman, “Judgment Under Uncertainty: Heuristics and Biases,” _Science_ 185, no. 4157 (1974): 1124–1131.\n\n^[2](#fn2x38-bk)^Fritz Strack and Thomas Mussweiler, “Explaining the Enigmatic Anchoring Effect: Mechanisms of Selective Accessibility,” _Journal of Personality and Social Psychology_ 73, no. 3 (1997): 437–446.\n\n^[3](#fn3x38-bk)^George A. Quattrone et al., “Explorations in Anchoring: The Effects of Prior Range, Anchor Extremity, and Suggestive Hints” (Unpublished manuscript, Stanford University, 1981)."
          },
          "voteCount": 50
        },
        {
          "name": "The Least Convenient Possible World",
          "type": "post",
          "slug": "the-least-convenient-possible-world",
          "_id": "neQ7eXuaXpiYw7SBy",
          "url": null,
          "title": "The Least Convenient Possible World",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Steelmanning"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Hypotheticals"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [Is That Your True Rejection?](http://www.overcomingbias.com/2008/12/your-true-rejec.html)\n\n_\"If you’re interested in being on the right side of disputes, you will refute your opponents’ arguments.  But if you’re interested in producing truth, you will fix your opponents’ arguments for them.  To win, you must fight not only the creature you encounter; you must fight the most horrible thing that can be constructed from its corpse.\"_\n\n_   --_ [Black Belt Bayesian](http://www.acceleratingfuture.com/steven/?p=155), via [Rationality Quotes 13](http://www.overcomingbias.com/2008/09/rationality-quo.html)\n\nYesterday [John Maxwell's post](/lw/2b/so_you_say_youre_an_altruist/) wondered [how much the average person would do](http://www.aaronsw.com/weblog/handwritingwall) to save ten people from a ruthless tyrant. I remember asking some of my friends a vaguely related question as part of an investigation of the [Trolley Problems](http://en.wikipedia.org/wiki/Trolley_Problem):\n\n> You are a doctor in a small rural hospital. You have ten patients, each of whom is dying for the lack of a separate organ; that is, one person needs a heart transplant, another needs a lung transplant, another needs a kidney transplant, and so on. A traveller walks into the hospital, mentioning how he has no family and no one knows that he's there. All of his organs seem healthy. You realize that by killing this traveller and distributing his organs among your patients, you could save ten lives. Would this be moral or not?\n\nI don't want to discuss the answer to this problem today. I want to discuss the answer one of my friends gave, because I think it illuminates a very interesting kind of defense mechanism that rationalists need to be watching for. My friend said:\n\n> It wouldn't be moral. After all, people often reject organs from random donors. The traveller would probably be a genetic mismatch for your patients, and the transplantees would have to spend the rest of their lives on immunosuppressants, only to die within a few years when the drugs failed.\n\nOn the one hand, I have to give my friend credit: his answer is biologically accurate, and beyond a doubt the technically correct answer to the question I asked. On the other hand, I don't have to give him very _much_ credit: he completely missed the point and lost a valuable effort to examine the nature of morality.  \n  \nSo I asked him, \"In the least convenient possible world, the one where everyone was genetically compatible with everyone else and this objection was invalid, what would you do?\"  \n  \nHe mumbled something about counterfactuals and refused to answer. But I learned something very important from him, and that is to always ask this question of _myself_. Sometimes the least convenient possible world is the only place where I can figure out my true motivations, or which step to take next. I offer three examples:\n\n**1:  Pascal's Wager**. Upon being presented with Pascal's Wager, one of the first things most atheists think of is this:\n\nPerhaps God values intellectual integrity so highly that He is prepared to reward honest atheists, but will punish anyone who practices a religion he does not truly believe simply for personal gain. Or perhaps, as the Discordians claim, \"Hell is reserved for people who believe in it, and the hottest levels of Hell are reserved for people who believe in it on the principle that they'll go there if they don't.\"  \n  \nThis is a good argument against Pascal's Wager, but it isn't the least convenient possible world. The least convenient possible world is the one where Omega, the completely trustworthy superintelligence who is always right, informs you that God definitely doesn't value intellectual integrity that much. In fact (Omega tells you) either God does not exist or the Catholics are right about absolutely everything.  \n  \nWould you become a Catholic in this world? Or are you willing to admit that maybe your rejection of Pascal's Wager has less to do with a hypothesized pro-atheism God, and more to do with a belief that it's wrong to abandon your intellectual integrity on the off chance that a crazy deity is playing a perverted game of blind poker with your eternal soul?\n\n**2:** **The God-Shaped Hole.** Christians claim there is one in every atheist, keeping him from spiritual fulfillment.\n\nSome commenters on [Raising the Sanity Waterline](/lw/1e/raising_the_sanity_waterline/) don't deny the existence of such a hole, if it is intepreted as a desire for purpose or connection to something greater than one's self. But, some commenters say, science and rationality can fill this hole even better than God can.  \n  \nWhat luck! Evolution has by a wild coincidence created us with a big rationality-shaped hole in our brains! Good thing we happen to be rationalists, so we can fill this hole in the best possible way! I don't know - despite my sarcasm this may even be true. But in the least convenient possible world, Omega comes along and tells you that sorry, the hole is exactly God-shaped, and anyone without a religion will lead a less-than-optimally-happy life. Do you head down to the nearest church for a baptism? Or do you admit that even if believing something makes you happier, you still don't want to believe it unless it's true?  \n\n**3**: **Extreme Altruism.** John Maxwell mentions the utilitarian argument for donating almost everything to charity.\n\nSome commenters object that many forms of charity, especially the classic \"give to starving African orphans,\" are counterproductive, either because they enable dictators or thwart the free market. This is quite true.  \n  \nBut in the least convenient possible world, here comes Omega again and tells you that Charity X has been proven to do exactly what it claims: help the poor without any counterproductive effects. So is your real objection the corruption, or do you just not believe that you're morally obligated to give everything you own to starving Africans?\n\nYou may argue that this citing of convenient facts is at worst a venial sin. If you still get to the correct answer, and you do it by a correct method, what does it matter if this method isn't really the one that's convinced you personally?  \n  \nOne easy answer is that it saves you from embarrassment later. If some scientist does a study and finds that people really do have a god-shaped hole that can't be filled by anything else, no one can come up to you and say \"Hey, didn't you say the reason you didn't convert to religion was because rationality filled the god-shaped hole better than God did? Well, I have some bad news for you...\"  \n  \nAnother easy answer is that your real answer teaches you something about yourself. My friend may have successfully avoiding making a distasteful moral judgment, but he didn't learn anything about morality. My refusal to take the easy way out on the transplant question helped me develop the form of precedent-utilitarianism I use today.  \n  \nBut more than either of these, it matters because it seriously influences where you go next.  \n  \nSay \"I accept the argument that I need to donate almost all my money to poor African countries, but my only objection is that corrupt warlords might get it instead\", and the _obvious_ next step is to see if there's a poor African country without corrupt warlords (see: Ghana, Botswana, etc.) and donate almost all your money to them. Another acceptable answer would be to donate to another warlord-free charitable cause like the Singularity Institute.  \n  \nIf you _just_ say \"Nope, corrupt dictators might get it,\" you may go off and spend the money on a new TV. Which is fine, _if_ a new TV is what you really want. But if you're the sort of person who _would have_ been convinced by John Maxwell's argument, but you dismissed it by saying \"Nope, corrupt dictators,\" then you've lost an opportunity to change your mind.  \n  \nSo I recommend: limit yourself to responses of the form \"I completely reject the entire basis of your argument\" or \"I accept the basis of your argument, but it doesn't apply to the real world because of contingent fact X.\" If you just say \"Yeah, well, contigent fact X!\" and walk away, you've left yourself too much wiggle room.  \n  \nIn other words: always have a plan for what you would do in the least convenient possible world."
          },
          "voteCount": 234
        },
        {
          "name": "Scarcity",
          "type": "post",
          "slug": "scarcity",
          "_id": "MCYp8g9EMAiTCTawk",
          "url": null,
          "title": "Scarcity",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Heuristics & Biases"
            },
            {
              "name": "Dark Arts"
            },
            {
              "name": "Censorship"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "What follows is taken primarily from Robert Cialdini's _Influence: The Psychology of Persuasion._  I own three copies of this book, one for myself, and two for loaning to friends.\n\nS_carcity,_ as that term is used in social psychology, is when things become _more desirable_ as they appear _less obtainable_.\n\n*   If you put a two-year-old boy in a room with two toys, one toy in the open and the other behind a Plexiglas wall, the two-year-old will ignore the easily accessible toy and go after the apparently forbidden one.  If the wall is low enough to be easily climbable, the toddler is no more likely to go after one toy than the other.  (Brehm and Weintraub 1977.)  \n*   When Dade County forbade use or possession of phosphate detergents, many Dade residents drove to nearby counties and bought huge amounts of phosphate laundry detergents.  Compared to Tampa residents not affected by the regulation, Dade residents rated phosphate detergents as gentler, more effective, more powerful on stains, and even believed that phosphate detergents poured more easily.  (Mazis 1975, Mazis et. al. 1973.)\n\nSimilarly, information that appears forbidden or secret, seems more important and trustworthy:\n\n*   When University of North Carolina students learned that a speech opposing coed dorms had been banned, they became more opposed to coed dorms (without even hearing the speech).  (Probably in Ashmore et. al. 1971.)  \n*   When a driver said he had liability insurance, experimental jurors awarded his victim an average of four thousand dollars more than if the driver said he had no insurance.  If the judge afterward informed the jurors that information about insurance was inadmissible and must be ignored, jurors awarded an average of thirteen thousand dollars more than if the driver had no insurance.  (Broeder 1959.)  \n*   Buyers for supermarkets, told by a supplier that beef was in scarce supply, gave orders for twice as much beef as buyers told it was readily available.  Buyers told that beef was in scarce supply, and furthermore, that the information about scarcity was itself scarce—that the shortage was not general knowledge—ordered six times as much beef.  (Since the study was conducted in a real-world context, the information provided was in fact correct.)  (Knishinsky 1982.)\n\nThe conventional theory for explaining this is \"psychological reactance\", social-psychology-speak for \"When you tell people they can't do something, they'll just try even harder.\"  The fundamental instincts involved appear to be preservation of status and preservation of options.  We resist dominance, when any human agency tries to restrict our freedom.  And when options seem to be in danger of disappearing, even from natural causes, we try to leap on the option before it's gone.\n\nLeaping on disappearing options may be a good adaptation in a [hunter-gatherer](/lw/l0/adaptationexecuters_not_fitnessmaximizers/) society—gather the fruits while the tree is still in bloom—but in a money-based society it can be rather costly.   Cialdini (1993) reports that in one appliance store he observed, a salesperson who saw that a customer was evincing signs of interest in an appliance would approach, and sadly inform the customer that the item was out of stock, the last one having been sold only twenty minutes ago.  Scarcity creating a sudden jump in desirability, the customer would often ask whether there was any chance that the salesperson could locate an unsold item in the back room, warehouse, or anywhere.  \"Well,\" says the salesperson, \"that's possible, and I'm willing to check; but do I understand that this is the model you want, and if I can find it at this price, you'll take it?\"\n\nAs Cialdini remarks, a chief sign of this malfunction is that you dream of _possessing_ something, rather than _using_ it.  (Timothy Ferriss offers similar advice on planning your life: ask which _ongoing experiences_ would make you happy, rather than which possessions or status-changes.)\n\nBut the really fundamental problem with desiring the unattainable is that [as soon as you actually _get_ it, it stops being unattainable](/lw/ou/if_you_demand_magic_magic_wont_help/).  If we cannot take joy in the merely available, our lives will _always_ be frustrated...\n\n* * *\n\nAshmore, R. D., Ramachandra, V. and Jones, R. A. (1971.) \"Censorship as an Attitude Change Induction.\"  Paper presented at Eastern Psychological Association meeting, New York, April 1971.\n\nBrehm, S. S. and Weintraub, M. (1977.) \"Physical Barriers and Psychological Reactance: Two-year-olds' Responses to Threats to Freedom.\"  _Journal of Personality and Social Psychology,_ **35**: 830-36.\n\nBroeder, D. (1959.)  \"The University of Chicago Jury Project.\" _Nebraska Law Review_ **38**: 760-74.\n\nCialdini, R. B. (1993.)  _Influence:  The Psychology of Persuasion: Revised Edition._  Pp. 237-71.  New York: Quill.\n\nKnishinsky, A. (1982.)  \"The Effects of Scarcity of Material and Exclusivity of Information on Industrial Buyer Perceived Risk in Provoking a Purchase Decision.\"  Doctoral dissertation, Arizona State University.\n\nMazis, M. B. (1975.) \"Antipollution Measures and Psychological Reactance Theory: A Field Experiment.\" _Journal of Personality and Social Psychology_ **31**: 654-66.\n\nMazis, M. B., Settle, R. B. and Leslie, D. C. (1973.) \"Elimination of Phosphate Detergents and Psychological Reactance.\"  _Journal of Marketing Research_ **10**: 390-95."
          },
          "voteCount": 47
        },
        {
          "name": "The Neglected Virtue of Scholarship",
          "type": "post",
          "slug": "the-neglected-virtue-of-scholarship",
          "_id": "64FdKLwmea8MCLWkE",
          "url": null,
          "title": "The Neglected Virtue of Scholarship",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Virtues"
            },
            {
              "name": "Scholarship & Learning"
            },
            {
              "name": "World Modeling"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Eliezer Yudkowsky identifies _scholarship_ as one of the [Twelve Virtues of Rationality](http://yudkowsky.net/rational/virtues):\n\n> Study many sciences and absorb their power as your own. Each field that you consume makes you larger... It is especially important to eat math and science which impinges upon rationality: Evolutionary psychology, heuristics and biases, social psychology, probability theory, decision theory. But these cannot be the only fields you study...\n\nI think he's right, and I think scholarship doesn't get enough praise - _even on Less Wrong_, where it is [regularly](/lw/2un/references_resources_for_lesswrong/) [encouraged](/lw/2sw/math_prerequisites_for_understanding_lw_stuff/).\n\nFirst, consider the evangelical atheist community [to which I belong](http://commonsenseatheism.com/). There is a tendency for lay atheists to write \"refutations\" of theism without first doing a modicum of research on the current state of the arguments. This can get atheists into trouble when they go toe-to-toe with a theist who _did_ do his homework. I'll share [two examples](http://commonsenseatheism.com/?p=11376):\n\n*   In a [debate](http://www.youtube.com/watch?v=AjOSNj97_gk&feature=related) with theist Bill Craig, agnostic Bart Ehrman paraphrased [David Hume's argument](http://en.wikipedia.org/wiki/Of_Miracles) that we can't demonstrate the occurrence of a miracle in the past. Craig [responded](http://commonsenseatheism.com/wp-content/uploads/2010/04/craig-ehrman.pdf) with a PowerPoint slide showing Bayes' Theorem, and explained that Ehrman was only considering prior probabilities, when of course he needed to consider the relevant conditional probabilities as well. Ehrman failed to respond to this, and looked as though he had never seen Bayes' Theorem before. Had Ehrman practiced the virtue of scholarship on this issue, he might have noticed that much of the scholarly work on Hume's argument in the past two decades [has](http://www.amazon.com/Resurrection-God-Incarnate-Richard-Swinburne/dp/0199257469/) [involved](http://www.amazon.com/Humes-Abject-Failure-Argument-Miracles/dp/0195127382/) [Bayes'](http://www.amazon.com/Arguing-about-Gods-Graham-Oppy/dp/0521122643/) [Theorem](http://www.lydiamcgrew.com/Resurrectionarticlesinglefile.pdf). He might also have discovered that the correct response to Craig's use of Bayes' Theorem can be found in [pages 298-341](http://commonsenseatheism.com/wp-content/uploads/2010/09/Sobel-on-Hume-on-Miracles.pdf) of J.H. Sobel’s _[Logic and Theism](http://commonsenseatheism.com/wp-content/uploads/2010/09/Sobel-on-Hume-on-Miracles.pdf)_.  \n      \n    \n*   In another [debate](http://commonsenseatheism.com/?p=1230) with Bill Craig, atheist Christopher Hitchens gave this objection: \"Who designed the Designer? Don’t you run the risk… of asking 'Well, where does that come from? And where does that come from?' and running into an infinite regress?\" But this is an _elementary_ misunderstanding in philosophy of science. Why? Because [every successful scientific explanation faces the exact same problem](http://commonsenseatheism.com/?p=6113). It’s called the “why regress” because no matter what explanation is given of something, you can always still ask “Why?” Craig pointed this out and handily won that part of the debate. Had Hitchens had a passing understanding of science or explanation, he could have avoided looking foolish, and also spent more time on _substantive_ objections to theism. (One _can_ give a \"Who made God?\" objection to theism that [has some meat](http://omnisaffirmatioestnegatio.wordpress.com/2010/06/12/dawkins-and-the-ultimate-747-gambit/), but that's not the one Hitchens gave. Hitchens' objection concerned an infinite regress of explanations, which is just as much a feature of science as it is of theism.)\n\nThe lesson I take from these and a hundred other examples is to employ the rationality virtue of scholarship. Stand on the shoulders of giants. We don't each need to cut our own path into a subject right from the point of near-total ignorance. That's silly. Just catch the bus on the road of knowledge paved by hundreds of diligent workers before you, and get off somewhere near where the road finally fades into fresh jungle. Study enough to have a view of the _current_ state of the debate so you don't waste your time on paths that have already dead-ended, or on arguments that have already been refuted. Catch up before you speak up.\n\nThis is why, in more than 1000 posts on [my own blog](http://commonsenseatheism.com/), I've said almost _nothing_ that is original. Most of my posts instead summarize what other experts have said, in an effort to bring myself and my readers up to the level of the _current_ debate on a subject before we try to make _new_ contributions to it.\n\nThe Less Wrong community is a particularly smart and well-read bunch, but of course it doesn't always embrace the virtue of scholarship.\n\nConsider the field of [formal epistemology](http://en.wikipedia.org/wiki/Formal_epistemology), an entire branch of philosophy devoted to (1) mathematically formalizing concepts related to induction, belief, choice, and action, and (2) arguing about the foundations of probability, statistics, game theory, decision theory, and algorithmic learning theory. These are central discussion topics at Less Wrong, and yet my own experience suggests that most Less Wrong readers have never heard of the entire field, let alone read any works by formal epistemologists, such as _[In Defense of Objective Bayesianism](http://www.amazon.com/Defence-Objective-Bayesianism-Jon-Williamson/dp/0199228000/)_ by Jon Williamson or _[Bayesian Epistemology](http://www.amazon.com/Bayesian-Epistemology-Luc-Bovens/dp/0199270406/)_ by Luc Bovens and Stephan Hartmann.\n\nOr, consider a recent post by Yudkowsky: [Working hurts less than procrastinating, we fear the twinge of starting](/lw/3kv/working_hurts_less_than_procrastinating_we_fear/). The post attempts to make progress against procrastination by practicing single-subject phenomenology, rather than by first catching up with [a quick summary of scientific research on procrastination](http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-The-Nature-of-Procrastination.pdf). The post's approach to the problem looks inefficient to me. It's not standing on the shoulders of giants.\n\nThis post probably looks harsher than I mean it to be. After all, Less Wrong _is_ pretty damn good at scholarship compared to most communities. But I think it could be better.\n\nHere's my suggestion. Every time you're tempted to tackle a serious question in a subject on which you're not already an expert, ask yourself: \"Whose giant shoulders can I stand on, here?\"\n\nUsually, you can answer the question by doing the following:\n\n1.  Read the Wikipedia article on the subject, and glance over the references.\n2.  Read the article on the subject in a field-specific encyclopedia. For example if you're probing a philosophical concept, find the relevant essay(s) in _[The Routledge Encyclopedia of Philosophy](http://www.rep.routledge.com/)_ or the _[Internet Encyclopedia of Philosophy](http://www.iep.utm.edu/)_ or the _[Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/)_. Often, the encyclopedia you want is at your local library or can be browsed at [Google Books](http://books.google.com/books?id=XHU8V3fkOXwC&printsec=frontcover#v=onepage&q&f=false).\n3.  Read or skim-read an entry-level university textbook on the subject.\n\nThere are so many resources for learning available today, the virtue of scholarship has never in human history been so easy to practice."
          },
          "voteCount": 251
        },
        {
          "name": "Scholarship: How to Do It Efficiently",
          "type": "post",
          "slug": "scholarship-how-to-do-it-efficiently",
          "_id": "37sHjeisS9uJufi4u",
          "url": null,
          "title": "Scholarship: How to Do It Efficiently",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Scholarship & Learning"
            },
            {
              "name": "Practical"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Review articles and textbooks are king",
                "anchor": "Review_articles_and_textbooks_are_king",
                "level": 1
              },
              {
                "title": "Going granular",
                "anchor": "Going_granular",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "146 comments"
              }
            ],
            "headingsCount": 4
          },
          "contents": {
            "markdown": "Scholarship is [an important virtue of rationality](/lw/3m3/the_neglected_virtue_of_scholarship/), but it can be [costly](/lw/4sn/costs_and_benefits_of_scholarship/). Its major costs are _time_ and _effort_. Thus, if you can reduce the time and effort required for scholarship - if you can learn to do scholarship _more efficiently_ \\- then scholarship will be worth your effort more often than it previously was.\n\nAs an autodidact who now consumes [whole](/lw/3w3/how_to_beat_procrastination/) [fields](/lw/4z7/the_neuroscience_of_desire/) [of](/lw/3gv/statistical_prediction_rules_outperform_expert/) [knowledge](/lw/4yq/the_neuroscience_of_pleasure/) [in](/lw/59v/intuition_and_unconscious_learning/) [mere](/lw/531/how_you_make_judgments_the_elephant_and_its_rider/) [weeks](/lw/4su/how_to_be_happy/), I've developed efficient habits that allow me to research topics quickly. I'll share my research habits with you now.\n\n#### Review articles and textbooks are king\n\nMy first task is to find scholarly review (or 'survey') articles on my chosen topic from the past five years (the more recent, the better). A good review article provides:\n\n1.  An overview of the subject matter of the field and the _terms_ being used (for [scholarly googling](http://scholar.google.com/) later).\n2.  An overview of the open and solved problems in the field, and which researchers are working on them.\n3.  Pointers to the key studies that give researchers their current understanding of the topic.\n\nIf you can find a recent scholarly edited volume of review articles on the topic, then you've _hit the jackpot_. (Edited volumes are better than single-author volumes, because when starting out you want to avoid reading only one particular researcher's perspective.) Examples from my own research of _just this year_ include:\n\n*   Affective neuroscience: _[Pleasures of the Brain](http://www.amazon.com/Pleasures-Affective-Science-Morten-Kringelbach/dp/0195331028/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009)\n*   Neuroeconomics: _[Decision Making and the Brain](http://www.amazon.com/Neuroeconomics-Decision-Paul-W-Glimcher/dp/0123741769/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2008)\n*   Dual process theories of psychology: _[In Two Minds](http://www.amazon.com/Two-Minds-Dual-Processes-Beyond/dp/0199230161/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009)\n*   Intuition and unconscious learning: _[Intuition in Judgment and Decision Making](http://www.amazon.com/Intuition-Judgment-Decision-Henning-Plessner/dp/0805857419/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2007)\n*   Goals: _[The Psychology of Goals](http://www.amazon.com/Psychology-Goals-Gordon-Moskowitz-PhD/dp/1606230298/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009)\n*   Catastrophic risks: _[Global Catastrophic Risks](http://www.amazon.com/Global-Catastrophic-Risks-Nick-Bostrom/dp/0198570503/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2008)\n\nIf the field is large enough, there may exist an edited 'Handbook' on the subject, which is basically just a _very large_ scholarly edited volume of review articles. Examples: _[Oxford Handbook of Evolutionary Psychology](http://www.amazon.com/Oxford-Handbook-Evolutionary-Psychology-Library/dp/0198568304/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2007), _[Oxford Handbook of Positive Psychology](http://www.amazon.com/Oxford-Handbook-Positive-Psychology-Library/dp/0195187245/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009), _[Oxford Handbook of Philosophy and Neuroscience](http://www.amazon.com/Oxford-Handbook-Philosophy-Neuroscience/dp/0195304780/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009), _[Handbook of Developmental Cognitive Neuroscience](http://www.amazon.com/Handbook-Developmental-Cognitive-Neuroscience-Charles/dp/0262141043/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2008), _[Oxford Handbook of Neuroethics](http://www.amazon.com/Oxford-Handbook-Neuroethics-Library-Psychology/dp/0199570701/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2011), _[Handbook of Relationship Intitiation](http://www.amazon.com/gp/product/0805861599/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2008), and _[Handbook of Implicit Social Cognition](http://www.amazon.com/Handbook-Implicit-Social-Cognition-Applications/dp/1606236733/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2010). For the humanities, see the [Blackwell Companions](http://www.amazon.com/s/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) and [Cambridge Companions](http://cco.cambridge.org/collection?id=complete).\n\nIf your questions are basic enough, a recent entry-level textbook on the subject may be just as good. Textbooks are basically book-length review articles written for undergrads. Textbooks I purchased this year include:\n\n*   _[Evolutionary Psychology: The New Science of Mind, 4th edition](http://www.amazon.com/Evolutionary-Psychology-New-Science-Mind/dp/020501562X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2011)\n*   _[Artificial Intelligence: A Modern Approach, 3rd edition](http://www.amazon.com/Artificial-Intelligence-Modern-Approach-3rd/dp/0136042597/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009)\n*   _[Psychology Applied to Modern Life, 10th edition](http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2011)\n*   _[Psychology, 9th edition](http://www.amazon.com/Psychology-David-G-Myers/dp/1429215976/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2009)\n\nUse [Google Books](http://books.google.com/) and Amazon's 'Look Inside' feature to see if the books appear to be of high quality, and likely to answer the questions you have. Also check the textbook recommendations [here](/lw/3gu/the_best_textbooks_on_every_subject/). You can save money by checking [Library Genesis](http://gen.lib.rus.ec/) and [library.nu](http://library.nu/) for a PDF copy first, or by buying [used books](http://www.bookfinder.com/), or by buying ebook versions from [Amazon](http://www.amazon.com/kindle-store-ebooks-newspapers-blogs/b?ie=UTF8&node=133141011), [B&N](http://www.barnesandnoble.com/ebooks/index.asp), or [Google](http://books.google.com/ebooks).\n\nKeep in mind that if you take the [virtue of scholarship](http://yudkowsky.net/rational/virtues) seriously, you may need to change how you think about the cost of obtaining knowledge. Purchasing the _right_ book can save you _dozens_ of hours of research. Because a huge part of my life these days is devoted to scholarship, a significant portion of my monthly budget is set aside for purchasing knowledge. So far this year I've averaged over $150/mo spent on textbooks and scholarly edited volumes.\n\nRecent scholarly review articles can also be found on [Google scholar](http://scholar.google.com/). Search for key terms, and review articles will often be listed near the top of the results because review articles are cited widely. For example, result #9 on Google scholar for [procrastination](http://scholar.google.com/scholar?q=procrastination&hl=en&btnG=Search&as_sdt=2001&as_sdtp=on) is \"[The nature of procrastination](http://commonsenseatheism.com/wp-content/uploads/2011/02/Steel-The-Nature-of-Procrastination.pdf)\" (2007) by Piers Steel, the first half of which is a review article, while the second half is a meta-analysis. Bingo.\n\nYou can also search Amazon for key terms. I recently searched Amazon for 'attention neuroscience.' Result #2 was [a 2004 scholarly edited volume](http://www.amazon.com/Attention-Action-Cognitive-Neuroscience-Behavioural/dp/1841693545/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) on the subject. A bit old, but not bad for my first search! I found the PDF on [library.nu](http://library.nu/search?q=attention%20in%20action).\n\nIn order to find good review articles, textbooks, and scholarly edited volumes you may first need to figure out what the terminology is. When I wanted to understand the neuroscience of pleasure and desire, it took me a while to figure out that the neuroscience of emotions is called _affective neuroscience_. After consuming that field, I had learned a lot about pleasure but not much about desire. I then realized that I didn't care about desire _as an emotion_ but instead as _a driver of action under uncertainty_. That aspect of desire, it turns out, is studied not under the field of affective neuroscience but instead _neuroeconomics_.\n\nSimilarly, when I was originally looking for 'scientific self-help', I had trouble finding review articles or textbooks on the subject. It took me months to discover that professionals call this the _psychology of adjustment_. Who would have guessed that? But once I knew the term, I quickly found [two](http://www.amazon.com/Psychology-Applied-Modern-Life-Adjustment/dp/1111186634/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [textbooks](http://www.amazon.com/Human-Adjustment--Psych-CD-ROM-Santrock/dp/0073111910/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) on the subject, which were good starting points for understanding [the field](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/).\n\nNote that not every scholarly edited volume is a volume of _review articles_. _[New Waves in Philosophy of Action](http://www.amazon.com/Waves-Philosophy-Action-Jes%C3%BAs-Aguilar/dp/0230230601/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ is a collection of new research articles, not a collection of review articles. It is a poor entry point into the field. Some edited volumes are _okay_ entry points into the field because they are a mix of review articles and original research, for example _[Machine Ethics](http://www.amazon.com/Machine-Ethics-Michael-Anderson/dp/0521112354/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_ (2011). But remember that a 'good' edited volume on a subject does not protect you from the entire _field_ being [mostly misguided](/lw/4ba/some_heuristics_for_evaluating_the_soundness_of/), like machine ethics or [mainstream philosophy](/lw/4zs/philosophy_a_diseased_discipline/).\n\nAlso note that if you can't find an edited volume on your subject, one may be just around the corner. In 2007 there was no decent edited volume on neuroeconomics, but there were [three](http://commonsenseatheism.com/wp-content/uploads/2011/03/Camerer-Neuroeconomics-how-neuroscience-can-inform-economics.pdf) [review](http://commonsenseatheism.com/wp-content/uploads/2011/03/Sanfey-Neuroeconomics-cross-currents-in-research-on-decision-making.pdf) [articles](http://commonsenseatheism.com/wp-content/uploads/2011/03/Glimcher-Neuroeconomics-the-consilience-of-brain-and-decision.pdf). Then in 2008, _[Decision Making and the Brain](http://www.amazon.com/Neuroeconomics-Decision-Paul-W-Glimcher/dp/0123741769/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) _was released.\n\n#### Going granular\n\nOnce textbooks and review articles have given you a good overview of the key concepts and terms, open and closed problems, studies and researchers on your chosen topic, it's time to go granular.\n\nTextbooks and review articles will point you to the articles most directly relevant for answering the questions you have, and the researchers working on the problems you care about. Visit researchers' home pages and check their 'recent publications' lists. Find the papers on [Google Scholar](http://scholar.google.com/) and read the abstracts. Make a list of the ones you need to read more closely. You'll be able to download many of them directly from links found on Google Scholar. For others, you'll need to visit a university library's computer lab to download the papers. The university will have subscribed to many of the databases that carry the papers, and university computers will let you past the paywall (but on-campus wifi will not). To get access to a paper you can't get at a nearby university, you can:\n\n1.  Contact the author via email and request a copy (or a preprint), explaining that you can't get it elsewhere.\n2.  Ask your friends at other universities to check if their university has access to it.\n3.  Look to see if the article has been published in a book that is available at your library or online.\n\nI've never purchased an article from an online database because the prices are outrageous: $15-$40 for a 20-page article, usually. If I absolutely can't get access to an article, I make a judgement as to how much weight to give the study's conclusions, inferring this from the researcher's history and the abstract and responses to the article I _can_ read and other factors.\n\nSkim through promising research articles for the information you want, watching for obvious problems in experimental design or quality of argument. This is where your time investment in scholarship can explode, so be conscious of the tradeoffs involved when reading 100 abstracts vs. reading 100 papers.\n\nYou can also try contacting individual researchers. This works best when the subject line of your email is very descriptive, and is obviously about a detail in their recent work. The content of your email should ask a very specific question or two, quoting directly from their paper(s). Researchers are often excited to hear that somebody is actually reading their work closely, though philosophers get more excited than neuroscientists (for example). Neuroscientists are called for comment by the media somewhat regularly. This doesn't happen to philosophers.\n\nFinally, if you've done all this work already and you're feeling generous, perhaps you could take a little time to write up the results of your research for the rest of us! Or, [help make Wikipedia better](http://en.wikipedia.org/wiki/Wikipedia:Contributing_to_Wikipedia)."
          },
          "voteCount": 158
        },
        {
          "name": "Excluding the Supernatural",
          "type": "post",
          "slug": "excluding-the-supernatural",
          "_id": "u6JzcFtPGiznFgDxP",
          "url": null,
          "title": "Excluding the Supernatural",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Occasionally, you hear someone claiming that creationism should not be taught in schools, especially not as a competing hypothesis to evolution, because creationism is _a priori and automatically_ excluded from scientific consideration, in that it invokes the \"supernatural\".\n\nSo... is the idea here, that creationism _could_ be true, but _even if it were true_, you wouldn't be _allowed_ to teach it in science class, because science is only about \"natural\" things?\n\nIt seems clear enough that this notion stems from the desire to [avoid a confrontation between science and religion](/lw/i8/religions_claim_to_be_nondisprovable/).  You don't want to come right out and say that science doesn't teach Religious Claim X because X has been [tested by the scientific method and found false](/lw/i8/religions_claim_to_be_nondisprovable/).  So instead, you can... um... claim that science is excluding hypothesis X _a priori_.  That way you don't have to discuss how experiment has falsified X _a posteriori._\n\nOf course this plays right into the creationist claim that Intelligent Design isn't getting a fair shake from science—that science has _prejudged_ the issue in favor of atheism, regardless of the evidence.  If science excluded Intelligent Design _a priori,_ this would be a justified complaint!\n\nBut let's back up a moment.  The one comes to you and says:  \"Intelligent Design is excluded from being science _a priori,_ because it is 'supernatural', and science only deals in 'natural' explanations.\"\n\nWhat exactly do they mean, \"supernatural\"?  Is any explanation invented by someone with the last name \"Cohen\" a supernatural one?  If we're going to summarily kick a set of hypotheses out of science, what is it that we're supposed to exclude?\n\nBy _far_ the best definition I've ever heard of the supernatural is [Richard Carrier's](http://richardcarrier.blogspot.com/2007/01/defining-supernatural.html):  A \"supernatural\" explanation appeals to _ontologically basic mental things,_ mental entities that cannot be reduced to nonmental entities.\n\nThis is the difference, for example, between saying that [water rolls downhill because it _wants_ to be lower](/lw/te/three_fallacies_of_teleology/), and setting forth differential equations that claim to describe only motions, not desires.  It's the difference between saying that a tree puts forth leaves because of a tree spirit, versus examining plant biochemistry.  Cognitive science [takes the fight against supernaturalism into the realm of the mind](/lw/tf/dreams_of_ai_design/).\n\nWhy is this an excellent definition of the supernatural?  I refer you to [Richard Carrier](http://richardcarrier.blogspot.com/2007/01/defining-supernatural.html) for the full argument.  But consider:  Suppose that you discover what seems to be a _spirit,_ inhabiting a tree: a dryad who can materialize outside or inside the tree, who speaks in English about the need to protect her tree, et cetera.  And then suppose that we turn a microscope on this tree spirit, and she turns out to be [made of parts](/lw/t5/when_anthropomorphism_became_stupid/)—not inherently spiritual and ineffable parts, like fabric of desireness and cloth of belief; but rather the same sort of parts as quarks and electrons, parts whose behavior is defined in motions rather than minds.  Wouldn't the dryad immediately be [demoted to the dull catalogue of common things](/lw/or/joy_in_the_merely_real/)?\n\nBut if we accept Richard Carrier's definition of the supernatural, then a dilemma arises: we _want_ to give religious claims a fair shake, but it seems that we have _very good_ grounds for excluding supernatural explanations _a priori._\n\nI mean, what _would_ the universe look like if reductionism were false?\n\nI previously [defined the reductionist thesis](/lw/on/reductionism/) as follows: human minds create multi-level _models_ of reality in which high-level patterns and low-level patterns are separately and explicitly _represented._  A physicist knows Newton's equation for gravity, Einstein's equation for gravity, and the derivation of the former as a low-speed approximation of the latter.  But these three separate mental representations, are only a convenience of human cognition.  It is not that _reality itself_ has an Einstein equation that governs at high speeds, a Newton equation that governs at low speeds, and a \"bridging law\" that smooths the interface.  Reality itself has only a single level, Einsteinian gravity.  It is only the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/) that makes some people talk as if the higher levels could have a separate existence—different levels of organization can have separate representations in human maps, but the territory itself is a single unified low-level mathematical object.\n\nSuppose this were wrong.\n\nSuppose that the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/) was not a fallacy, but simply true.\n\nSuppose that a 747 had a fundamental physical existence apart from the quarks making up the 747.\n\nWhat experimental observations would you expect to make, if you found yourself in such a universe?\n\nIf you can't come up with a good answer to that, it's not _observation_ that's ruling out \"non-reductionist\" beliefs, but _a priori_ logical incoherence.  If you can't say what predictions the \"non-reductionist\" model makes, how can you say that experimental evidence rules it out?\n\nMy thesis is that non-reductionism is a _confusion;_ and once you realize that an idea is a confusion, it becomes a tad difficult to envision what the universe would look like if the confusion were _true._ Maybe I've got some multi-level model of the world, and the multi-level model has a one-to-one direct correspondence with the causal elements of the physics?  But once all the rules are specified, why wouldn't the model just flatten out into yet another list of fundamental things and their interactions?  Does everything I can _see in_ the model, like a 747 or a human mind, have to become a separate real thing?  But what if I see a pattern in that new supersystem?\n\nSupernaturalism is a special case of non-reductionism, where it is not 747s that are irreducible, but just (some) mental things.  Religion is a special case of supernaturalism, where the irreducible mental things are God(s) and souls; and perhaps also sins, angels, karma, etc.\n\nIf I propose the existence of a powerful entity with the ability to survey and alter each element of our observed universe, but with the entity reducible to nonmental parts that interact with the elements of our universe in a lawful way; if I propose that this entity wants certain particular things, but \"wants\" using a brain composed of particles and fields; then this is not yet a religion, just a naturalistic hypothesis about a naturalistic Matrix.  If tomorrow the clouds parted and a vast glowing amorphous figure thundered forth the above description of reality, then this would not imply that the figure was necessarily honest; but I would show the movies in a science class, and I would try to derive testable predictions from the theory.\n\nConversely, [religions have ignored](/lw/kr/an_alien_god/) the discovery of that [ancient bodiless thing](/lw/kr/an_alien_god/): omnipresent in the working of Nature and immanent in every falling leaf: vast as a planet's surface and billions of years old: itself unmade and arising from the structure of physics: designing without brain to shape all life on Earth and the minds of humanity.  Natural selection, when Darwin proposed it, was not hailed as the long-awaited Creator:  It wasn't _fundamentally_ mental.\n\nBut now we get to the dilemma: if the staid conventional normal boring understanding of physics and the brain _is_ correct, there's no way _in principle_ that a human being can concretely envision, and derive testable experimental predictions about, an alternate universe in which things _are_ irreducibly mental.  Because, if the boring old normal model is correct, your brain is made of quarks, and so your brain will only be able to envision and concretely predict things that can predicted by quarks.  You will only ever be able to construct models made of interacting simple things.\n\nPeople who live in reductionist universes cannot concretely envision non-reductionist universes.  They can [pronounce the syllables](/lw/i6/professing_and_cheering/) \"non-reductionist\" but they can't _imagine_ it.\n\nThe basic error of anthropomorphism, and the reason why [supernatural explanations sound much simpler than they really are](/lw/jp/occams_razor/), is your brain using itself as an opaque black box to predict other things labeled \"mindful\".  Because you already have big, complicated webs of neural circuitry that implement your \"wanting\" things, it seems like you can easily describe water that \"wants\" to flow downhill—the one word \"want\" acts as a [lever](/lw/sp/detached_lever_fallacy/) to set your _own_ complicated wanting-machinery in motion.\n\nOr you imagine that God likes beautiful things, and therefore made the flowers.  Your own \"beauty\" circuitry determines what is \"beautiful\" and \"not beautiful\".  But you don't know the diagram of your own synapses.  You can't describe a _nonmental_ system that computes the same label for what is \"beautiful\" or \"not beautiful\"—can't write a computer program that predicts your own labelings.  But this is just a defect of knowledge on your part; it doesn't mean that the brain [has no explanation](/lw/iu/mysterious_answers_to_mysterious_questions/).\n\nIf the \"boring view\" of reality is correct, then you can _never_ predict anything irreducible because _you_ are reducible.  You can never get Bayesian confirmation for a hypothesis of irreducibility, because any _prediction you can make_ is, therefore, something that could also be predicted by a reducible thing, namely your brain.\n\nSome boxes you really _can't_ think outside.  If our universe _really is_ Turing computable, we will never be able to _concretely_ envision anything that isn't Turing-computable—no matter how many levels of halting oracle hierarchy our mathematicians can talk _about,_ we won't be able to predict what a halting oracle would actually _say,_ in such fashion as to experimentally discriminate it from merely computable reasoning.\n\nOf course, that's all assuming the \"boring view\" is correct.  _To the extent_ that you believe evolution is true, you should not expect to encounter strong evidence against evolution.  To the extent you believe reductionism is true, you should expect non-reductionist hypotheses to be _incoherent_ as well as wrong.  To the extent you believe supernaturalism is false, you should expect it to be _inconceivable_ as well.\n\nIf, on the other hand, a supernatural hypothesis turns out to be true, then presumably you will also discover that it is not inconceivable.\n\nSo let us bring this back full circle to the matter of Intelligent Design:\n\nShould ID be excluded _a priori_ from experimental falsification and science classrooms, because, by invoking the supernatural, it has placed itself outside of natural philosophy?\n\nI answer:  \"Of course not.\"  The _irreducibility_ of the intelligent designer is not an indispensable part of the ID hypothesis.  For every irreducible God that can be proposed by the IDers, there exists a corresponding reducible alien that behaves in accordance with the same predictions—since the IDers themselves are reducible; to the extent I believe reductionism is in fact correct, which is a rather strong extent, I must expect to discover reducible formulations of all supposedly supernatural predictive models.\n\nIf we're going over the archeological records to test the assertion that Jehovah parted the Red Sea out of an explicit desire to display its superhuman power, then it makes little difference whether Jehovah is ontologically basic, or an alien with nanotech, or a Dark Lord of the Matrix.  You do some archeology, find no skeletal remnants or armor at the Red Sea site, and indeed find records that Egypt ruled much of Canaan at the time.  So you stamp the historical record in the Bible \"disproven\" and carry on.  The hypothesis is coherent, falsifiable and wrong.\n\nLikewise with the evidence from biology that foxes are designed to chase rabbits, rabbits are designed to evade foxes, and [neither is designed \"to carry on their species\" or \"protect the harmony of Nature\"](/lw/l5/evolving_to_extinction/); likewise with the retina being designed backwards with the light-sensitive parts at the bottom; and so on through a thousand other items of evidence for splintered, immoral, [incompetent](/lw/kt/evolutions_are_stupid_but_work_anyway/) design.  The Jehovah model of our [alien god](/lw/kr/an_alien_god/) is coherent, falsifiable, and wrong—coherent, that is, so long as you don't care whether Jehovah is ontologically basic or just an alien.\n\nJust convert the supernatural hypothesis into the corresponding natural hypothesis.  Just make the same predictions the same way, without asserting any mental things to be ontologically basic.  Consult your brain's black box if necessary to make predictions—say, if you want to talk about an \"angry god\" without building a full-fledged angry AI to label behaviors as angry or not angry.  So you derive the predictions, or look up the predictions made by ancient theologians without advance knowledge of our experimental results.  If experiment conflicts with those predictions, then it is fair to speak of the religious claim having been scientifically refuted.  It was given its just chance at confirmation; it is being excluded _a posteriori,_ not _a priori._\n\nUltimately, reductionism is just disbelief in _fundamentally complicated_ things.  If \"fundamentally complicated\" sounds like an oxymoron... well, that's why I think that the doctrine of non-reductionism is a _confusion,_ rather than a way that things could be, but aren't.  You would be wise to be wary, if you find yourself supposing such things.\n\nBut the ultimate rule of science is to look and see.  If ever a God appeared to thunder upon the mountains, it would be something that people looked at and saw.\n\n_Corollary:_  Any supposed [designer](/lw/tf/dreams_of_ai_design/) of Artificial General Intelligence who [talks about religious beliefs in respectful tones](/lw/gv/outside_the_laboratory/), is clearly not an [expert on](http://www.overcomingbias.com/2007/04/expert_at_versu.html) reducing mental things to nonmental things; and indeed knows so very little of the uttermost basics, as for it to be scarcely plausible that they could be [expert at](http://www.overcomingbias.com/2007/04/expert_at_versu.html) the art; unless their _idiot savancy_ is complete.  Or, of course, if they're outright lying.  We're not talking about a subtle mistake."
          },
          "voteCount": 65
        },
        {
          "name": "Joy in the Merely Real",
          "type": "post",
          "slug": "joy-in-the-merely-real",
          "_id": "x4dG4GhpZH2hgz59x",
          "url": null,
          "title": "Joy in the Merely Real",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Reductionism"
            },
            {
              "name": "Emotions"
            },
            {
              "name": "Fun Theory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": ">                     ...Do not all charms fly  \n> At the mere touch of cold philosophy?  \n> There was an awful rainbow once in heaven:  \n> We know her woof, her texture; she is given  \n> In the dull catalogue of common things.  \n>         —John Keats, _Lamia_\n> \n> \"Nothing is 'mere'.\"  \n>         —Richard Feynman\n\nYou've got to admire that phrase, \"dull catalogue of common things\".  What is it, exactly, that goes in this catalogue?  Besides rainbows, that is?\n\nWhy, things that are mundane, of course.  Things that are normal; things that are unmagical; things that are known, or knowable; things that play by the rules (or that play by _any_ rules, which makes them boring); things that are part of the ordinary universe; things that are, in a word, _real._\n\nNow that's what I call setting yourself up for a fall.\n\nAt that rate, sooner or later you're going to be disappointed in _everything_—either it will turn out not to exist, or even worse, it will turn out to be real.\n\nIf we cannot take joy in things that are merely real, our lives will _always_ be empty.\n\nFor what sin are rainbows demoted to the dull catalogue of common things?  For the sin of having a scientific explanation.  \"We know her woof, her texture\", says Keats—an interesting use of the word \"we\", because I [suspect that Keats didn't know](/lw/op/fake_reductionism/) the explanation himself.  I suspect that just being [told that someone else knew](/lw/j3/science_as_curiositystopper/) was too much for him to take.  I suspect that just the notion of rainbows being scientifically explicable _in principle_ would have been too much to take.  And if Keats didn't think like that, well, I know plenty of people who do.\n\nI have already remarked that nothing is _inherently_ [mysterious](/lw/iu/mysterious_answers_to_mysterious_questions/)—nothing that actually exists, that is.  If I am [ignorant](/lw/oj/probability_is_in_the_mind/) about a phenomenon, that is [a fact about my state of mind](/lw/oi/mind_projection_fallacy/), not a fact about the phenomenon; to [worship](/lw/j2/explainworshipignore/) a phenomenon because it seems so wonderfully mysterious, is to worship your own ignorance; a blank map does not correspond to a blank territory, it is just somewhere we haven't visited yet, etc. etc...\n\nWhich is to say that _everything_—everything that _actually_ exists—is liable to end up in \"the dull catalogue of common things\", sooner or later.\n\nYour choice is either:\n\n*   Decide that things are allowed to be unmagical, knowable, scientifically explicable, in a word, _real,_ and yet still worth caring about;\n*   Or go about the rest of your life suffering from existential ennui that is _unresolvable._\n\n(Self-deception might be an option for others, but [not for you](/lw/je/doublethink_choosing_to_be_biased/).)\n\nThis puts quite a different complexion on the bizarre habit indulged by those strange folk called _scientists,_ wherein they suddenly become fascinated by pocket lint or bird droppings or rainbows, or some other ordinary thing which world-weary and sophisticated folk would never give a second glance.\n\nYou might say that scientists—at least _some_ scientists—are those folk who are _in principle_ capable of enjoying life in the real universe."
          },
          "voteCount": 108
        },
        {
          "name": "Affective Death Spirals",
          "type": "post",
          "slug": "affective-death-spirals",
          "_id": "XrzQW69HpidzvBxGr",
          "url": null,
          "title": "Affective Death Spirals",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Affect Heuristic"
            },
            {
              "name": "Emotions"
            },
            {
              "name": "Affective Death Spiral"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Many, many, many are the flaws in human reasoning which lead us to overestimate how well our beloved theory explains the facts. The phlogiston theory of chemistry could explain just about anything, so long as it didn’t have to predict it in advance. And the more phenomena you use your favored theory to explain, the truer your favored theory seems—has it not been confirmed by these many observations? As the theory seems truer, you will be more likely to question evidence that conflicts with it. As the favored theory seems more general, you will seek to use it in more explanations.\n\nIf you know anyone who believes that Belgium secretly controls the US banking system, or that they can use an invisible blue spirit force to detect available parking spaces, that’s probably how they got started.\n\n(Just keep an eye out, and you’ll observe much that seems to confirm this theory . . .)\n\nThis positive feedback cycle of credulity and confirmation is indeed fearsome, and responsible for much error, both in science and in everyday life.\n\nBut it’s nothing compared to the death spiral that begins with a charge of positive affect—a thought that _feels really good._\n\nA new political system that can save the world. A great leader, strong and noble and wise. An amazing tonic that can cure upset stomachs and cancer.\n\nHeck, why not go for all three? A great cause needs a great leader. A great leader should be able to brew up a magical tonic or two.\n\nThe halo effect is that any perceived positive characteristic (such as attractiveness or strength) increases perception of any other positive characteristic (such as intelligence or courage). Even when it makes no sense, or less than no sense.\n\nPositive characteristics enhance perception of every other positive characteristic? That sounds a lot like how a fissioning uranium atom sends out neutrons that fission other uranium atoms.\n\nWeak positive affect is subcritical; it doesn’t spiral out of control. An attractive person seems more honest, which, perhaps, makes them seem more attractive; but the effective neutron multiplication factor is less than one. Metaphorically speaking. The resonance confuses things a little, but then dies out.\n\nWith intense positive affect attached to the Great Thingy, the resonance touches everywhere. A believing Communist sees the wisdom of Marx in every hamburger bought at McDonald’s; in every promotion they’re denied that would have gone to them in a true worker’s paradise; in every election that doesn’t go to their taste; in every newspaper article “slanted in the wrong direction.” Every time they use the Great Idea to interpret another event, the Great Idea is confirmed all the more. It feels better—positive reinforcement—and of course, when something feels good, that, alas, makes us _want_ to believe it all the more.\n\nWhen the Great Thingy feels good enough to make you _seek out_ new opportunities to feel even better about the Great Thingy, applying it to interpret new events every day, the resonance of positive affect is like a chamber full of mousetraps loaded with ping-pong balls.\n\nYou could call it a “happy attractor,” “overly positive feedback,” a “praise locked loop,” or “funpaper.” Personally I prefer the term “affective death spiral.”\n\nComing up next: How to resist an affective death spiral.^[1](#fn1x54)^\n\n^[1](#fn1x54-bk)^Hint: It’s not by refusing to ever admire anything again, nor by keeping the things you admire in safe little restricted magisteria."
          },
          "voteCount": 70
        },
        {
          "name": "The Lens That Sees Its Flaws",
          "type": "post",
          "slug": "the-lens-that-sees-its-flaws",
          "_id": "46qnWRSR7L2eyNbMA",
          "url": null,
          "title": "The Lens That Sees Its Flaws",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Gears-Level"
            },
            {
              "name": "Epistemology"
            },
            {
              "name": "Map and Territory"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Light leaves the Sun and strikes your shoelaces and bounces off; some photons enter the pupils of your eyes and strike your retina; the energy of the photons triggers neural impulses; the neural impulses are transmitted to the visual-processing areas of the brain; and there the optical information is processed and reconstructed into a 3D model that is recognized as an untied shoelace; and so you believe that your shoelaces are untied.\n\nHere is the secret of *deliberate rationality—*this whole process is not [magic](https://www.lesswrong.com/lw/iu/mysterious_answers_to_mysterious_questions/), and you can *understand* it. You can *understand* how you see your shoelaces. You can *think* about which sort of thinking processes will create beliefs which mirror reality, and which thinking processes will not.\n\nMice can see, but they can’t understand seeing. *You* can understand seeing, and because of that, you can do things that mice cannot do. Take a moment to [marvel](https://www.lesswrong.com/lw/j3/science_as_curiositystopper/) at this, for it is indeed marvelous.\n\nMice see, but they don’t know they have visual cortexes, so they can’t correct for optical illusions. A mouse lives in a mental world that includes cats, holes, cheese and mousetraps—but not mouse brains. Their camera does not take pictures of its own lens. But we, as humans, can look at a [seemingly bizarre image](http://www.richrock.com/gifs/optical-illusion-wheels-circles-rotating.png), and realize that part of what we’re seeing is the lens itself. You don’t always have to believe your own eyes, but you have to realize that you *have* eyes—you must have distinct mental buckets for the map and the territory, for the senses and reality. Lest you think this a trivial ability, remember how rare it is in the animal kingdom.\n\nThe whole idea of Science is, simply, reflective reasoning about a more reliable process for making the contents of your mind mirror the contents of the world. It is the sort of thing mice would never invent. Pondering this business of “performing replicable experiments to falsify theories,” we can see *why* it works. Science is not a [separate magisterium](https://www.lesswrong.com/lw/i8/religions_claim_to_be_nondisprovable/), far away from real life and the understanding of ordinary mortals. Science is not something that only applies to the [inside of laboratories](https://www.lesswrong.com/lw/gv/outside_the_laboratory/). Science, itself, is an understandable process-in-the-world that correlates brains with reality.\n\nScience *makes sense*, when you think about it. But mice can’t think about thinking, which is why they don’t have Science. One should not overlook the wonder of this—or the potential power it bestows on us as individuals, not just scientific societies.\n\nAdmittedly, understanding the engine of thought may be *a little more complicated* than understanding a steam engine—but it is not a *fundamentally* different task.\n\nOnce upon a time, I went to EFNet’s #philosophy chatroom to ask, “Do you believe a nuclear war will occur in the next 20 years? If no, why not?” One person who answered the question said he didn’t expect a nuclear war for 100 years, because “All of the players involved in decisions regarding nuclear war are not interested right now.” “But why extend that out for 100 years?” I asked. “Pure hope,” was his reply.\n\nReflecting on this whole thought process, we can see why the thought of nuclear war makes the person unhappy, and we can see how his brain therefore rejects the belief. But if you imagine a billion worlds—Everett branches, or Tegmark duplicates^1^—this thought process will not [systematically correlate](https://www.lesswrong.com/lw/jl/what_is_evidence/) optimists to branches in which no nuclear war occurs.^2^\n\nTo ask which beliefs make you happy is to turn inward, not outward—it tells you something about yourself, but it is not evidence entangled with the environment. I have nothing against happiness, but it should follow from your picture of the world, rather than tampering with the mental paintbrushes.\n\nIf you can see this—if you can see that hope is shifting your *first-order* thoughts by too large a degree—if you can understand your mind as a mapping engine that has flaws—then you can apply a reflective correction. The brain is a flawed lens through which to see reality. This is true of both mouse brains and human brains. But a human brain is a flawed lens that can understand its own flaws—its systematic errors, its biases—and apply second-order corrections to them. This, *in practice,* makes the lens far more powerful. Not perfect, but far more powerful.\n\n* * *\n\n^1^ Max Tegmark, “Parallel Universes,” in *Science and Ultimate Reality: Quantum Theory,* *Cosmology, and Complexity*, ed. John D. Barrow, Paul C. W. Davies, and Charles L. Harper Jr. (New York: Cambridge University Press, 2004), 459–491, [http://arxiv.org/abs/astro-ph/0302131](http://arxiv.org/abs/astro-ph/0302131).\n\n^2^ Some clever fellow is bound to say, “Ah, but since I have hope, I'll work a little harder at my job, pump up the global economy, and thus help to prevent countries from sliding into the angry and hopeless state where nuclear war is a possibility. So the two events are related after all.” At this point, we have to drag in Bayes’s Theorem and measure the relationship quantitatively. Your optimistic nature cannot have *that* large an effect on the world; it cannot, of itself, decrease the probability of nuclear war by 20%, or however much your optimistic nature shifted your beliefs. Shifting your beliefs by a large amount, due to an event that only slightly increases your chance of being right, will still mess up your mapping."
          },
          "voteCount": 146
        },
        {
          "name": "The Virtue of Narrowness",
          "type": "post",
          "slug": "the-virtue-of-narrowness",
          "_id": "yDfxTj9TKYsYiWH5o",
          "url": null,
          "title": "The Virtue of Narrowness",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Virtues"
            },
            {
              "name": "Reductionism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "> What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world.\n> \n> —“The Twelve Virtues of Rationality”\n\nWithin their own professions, people grasp the importance of narrowness; a car mechanic knows the difference between a carburetor and a radiator, and would not think of them both as “car parts.” A hunter-gatherer knows the difference between a lion and a panther. A janitor does not wipe the floor with window cleaner, even if the bottles look similar to one who has not mastered the art.\n\nOutside their own professions, people often commit the misstep of trying to broaden a word as widely as possible, to cover as much territory as possible. Is it not more glorious, more wise, more impressive, to talk about *all* the apples in the world? How much loftier it must be to *explain human thought in general*, without being distracted by smaller questions, such as how humans invent techniques for solving a Rubik’s Cube. Indeed, it scarcely seems necessary to consider *specific* questions at all; isn’t a general theory a worthy enough accomplishment on its own?\n\nIt is the way of the curious to lift up one pebble from among a million pebbles on the shore, and see something new about it, something interesting, something different. You call these pebbles “diamonds,” and ask what might be special about them—what inner qualities they might have in common, beyond the glitter you first noticed. And then someone else comes along and says: “Why not call *this* pebble a diamond too? And this one, and this one?” They are enthusiastic, and they mean well. For it seems undemocratic and exclusionary and elitist and unholistic to call some pebbles “diamonds,” and others not. It seems . . . *narrow-minded . . .* if you’ll pardon the phrase. Hardly *open*, hardly *embracing*, hardly *communal.*\n\nYou might think it poetic, to give one word many meanings, and thereby spread shades of connotation all around. But even poets, if they are good poets, must learn to see the world precisely. It is not enough to compare love to a flower. Hot jealous unconsummated love is not the same as the love of a couple married for decades. If you need a flower to symbolize jealous love, you must go into the garden, and look, and make subtle distinctions—find a flower with a heady scent, and a bright color, and thorns. Even if your intent is to shade meanings and cast connotations, you must keep precise track of exactly which meanings you shade and connote.\n\nIt is a necessary part of the rationalist’s art—or even the poet’s art!—to focus narrowly on unusual pebbles which possess some special quality. And look at the details which those pebbles—and those pebbles alone!—share among each other. This is not a sin.\n\nIt is perfectly all right for modern evolutionary biologists to explain *just* the patterns of living creatures, and not the “evolution” of stars or the “evolution” of technology. Alas, some unfortunate souls use the same word “evolution” to cover the naturally selected patterns of replicating life, *and* the strictly accidental structure of stars, *and* the intelligently configured structure of technology. And as we all know, if people use the same word, it must all be the same thing. These biologists must just be too dumb to see the connections.\n\nAnd what could be more virtuous than seeing connections? Surely the wisest of all human beings are the New Age gurus who say, “Everything is connected to everything else.” If you ever say this aloud, you should pause, so that everyone can absorb the sheer shock of this Deep Wisdom.\n\nThere is a trivial mapping between a graph and its complement. A fully connected graph, with an edge between every two vertices, conveys the same amount of information as a graph with no edges at all. The important graphs are the ones where some things are *not* connected to some other things.\n\nWhen the unenlightened ones try to be profound, they draw endless verbal comparisons between this topic, and that topic, which is like this, which is like that; until their graph is fully connected and also totally useless. The remedy is specific knowledge and in-depth study. When you understand things in detail, you can see how they are *not* alike, and start enthusiastically subtracting edges *off* your graph.\n\nLikewise, the important categories are the ones that do not contain everything in the universe. Good hypotheses can only explain some possible outcomes, and not others.\n\nIt was perfectly all right for Isaac Newton to explain *just* gravity, *just* the way things fall down—and how planets orbit the Sun, and how the Moon generates the tides—but *not* the role of money in human society or how the heart pumps blood. Sneering at narrowness is rather reminiscent of ancient Greeks who thought that going out and actually *looking* at things was manual labor, and manual labor was for slaves.\n\nAs Plato put it in *The Republic, Book VII*:\n\n> If anyone should throw back his head and learn something by staring at the varied patterns on a ceiling, apparently you would think that he was contemplating with his reason, when he was only staring with his eyes . . . I cannot but believe that no study makes the soul look on high except that which is concerned with real being and the unseen. Whether he gape and stare upwards, or shut his mouth and stare downwards, if it be things of the senses that he tries to learn something about, I declare he never could learn, for none of these things admit of knowledge: I say his soul is looking down, not up, even if he is floating on his back on land or on sea!\n\nMany today make a similar mistake, and think that narrow concepts are as lowly and unlofty and unphilosophical as, say, going out and looking at things—an endeavor only suited to the underclass. But rationalists—and also poets—need narrow words to express precise thoughts; they need categories that include only some things, and exclude others. There’s nothing wrong with focusing your mind, narrowing your categories, excluding possibilities, and sharpening your propositions. Really, there isn’t! If you make your words too broad, you end up with something that isn’t true and doesn’t even make good poetry.\n\n*And DON’T EVEN GET ME STARTED on people who think Wikipedia is an “Artificial Intelligence,” the invention of LSD was a “Singularity,” or that corporations are “superintelligent”!*"
          },
          "voteCount": 93
        },
        {
          "name": "Generalizing From One Example",
          "type": "post",
          "slug": "generalizing-from-one-example",
          "_id": "baTWMegR42PAsH9qJ",
          "url": null,
          "title": "Generalizing From One Example",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Inferential Distance"
            },
            {
              "name": "Fallacies"
            },
            {
              "name": "Typical Mind Fallacy"
            },
            {
              "name": "Psychiatry"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [The Psychological Unity of Humankind](http://www.overcomingbias.com/2008/06/psychological-u.html), [Instrumental vs. Epistemic: A Bardic Perspective](/lw/cn/instrumental_vs_epistemic_a_bardic_perspective/)  \n  \n\"_Everyone generalizes from one example. At least, I do.\"_\n\n   \\-\\- Vlad Taltos (_Issola,_ Steven Brust)  \n  \nMy old professor, David Berman, liked to talk about what he called the \"typical mind fallacy\", which he illustrated through the following example:  \n  \nThere was a debate, in the late 1800s, about whether \"imagination\" was simply a turn of phrase or a real phenomenon. That is, can people actually create images in their minds which they see vividly, or do they simply say \"I saw it in my mind\" as a metaphor for considering what it looked like?  \n  \nUpon hearing this, my response was \"How the stars was this actually a real debate? Of course we have mental imagery. Anyone who doesn't think we have mental imagery is either such a fanatical Behaviorist that she doubts the evidence of her own senses, or simply insane.\" Unfortunately, the professor was able to parade a long list of famous people who denied mental imagery, including some leading scientists of the era. And this was all before Behaviorism even existed.  \n  \nThe debate was resolved by Francis Galton, a fascinating man who among other achievements invented eugenics, the \"wisdom of crowds\", and standard deviation. Galton gave people some very detailed surveys, and found that some people did have mental imagery and others didn't. The ones who did had simply assumed everyone did, and the ones who didn't had simply assumed everyone didn't, to the point of coming up with absurd justifications for why they were lying or misunderstanding the question. There was a wide spectrum of imaging ability, from about five percent of people with perfect eidetic imagery^1^ to three percent of people completely unable to form mental images^2^.\n\nDr. Berman dubbed this the Typical Mind Fallacy: the human tendency to believe that one's own mental structure can be generalized to apply to everyone else's.\n\nHe kind of took this idea and ran with it. He interpreted certain passages in George Berkeley's biography to mean that Berkeley was an eidetic imager, and that this was why the idea of the universe as sense-perception held such interest to him. He also suggested that experience of consciousness and qualia were as variable as imaging, and that philosophers who deny their existence (Ryle? Dennett? Behaviorists?) were simply people whose mind lacked the ability to easily experience qualia. In general, he believed philosophy of mind was littered with examples of philosophers taking their own mental experiences and building theories on them, and other philosophers with different mental experiences critiquing them and wondering why they disagreed.  \n  \nThe formal typical mind fallacy is about serious matters of mental structure. But I've also run into something similar with something more like the psyche than the mind: a tendency to generalize from our personalities and behaviors.  \n  \nFor example, I'm about as introverted a person as you're ever likely to meet - anyone more introverted than I am doesn't communicate with anyone. All through elementary and middle school, I suspected that the other children were out to get me. They kept on grabbing me when I was busy with something and trying to drag me off to do some rough activity with them and their friends. When I protested, they counter-protested and told me I really needed to stop whatever I was doing and come join them. I figured they were bullies who were trying to annoy me, and found ways to hide from them and scare them off.  \n  \nEventually I realized that it was a double misunderstanding. They figured I must be like them, and the only thing keeping me from playing their fun games was that I was too shy. I figured they must be like me, and that the only reason they would interrupt a person who was obviously busy reading was that they wanted to annoy him.  \n  \nLikewise: I can't deal with noise. If someone's being loud, I can't sleep, I can't study, I can't concentrate, I can't do anything except bang my head against the wall and hope they stop. I once had a noisy housemate. Whenever I asked her to keep it down, she told me I was being oversensitive and should just mellow out. I can't claim total victory here, because she was very neat and kept yelling at me for leaving things out of place, and I told her she needed to just mellow out and you couldn't even tell that there was dust on that dresser anyway. It didn't occur to me then that neatness to her might be as necessary and uncompromisable as quiet was to me, and that this was an actual feature of how our minds processed information rather than just some weird quirk on her part.\n\n\"Just some weird quirk on her part\" and \"just being oversensitive\" are representative of the problem with the typical psyche fallacy, which is that it's invisible. We tend to neglect the role of differently-built minds in disagreements, and attribute the problems to the other side being deliberately perverse or confused. I happen to know that loud noise seriously pains and debilitates me, but when I say this to other people they think I'm just expressing some weird personal preference for quiet. Think about all those poor non-imagers who thought everyone else was just taking a metaphor about seeing mental images _way_ too far and refusing to give it up.\n\nAnd the reason I'm posting this here is because it's rationality that helps us deal with these problems.  \n  \nThere's some evidence that the usual method of interacting with people involves something sorta like emulating them within our own brain. We think about how we would react, adjust for the other person's differences, and then assume the other person would react that way. This method of interaction is very tempting, and it always feels like it ought to work.  \n  \nBut when statistics tell you that the method that would work on you doesn't work on anyone else, then continuing to follow that gut feeling is a Typical Psyche Fallacy. You've got to be a good rationalist, reject your gut feeling, and follow the data.  \n  \nI only really discovered this in my last job as a school teacher. There's a lot of data on teaching methods that students enjoy and learn from. I had some of these methods...inflicted...on me during my school days, and I had no intention of abusing my own students in the same way. And when I tried the sorts of really creative stuff I would have loved as a student...it fell completely flat. What ended up working? Something pretty close to the teaching methods I'd hated as a kid. Oh. Well. Now I know why people use them so much. And here I'd gone through life thinking my teachers were just inexplicably bad at what they did, never figuring out that I was just the odd outlier who couldn't be reached by this sort of stuff.  \n  \nThe other reason I'm posting this here is because I think it relates to some of the discussions of seduction that are going on in MBlume's Bardic thread. There are a lot of not-particularly-complimentary things about women that many men tend to believe. Some guys say that women will never have romantic relationships with their actually-decent-people male friends because they prefer alpha-male jerks who treat them poorly. Other guys say women want to be lied to and tricked. I could go on, but I think most of them are covered in that thread anyway.  \n  \nThe response I hear from most of the women I know is that this is complete balderdash and women aren't like that at all. So what's going on?  \n  \nWell, I'm afraid I kind of trust the seduction people. They've put a lot of work into their \"art\" and at least according to their self-report are pretty successful. And unhappy romantically frustrated nice guys everywhere can't be completely wrong.  \n  \nMy theory is that the women in this case are committing a Typical Psyche Fallacy. The women I ask about this are not even remotely close to being a representative sample of all women. They're the kind of women whom a shy and somewhat geeky guy knows and talks about psychology with. Likewise, the type of women who publish strong opinions about this on the Internet aren't close to a representative sample. They're well-educated women who have strong opinions about gender issues and post about them on blogs.  \n  \nAnd lest I sound chauvinistic, the same is certainly true of men. I hear a lot of bad things said about men (especially with reference to what they want romantically) that I wouldn't dream of applying to myself, my close friends, or to any man I know. But they're so common and so well-supported that I have excellent reason to believe they're true.  \n  \nThis post has gradually been getting less rigorous and less connected to the formal Typical Mind Fallacy. First I changed it to a Typical Psyche Fallacy so I could talk about things that were more psychological and social than mental. And now it's expanding to cover the related fallacy of believing your own social circle is at least a little representative of society at large, which it very rarely is^3^.  \n  \nIt was originally titled \"The Typical Mind Fallacy\", but I'm taking a hint fromt the quote and changing it to \"Generalizing From One Example\", because that seems to be the link between all of these errors. We only have direct first-person knowledge one one mind, one psyche, and one social circle, and we find it tempting to treat it as typical even in the face of contrary evidence.  \n  \nThis, I think, is especially important for the sort of people who enjoy Less Wrong, who as far as I can tell are with few exceptions the sort of people who are extreme outliers on every psychometric test ever invented.\n\n  \n**Footnotes**  \n  \n**1.** Eidetic imagery, vaguely related to the idea of a \"photographic memory\", is the ability to visualize something and have it be exactly as clear, vivid and obvious as actually seeing it. My professor's example (which Michael Howard somehow remembers even though I only mentioned it once a few years ago) is that although many people can imagine a picture of a tiger, only an eidetic imager would be able to count the number of stripes.  \n  \n**2.** According to Galton, people incapable of forming images were overrepresented in math and science. I've since heard that this idea has been challenged, but I can't access the study.  \n  \n**3.** The example that really drove this home to me: what percent of high school students do you think cheat on tests? What percent have shoplifted? Someone did a survey on this recently and found that the answer was nobhg gjb guveqf unir purngrq naq nobhg bar guveq unir fubcyvsgrq ([rot13ed](http://www.rot13.com/) so you have to actually take a guess first). This shocked me and everyone I knew, because we didn't cheat or steal during high school and we didn't know anyone who did. I spent an afternoon trying to find some proof that the study was wrong or unrepresentative and coming up with nothing."
          },
          "voteCount": 352
        },
        {
          "name": "Belief in Belief",
          "type": "post",
          "slug": "belief-in-belief",
          "_id": "CqyJzDZWvGhhFJ7dY",
          "url": null,
          "title": "Belief in Belief",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Self-Deception"
            },
            {
              "name": "Alief"
            },
            {
              "name": "Motivated Reasoning"
            },
            {
              "name": "Anticipated Experiences"
            },
            {
              "name": "Religion"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Carl Sagan once told a [parable](http://www.godlessgeeks.com/LINKS/Dragon.htm) of someone who comes to us and claims: “There is a dragon in my garage.” Fascinating! We reply that we wish to see this dragon—let us set out at once for the garage! “But wait,” the claimant says to us, “it is an *invisible* dragon.”\n\nNow as Sagan points out, this doesn’t make the hypothesis unfalsifiable. Perhaps we go to the claimant’s garage, and although we see no dragon, we hear heavy breathing from no visible source; footprints mysteriously appear on the ground; and instruments show that something in the garage is consuming oxygen and breathing out carbon dioxide.\n\nBut now suppose that we say to the claimant, “Okay, we’ll visit the garage and see if we can hear heavy breathing,” and the claimant quickly says no, it’s an *inaudible* dragon. We propose to measure carbon dioxide in the air, and the claimant says the dragon does not breathe. We propose to toss a bag of flour into the air to see if it outlines an invisible dragon, and the claimant immediately says, “The dragon is permeable to flour.”\n\nCarl Sagan used this parable to illustrate the classic moral that poor hypotheses need to do fast footwork to avoid falsification. But I tell this parable to make a different point: The claimant must have an accurate model of the situation *somewhere* in their mind, because they can anticipate, in advance, *exactly which experimental results they’ll need to excuse.*\n\nSome philosophers have been much confused by such scenarios, asking, “Does the claimant *really* believe there’s a dragon present, or not?” As if the human brain only had enough disk space to represent one belief at a time! Real minds are more tangled than that. There are different types of belief; not all beliefs are direct anticipations. The claimant clearly does not *anticipate* seeing anything unusual upon opening the garage door. Otherwise they wouldn’t make advance excuses. It may also be that the claimant’s pool of propositional beliefs contains the free-floating statement *There is a dragon in my garage.* It may seem, to a rationalist, that these two beliefs should collide and conflict even though they are of different types. Yet it is a physical fact that you can write “The sky is green!” next to a picture of a blue sky without the paper bursting into flames.\n\nThe rationalist virtue of empiricism is supposed to prevent us from making this class of mistake. We’re supposed to constantly ask our beliefs which experiences they predict, make them pay rent in anticipation. But the dragon-claimant’s problem runs deeper, and cannot be cured with such simple advice. It’s not exactly *difficult* to connect belief in a dragon to anticipated experience of the garage. If you believe there’s a dragon in your garage, then you can expect to open up the door and see a dragon. If you don’t see a dragon, then that means there’s no dragon in your garage. This is pretty straightforward. You can even try it with your own garage.\n\nNo, this invisibility business is a symptom of something much worse.\n\nDepending on how your childhood went, you may remember a time period when you first began to doubt Santa Claus’s existence, but you still believed that you were *supposed* to believe in Santa Claus, so you tried to deny the doubts. As Daniel Dennett observes, where it is difficult to believe a thing, it is often much easier to believe that you *ought* to believe it. What does it mean to believe that the Ultimate Cosmic Sky is both perfectly blue and perfectly green? The statement is confusing; it’s not even clear what it would *mean* to believe it—what exactly would *be* believed, if you believed. You can much more easily believe that it is *proper*, that it is *good* and *virtuous* and *beneficial*, to believe that the Ultimate Cosmic Sky is both perfectly blue and perfectly green. Dennett calls this “belief in belief.”^1^\n\nAnd here things become complicated, as human minds are wont to do—I think even Dennett oversimplifies how this psychology works in practice. For one thing, if you believe in belief, you cannot admit to yourself that you merely believe in belief. What’s virtuous is to *believe*, not to believe in believing; and so if you only believe in belief, instead of believing, you are not virtuous. Nobody will *admit* to themselves, “I don’t believe the Ultimate Cosmic Sky is blue and green, but I believe I ought to believe it”—not unless they are unusually capable of acknowledging their own lack of virtue. People don’t believe in belief in belief, they just believe in belief.\n\n(Those who find this confusing may find it helpful to study mathematical logic, which trains one to make very sharp distinctions between the proposition P, a proof of P, and a proof that P is provable. There are similarly sharp distinctions between P, wanting P, believing P, wanting to believe P, and believing that you believe P.)\n\nThere are different kinds of belief in belief. You may believe in belief explicitly; you may recite in your deliberate stream of consciousness the verbal sentence “It is virtuous to believe that the Ultimate Cosmic Sky is perfectly blue and perfectly green.” (While also believing that you believe this, unless you are unusually capable of acknowledging your own lack of virtue.) But there are also less explicit forms of belief in belief. Maybe the dragon-claimant fears the public ridicule that they imagine will result if they publicly confess they were wrong.^2^ Maybe the dragon-claimant flinches away from the prospect of admitting to themselves that there is no dragon, because it conflicts with their self-image as the glorious discoverer of the dragon, who saw in their garage what all others had failed to see.\n\nIf all our thoughts were deliberate verbal sentences like philosophers manipulate, the human mind would be a great deal easier for humans to understand. Fleeting mental images, unspoken flinches, desires acted upon without acknowledgement—these account for as much of ourselves as words.\n\nWhile I disagree with Dennett on some details and complications, I still think that Dennett’s notion of *belief in belief* is the key insight necessary to understand the dragon-claimant. But we need a wider concept of *belief*, not limited to verbal sentences. “Belief” should include unspoken anticipation-controllers. “Belief in belief” should include unspoken cognitive-behavior-guiders. It is not psychologically realistic to say, “The dragon-claimant does not believe there is a dragon in their garage; they believe it is beneficial to believe there is a dragon in their garage.” But it is realistic to say the dragon-claimant *anticipates as if* there is no dragon in their garage, and *makes excuses as if* they believed in the belief.\n\nYou can possess an ordinary mental picture of your garage, with no dragons in it, which correctly predicts your experiences on opening the door, and never once think the verbal phrase *There is no dragon in my garage.* I even bet it’s happened to you—that when you open your garage door or bedroom door or whatever, and expect to see no dragons, no such verbal phrase runs through your mind.\n\nAnd to flinch away from giving up your belief in the dragon—or flinch away from giving up your *self-image* as a person who believes in the dragon—it is not necessary to explicitly think *I want to believe there’s a dragon in my garage.* It is only necessary to flinch away from the prospect of admitting you don’t believe.\n\nIf someone believes in their belief in the dragon, and also believes in the dragon, the problem is much less severe. They will be willing to stick their neck out on experimental predictions, and perhaps even agree to give up the belief if the experimental prediction is wrong.^3^ But when someone makes up excuses *in advance*, it would seem to require that belief and belief in belief have become unsynchronized.\n\n* * *\n\n^1^ Daniel C. Dennett, *Breaking the Spell: Religion as a Natural Phenomenon* (Penguin, 2006).\n\n^2^ Although, in fact, a rationalist would congratulate them, and others are more likely to ridicule the claimant if they go on claiming theres a dragon in their garage.\n\n^3^ Although belief in belief can still interfere with this, if the belief itself is not absolutely confident."
          },
          "voteCount": 132
        }
      ]
    },
    {
      "title": "Top-100",
      "children": [
        {
          "name": "Beyond the Reach of God",
          "type": "post",
          "slug": "beyond-the-reach-of-god",
          "_id": "sYgv4eYH82JEsTD34",
          "url": null,
          "title": "Beyond the Reach of God",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "World Optimization"
            },
            {
              "name": "Religion"
            },
            {
              "name": "Courage"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Today's post is a tad gloomier than usual, as I measure such things.  It deals with a thought experiment I invented to smash my own optimism, after [I realized that optimism had misled me](http://www.overcomingbias.com/2008/09/it-was-pretty-b.html).  Those readers sympathetic to arguments like, \"It's important to keep our biases because they help us stay happy,\" should consider not reading.  (Unless they have [something to protect](http://www.overcomingbias.com/2008/01/something-to-pr.html), including their own life.)\n\nSo!  Looking back on the magnitude of my own folly, I realized that at the root of it had been a disbelief in the Future's vulnerability—a reluctance to accept that things could _really_ turn out wrong.  Not as the result of any explicit propositional verbal belief.  More like something inside that persisted in believing, even in the face of adversity, that everything would be all right in the end.\n\nSome would account this a virtue (_zettai daijobu da yo_), and others would say that it's a thing necessary for mental health.\n\nBut we don't live in that world.  We live in the world beyond the reach of God.\n\nIt's been a long, long time since I believed in God.  Growing up in an Orthodox Jewish family, I can recall the last remembered time I asked God for something, though I don't remember how old I was.  I was putting in some request on behalf of the next-door-neighboring boy, I forget what exactly—something along the lines of, \"I hope things turn out all right for him,\" or maybe \"I hope he becomes Jewish.\"\n\nI remember what it was like to have some higher authority to appeal to, to take care of things I couldn't handle myself.  I didn't think of it as \"warm\", because I had no alternative to compare it to.  I just took it for granted.\n\nStill I recall, though only from distant childhood, what it's like to live in the [conceptually impossible possible world](http://www.overcomingbias.com/2008/09/excluding-the-s.html) where God exists.  _Really_ exists, in the way that children and rationalists take all their [beliefs](http://www.overcomingbias.com/2007/07/belief-in-belie.html) at face value.\n\nIn the world where God exists, does God intervene to optimize _everything?_  Regardless of what rabbis assert about the fundamental nature of reality, the take-it-seriously operational answer to this question is obviously \"No\".  You can't ask God to bring you a lemonade from the refrigerator instead of getting one yourself.  When I believed in God after the serious fashion of a child, so very long ago, I didn't believe that.\n\nPostulating that particular divine inaction doesn't provoke a full-blown theological crisis.  If you said to me, \"I have constructed a benevolent superintelligent nanotech-user\", and I said \"Give me a banana,\" and no banana appeared, this would not _yet_ disprove your statement.  Human parents don't always do everything their children ask.  There are some decent fun-theoretic arguments—I even believe them myself—against the idea that the _best_ kind of help you can offer someone, is to always immediately give them everything they want.  I don't think that eudaimonia is formulating goals and having them instantly fulfilled; I don't _want_ to become a simple wanting-thing that never has to plan or act or think.\n\nSo it's not necessarily an attempt to avoid falsification, to say that God does not grant all prayers.  Even a Friendly AI might not respond to every request.\n\nBut clearly, there exists _some_ threshold of horror awful enough that God will intervene.  I remember that being true, when I believed after the fashion of a child.\n\nThe God who does not intervene _at all_, no matter how bad things get—_that's_ an obvious attempt to avoid falsification, to protect a [belief-in-belief](http://www.overcomingbias.com/2007/07/belief-in-belie.html).  Sufficiently young children don't have the deep-down knowledge that God doesn't really exist.  They really expect to see [a dragon in their garage](http://www.overcomingbias.com/2007/07/belief-in-belie.html).  They have no reason to imagine a loving God who never acts.  Where exactly is the boundary of sufficient awfulness?  Even a child can imagine arguing over the precise threshold.  But of course God will draw the line somewhere.  Few indeed are the loving parents who, desiring their child to grow up strong and self-reliant, would let their toddler be run over by a car.\n\nThe obvious example of a horror so great that God cannot tolerate it, is death—true death, mind-annihilation.  I don't think that even Buddhism allows that.  So long as there is a God in the classic sense—full-blown, ontologically fundamental, _the_ God—we can rest assured that no _sufficiently_ awful event will ever, ever happen.  There is no soul anywhere that need fear true annihilation; God will prevent it.\n\nWhat if you build your own simulated universe?  The classic example of a simulated universe is Conway's Game of Life.  I do urge you to [investigate](http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life) Life if you've never played it—it's important for comprehending the notion of \"physical law\".  Conway's Life has been proven Turing-complete, so it would be possible to build a sentient being in the Life universe, albeit it might be rather fragile and awkward.  Other cellular automata would make it simpler.\n\nCould you, by creating a simulated universe, escape the reach of God?  Could you simulate a Game of Life containing sentient entities, and torture the beings therein?  But if God is watching everywhere, then trying to build an unfair Life just results in _the_ God stepping in to modify your computer's transistors.  If the physics you set up in your computer program calls for a sentient Life-entity to be endlessly tortured for no particular reason, _the_ God will intervene.  God being omnipresent, there is no refuge _anywhere_ for true horror:  Life is fair.\n\nBut suppose that instead you ask the question:\n\n_Given_ such-and-such initial conditions, and _given_ such-and-such cellular automaton rules, what _would be_ the mathematical result?\n\nNot even God can modify the answer to this question, unless you believe that God can implement logical impossibilities.  Even as a very young child, I don't remember believing that.  (And why would you need to believe it, if God can modify anything that _actually_ exists?)\n\nWhat does Life look like, in this imaginary world where every step follows _only_ from its immediate predecessor?  Where things _only_ ever happen, or don't happen, because of the cellular automaton rules?  Where the initial conditions and rules _don't_ describe any God that checks over each state?  What does it look like, the world beyond the reach of God?\n\nThat world wouldn't be fair.  If the initial state contained the seeds of something that could self-replicate, natural selection might or might not take place, and complex life might or might not evolve, and that life might or might not become sentient, with no God to guide the evolution.  That world might evolve the equivalent of conscious cows, or conscious dolphins, that lacked hands to improve their condition; maybe they would be eaten by conscious wolves who never thought that they were doing wrong, or cared.\n\nIf in a vast plethora of worlds, something like humans evolved, then they would suffer from diseases—not to teach them any lessons, but only because viruses happened to evolve as well, under the cellular automaton rules.\n\nIf the people of that world are happy, or unhappy, the causes of their happiness or unhappiness may have nothing to do with good or bad choices they made.  Nothing to do with free will or lessons learned.  In the what-if world where every step follows only from the cellular automaton rules, the equivalent of Genghis Khan can murder a million people, and laugh, and be rich, and never be punished, and live his life much happier than the average.  Who prevents it?  God would prevent it from ever _actually_ happening, of course; He would at the very least visit some shade of gloom in the Khan's heart.  But in the mathematical answer to the question _What if?_ there is no God in the axioms.  So if the cellular automaton rules say that the Khan is happy, that, simply, is the whole and only answer to the what-if question.  There is nothing, absolutely nothing, to prevent it.\n\nAnd if the Khan tortures people horribly to death over the course of days, for his own amusement perhaps?  They will call out for help, perhaps imagining a God.  And if you _really_ wrote that cellular automaton, God would intervene in your program, of course.  But in the what-if question, what the cellular automaton _would_ do under the mathematical rules, there isn't any God in the system.  Since the physical laws contain no specification of a utility function—in particular, no prohibition against torture—then the victims will be saved only if the right cells happen to be 0 or 1.  And it's not likely that anyone will defy the Khan; if they did, someone would strike them with a sword, and the sword would disrupt their organs and they would die, and that would be the end of that.  So the victims die, screaming, and no one helps them; that is the answer to the what-if question.\n\nCould the victims be completely innocent?  Why not, in the what-if world?  If you look at the rules for Conway's Game of Life (which is Turing-complete, so we can embed arbitrary computable physics in there), then the rules are really very simple.  Cells with three living neighbors stay alive; cells with two neighbors stay the same, all other cells die.  There isn't anything in there about only innocent people not being horribly tortured for indefinite periods.\n\nIs this world starting to sound familiar?\n\nBelief in a fair universe often manifests in more subtle ways than thinking that horrors should be outright prohibited:  Would the twentieth century have gone differently, if Klara Pölzl and Alois Hitler had made love one hour earlier, and a different sperm fertilized the egg, on the night that Adolf Hitler was conceived?\n\nFor so many lives and so much loss to turn on a single event, seems _disproportionate._  The Divine Plan ought to make more _sense_ than that.  You can believe in a Divine Plan without believing in God—Karl Marx surely did.  You shouldn't have millions of lives depending on a casual choice, an hour's timing, the speed of a microscopic flagellum.  It ought not to be allowed.  It's _too_ disproportionate.  Therefore, if Adolf Hitler had been able to go to high school and become an architect, there would have been someone else to take his role, and World War II would have happened the same as before.\n\nBut in the world beyond the reach of God, there isn't any clause in the physical axioms which says \"things have to make sense\" or \"big effects need big causes\" or \"history runs on reasons too important to be so fragile\".  There is no God to _impose_ that order, which is so severely violated by having the lives and deaths of millions depend on one small molecular event.\n\nThe point of the thought experiment is to lay out the God-universe and the Nature-universe side by side, so that we can recognize what kind of thinking belongs to the God-universe.  Many who are atheists, still think as if certain things are _not allowed_.  They would lay out arguments for why World War II was inevitable and would have happened in more or less the same way, even if Hitler had become an architect.  But in sober historical fact, this is an unreasonable belief; I chose the example of World War II because from my reading, it seems that events were mostly driven by Hitler's personality, often in defiance of his generals and advisors.  There is no particular empirical justification that I happen to have heard of, for doubting this.  The main reason to doubt would be _refusal to accept_ that the universe could make so little sense—that horrible things could happen so _lightly,_ for no more reason than a roll of the dice.\n\nBut why not?  What prohibits it?\n\nIn the God-universe, God prohibits it.  To recognize this is to recognize that we don't live in that universe.  We live in the what-if universe beyond the reach of God, driven by the mathematical laws and nothing else.  Whatever physics says will happen, will happen.  Absolutely _anything,_ good or bad, will happen.  And there is nothing in the laws of physics to lift this rule even for the _really extreme_ cases, where you might expect Nature to be a little more reasonable.\n\nReading William Shirer's _The Rise and Fall of the Third Reich,_ listening to him describe the disbelief that he and others felt upon discovering the full scope of Nazi atrocities, I thought of what a strange thing it was, to read all that, and know, already, that there wasn't a single protection against it.  To just read through the whole book and accept it; horrified, but not at all disbelieving, because I'd already understood what kind of world I lived in.\n\nOnce upon a time, I believed that the extinction of humanity was not allowed.  And others who call themselves rationalists, may yet have things they trust.  They might be called \"positive-sum games\", or \"democracy\", or \"technology\", but they are sacred.  The mark of this sacredness is that the trustworthy thing can't lead to anything _really_ bad; or they can't be _permanently_ defaced, at least not without a compensatory silver lining.  In that sense they can be trusted, even if a few bad things happen here and there.\n\nThe unfolding history of Earth can't ever turn from its positive-sum trend to a negative-sum trend; that is not allowed.  [Democracies](http://www.overcomingbias.com/2007/09/applause-lights.html)—_modern_ _liberal_ democracies, anyway—won't ever legalize torture.  [Technology](http://www.overcomingbias.com/2008/09/raised-in-sf.html) has done so much good up until now, that there can't possibly be a Black Swan technology that breaks the trend and does more harm than all the good up until this point.\n\nThere are all sorts of clever arguments why such things can't possibly happen.  But the source of these arguments is a much deeper belief that such things are _not allowed_.  Yet who prohibits?  Who prevents it from happening?  If you can't visualize at least one lawful universe where physics say that such dreadful things happen—and so they _do_ happen, there being nowhere to appeal the verdict—then you aren't yet ready to argue _probabilities_.\n\nCould it really be that sentient beings have died absolutely for thousands or millions of years, with no soul and no afterlife—and _not_ as part of any grand plan of Nature—_not_ to teach any great lesson about the meaningfulness or meaninglessness of life—not even to teach any profound lesson about what is impossible—so that a trick as simple and stupid-sounding as [vitrifying people in liquid nitrogen](http://www.overcomingbias.com/2008/06/timeless-identi.html) can save them from total annihilation—and a 10-second rejection of the silly idea can destroy someone's soul?  Can it be that a computer programmer who signs a few papers and buys a life-insurance policy continues into the far future, while Einstein rots in a grave?  We can be sure of one thing:  God wouldn't allow it.  Anything that ridiculous and disproportionate would be ruled out.  It would make a mockery of the Divine Plan—a mockery of the _strong reasons_ why things must be the way they are.\n\nYou can have secular rationalizations for things being _not allowed_.  So it helps to imagine that there _is_ a God, benevolent as you understand goodness—a God who enforces throughout Reality a _minimum_ of fairness and justice—whose plans make sense and depend proportionally on people's choices—who will never permit absolute horror—who does not always intervene, but who at least prohibits universes wrenched _completely_ off their track... to imagine all this, but also imagine that _you,_ yourself, live in a what-if world of pure mathematics—a world beyond the reach of God, an utterly unprotected world where anything at all can happen.\n\nIf there's any reader still reading this, who thinks that being happy counts for more than anything in life, then maybe they _shouldn't_ spend much time pondering the unprotectedness of their existence.  Maybe think of it _just_ long enough to sign up themselves and their family for cryonics, and/or write a check to an existential-risk-mitigation agency now and then.  And wear a seatbelt and get health insurance and all those other dreary necessary things that can destroy your life if you miss that one step... but aside from that, if you want to be happy, meditating on the fragility of life isn't going to help.\n\nBut this post was written for those who have [something to protect](http://www.overcomingbias.com/2008/01/something-to-pr.html).\n\nWhat can a twelfth-century peasant do to save themselves from annihilation?  Nothing.  Nature's little challenges aren't always fair.  When you run into a challenge that's too difficult, you suffer the penalty; when you run into a lethal penalty, you die.  That's how it is for people, and it isn't any different for planets.  Someone who wants to dance the deadly dance with Nature, does need to understand what they're up against:  Absolute, utter, exceptionless neutrality.\n\nKnowing this won't always save you.  It wouldn't save a twelfth-century peasant, even if they knew.  If you think that a rationalist who fully understands the mess they're in, must _surely_ be able to find a way out—then you [trust rationality](http://www.overcomingbias.com/2008/05/no-defenses.html), enough said.\n\nSome commenter is bound to castigate me for putting too dark a tone on all this, and in response they will list out all the reasons why it's lovely to live in a neutral universe.  Life is allowed to be a _little_ dark, after all; but not darker than a certain point, unless there's a silver lining.\n\nStill, because I don't want to create _needless_ despair, I will say a few hopeful words at this point:\n\nIf humanity's future unfolds in the right way, we might be able to make our future light cone fair(er).  We can't modify fundamental physics, but on a higher level of organization we could build some guardrails and put down some padding; organize the particles into a pattern that does some internal checks against catastrophe.  There's a lot of stuff out there that we can't touch—but it may help to consider everything that isn't in our future light cone, as being part of the \"generalized past\".  As if it had all already happened.  There's at least the _prospect_ of defeating neutrality, in the only future we can touch—the only world that it accomplishes something to care about.\n\nSomeday, maybe, immature minds will reliably be sheltered.  Even if children go through the equivalent of not getting a lollipop, or even burning a finger, they won't ever be run over by cars.\n\nAnd the adults wouldn't be in so much danger.  A superintelligence—a mind that could think a trillion thoughts without a misstep—would not be intimidated by a challenge where death is the price of a single failure.  The raw universe wouldn't seem so harsh, would be only another problem to be solved.\n\nThe problem is that building an adult is itself an adult challenge.  That's what I finally realized, years ago.\n\nIf there is a fair(er) universe, we have to get there starting from _this_ world—the neutral world, the world of hard concrete with no padding, the world where challenges are not calibrated to your skills.\n\nNot every child needs to stare Nature in the eyes.  Buckling a seatbelt, or writing a check, is not that complicated or deadly.  I don't say that every rationalist should meditate on neutrality.  I don't say that every rationalist should think all these unpleasant thoughts.  But anyone who plans on confronting an uncalibratedchallenge of instant death, must not avoid them.\n\nWhat does a child need to do—what rules should they follow, how should they behave—to solve an adult problem?"
          },
          "voteCount": 133
        },
        {
          "name": "Experiential Pica",
          "type": "post",
          "slug": "experiential-pica",
          "_id": "9ZodFr54FtpLThHZh",
          "url": null,
          "title": "Experiential Pica",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Pica"
            },
            {
              "name": "Akrasia"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "tl;dr version: Akrasia might be like an eating disorder!\n\nWhen I was a teenager, I ate ice.  Lots of ice.  Cups and cups and cups of ice, constantly, all day long, when it was freely available.  This went on for years, during which time I ignored the fact that others found it peculiar. (\"Oh,\" I would joke to curious people at the school cafeteria, ignoring the opportunity to detect the strangeness of my behavior, \"it's for my pet penguin.\")  I had my cache of excuses: it keeps my mouth occupied.  It's so nice and cool in the summer.  I don't drink enough water anyway, it keeps me hydrated.  Yay, zero-calorie snack!\n\nThen I turned seventeen and attempted to donate blood, and was basically told, when they did the finger-stick test, \"Either this machine is broken or you should be in a dead faint.\"  I got some more tests done, confirmed that extremely scary things were wrong with my blood, and started taking iron supplements.  I stopped eating ice.  I stopped having any interest in eating ice at all.\n\n[Pica](http://en.wikipedia.org/wiki/Pica_%28disorder%29) is an impulse to eat things that are not actually food.  Compared to some of the things that people with pica eat, I got off very easy: ice did not do me any harm on its own, and was merely a symptom.  But here's the kicker: What I needed was iron.  If I'd been consciously aware of that need, I'd have responded to it with the supplements far earlier, or with steak^1^ and spinach and cereals fortified with 22 essential vitamins & minerals.  Ice does not contain iron.  And yet when what I needed was iron, what I wanted was ice.\n\nWhat if akrasia is _experiential pica_? What if, when you want to play Tetris or watch TV or tat doilies instead of doing your Serious Business, that _means_ that you aren't going to art museums enough, or that you should get some exercise, or that what your brain really craves is the chance to write a symphony?\n\nThe existence - indeed, prevalence - of pica is a perfect example of how the brain is _very bad_ at communicating certain needs to the systems that can get those needs met.  Even when the same mechanism - that of instilling the desire to eat something, in the case of pica - could be used to meet the need, the brain misfires^2^.  It didn't make me crave liver and shellfish and molasses, it made me crave water in frozen form.  A substance which did nothing to help, and was very inconvenient to continually keep around and indulge in, and which made people look at me funny when I held up the line at the drink dispenser for ten minutes filling up half a dozen paper cups.\n\nSo why shouldn't I believe that, for lack of some _non_-food X, my brain just might force me to seek out unrelated non-food Y and make me think it was all my own brilliant idea?  (\"Yay, zero-calorie snack that hydrates, cools, and is free or mere pennies from fast food outlets when I have completely emptied the icemaker!  I'm _so clever!_\")\n\nThe trouble, if one hopes to take this hypothesis any farther, is that it's hard to tell what your experiential deficiencies might be^3^.  The baseline needs for figure-skating and flan-tasting probably vary person-to-person a lot more than nutrient needs do.  You can't stick your finger, put a drop of blood into a little machine that goes \"beep\", and see if it says that you spend too little time weeding peonies.  I also have no way to solve the problem of being akratic about attempted substitutions for akrasia-related activities: even if you discovered for sure that by baking a batch of muffins once a week, you would lose the crippling desire to play video games constantly, nothing's stopping the desire to play video games from obstructing initial attempts at muffin-baking.\n\nPossible next steps to explore the experiential pica idea and see how it pans out:\n\n*   Study the habits of highly effective people.  Do you know somebody who seems unplagued by akrasia?  What does (s)he do during downtime?  Maybe someone you're acquainted with has hit on a good diet of experience that we could try to emulate.\n*   If you are severely plagued by akrasia, and there is some large class of experiences that you completely leave out of your life, attempt to find a way to incorporate something from that class.  See if it helps.  For instance, if you are practically never outdoors, take a short walk or just sit in the yard; if you practically never do anything for aesthetic reasons, find something pretty to look at or listen to; etc.\n*   You might already have periods when you are less akratic than usual.  Notice what experiences you have had around those times that could have contributed.\n\n^1^I was not a vegetarian until I had already been eating ice for a very long time.  The switch can only have exacerbated the problem.\n\n^2^Some pica sufferers do in fact eat things that contain the mineral they're deficient in, but not all.\n\n^3^Another problem is that this theory only covers what might be called \"enticing\" akrasia, the positive desire to do non-work things.  It has nothing to say about aversive akrasia, where you would do _anything but_ what you metawant to do."
          },
          "voteCount": 115
        },
        {
          "name": "Money: The Unit of Caring",
          "type": "post",
          "slug": "money-the-unit-of-caring",
          "_id": "ZpDnRCeef2CLEFeKM",
          "url": null,
          "title": "Money: The Unit of Caring",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Consequentialism"
            },
            {
              "name": "Effective Altruism"
            },
            {
              "name": "Economics"
            },
            {
              "name": "Shut Up and Multiply"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Steve Omohundro has suggested a [folk theorem](http://www.overcomingbias.com/2008/10/economic-defini.html) to the effect that, within the interior of any approximately rational, self-modifying agent, the marginal benefit of investing additional resources in anything ought to be about equal.  Or, to put it a bit more exactly, shifting a unit of resource between any two tasks should produce no increase in expected utility, relative to the agent's utility function and its probabilistic expectations about its own algorithms.\n\nThis resource balance principle implies that—over a very wide range of approximately rational systems, including even the interior of a self-modifying mind—there will exist some common currency of expected utilons, by which everything worth doing can be measured.\n\nIn our society, this common currency of expected utilons is called \"money\".  It is the measure of how much society cares about something.\n\nThis is a brutal yet obvious point, which many are motivated to deny.\n\nWith this audience, I hope, I can simply state it and move on.  It's not as if you thought \"society\" was intelligent, benevolent, and sane up until this point, right?\n\nI say this to make a certain point [held in common across many good causes](/lw/66/rationality_common_interest_of_many_causes/).  Any charitable institution you've ever had a kind word for, certainly _wishes_ you would appreciate this point, whether or not they've ever said anything out loud.  For I have listened to others in the nonprofit world, and I know that I am not speaking only for myself here...\n\nMany people, when they see something that they think is worth doing, would like to volunteer a few hours of spare time, or maybe mail in a five-year-old laptop and some canned goods, or walk in a march somewhere, but at any rate, not spend _money._\n\nBelieve me, I understand the feeling.  Every time I spend money I feel like I'm losing hit points.  That's the problem with having a unified quantity describing your net worth:  Seeing that number go down is not a pleasant feeling, even though it has to fluctuate in the ordinary course of your existence.  There ought to be a [fun-theoretic](http://www.overcomingbias.com/2009/01/fun-theory-sequence.html) principle against it.\n\nBut, well...\n\nThere _is_ this very, very old puzzle/observation in economics about the lawyer who spends an hour volunteering at the soup kitchen, instead of working an extra hour and donating the money to hire someone to work for five hours at the soup kitchen.\n\nThere's this thing called \"Ricardo's Law of Comparative Advantage\".  There's this idea called \"professional specialization\".  There's this notion of \"economies of scale\".  There's this concept of \"gains from trade\".  The whole reason why we have money is to realize the _tremendous_ gains possible from each of us doing what we do _best_.\n\nThis is what grownups do.  This is what you do when you want something to actually get _done._  You use _money_ to employ _full-time specialists._\n\nYes, people are sometimes limited in their ability to trade time for money (underemployed), so that it is better for them if they can directly donate that which they would usually trade for money.  _If_ the soup kitchen _needed_ a lawyer, and the lawyer donated a _large contiguous high-priority_ block of lawyering, then _that_ sort of volunteering makes sense—that's the same _specialized_ capability the lawyer ordinarily trades for money.  But \"volunteering\" just one hour of legal work, constantly delayed, spread across three weeks in casual minutes between other jobs?  This is not the way something gets done _when anyone actually cares about it_, or to state it near-equivalently, _when money is involved_.\n\nTo the extent that individuals fail to grasp this principle on a _gut level_, they may think that the use of money is somehow _optional_ in the pursuit of things that merely seem _morally_ desirable—as opposed to tasks like feeding ourselves, whose desirability seems to be treated oddly differently.  This factor may be sufficient _by itself_ to [prevent us from pursuing](/lw/64/helpless_individuals/) our collective common interest in groups larger than 40 people.\n\nEconomies of trade and professional specialization are not just vaguely good yet unnatural-sounding ideas, _they are the only way that anything ever gets done in this world._  Money is not pieces of paper, it is the _common currency of caring._\n\nHence the old saying:  \"Money makes the world go 'round, love barely keeps it from blowing up.\"\n\nNow, we do have the problem of akrasia—of not being able to do what we've decided to do—which is a part of the art of rationality that I hope someone else will develop; I specialize more in the impossible questions business.  And yes, spending money is more painful than volunteering, because you can see the bank account number go down, whereas the remaining hours of our span are not visibly numbered.  But when it comes time to feed yourself, do you think, \"Hm, maybe I should try raising my own cattle, that's less painful than spending money on beef?\"  Not everything can get done _without_ invoking Ricardo's Law; and on the other end of that trade are people who feel just the same pain at the thought of having less money.\n\nIt does seem to me offhand that there ought to be things doable to diminish the pain of losing hit points, and to increase the felt strength of the connection from donating money to \"I did a good thing!\"  Some of that I am trying to accomplish right now, by emphasizing the true nature and power of money; and by inveighing against the poisonous meme saying that someone who [gives mere money](http://www.overcomingbias.com/2009/03/as-ye-judge-those-who-fund-thee-ye-shall-be-judged.html) must not care enough to get personally involved.  This is a mere reflection of a mind that doesn't understand the post-hunter-gatherer concept of a market economy.  The act of donating money is not the momentary act of writing the check, it is the act of every hour you spent to earn the money to write that check—just as though you worked at the charity itself _in your professional capacity,_ at maximum, grownup efficiency.\n\nIf the lawyer needs to work an hour at the soup kitchen to keep himself motivated and remind himself why he's doing what he's doing, _that's fine._  But he should _also_ be donating some of the hours he worked at the office, because that is the power of professional specialization and it is how grownups really get things done.  One might consider the check as buying the right to volunteer at the soup kitchen, or validating the time spent at the soup kitchen.  I may post more about this later.\n\nTo a first approximation, money is the unit of caring up to a positive scalar factor—the unit of relative caring.  Some people are frugal and spend less money on _everything;_ but if you would, in fact, spend $5 on a burrito, then whatever you will not spend $5 on, you care about _less than_ you care about the burrito.  If you don't spend two months salary on a diamond ring, it doesn't mean you don't love your Significant Other.  (\"De Beers: It's Just A Rock.\")  But conversely, if you're _always_ reluctant to spend _any_ money on your SO, and yet seem to have no emotional problems with spending $1000 on a flat-screen TV, then yes, this _does_ say something about your relative values.\n\nYes, frugality is a virtue.  Yes, spending money hurts.  But in the end, if you are never willing to spend any units of caring, it means you don't care."
          },
          "voteCount": 166
        },
        {
          "name": "The Importance of Goodhart's Law",
          "type": "post",
          "slug": "the-importance-of-goodhart-s-law",
          "_id": "YtvZxRpZjcFNwJecS",
          "url": null,
          "title": "The Importance of Goodhart's Law",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Goodhart's Law"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "A speculative origin of Goodhart's law",
                "anchor": "A_speculative_origin_of_Goodhart_s_law",
                "level": 1
              },
              {
                "title": "The mitigations to Goodhart's law",
                "anchor": "The_mitigations_to_Goodhart_s_law",
                "level": 1
              },
              {
                "title": "Hansonian Cynicism",
                "anchor": "Hansonian_Cynicism",
                "level": 2
              },
              {
                "title": "Better measures",
                "anchor": "Better_measures",
                "level": 2
              },
              {
                "title": "Balanced scorecards",
                "anchor": "Balanced_scorecards",
                "level": 3
              },
              {
                "title": "Optimization around the constraint",
                "anchor": "Optimization_around_the_constraint",
                "level": 3
              },
              {
                "title": "Extrapolated Volition",
                "anchor": "Extrapolated_Volition",
                "level": 3
              },
              {
                "title": "Solutions centred around Human discretion",
                "anchor": "Solutions_centred_around_Human_discretion",
                "level": 2
              },
              {
                "title": "Left Anarchist ideas",
                "anchor": "Left_Anarchist_ideas",
                "level": 3
              },
              {
                "title": "Hierarchical rule",
                "anchor": "Hierarchical_rule",
                "level": 3
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "123 comments"
              }
            ],
            "headingsCount": 12
          },
          "contents": {
            "markdown": "This article introduces Goodhart's law, provides a few examples, tries to explain an origin for the law and lists out a few general mitigations.\n\nGoodhart's law states that once a social or economic measure is turned into a target for policy, it will lose any information content that had qualified it to play such a role in the first place. [wikipedia](http://en.wikipedia.org/wiki/Goodhart's_law) The law was named for its developer, Charles Goodhart, a chief economic advisor to the Bank of England.\n\nThe much more famous [Lucas critique](http://en.wikipedia.org/wiki/Lucas_critique) is a relatively specific formulation of the same. \n\nThe most famous examples of Goodhart's law should be the soviet factories which when given targets on the basis of numbers of nails produced many tiny useless nails and when given targets on basis of weight produced a few giant nails. Numbers and weight both correlated well in a pre-central plan scenario. After they are made targets (in different times and periods), they lose that value.\n\nWe laugh at such ridiculous stories, because our societies are generally much better run than Soviet Russia. But the key with Goodhart's law is that it is applicable at every level. The japanese countryside is apparently full of constructions that are going on because constructions once started in recession era are getting to be almost impossible to stop. Our society centres around money, which is supposed to be a relatively good measure of reified human effort. But many unscruplous institutions have got rich by pursuing money in many ways that people would find extremely difficult to place as value-adding.\n\nRecently [GDP Fetishism](http://www.econlib.org/library/Columns/y2010/HendersonGDP.html) by David henderson is another good article on how Goodhart's law is affecting societies.\n\nThe way I look at Goodhart's law is [Guess the teacher's password](/lw/iq/guessing_the_teachers_password/) writ large. People and instituitions try to achieve their explicitly stated targets in the easiest way possible, often obeying the letter of the law. \n\nA speculative origin of Goodhart's law\n--------------------------------------\n\nThe way I see Goodhart's law work, or a target's utility break down, is the following.\n\n*   Superiors want an undefined goal G.\n*   They formulate G* which is not G, but until now in usual practice, G and G* have correlated.\n*   Subordinates are given the target G*.\n*   The well-intentioned subordinate may recognise G and suggest G** as a substitute, but such people are relatively few and far inbetween. Most people try to achieve G*. \n*   As time goes on, every means of achieving G* is sought. \n*   Remember that G* was formulated precisely because it is simple and more explicit than G. Hence, the persons, processes and organizations which aim at maximising G* achieve competitive advantage over those trying to juggle both G* and G. \n*   P(G|G*) reduces with time and after a point, the correlation completely breaks down.\n\nThe mitigations to Goodhart's law\n---------------------------------\n\nIf you consider the law to be true, solutions to Goodhart's law are an impossibility in a non-singleton scenario. So let's consider mitigations.\n\n*   Hansonian Cynicism\n*   Better Measures\n*   Solutions centred around Human Discretion\n\n### Hansonian Cynicism\n\nPointing out what most people would have in mind as G and showing that institutions all around are not following G, but their own convoluted G*s. Hansonian cynicism is definitely the second step to mitigation in many many cases (Knowing about Goodhart's law is the first). Most people expect universities to be about education and hospitals to be about health. Pointing out that they aren't doing what they are supposed to be doing creates a huge cognitive dissonance in the thinking person.\n\n### Better measures\n\n#### Balanced scorecards\n\nTaking multiple factors into consideration, trying to make G* as strong and spoof-proof as possible.   The Scorecard approach is mathematically, the simplest solution that strikes a mind when confronted with Goodhart's law.\n\n#### Optimization around the constraint\n\nThere are no generic solutions to bridging the gap between G and G*, but the body of knowledge of [theory of constraints](http://en.wikipedia.org/wiki/Thinking_Processes_(Theory_of_Constraints)) is a very good starting point for formulating better measures for corporates.\n\n#### Extrapolated Volition\n\n[CEV](http://en.wikipedia.org/wiki/Coherent_Extrapolated_Volition) tries to mitigate Goodhart's law in a better way than mechanical measures by trying to create a complete map of human morality. If G is defined fully, there is no need for a G*. CEV tries to do it for all humanity, but as an example, individual extrapolated volition should be enough. The attempt is incomplete as of now, but it is promising.\n\n### Solutions centred around Human discretion\n\nHuman discretion is the one thing that can presently beat Goodhart's law because the constant checking and rechecking that G and G* match. Nobody will attempt to pull off anything as weird as the large nails in such a scenario. However, this is not scalable in a strict sense because of the added testing and quality control requirements.\n\n#### Left Anarchist ideas\n\nLeft anarchist ideas about small firms and workgroups are based on the fact that hierarchy will inevitably introduce goodhart's law related problems and thus the best groups are small ones doing simple things.\n\n#### Hierarchical rule\n\nOn the other end of the political spectrum, Molbuggian hierarchical rule completely eliminates the mechanical aspects of the law. There is no letter of the law, its all spirit. I am supposed to take total care of my slaves and have total obedience to my master. The scalability is ensured through hierarchy.\n\nOf all proposed solutions to the Goodhart's law problem confronted, I like CEV the most, but that is probably a reflection on me more than anything, wanting a relatively scalable and automated solution. I'm not sure whether the human discretion supporting people are really correct in this matter.\n\nYour comments are invited and other mitigations and solutions to Goodhart's law are also invited."
          },
          "voteCount": 91
        },
        {
          "name": "Policy Debates Should Not Appear One-Sided",
          "type": "post",
          "slug": "policy-debates-should-not-appear-one-sided",
          "_id": "PeSzc9JTBxhaYRp9b",
          "url": null,
          "title": "Policy Debates Should Not Appear One-Sided",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Politics"
            },
            {
              "name": "Libertarianism"
            },
            {
              "name": "Conflict vs Mistake"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Robin Hanson proposed stores where banned products could be sold.[^1^](#fn1x15) There are a number of excellent arguments for such a policy—an inherent right of individual liberty, the career incentive of bureaucrats to prohibit *everything*, legislators being just as biased as individuals. But even so (I replied), *some* poor, honest, not overwhelmingly educated mother of five children is going to go into these stores and buy a “Dr. Snakeoil’s Sulfuric Acid Drink” for her arthritis and die, leaving her orphans to weep on national television.\n\nI was just making a factual observation. Why did some people think it was an argument in favor of regulation?\n\nOn questions of simple fact (for example, whether Earthly life arose by natural selection) there’s a legitimate expectation that the argument should be a one-sided battle; the facts themselves are either one way or another, and the so-called “balance of evidence” should reflect this. Indeed, under the Bayesian definition of evidence, “strong evidence” is just that sort of evidence which we only expect to find on one side of an argument.\n\nBut there is no reason for complex actions with many consequences to exhibit this onesidedness property. Why do people seem to want their *policy* debates to be one-sided?\n\nPolitics is the mind-killer. Arguments are soldiers. Once you know which side you’re on, you must support all arguments of that side, and attack all arguments that appear to favor the enemy side; otherwise it’s like stabbing your soldiers in the back. If you abide within that pattern, policy debates will also appear one-sided to you—the costs and drawbacks of your favored policy are enemy soldiers, to be attacked by any means necessary.\n\nOne should also be aware of a related failure pattern: thinking that the course of Deep Wisdom is to compromise with perfect evenness between whichever two policy positions receive the most airtime. A policy may legitimately have *lopsided* costs or benefits. If policy questions were not tilted one way or the other, we would be unable to make decisions about them. But there is also a human tendency to deny all costs of a favored policy, or deny all benefits of a disfavored policy; and people will therefore tend to think policy tradeoffs are tilted much further than they actually are.\n\nIf you allow shops that sell otherwise banned products, some poor, honest, poorly educated mother of five kids is going to buy something that kills her. This is a prediction about a factual consequence, and as a factual question it appears rather straightforward—a sane person should readily confess this to be true regardless of which stance they take on the policy issue. You may *also* think that making things illegal just makes them more expensive, that regulators will abuse their power, or that her individual freedom trumps your desire to meddle with her life. But, as a matter of simple fact, she’s still going to die.\n\nWe live in an unfair universe. Like all primates, humans have strong negative reactions to perceived unfairness; thus we find this fact stressful. There are two popular methods of dealing with the resulting cognitive dissonance. First, one may change one’s view of the facts—deny that the unfair events took place, or edit the history to make it appear fair.[^2^](#fn2x15) Second, one may change one’s morality—deny that the events are unfair.\n\nSome libertarians might say that if you go into a “banned products shop,” passing clear warning labels that say THINGS IN THIS STORE MAY KILL YOU, and buy something that kills you, then it’s your own fault and you deserve it. If that were a moral truth, there would be *no downside* to having shops that sell banned products. It wouldn’t just be a *net benefit*, it would be a *one-sided* tradeoff with no drawbacks.\n\nOthers argue that regulators can be trained to choose rationally and in harmony with consumer interests; if those were the facts of the matter then (in their moral view) there would be *no downside* to regulation.\n\nLike it or not, there’s a birth lottery for intelligence—though this is one of the cases where the universe’s unfairness is so extreme that many people choose to deny the facts. The experimental evidence for a purely genetic component of 0.6–0.8 is overwhelming, but even if this were to be denied, you don’t choose your parental upbringing or your early schools either.\n\nI was raised to believe that denying reality is a *moral wrong.* If I were to engage in wishful optimism about how Sulfuric Acid Drink was likely to benefit me, I would be doing something that I was *warned* against and raised to regard as unacceptable. Some people are born into environments—we won’t discuss their genes, because that part is too unfair—where the local witch doctor tells them that it is *right* to have faith and *wrong* to be skeptical. In all goodwill, they follow this advice and die. Unlike you, they weren’t raised to believe that people are responsible for their individual choices to follow society’s lead. Do you really think you’re so smart that you would have been a proper scientific skeptic even if you’d been born in 500 CE? Yes, there is a birth lottery, no matter what you believe about genes.\n\nSaying “People who buy dangerous products deserve to get hurt!” is not tough-minded. It is a way of refusing to live in an unfair universe. Real tough-mindedness is saying, “Yes, sulfuric acid is a horrible painful death, and no, that mother of five children didn’t deserve it, but we’re going to keep the shops open anyway because we did this cost-benefit calculation.” Can you imagine a politician saying that? Neither can I. But insofar as economists have the power to influence policy, it might help if they could think it privately—maybe even say it in journal articles, suitably dressed up in polysyllabismic obfuscationalization so the media can’t quote it.\n\nI don’t think that when someone makes a stupid choice and dies, this is a cause for celebration. I count it as a tragedy. It is not always helping people, to save them from the consequences of their own actions; but I draw a moral line at capital punishment. If you’re dead, you can’t learn from your mistakes.\n\nUnfortunately the universe doesn’t agree with me. We’ll see which one of us is still standing when this is over.\n\n* * *\n\n[^1^](#fn1x15-bk)Robin Hanson et al., “The Hanson-Hughes Debate on ‘The Crack of a Future Dawn,’” 16, no. 1 (2007): 99–126, [http://jetpress.org/v16/hanson.pdf](http://jetpress.org/v16/hanson.pdf).\n\n[^2^](#fn2x15-bk)This is mediated by the affect heuristic and the just-world fallacy."
          },
          "voteCount": 171
        },
        {
          "name": "The mathematical universe: the map that is the territory",
          "type": "post",
          "slug": "the-mathematical-universe-the-map-that-is-the-territory",
          "_id": "fZJRxYLtNNzpbWZAA",
          "url": null,
          "title": "The mathematical universe: the map that is the territory",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Physics"
            },
            {
              "name": "Logic & Mathematics "
            },
            {
              "name": "Simulation Hypothesis"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_This post is for people who are not familiar with the Level IV Multiverse/Ultimate Ensemble/Mathematical Universe Hypothesis, people who are not convinced that there’s any reason to believe it, and people to whom it appears believable or useful but not satisfactory as an actual explanation for anything._\n\n_I’ve found that while it’s fairly easy to understand what this idea asserts, it is more difficult to get to the point where it actually seems convincing and intuitively correct, until you independently invent it for yourself. Doing so can be fun, but for those who want to skip that part, I’ve tried to write this post as a kind of [intuition pump](http://en.wikipedia.org/wiki/Intuition_pump) (of the variety, I hope, that deserves the non-derogatory use of that term) with the goal of leading you along the same line of thinking that I followed, but in a few minutes rather than a few years._\n\n* * *\n\nOnce upon a time, I was reading some Wikipedia articles on physics, [clicking links aimlessly](http://xkcd.com/214/), when I happened upon a page then titled “Ultimate Ensemble”. It described a multiverse of all internally-consistent mathematical structures, thereby allegedly explaining our own universe — it’s mathematically possible, so it exists along with every other possible structure.\n\nNow, I was certainly interested in the question it was attempting to answer. It’s one that most young aspiring deep thinkers (and many very successful deep thinkers) end up at eventually: why is there a universe at all? A friend of mine calls himself an agnostic because, he says, “Who created God?” and “What caused the Big Bang?” are the same question. Of course, they’re not _quite_ the same, but the fundamental point is valid: although nothing happened “before” the Big Bang (as a more naïve version of this query might ask), saying that it caused the universe to exist still requires us to explain what brought about the laws and circumstances allowing the Big Bang to happen. There are some hypotheses that try to explain this universe in terms of a more general multiverse, but all of them seemed to lead to another question: “Okay, fine, then what caused _that_ to be the case?”\n\nThe Ultimate Ensemble, although interesting, looked like yet another one of those non-explanations to me. “Alright, so every mathematical structure ‘exists’. Why? Where? If there are all these mathematical structures floating around in some multiverse, what are the laws of this multiverse, and what caused _those_ laws? What’s the evidence for it?” It seemed like every explanation would lead to an infinite regress of multiverses to explain, or a [stopsign](/lw/it/semantic_stopsigns/) like “God did it” or “it just exists because it exists and that’s the end of it” (I’ve seen that from several atheists trying to convince themselves or others that this is a non-issue) or “science can never know what lies beyond this point” or “here be dragons”. This was deeply vexing to my 15-year-old self, and after a completely secular upbringing, I suffered a mild bout of spirituality over the following year or so. Fortunately I made a full recovery, but I gave in and decided that Stephen Hawking was right that “Why does the universe bother to exist?” would remain permanently unanswerable.\n\nLast year, I found myself thinking about this question again — but only after unexpectedly making my way back to it while thinking about the idea of an AI being conscious. And the path I took actually suggested an answer this time. As I worked on writing it up, I noticed that it sounded familiar. After I remembered what that Wikipedia article was called, and after actually looking up Max Tegmark’s papers on it this time, I confirmed that it was indeed the same essential idea. (Don’t you hate/love it when you find out that your big amazing groundbreaking idea has already been advocated by someone smarter and more important than you? It’s so disappointing/validating.) One of the papers briefly explores reasoning similar to that which I had accidentally used to convince myself of it, but it’s an argument that I haven’t seen emphasized in any discussions of it hereabouts, and it’s one which seems inescapable with no assumptions outside of ordinary materialism and reductionism.\n\nI shall now get to the point.\n\n* * *\n\nSuppose this universe is a computer simulation.\n\nIt isn’t, but we’ll imagine for the next few paragraphs that it is.\n\nSuppose everything we see — and all of the Many Worlds that we don’t see, and everything in this World that is too distant for us to ever see — is the product of a precise simulation being performed by some amazing supercomputer. Let’s call it the Grand Order Deducer, or G.O.D. for short.\n\nActually, let’s say that G.O.D. is not an amazing supercomputer, but a 386 with an insanely large hard drive. Obviously, we wouldn’t notice the slowness from the inside, any more than the characters in a movie would notice that your DVD player is being choppy.\n\nClearly, then, if G.O.D. were turned off for a billion years, and then reactivated at the point where it left off, we wouldn’t notice anything either. How about if the state of the simulation were copied to a very different kind of computer (say, a prototypical tape-based universal Turing machine, or an immortal person doing lambda calculus operations by hand) and continued? If our universe’s physics turns out to be fundamentally time-symmetrical, then if G.O.D. started from the end of the universe and simulated backwards, would we experience our lives backwards? If it saved a copy of the universe at the beginning of your life and repeatedly ran the simulation from there until your death (if any), would it mean anything to say that you are experiencing your life multiple times? If the state of the simulation were copied onto a million identical computers, and continued thence on all of them, would we feel a million times as real (or would there be a million “more” of each of us in any meaningful sense), and would the implausibly humanlike agent who hypothetically created this simulation feel a million times more culpable for any suffering taking place within it? It would be hard to argue that any of this should be the case without resorting to some truly ridiculous metaphysics. Every computer is calculating the same thing, even the ones that don’t seem plausible as universe-containers under our intuitions about what a simulation would look like.\n\nBut what, then, makes us feel real? What if, after G.O.D. has been turned off for a billion years… it stays off? If we can feel real while being simulated by a hundred computers, and no less real while being simulated by [one computer](/lw/1hg/the_moral_status_of_independent_identical_copies/), how about if we’re being simulated by zero computers? More concretely, and perhaps more disturbingly, if [torturing a million identical simulations](/lw/1pz/the_ai_in_a_box_boxes_you/) is the same thing as torturing one (I’d argue that it is), is torturing one the same as torturing zero?\n\n2 + 2 will always be 4 whether somebody is computing it or not. (No Platonism is necessary here; only the Simple Truth that taking the string “2 + 2” and applying certain rules of inference to it always results in the string “4”.) Similarly, even if this universe is nothing but a hypothetical, not being computed by anyone, not existing in anything larger, there are certain things that are necessarily true _about_ the hypothetical, including facts about the subjective mental states of us self-aware substructures. Nothing magical happens when a simulation runs. Most of us agree that consciousness is probably purely mechanistic, and that we could therefore create a conscious AI or emulate an uploaded brain, and that it would be just as conscious as we are; that if we could simulate Descartes, we’d hear him make the usual arguments about the duality of the material body and the extraphysical mind, and if we could simulate Chalmers, he’d come to the same familiar nonsensical conclusions about qualia and zombies. But the fact remains that it’s just a computer doing what computers always do, with no special EXIST or FEEL opcodes added to its instruction set. If a mind, from the outside, can be a self-contained and timeless structure, and the full structure can be calculated (within given finite limits) from some initial state by a normal computer, then its consciousness is a property of the structure itself, not of the computer or the program — the program is not causing it, it’s just letting someone notice it. So deep runs the dualist intuition that even when we have reduced spirits and consciousness and free will to normal physical causality, there’s still sometimes a tendency to think as though turning on a sufficiently advanced calculator causes something to mysteriously blink into existence or awareness, when all it is doing is reporting facts about some very large numbers that would be true one way or the other.\n\nG.O.D. is doing the very same thing, just with numbers that are even more unimaginably huge: a universe instead of an individual mind. The distilled and generalized argument is thus: _If we can feel real inside a non-magical computer simulation, then our feeling of reality must be due to necessary properties of the information being computed, because such properties do not exist in the abstract process of computing, and those properties will not cease to be true about the underlying information if the simulation is stopped or is never created in the first place._ This is identically true about every other possible reality.\n\nBy Occam’s Razor, I conclude that if a universe _can_ exist in this way — as one giant subjunctive — then we must accept that that is how and why our universe _does_ exist; even if we are being simulated on a computer in some outer universe, or if we were created by an actual deity (which, from a non-intervening deity’s perspective, would probably look about the same as running a simulation anyway), or if there is some other explanation for this particular universe, we now see that this would not actually be the cause of our existence. Existence is what mathematical possibility feels like from the inside. Turn off G.O.D., and we’ll go on with our lives, not noticing that anything has changed. Because the only thing that _has_ changed is that the people who were running the simulation won’t get to find out what happens next.\n\n* * *\n\nTegmark has described this as a “theory of everything”. I’d discourage that use, merely as a matter of consistency with common usage; conventionally, “theory of everything” refers to the underlying laws that define the regularities of _this_ universe, and whatever heroic physicists eventually discover those laws should retain the honour of having their theory known as such. As a metaphysical theory (less arbitrary than conventional metaphysics, but metaphysical nonetheless), this does not fit that description; it gives us almost no useful information about our own universe. It is a theory of more than everything, and a theory of nothing (in the same way that a program that prints out every possible bit string will eventually print out any given piece of information, while its actual information content is near zero).\n\nThat said, this theory and the argument I presented are not _entirely_ free of implications about and practical applications within this particular universe. Here are some of them.\n\n*   The [simulation argument](http://www.simulation-argument.com/) is dissolved. At this point, the idea of “living in a computer simulation” is meaningless. Simulating a universe should properly be viewed as comparable more to looking in a window than building the house. (Most of Robin Hanson’s [thoughts about metaethics and self-preservation within a simulation](http://www.jetpress.org/volume7/simulation.htm) are similarly dissolved, since a reality doesn’t pop out of existence when people stop simulating it; the only relevant part is the section about “If our descendants sometimes play parts in their simulations”, and this doesn’t seem to be the case anyway.)\n    \n*   As I mentioned, this significantly changes the dynamics of thought experiments like [The AI In A Box Boxes You](/lw/1pz/the_ai_in_a_box_boxes_you/). Torturing a thousand identical simulations is the same as torturing one, and torturing one is the same as torturing zero — _if and only if_ the structure within the simulation(s) is not being causally influenced by any ongoing circumstances in _this_ universe. If it is, then the two realities are entangled to the point where they are essentially different parts of the same structure, and it is worth thinking about how much we should care about each one.\n    \n*   That leads me to a more general point about metaethics: although there are other realities out there where there are very sentient and very intelligent beings experiencing suffering literally 3^^^3 times greater than anything we can imagine, and others where there are beings experiencing bliss in the same proportions, we must resist the urge to feel (respectively) sorry for them or jealous of them. Your intuitive sense of what “really exists” should remain limited to this universe.\n    \n    Perhaps this caution only applies to me in the first place. I am, admittedly, the only person I know who has to leave the room when people are playing _The Sims_ because I can’t stand to watch those little nowhere-near-sentient structures being put in torturous or even merely uncomfortable situations, so maybe it’s only my own empathy that’s a bit overactive. However, when we’re talking about sentient, sapient structures, we really do need to think about where to draw the line. I’d draw it at the point where a simulation starts to interact with this universe, in both directions — of course it will affect our universe if we are observing the simulation and reacting based on it, but we should only start caring about its feelings if we have designed the software such that _it_ is affected by _our actions_ beyond our choices for its initial conditions. That’s what I referred to as entanglement earlier. Once there’s that bilateral feedback, it’s no longer one structure observing another; they are both part of the same reality. (Take that as a practicality, not as a statement of an alleged metaphysical law. We’re trying to eliminate the need for metaphysical laws here.)\n    \n*   This theory results in a variation on the [Boltzmann brain scenario](/lw/17d/forcing_anthropics_boltzmann_brains/): regardless of _this_ universe’s ability to create Boltzmann brains, there’s also the possibility (and, therefore, necessity) of disembodied mind-structures hallucinating their own realities. My best guess as to the solution to this problem (if we’re to take it as a problem) is that any mind-structure that contains enough information to reliably hallucinate an orderly, mechanistic reality must be isomorphic to that reality.\n    \n*   It raises other strange anthropic questions too. The one that comes most immediately to my mind is this: If every possible mathematical structure is real in the same way that this universe is, then isn’t there only an infinitesimal probability that this universe will turn out to be ruled entirely by simple regularities? Given a universe governed by a small set of uniformly applied laws, there will be an infinity of universes governed by the same laws plus arbitrary variations, possibly affecting the internally observable structure only at very specific points in space and time. This results in a sort of anti–Occam’s Razor (Macco’s Rozar? Occam’s Beard Tonic?), where the larger the irregularity, the more likely it becomes over the space of all possible universes, because there are that many more ways for it to happen. (For example, there is a universe — actually, a huge number (possibly infinity) of barely different universes — identical to this one, except that, for no reason explainable by the usual laws of quantum mechanics, but not ruled out as a logically possible law unto itself, your head will explode as soon as you finish reading this post. I hope that possibility does not dissuade you from doing so, but I accept no responsibility if this _does_ turn out to be one of those universes.)\n    \n    From the outside, this would appear to be a non-issue. Consider people in some other reality simulating this one (assuming that this one really _is_ as simple and consistent as it appears). By some extraordinary luck, they’ve zoomed in on this exact planet in this exact Everett branch, and from there, they’ve even zoomed in on me writing this. “What does this guy mean,” they ask themselves, “wondering what the probability is that this particular reality will have the laws that it does? It’s not like anyone had any choice in the matter.” Yes, there will be versions of the universe that really are that orderly, and if this is one of them, than that would be why this universe’s version of me is wondering about the apparent astronomical unlikelihood of being in this universe. But from the inside, this seems terribly unsatisfying — if these slightly-irregular universes are possible, then we don’t know for sure what kind we’re in, so _should_ we expect to find such irregularities? Perhaps such exceptions would constitute such a departure from quantum mechanics that they couldn’t be made consistent with it even as a special case. (Tegmark makes a related point in one paper: the hypothesis “does certainly _not_ imply that all imaginable universes exist. We humans can imagine many things that are mathematically undefined and hence do not correspond to mathematical structures.”) Or perhaps the infinity of universes where such irregularities exist in places we’ll never observe (outside our light cone, in vast areas of empty space, etc.) is a much larger infinity (in probability density, not cardinality) than that of those universes where any of those irregularities will actually affect us. I’m leaning toward that explanation, but maybe a simpler one is that I’m reasoning about this incorrectly — after a conversation about this with Justin Shovelain, I’m reconsidering whether it’s actually correct to use probabilities to reason about an infinite space of apparently equally-likely items — or maybe this reasoning is correct and it observationally refutes the hypothesis. We’ll see.\n    \n\n* * *\n\nOne last comment: some people I’ve discussed this with have actually taken it as a _reductio ad absurdum_ against the idea that a being within a simulation _could_ feel real. As we say, one person’s _modus ponens_ is another person’s _modus tollens_. Since the conclusion I’m arguing for is merely unusual, not inconsistent (as far as I can tell), that takes out the _absurdum_; therefore, in the apparent absence of any specific alternatives at all, you can weigh the probability of this hypothesis against the stand-in alternatives that there _is_ something extraphysical about our own existence, something noncomputable about consciousness, or something metaphysically significant about processes equivalent to universal computation (or any other alternatives that I’ve neglected to think of).\n\nFinally, as I mentioned, the main goal of this post was to serve as an intuition pump for the Level IV Multiverse idea (and to point out some of the rationality-related questions it raises, so we’ll have something apropos to discuss here), not to explore it in depth. So if this was your first exposure to it, you should probably read Max Tegmark’s [The Mathematical Universe](http://arxiv.org/abs/0704.0646) now."
          },
          "voteCount": 96
        },
        {
          "name": "Beware of Other-Optimizing",
          "type": "post",
          "slug": "beware-of-other-optimizing",
          "_id": "6NvbSwuSAooQxxf7f",
          "url": null,
          "title": "Beware of Other-Optimizing",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Practical"
            },
            {
              "name": "Principles"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "I've noticed a serious problem in which aspiring rationalists vastly overestimate their ability to optimize other people's lives.  And I think I have some idea of how the problem arises.\n\nYou read nineteen different webpages advising you about personal improvement—productivity, dieting, saving money.  And the writers all sound bright and enthusiastic about Their Method, they tell tales of how it worked for them and promise _amazing_ results...\n\nBut most of the advice rings so false as to not even seem worth considering.  So you sigh, mournfully pondering the wild, childish enthusiasm that people can seem to work up for just about anything, no matter how silly.  Pieces of advice #4 and #15 sound interesting, and you try them, but... they don't... quite... well, it fails miserably.  The advice was wrong, or you couldn't do it, and either way you're not any better off.\n\nAnd then you read the twentieth piece of advice—or even more, you discover a twentieth method that wasn't in any of the pages—and STARS ABOVE IT ACTUALLY WORKS THIS TIME.\n\nAt long, long last you have discovered the _real_ way, the _right_ way, the way that actually _works._  And when someone else gets into the sort of trouble you used to have—well, this time you _know_ how to help them.  You can save them all the trouble of reading through nineteen useless pieces of advice and skip directly to the correct answer.  As an aspiring rationalist you've already learned that most people don't listen, and you usually don't bother—but this person is a friend, someone you know, someone you trust and respect to listen.\n\nAnd so you put a comradely hand on their shoulder, look them straight in the eyes, and tell them how to do it.\n\nI, personally, get quite a lot of this.  Because you see... when you've discovered the way that _really works_... well, you know better by now than to run out and tell your friends and family.  But you've got to try telling Eliezer Yudkowsky.  He _needs_ it, and there's a pretty good chance that _he'll_ understand.\n\nIt actually did take me a while to understand.  One of the critical events was when someone on the Board of the Institute Which May Not Be Named, told me that I didn't need a salary increase to keep up with inflation—because I could be spending substantially less money on food if I used an online coupon service.  And I believed this, because it was a friend I trusted, and it was delivered in a tone of such confidence.  So my girlfriend started trying to use the service, and a couple of weeks later she gave up.\n\nNow here's the the thing: if I'd run across exactly the same advice about using coupons on some blog somewhere, I probably wouldn't even have paid much attention, just read it and moved on.  Even if it were written by Scott Aaronson or some similar person known to be intelligent, I still would have read it and moved on.  But because it was delivered to me personally, by a friend who I knew, my brain processed it differently—as though I were being told _the_ secret; and that indeed is the tone in which it was told to me.  And it was something of a delayed reaction to realize that I'd simply been told, as personal advice, what otherwise would have been just a blog post somewhere; no more and no less likely to work for me, than a productivity blog post written by any other intelligent person.\n\nAnd because I have encountered a great many people trying to optimize me, I can attest that the advice I get is as wide-ranging as the productivity blogosphere.  But others don't see this plethora of productivity advice as indicating that people are _diverse_ in which advice works for them.  Instead they see a lot of obviously wrong poor advice.  And then they finally discover the right way—the way that works, unlike all those other blog posts that don't work—and then, quite often, they decide to use it to optimize Eliezer Yudkowsky.\n\nDon't get me wrong.  Sometimes the advice is helpful.  Sometimes it works.  [\"Stuck In The Middle With Bruce\"](/lw/9o/stuck_in_the_middle_with_bruce/)—that resonated, for me.  It may prove to be the most helpful thing I've read on the new _Less Wrong_ so far, though that has yet to be determined.\n\nIt's just that your earnest personal advice, that amazing thing you've found to actually work by golly, is no more and no less likely to work for me than a random personal improvement blog post written by an intelligent author is likely to work for you.\n\n\"Different things work for different people.\"  That sentence may give you a squicky feeling; I know it gives me one.  Because this sentence is a tool wielded by [Dark Side Epistemology](http://www.overcomingbias.com/2008/10/the-dark-side.html) to shield from criticism, used in a way closely akin to \"Different things are true for different people\" (which is simply false).\n\nBut until you grasp the laws that are near-universal generalizations, sometimes you end up messing around with surface tricks that work for one person and not another, without your understanding why, because you don't know the general laws that would dictate what works for who.  And the best you can do is remember that, and be willing to take \"No\" for an answer.\n\nYou _especially_ had better be willing to take \"No\" for an answer, if you have _power_ over the Other.  Power is, in general, a very dangerous thing, which is tremendously easy to abuse, without your being aware that you're abusing it.  There are things you can do to prevent yourself from abusing power, but you have to actually do them or they don't work.  There was a post on OB on how being in a position of power has been shown to decrease our ability to empathize with and understand the other, though I can't seem to locate it now.  I have seen a rationalist who did not think he had power, and so did not think he needed to be cautious, who was amazed to learn that he might be feared...\n\nIt's even worse when their discovery that works for them, requires a little _willpower._  Then if you say it doesn't work for you, the answer is clear and obvious: you're just being _lazy_, and they need to exert some _pressure_ on you to get you to do the _correct_ thing, the advice they've found that actually works.\n\nSometimes—I suppose—people are being lazy.  But be very, very, _very_ careful before you assume that's the case and wield power over others to \"get them moving\".  Bosses who can tell when something actually _is_ in your capacity if you're a little more motivated, without it burning you out or making your life incredibly painful—these are the bosses who are a pleasure to work under.  _That ability is extremely rare,_ and the bosses who have it are worth their weight in silver.  It's a high-level interpersonal technique that most people do not have.  I surely don't have it.  Do not assume you have it, because your intentions are good.  Do not assume you have it, because you'd never do anything to _others_ that you didn't want done to _yourself._  Do not assume you have it, because no one has ever complained to you.  Maybe they're just scared.  That rationalist of whom I spoke—who did not think he held power and threat, though it was certainly obvious enough to me—he did not realize that anyone could be scared of him.\n\nBe careful even when you hold _leverage,_ when you hold an important decision in your hand, or a threat, or something that the other person needs, and all of a sudden the temptation to optimize them seems overwhelming.\n\nConsider, if you would, that [Ayn Rand](http://www.overcomingbias.com/2007/12/ayn-rand.html)'s whole reign of terror over Objectivists can be seen in just this light—that she found herself with power and leverage, and could not resist the temptation to optimize.\n\nWe underestimate the distance between ourselves and others.  Not just [inferential distance](http://www.overcomingbias.com/2007/10/inferential-dis.html), but distances of temperament and ability, distances of situation and resource, distances of unspoken knowledge and unnoticed skills and luck, distances of interior landscape.\n\nEven I am often surprised to find that X, which worked so well for me, doesn't work for someone else.  But with so many others having tried to optimize me, I can at least recognize distance when I'm hit over the head with it.\n\nMaybe being pushed on does work... for you.  Maybe _you_ don't get sick to the stomach when someone with power over you starts helpfully trying to reorganize your life the correct way.  I don't know what makes you tick.  In the realm of willpower and akrasia and productivity, as in other realms, I don't know the generalizations deep enough to hold almost always.  I don't possess the deep keys that would tell me _when_ and _why_ and for _who_ a technique works or doesn't work.  All I can do is be willing to accept it, when someone tells me it doesn't work... and go on looking for the deeper generalizations that will hold everywhere, the deeper laws governing both the rule and the exception, waiting to be found, someday."
          },
          "voteCount": 130
        },
        {
          "name": "A Much Better Life?",
          "type": "post",
          "slug": "a-much-better-life",
          "_id": "5Qvvi23WT2unNCoS9",
          "url": null,
          "title": "A Much Better Life?",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Wireheading"
            },
            {
              "name": "Fiction"
            },
            {
              "name": "Humor"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "(Response to: [You cannot be mistaken about (not) wanting to wirehead](/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to),  [Welcome to Heaven)](/lw/1o9/welcome_to_heaven)\n\nThe Omega Corporation  \nInternal Memorandum  \nTo: Omega, CEO  \nFrom: Gamma, Vice President, Hedonic Maximization\n\nSir, this concerns the newest product of our Hedonic Maximization Department, the Much-Better-Life Simulator. This revolutionary device allows our customers to essentially plug into the Matrix, except that instead of providing robots with power in flagrant disregard for the basic laws of thermodynamics, they experience a life that has been determined by rigorously tested algorithms to be the most enjoyable life they could ever experience. The MBLS even eliminates all memories of being placed in a simulator, generating a seamless transition into a life of realistic perfection.\n\nOur department is baffled. Orders for the MBLS are significantly lower than estimated. We cannot fathom why every customer who could afford one has not already bought it. It is simply impossible to have a better life otherwise. Literally. Our customers' best possible real life has already been modeled and improved upon many times over by our programming. Yet, many customers have failed to make the transition. Some are even expressing shock and outrage over this product, and condemning its purchasers.\n\nExtensive market research has succeeded only at baffling our researchers. People have even refused free trials of the device. Our researchers explained to them in perfectly clear terms that their current position is misinformed, and that once they tried the MBLS, they would never want to return to their own lives again. Several survey takers went so far as to specify _that statement_ as their reason for refusing the free trial! They _know_ that the MBLS will make their life so much better that they won't want to live without it, and they refuse to try it _for that reason!_ Some cited their \"utility\" and claimed that they valued \"reality\" and \"actually accomplishing something\" over \"mere hedonic experience.\" Somehow these organisms are incapable of comprehending that, inside the MBLS simulator, they will be able to experience the feeling of actually accomplishing feats far greater than they could ever accomplish in real life. Frankly, it's remarkable such people amassed enough credits to be able to afford our products in the first place!\n\nYou may recall that a Beta version had an off switch, enabling users to deactivate the simulation after a specified amount of time, or could be terminated externally with an appropriate code. These features received somewhat positive reviews from early focus groups, but were ultimately eliminated. No agent could reasonably want a device that could allow for the interruption of its perfect life. Accounting has suggested we respond to slack demand by releasing the earlier version at a discount; we await your input on this idea.\n\nProfits aside, the greater good is at stake here. We feel that we should find every customer with sufficient credit to purchase this device,  forcibly install them in it, and bill their accounts. They will immediately forget our coercion, and they will be many, many times happier. To do anything less than this seems criminal. Indeed, our ethics department is currently determining if we can justify delaying putting such a plan into action. Again, your input would be invaluable.\n\nI can't help but worry there's something we're just not getting."
          },
          "voteCount": 75
        },
        {
          "name": "Less Wrong Rationality and Mainstream Philosophy",
          "type": "post",
          "slug": "less-wrong-rationality-and-mainstream-philosophy",
          "_id": "oTX2LXHqXqYg2u4g6",
          "url": null,
          "title": "Less Wrong Rationality and Mainstream Philosophy",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Site Meta"
            },
            {
              "name": "Scholarship & Learning"
            },
            {
              "name": "Philosophy"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Non-Quinean philosophy",
                "anchor": "Non_Quinean_philosophy",
                "level": 1
              },
              {
                "title": "Conclusion",
                "anchor": "Conclusion",
                "level": 1
              },
              {
                "title": "References",
                "anchor": "References",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "336 comments"
              }
            ],
            "headingsCount": 5
          },
          "contents": {
            "markdown": "Part of the sequence: [Rationality and Philosophy](http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy)\n\nDespite Yudkowsky's [distaste](/lw/tg/against_modal_logics/) for mainstream philosophy, Less Wrong is largely a philosophy blog. Major topics include [epistemology](http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions), [philosophy of language](/lw/od/37_ways_that_words_can_be_wrong/), [free will](http://wiki.lesswrong.com/wiki/Free_will), [metaphysics](http://wiki.lesswrong.com/wiki/Reductionism_(sequence)), [metaethics](http://wiki.lesswrong.com/wiki/Metaethics_sequence), [normative ethics](/lw/n3/circular_altruism/), [machine ethics](http://wiki.lesswrong.com/wiki/Friendly_artificial_intelligence), [axiology](/lw/xy/the_fun_theory_sequence/), [philosophy of mind](http://wiki.lesswrong.com/wiki/Zombies_(sequence)), and more.\n\nMoreover, standard Less Wrong positions on philosophical matters have been standard positions in a movement _within_ mainstream philosophy for half a century. That movement is sometimes called \"Quinean naturalism\" after Harvard's [W.V. Quine](http://en.wikipedia.org/wiki/W._V._Quine), who articulated the Less Wrong approach to philosophy in the 1960s. Quine was one of the [most influential](http://leiterreports.typepad.com/blog/2009/03/so-who-is-the-most-important-philosopher-of-the-past-200-years.html) philosophers of the last 200 years, so I'm not talking about an _obscure_ movement in philosophy.\n\nLet us survey the connections. Quine thought that philosophy was continuous with science - and where it wasn't, it was _bad_ philosophy. He embraced empiricism and reductionism. He rejected the notion of libertarian free will. He regarded postmodernism as sophistry. Like Wittgenstein and Yudkowsky, Quine didn't try to straightforwardly _solve_ traditional Big Questions as much as he either [dissolved those questions](/lw/of/dissolving_the_question/) or reframed them such that they _could_ be solved. He dismissed endless semantic arguments about the meaning of vague terms like _knowledge_. He rejected _a priori_ knowledge. He rejected the notion of privileged philosophical insight: knowledge comes from ordinary knowledge, as best refined by science. Eliezer once [said](/lw/tg/against_modal_logics/) that philosophy should be about cognitive science, and Quine would agree. Quine famously [wrote](http://commonsenseatheism.com/wp-content/uploads/2011/03/Quine-Epistemology-Naturalized.pdf):\n\n> The stimulation of his sensory receptors is all the evidence anybody has had to go on, ultimately, in arriving at his picture of the world. Why not just see how this construction really proceeds? Why not settle for psychology?\n\nBut isn't this using science to justify science? Isn't that circular? Not quite, say Quine and Yudkowsky. It is merely \"[reflecting on your mind's degree of trustworthiness, using your current mind as opposed to something else](/lw/s0/where_recursive_justification_hits_bottom/).\" Luckily, the brain is [the lens that sees its flaws](/lw/jm/the_lens_that_sees_its_flaws/). And thus, says Quine:\n\n> Epistemology, or something like it, simply falls into place as a chapter of psychology and hence of natural science.\n\nYudkowsky once [wrote](/lw/tg/against_modal_logics/), \"If there's any centralized repository of reductionist-grade naturalistic cognitive philosophy, I've never heard mention of it.\"\n\nWhen I read that I thought: _What? That's Quinean naturalism! That's [Kornblith](http://en.wikipedia.org/wiki/Hilary_Kornblith) and [Stich](http://en.wikipedia.org/wiki/Stephen_Stich) and [Bickle](http://www.philosophyandreligion.msstate.edu/faculty/bickle.php) and [the Churchlands](http://commonsenseatheism.com/?p=11226) and [Thagard](http://cogsci.uwaterloo.ca/Biographies/pault.html) and [Metzinger](http://www.philosophie.uni-mainz.de/metzinger/) and [Northoff](http://www.imhr.ca/research/northofflab/index-e.cfm)! There are_ hundreds _of philosophers who do that!_\n\n####   \n\n####   \n\n#### Non-Quinean philosophy\n\nBut I should also mention that LW philosophy / Quinean naturalism is not the _largest_ strain of mainstream philosophy. Most philosophy is still done in relative ignorance (or ignoring) of cognitive science. Consider the preface to _[Rethinking Intuition](http://www.amazon.com/Rethinking-Intuition-Michael-R-DePaul/dp/0847687953/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_:\n\n> Perhaps more than any other intellectual discipline, philosophical inquiry is driven by intuitive judgments, that is, by what \"we would say\" or by what seems true to the inquirer. For most of philosophical theorizing and debate, intuitions serve as something like a source of evidence that can be used to defend or attack particular philosophical positions.\n> \n> One clear example of this is a traditional philosophical enterprise commonly known as _conceptual analysis_. Anyone familiar with Plato's dialogues knows how this type of inquiry is conducted. We see Socrates encounter someone who claims to have figured out the true essence of some abstract notion... the person puts forward a definition or analysis of the notion in the form of necessary and sufficient conditions that are thought to capture all and only instances of the concept in question. Socrates then refutes his interlocutor's definition of the concept by pointing out various counterexamples...\n> \n> For example, in Book I of the _Republic_, when Cephalus defines justice in a way that requires the returning of property and total honesty, Socrates responds by pointing out that it would be unjust to return weapons to a person who had gone mad or to tell the whole truth to such a person. What is the status of these claims that certain behaviors would be unjust in the circumstances described? Socrates does not argue for them in any way. They seem to be no more than spontaneous judgments representing \"common sense\" or \"what we would say.\" So it would seem that the proposed analysis is rejected because it fails to capture our intuitive judgments about the nature of justice.\n> \n> After a proposed analysis or definition is overturned by an intuitive counterexample, the idea is to revise or replace the analysis with one that is not subject to the counterexample. Counterexamples to the new analysis are sought, the analysis revised if any counterexamples are found, and so on...\n> \n> Refutations by intuitive counterexamples figure as prominently in today's philosophical journals as they did in Plato's dialogues...\n> \n> ...philosophers have continued to rely heavily upon intuitive judgments in pretty much the way they always have. And they continue to use them in the absence of any well articulated, generally accepted account of intuitive judgment - in particular, an account that establishes their epistemic credentials.\n> \n> However, what appear to be serious new challenges to the way intuitions are employed have recently emerged from an unexpected quarter - empirical research in cognitive psychology.\n> \n> With respect to the tradition of seeking definitions or conceptual analyses that are immune to counterexample, the challenge is based on the work of psychologists studying the nature of concepts and categorization of judgments. (See, e.g., Rosch 1978; Rosch and Mervis 1975; Rips 1975; Smith and Medin 1981). Psychologists working in this area have been pushed to abandon the view that we represent concepts with simple sets of necessary and sufficient conditions. The data seem to show that, except for some mathematical and geometrical concepts, it is not possible to use simple sets of conditions to capture the intuitive judgments people make regarding what falls under a given concept...\n> \n> With regard to the use of intuitive judgments exemplified by reflective equilibrium, the challenge from cognitive psychology stems primarily from studies of inference strategies and belief revision. (See, e.g., Nisbett and Ross 1980; Kahneman, Slovic, and Tversky 1982.) Numerous studies of the patterns of inductive inference people use and judge to be intuitively plausible have revealed that people are prone to commit various fallacies. Moreover, they continue to find these fallacious patterns of reasoning to be intuitively acceptable upon reflection... Similarly, studies of the \"intuitive\" heuristics ordinary people accept reveal various gross departures from empirically correct principles...\n> \n> There is a growing consensus among philosophers that there is a serious and fundamental problem here that needs to be addressed. In fact, we do not think it is an overstatement to say that Western analytic philosophy is, in many respects, undergoing a crisis where there is considerable urgency and anxiety regarding the status of intuitive analysis.\n\n#### Conclusion\n\nSo Less Wrong-style philosophy is part of a movement within mainstream philosophy to massively reform philosophy in light of recent cognitive science - a movement that has been active for at least two decades. Moreover, Less Wrong-style philosophy has its roots in Quinean naturalism from _fifty_ years ago.\n\nAnd I haven't even covered all the work in [formal epistemology](/lw/3n0/an_overview_of_formal_epistemology_links/) toward (1) mathematically formalizing concepts related to induction, belief, choice, and action, and (2) arguing about the foundations of probability, statistics, game theory, decision theory, and algorithmic learning theory.\n\nSo: Rationalists need not dismiss or avoid philosophy.\n\n**Update:** To be clear, though, I _don't_ recommend reading Quine. Most people should not spend their time reading even Quinean philosophy; learning statistics and AI and cognitive science will be far more useful. All I'm saying is that mainstream philosophy, especially Quinean philosophy, _does_ make some useful contributions. **I've listed more than 20 of mainstream philosophy's useful contributions [here](/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qe4), including several instances of classic LW dissolution-to-algorithm**.\n\nBut maybe it's a testament to the epistemic utility of Less Wrong-ian rationality training and _thinking like an AI researcher_ that Less Wrong got so many things right _without_ much interaction with Quinean naturalism. As Daniel Dennett (2006) said, \"AI makes philosophy honest.\"\n\nNext post: [Philosophy: A Diseased Discipline](/lw/4zs/philosophy_a_diseased_discipline/)\n\n#### References\n\nDennett (2006). Computers as Prostheses for the Imagination. Talk presented at the International Computers and Philosophy Conference, Laval, France, May 3, 2006.\n\nKahneman, Slovic, & Tversky (1982)._ [Judgment Under Uncertainty: Heuristics and Biases](http://www.amazon.com/Judgment-under-Uncertainty-Heuristics-Biases/dp/0521284147/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Cambridge University Press.\n\nNisbett and Ross (1980). _[Human Inference: Strategies and Shortcomings of Social Judgment](http://www.amazon.com/Human-Inference-Strategies-Shortcomings-Judgement/dp/0134450736/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. Prentice-Hall.\n\nRips (1975). Inductive judgments about natural categories. _Journal of Verbal Learning and Behavior, 12_: 1-20.\n\nRosch (1978). Principles of categorization. In Rosch & Lloyd (eds.), _Cognition and Categorization_ (pp. 27-48). Lawrence Erlbaum Associates.\n\nRosch & Mervis (1975). Family resemblances: studies in the internal structure of categories. Cognitive Psychology, 8: 382-439.\n\nSmith & Medin (1981). _[Concepts and Categories](http://www.amazon.com/Categories-Concepts-Cognitive-science-Edward/dp/0674102754/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)_. MIT Press."
          },
          "voteCount": 149
        },
        {
          "name": "Philosophy: A Diseased Discipline",
          "type": "post",
          "slug": "philosophy-a-diseased-discipline",
          "_id": "FwiPfF8Woe5JrzqEu",
          "url": null,
          "title": "Philosophy: A Diseased Discipline",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Philosophy"
            },
            {
              "name": "World Modeling"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Failed methods",
                "anchor": "Failed_methods",
                "level": 2
              },
              {
                "title": "A diseased discipline",
                "anchor": "A_diseased_discipline",
                "level": 2
              },
              {
                "title": "Philosophy: the way forward",
                "anchor": "Philosophy__the_way_forward",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "448 comments"
              }
            ],
            "headingsCount": 5
          },
          "contents": {
            "markdown": "Part of the sequence: [Rationality and Philosophy](http://wiki.lesswrong.com/wiki/Rationality_and_Philosophy)\n\nEliezer's anti-philosophy post [Against Modal Logics](https://www.lesserwrong.com/lw/tg/against_modal_logics/) was pretty controversial, while my recent pro-philosophy (by LW standards) [post](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/) and my [list of useful mainstream philosophy contributions](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qe4) were massively up-voted. This suggests a significant appreciation for mainstream philosophy on Less Wrong - not surprising, since [Less](http://wiki.lesswrong.com/wiki/Mysterious_Answers_to_Mysterious_Questions) [Wrong](https://www.lesserwrong.com/lw/od/37_ways_that_words_can_be_wrong/) [covers](http://wiki.lesswrong.com/wiki/Free_will) [so](http://wiki.lesswrong.com/wiki/Reductionism_(sequence)) [many](http://wiki.lesswrong.com/wiki/Metaethics_sequence) [philosophical](https://www.lesserwrong.com/lw/n3/circular_altruism/) [topics](http://wiki.lesswrong.com/wiki/Zombies_(sequence)).\n\nIf you followed the recent [very](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3q6b) [long](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qan) [debate](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qth) between Eliezer and I over the value of mainstream philosophy, you may have gotten the impression that Eliezer and I strongly diverge on the subject. But I suspect I agree more with Eliezer on the value of mainstream philosophy than I do with many Less Wrong readers - perhaps most.\n\nThat might sound odd coming from someone who writes [a philosophy blog](http://commonsenseatheism.com/) and spends most of his spare time doing philosophy, so let me explain myself. (Warning: broad generalizations ahead! There are exceptions.)\n\nFailed methods\n--------------\n\nLarge swaths of philosophy (e.g. [continental](http://en.wikipedia.org/wiki/Continental_philosophy) and [postmodern](http://en.wikipedia.org/wiki/Postmodern_philosophy) philosophy) often don't even _try_ to be clear, rigorous, or scientifically respectable. This is philosophy of the \"Uncle Joe's musings on the meaning of life\" sort, except that it's [dressed up](http://el-prod.baylor.edu/certain_doubts/?p=453) in big words and long footnotes. You will occasionally stumble upon an _argument_, but it falls prey to [magical categories](https://www.lesserwrong.com/lw/td/magical_categories/) and [language confusions](https://www.lesserwrong.com/lw/od/37_ways_that_words_can_be_wrong/) and [non-natural hypotheses](https://www.lesserwrong.com/lw/on/reductionism/). You may also stumble upon science or math, but they are used to 'prove' things [irrelevant](http://richarddawkins.net/articles/824-postmodernism-disrobed) to the actual scientific data or the equations used.\n\n[Analytic](http://commonsenseatheism.com/?p=13499) philosophy is clearer, more rigorous, and better with math and science, but only does a slightly better job of avoiding magical categories, language confusions, and non-natural hypotheses. Moreover, its central tool is [intuition](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/#non-quine), and this displays a near-total ignorance of [how](http://www.amazon.com/Kluge-Haphazard-Evolution-Human-Mind/dp/054723824X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [brains](http://www.amazon.com/Heuristics-Biases-Psychology-Intuitive-Judgment/dp/0521796792/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [work](http://www.amazon.com/Foundations-Neuroeconomic-Analysis-Paul-Glimcher/dp/0199744254/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20). As [Michael Vassar](http://en.wikipedia.org/wiki/Michael_Vassar) observes, philosophers are \"spectacularly bad\" at understanding that their intuitions are [generated by cognitive algorithms](https://www.lesserwrong.com/lw/no/how_an_algorithm_feels_from_inside/).\n\nA diseased discipline\n---------------------\n\nWhat about [Quinean naturalists](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy)? Many of them at _least_ understand the basics: that [things are made of atoms](https://www.lesserwrong.com/lw/on/reductionism/), that many questions don't need to be answered but instead [dissolved](https://www.lesserwrong.com/lw/of/dissolving_the_question/), that [the brain is not an _a priori_ truth factory](https://www.lesserwrong.com/lw/k2/a_priori/), that [intuitions come from cognitive algorithms](https://www.lesserwrong.com/lw/no/how_an_algorithm_feels_from_inside/), that [humans are loaded with bias](http://wiki.lesswrong.com/wiki/Bias), that [language is full of tricks](https://www.lesserwrong.com/lw/od/37_ways_that_words_can_be_wrong/), and that [justification rests](https://www.lesserwrong.com/lw/s0/where_recursive_justification_hits_bottom/) in [the lens that can see its flaws](https://www.lesserwrong.com/lw/jm/the_lens_that_sees_its_flaws/). Some of them are even [Bayesians](http://commonsenseatheism.com/?p=13156).\n\nLike I said, a few naturalistic philosophers are doing [some useful work](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/3qe4). But the signal-to-noise ratio is _much_ lower even in naturalistic philosophy than it is in, say, behavioral economics or cognitive neuroscience or artificial intelligence or statistics. Why? Here are some hypotheses, based on my thousands of hours in the literature:\n\n1.  Many philosophers have been infected (often by [later Wittgenstein](http://en.wikipedia.org/wiki/Philosophical_Investigations)) with the idea that philosophy is _supposed_ to be useless. If it's useful, then it's science or math or something else, but not philosophy. Michael Bishop [says](http://commonsenseatheism.com/?p=10553) a common complaint from his colleagues about [his 2004 book](http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) is that it is _too useful_.\n2.  Most philosophers _don't_ understand the basics, so naturalists spend much of their time coming up with new ways to argue that people are made of atoms and intuitions don't trump science. They fight beside the poor atheistic philosophers who keep coming up with [new](http://www.amazon.com/Logic-Theism-Arguments-Against-Beliefs/dp/0521108667/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) [ways](http://www.amazon.com/Arguing-about-Gods-Graham-Oppy/dp/0521122643/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) to argue that the universe was not created by someone's invisible magical friend.\n3.  Philosophy has grown into an abnormally backward-looking discipline. Scientists like to put their work in the context of what old dead guys said, too, but philosophers have a real _fetish_ for it. Even naturalists spend a fair amount of time re-interpreting Hume and Dewey yet again.\n4.  Because they were trained in traditional philosophical ideas, arguments, and frames of mind, naturalists will [anchor and adjust](https://www.lesserwrong.com/lw/j7/anchoring_and_adjustment/) from traditional philosophy when they make progress, rather than scrapping the whole mess and starting from scratch with a correct understanding of language, physics, and cognitive science. Sometimes, philosophical work is useful to build from: Judea Pearl's triumphant [work on causality](http://www.amazon.com/Causality-Reasoning-Inference-Judea-Pearl/dp/052189560X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20) built on earlier counterfactual accounts of causality from philosophy. Other times, it's best to ignore the past confusions. Eliezer made most of his philosophical progress on his own, in order to solve problems in AI, and only later looked around in philosophy to see which standard position his own theory was most similar to.\n5.  Many naturalists aren't trained in cognitive science or AI. Cognitive science is essential because the tool we use to philosophize is the brain, and if you don't know how your tool works then you'll use it poorly. AI is useful because it keeps you honest: you can't write confused concepts or non-natural hypotheses in a programming language.\n6.  Mainstream philosophy publishing favors the established positions and arguments. You're more likely to get published if you can write about how intuitions are useless in solving Gettier problems (which is a confused set of non-problems anyway) than if you write about how to make a superintelligent machine preserve its utility function across millions of self-modifications.\n7.  Even much of the _useful_ work naturalistic philosophers do is not at the cutting-edge. Chalmers' [update](https://www.lesserwrong.com/lw/42l/david_chalmers_the_singularity_a_philosophical/) for I.J. Good's 'intelligence explosion' argument is the best one-stop summary available, but it doesn't get as far as the [Hanson-Yudkowsky AI-Foom debate](http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate) in 2008 did. Talbot ([2009](http://commonsenseatheism.com/wp-content/uploads/2011/03/Talbot-How-to-use-intuitions-in-philosophy.pdf)) and Bishop & Trout ([2004](http://www.amazon.com/Epistemology-Psychology-Judgment-Michael-Bishop/dp/0195162307/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0321928423&linkCode=as2&tag=lesswrong-20)) provide handy summaries of much of the heuristics and biases literature, just like Eliezer has so usefully done on Less Wrong, but of course this isn't cutting edge. You could always just read it in the primary literature by Kahneman and Tversky and others.\n\nOf course, there _is_ mainstream philosophy that is both good and cutting-edge: the work of [Nick Bostrom](http://www.nickbostrom.com/) and [Daniel Dennett](http://ase.tufts.edu/cogstud/incbios/dennettd/dennettd.htm) stands out. And of course there _is_ a role for those who keep arguing for atheism and reductionism and so on. I was a fundamentalist Christian [until](http://commonsenseatheism.com/?p=12) I read some contemporary atheistic philosophy, so that kind of work definitely does some good.\n\nBut if you're looking to solve cutting-edge problems, mainstream philosophy is one of the _last_ places you should look. Try to find the answer in the cognitive science or AI literature first, or try to solve the problem by applying rationalist thinking: [like this](https://www.lesserwrong.com/lw/2as/diseased_thinking_dissolving_questions_about/).\n\nSwimming the murky waters of mainstream philosophy is perhaps a job best left for those who already spent several years studying it - that is, people like _me_. I already know what things are called and where to look, and I have an efficient filter for skipping past the 95% of philosophy that isn't useful to me. And hopefully my rationalist training will protect me from picking up bad habits of thought.\n\nPhilosophy: the way forward\n===========================\n\nUnfortunately, many important problems are fundamentally _philosophical_ problems. Philosophy itself is unavoidable. How can we proceed?\n\nFirst, we must remain vigilant with our rationality training. It is not easy to overcome millions of years of brain evolution, and as long as you are human there is no final victory. You will always wake up the next morning as _homo sapiens_.\n\nSecond, if you want to contribute to cutting-edge problems, even ones that seem philosophical, it's far more productive to study math and science than it is to study philosophy. You'll learn more in math and science, and your learning will be of a higher quality. Ask a fellow rationalist who is knowledgeable about philosophy what the standard positions and arguments in philosophy are on your topic. If any of them seem _really_ useful, grab those particular works and read them. But again: you're probably better off trying to solve the problem by thinking like a cognitive scientist or an AI programmer than by ingesting mainstream philosophy.\n\nHowever, I must say that I wish so much of Eliezer's cutting-edge work wasn't spread out across hundreds of Less Wrong blog posts and long SIAI articles written in with an idiosyncratic style and vocabulary. I would rather these ideas were written in standard academic form, even if they transcended the standard game of mainstream philosophy.\n\nBut it's one thing to complain; another to offer solutions. So let me tell you what I think cutting-edge philosophy should be. As you might expect, my vision is to combine what's good in LW-style philosophy with what's good in mainstream philosophy, and toss out the rest:\n\n1.  Write short articles. One or two major ideas or arguments per article, maximum. Try to keep each article under 20 pages. It's hard to follow a [hundred-page argument](http://intelligence.org/upload/TDT-v01o.pdf).\n2.  Open each article by explaining the context and goals of the article (even if you cover mostly the same ground in the opening of 5 other articles). What topic are you discussing? Which problem do you want to solve? What have other people said about the problem? What will you accomplish in the paper? Introduce key terms, cite standard sources and positions on the problem you'll be discussing, even if you disagree with them.\n3.  If possible, use the standard terms in the field. If the standard terms are flawed, explain why they are flawed and then introduce your new terms in that context so everybody knows what you're talking about. This requires that you research your topic so you know what the standard terms and positions _are_. If you're talking about a problem in cognitive science, you'll need to read cognitive science literature. If you're talking about a problem in social science, you'll need to read social science literature. If you're talking about a problem in epistemology or morality, you'll need to read philosophy.\n4.  Write as clearly and simply as possible. Organize the paper with lots of heading and subheadings. Put in lots of 'hand-holding' sentences to help your reader along: explain the point of the previous section, then explain why the next section is necessary, etc. Patiently guide your reader through every step of the argument, especially if it is long and complicated.\n5.  Always cite the relevant literature. If you [can't find](http://intelligence.org/upload/artificial-intelligence-risk.pdf) much work relevant to your topic, you almost certainly [haven't looked hard enough](https://www.lesserwrong.com/lw/3m3/the_neglected_virtue_of_scholarship/). Citing the relevant literature not only lends weight to your argument, but also enables the reader to track down and examine the ideas or claims you are discussing. Being lazy with your citations is a sure way to frustrate precisely those readers who care enough to read your paper closely.\n6.  Think like a cognitive scientist and AI programmer. Watch out for biases. Avoid magical categories and language confusions and non-natural hypotheses. Look at your intuitions from the outside, as cognitive algorithms. Update your beliefs in response to evidence. \\[**This one is central. This is LW-style philosophy**.\\]\n7.  Use your rationality training, but avoid language that is unique to Less Wrong. Nearly all these terms and ideas have standard names outside of Less Wrong (though in many cases Less Wrong already uses the standard language).\n8.  Don't dwell too long on what old dead guys said, nor on semantic debates. Dissolve semantic problems and move on.\n9.  Conclude with a summary of your paper, and suggest directions for future research.\n10.  Ask fellow rationalists to read drafts of your article, then re-write. Then rewrite again, adding more citations and hand-holding sentences.\n11.  Format the article attractively. A well-chosen font makes for an [easier read](http://en.wikipedia.org/wiki/Typography#Readability_and_legibility). Then publish (in a journal or elsewhere).\n\nNote that this is _not_ just my vision of [how to get published _in journals_](https://www.lesserwrong.com/lw/4r1/how_siai_could_publish_in_mainstream_cognitive/). It's my vision of _how to do philosophy_.\n\nMeeting journals standards is _not_ the most important reason to follow the suggestions above. Write short articles because they're easier to follow. Open with the context and goals of your article because that makes it easier to understand, and lets people decide right away whether your article fits their interests. Use standard terms so that people already familiar with the topic aren't annoyed at having to learn a whole new vocabulary just to read your paper. Cite the relevant positions and arguments so that people have a sense of the context of what you're doing, and can look up what other people have said on the topic. Write clearly and simply and with much organization so that your paper is not wearying to read. Write lots of hand-holding sentences because we always communicate less effectively then we _thought_ we did. Cite the relevant literature as much as possible to assist your most careful readers in getting the information they want to know. Use your rationality training to remain sharp at all times. And so on.\n\n_That_ is what cutting-edge philosophy could look like, I think.\n\n  \n\nNext post: [How You Make Judgments](https://www.lesserwrong.com/lw/531/how_you_make_judgments_the_elephant_and_its_rider/)\n\nPrevious post: [Less Wrong Rationality and Mainstream Philosophy](https://www.lesserwrong.com/lw/4vr/less_wrong_rationality_and_mainstream_philosophy/)"
          },
          "voteCount": 124
        },
        {
          "name": "Eight Short Studies On Excuses",
          "type": "post",
          "slug": "eight-short-studies-on-excuses",
          "_id": "gFMH3Cqw4XxwL69iy",
          "url": null,
          "title": "Eight Short Studies On Excuses",
          "author": "Scott Alexander",
          "question": false,
          "tags": [
            {
              "name": "Game Theory"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Practical"
            },
            {
              "name": "Whole Brain Emulation"
            },
            {
              "name": "AI Risk"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "The Clumsy Game-Player",
                "anchor": "The_Clumsy_Game_Player",
                "level": 1
              },
              {
                "title": "The Lazy Student",
                "anchor": "The_Lazy_Student",
                "level": 1
              },
              {
                "title": "The Grieving Student",
                "anchor": "The_Grieving_Student",
                "level": 1
              },
              {
                "title": "The Sports Fan",
                "anchor": "The_Sports_Fan",
                "level": 1
              },
              {
                "title": "The Murderous Husband",
                "anchor": "The_Murderous_Husband",
                "level": 1
              },
              {
                "title": "The Bellicose Dictator",
                "anchor": "The_Bellicose_Dictator",
                "level": 1
              },
              {
                "title": "The Peyote-Popping Native",
                "anchor": "The_Peyote_Popping_Native",
                "level": 1
              },
              {
                "title": "The Well-Disguised Atheist",
                "anchor": "The_Well_Disguised_Atheist",
                "level": 1
              },
              {
                "title": "Conclusion",
                "anchor": "Conclusion",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "244 comments"
              }
            ],
            "headingsCount": 11
          },
          "contents": {
            "markdown": "**The Clumsy Game-Player**\n\nYou and a partner are playing an Iterated Prisoner's Dilemma. Both of you have publicly pre-committed to the tit-for-tat strategy. By iteration 5, you're going happily along, raking up the bonuses of cooperation, when your partner unexpectedly presses the \"defect\" button.\n\n\"Uh, sorry,\" says your partner. \"My finger slipped.\"\n\n\"I still have to punish you just in case,\" you say. \"I'm going to defect next turn, and we'll see how you like it.\"\n\n\"Well,\" said your partner, \"knowing that, I guess I'll defect next turn too, and we'll both lose out. But hey, it was just a slipped finger. By not trusting me, you're costing us both the benefits of one turn of cooperation.\"\n\n\"True\", you respond \"but if I don't do it, you'll feel free to defect whenever you feel like it, using the 'finger slipped' excuse.\"\n\n\"How about this?\" proposes your partner. \"I promise to take extra care that my finger won't slip again. You promise that if my finger does slip again, you will punish me terribly, defecting for a bunch of turns. That way, we trust each other again, and we can still get the benefits of cooperation next turn.\"\n\nYou don't believe that your partner's finger really slipped, not for an instant. But the plan still seems like a good one. You accept the deal, and you continue cooperating until the experimenter ends the game.\n\nAfter the game, you wonder what went wrong, and whether you could have played better. You decide that there was no better way to deal with your partner's \"finger-slip\" - after all, the plan you enacted gave you maximum possible utility under the circumstances. But you wish that you'd pre-committed, at the beginning, to saying \"and I will punish finger slips equally to deliberate defections, so make sure you're careful.\"\n\n**The Lazy Student**\n\nYou are a perfectly utilitarian school teacher, who attaches exactly the same weight to others' welfare as to your own. You have to have the reports of all fifty students in your class ready by the time midterm grades go out on January 1st. You don't want to have to work during Christmas vacation, so you set a deadline that all reports must be in by December 15th or you won't grade them and the students will fail the class. Oh, and your class is Economics 101, and as part of a class project all your students have to behave as selfish utility-maximizing agents for the year.\n\nIt costs your students 0 utility to turn in the report on time, but they gain +1 utility by turning it in late (they enjoy procrastinating). It costs you 0 utility to grade a report turned in before December 15th, but -30 utility to grade one after December 15th. And students get 0 utility from having their reports graded on time, but get -100 utility from having a report marked incomplete and failing the class.\n\nIf you say \"There's no penalty for turning in your report after deadline,\" then the students will procrastinate and turn in their reports late, for a total of +50 utility (1 per student times fifty students). You will have to grade all fifty reports during Christmas break, for a total of - 1500 utility (-30 per report times fifty reports). Total utility is -1450.\n\nSo instead you say \"If you don't turn in your report on time, I won't grade it.\" All students calculate the cost of being late, which is +1 utility from procrastinating and -100 from failing the class, and turn in their reports on time. You get all reports graded before Christmas, no students fail the class, and total utility loss is zero. Yay!\n\nOr else - one student comes to you the day after deadline and says \"Sorry, I was really tired yesterday, so I really didn't want to come all the way here to hand in my report. I expect you'll grade my report anyway, because I know you to be a perfect utilitarian, and you'd rather take the -30 utility hit to yourself than take the -100 utility hit to me.\"\n\nYou respond \"Sorry, but if I let you get away with this, all the other students will turn in their reports late in the summer.\" She says \"Tell you what - our school has procedures for changing a student's previously given grade. If I ever do this again, or if I ever tell anyone else about this, you can change my grade to a fail. Now you know that passing me this one time won't affect anything in the future. It certainly can't affect the past. So you have no reason not to do it.\" You believe her when she says she'll never tell, but you say \"You made this argument because you believed me to be the sort of person who would accept it. In order to prevent other people from making the same argument, I have to be the sort of person who wouldn't accept it. To that end, I'm going to not accept your argument.\"\n\n**The Grieving Student**\n\nA second student comes to you and says \"Sorry I didn't turn in my report yesterday. My mother died the other day, and I wanted to go to her funeral.\"\n\nYou say \"Like all economics professors, I have no soul, and so am unable to sympathize with your loss. Unless you can make an argument that would apply to all rational actors in my position, I can't grant you an extension.\"\n\nShe says \"If you did grant this extension, it wouldn't encourage other students to turn in their reports late. The other students would just say 'She got an extension because her mother died'. They know they won't get extensions unless they kill their own mothers, and even economics students aren't that evil. Further, if you don't grant the extension, it won't help you get more reports in on time. Any student would rather attend her mother's funeral than pass a course, so you won't be successfully motivating anyone else to turn in their reports early.\"\n\nYou think for a while, decide she's right, and grant her an extension on her report.\n\n**The Sports Fan**\n\nA third student comes to you and says \"Sorry I didn't turn in my report yesterday. The Bears' big game was on, and as I've told you before, I'm a huge Bears fan. But don't worry! It's very rare that there's a game on this important, and not many students here are sports fans anyway. You'll probably never see a student with this exact excuse again. So in a way, it's not that different from the student here just before me, the one whose mother died.\"\n\nYou respond \"It may be true that very few people will be able to say both that they're huge Bears fans, and that there's a big Bears game on the day before the report comes due. But by accepting your excuse, I establish a precedent of accepting excuses that are *approximately this good*. And there are many other excuses approximately as good as yours. Maybe someone's a big soap opera fan, and the season finale is on the night before the deadline. Maybe someone loves rock music, and there's a big rock concert on. Maybe someone's brother is in town that week. Practically anyone can come up with an excuse as good as yours, so if I accept your late report, I have to accept everyone's.\n\n\"The student who was here before you, that's different. We, as a society, already have an ordering in which a family member's funeral is one of the most important things around. By accepting her excuse, I'm establishing a precedent of accepting any excuse approximately that good, but almost no one will ever have an excuse that good. Maybe a few people who are really sick, someone struggling with a divorce or a breakup, that kind of thing. Not the hordes of people who will be coming to me if I give you your exemption.\n\n**The Murderous Husband**\n\nYou are the husband of a wonderful and beautiful lady whom you love very much - and whom you just found in bed with another man. In a rage, you take your hardcover copy of Introduction To Game Theory and knock him over the head with it, killing him instantly (it's a pretty big book).\n\nAt the murder trial, you plead to the judge to let you go free. \"Society needs to lock up murderers, as a general rule. After all, they are dangerous people who cannot be allowed to walk free. However, I only killed that man because he was having an affair with my wife. In my place, anyone would have done the same. So the crime has no bearing on how likely I am to murder someone else. I'm not a risk to anyone who isn't having an affair with my wife, and after this incident I plan to divorce and live the rest of my days a bachelor. Therefore, you have no need to deter me from future murders, and can safely let me go free.\"\n\nThe judge responds: \"You make a convincing argument, and I believe that you will never kill anyone else in the future. However, other people will one day be in the position you were in, where they walk in on their wives having an affair. Society needs to have a credible pre-commitment to punishing them if they succumb to their rage, in order to deter them from murder.\"\n\n\"No,\" you say, \"I understand your reasoning, but it won't work. If you've never walked in on your wife having an affair, you can't possibly understand the rage. No matter how bad the deterrent was, you'd still kill the guy.\"\n\n\"Hm,\" says the judge. \"I'm afraid I just can't believe anyone could ever be quite that irrational. But I see where you're coming from. I'll give you a lighter sentence.\"\n\n**The Bellicose Dictator**\n\nYou are the dictator of East Examplestan, a banana republic subsisting off its main import, high quality hypothetical scenarios. You've always had it in for your ancestral enemy, West Examplestan, but the UN has made it clear that any country in your region that aggressively invades a neighbor will be severely punished with sanctions and possible enforced \"regime change.\" So you decide to leave the West alone for the time being.\n\nOne day, a few West Examplestanis unintentionally wander over your unmarked border while prospecting for new scenario mines. You immediately declare it a \"hostile incursion\" by \"West Examplestani spies\", declare war, and take the Western capital in a sneak attack.\n\nThe next day, Ban Ki-moon is on the phone, and he sounds angry. \"I thought we at the UN had made it perfectly clear that countries can't just invade each other anymore!\"\n\n\"But didn't you read our propaganda mouthpi...ahem, official newspaper? We didn't *just* invade. We were responding to Western aggression!\"\n\n\"Balderdash!\" says the Secretary-General. \"Those were a couple of lost prospectors, and you know it!\"\n\n\"Well,\" you say. \"Let's consider your options. The UN needs to make a credible pre-commitment to punish aggressive countries, or everyone will invade their weaker neighbors. And you've got to follow through on your threats, or else the pre-commitment won't be credible anymore. But you don't actually like following through on your threats. Invading rogue states will kill a lot of people on both sides and be politically unpopular, and sanctions will hurt your economy *and* lead to heart-rending images of children starving. What you'd really like to do is let us off, but in a way that doesn't make other countries think they'll get off too.\n\n\"Luckily, we can make a credible story that we were following international law. Sure, it may have been stupid of us to mistake a few prospectors for an invasion, but there's no international law against being stupid. If you dismiss us as simply misled, you don't have to go through the trouble of punishing us, and other countries won't think they can get away with anything.\n\n\"Nor do you need to live in fear of us doing something like this again. We've already demonstrated that we won't go to war without a casus belli. If other countries can refrain from giving us one, they have nothing to fear.\"\n\nBan Ki-moon doesn't believe your story, but the countries that would bear the economic brunt of the sanctions and regime change decide they believe it just enough to stay uninvolved.\n\n**The Peyote-Popping Native**\n\nYou are the governor of a state with a large Native American population. You have banned all mind-altering drugs, with the honorable exceptions of alcohol, tobacco, caffeine, and several others, because you are a red-blooded American who believes that they would drive teenagers to commit crimes.\n\nA representative of the state Native population comes to you and says: \"Our people have used peyote religiously for hundreds of years. During this time, we haven't become addicted or committed any crimes. Please grant us a religious exemption under the First Amendment to continue practicing our ancient rituals.\" You agree.\n\nA leader of your state's atheist community breaks into your office via the ventilation systems (because seriously, how else is an atheist leader going to get access to a state governor?) and says: \"As an atheist, I am offended that you grant exemptions to your anti-peyote law for religious reasons, but not for, say, recreational reasons. This is unfair discrimination in favor of religion. The same is true of laws that say Sikhs can wear turbans in school to show support for God, but my son can't wear a baseball cap in school to show support for the Yankees. Or laws that say Muslims can get time off state jobs to pray five times a day, but I can't get time off my state job for a cigarette break. Or laws that say state functions will include special kosher meals for Jews, but not special pasta meals for people who really like pasta.\"\n\nYou respond \"Although my policies may seem to be saying religion is more important than other potential reasons for breaking a rule, one can make a non-religious case justifying them. One important feature of major world religions is that their rituals have been fixed for hundreds of years. Allowing people to break laws for religious reasons makes religious people very happy, but does not weaken the laws. After all, we all know the few areas in which the laws of the major US religions as they are currently practiced conflict with secular law, and none of them are big deals. So the general principle 'I will allow people to break laws if it is necessary to established and well-known religious rituals\" is relatively low-risk and makes people happy without threatening the concept of law in general. But the general principle 'I will allow people to break laws for recreational reasons' is *very* high risk, because it's sufficient justification for almost anyone breaking any law.\"\n\n\"I would love to be able to serve everyone the exact meal they most wanted at state dinners. But if I took your request for pasta because you liked pasta, I would have to follow the general principle of giving everyone the meal they most like, which would be prohibitively expensive. By giving Jews kosher meals, I can satisfy a certain particularly strong preference without being forced to satisfy anyone else's.\"\n\n**The Well-Disguised Atheist**\n\nThe next day, the atheist leader comes in again. This time, he is wearing a false mustache and sombrero. \"I represent the Church of Driving 50 In A 30 Mile Per Hour Zone,\" he says. \"For our members, going at least twenty miles per hour over the speed limit is considered a sacrament. Please grant us a religious exemption to traffic laws.\"\n\nYou decide to play along. \"How long has your religion existed, and how many people do you have?\" you ask.\n\n\"Not very long, and not very many people,\" he responds.\n\n\"I see,\" you say. \"In that case, you're a cult, and not a religion at all. Sorry, we don't deal with cults.\"\n\n\"What, exactly, is the difference between a cult and a religion?\"\n\n\"The difference is that cults have been formed recently enough, and are small enough, that we are suspicious of them existing for the purpose of taking advantage of the special place we give religion. Granting an exemption for your cult would challenge the credibility of our pre-commitment to punish people who break the law, because it would mean anyone who wants to break a law could just found a cult dedicated to it.\"\n\n\"How can my cult become a real religion that deserves legal benefits?\"\n\n\"You'd have to become old enough and respectable enough that it becomes implausible that it was created for the purpose of taking advantage of the law.\"\n\n\"That sounds like a lot of work.\"\n\n\"Alternatively, you could try writing awful science fiction novels and hiring a ton of lawyers. I hear that also works these days.\"\n\n**Conclusion**\n\nIn all these stories, the first party wants to credibly pre-commit to a rule, but also has incentives to forgive other people's deviations from the rule. The second party breaks the rules, but comes up with an excuse for why its infraction should be forgiven.\n\nThe first party's response is based not only on whether the person's excuse is believable, not even on whether the person's excuse is morally valid, but on whether the excuse can be accepted without straining the credibility of their previous pre-commitment.\n\nThe general principle is that by accepting an excuse, a rule-maker is also committing themselves to accepting all equally good excuses in the future. There are some exceptions - accepting an excuse in private but making sure no one else ever knows, accepting an excuse once with the express condition that you will never accept any other excuses - but to some degree these are devil's bargains, as anyone who can predict you will do this can take advantage of you.\n\nThese stories give an idea of excuses different from the one our society likes to think it uses, namely that it accepts only excuses that are true and that reflect well upon the character of the person giving the excuse. I'm not saying that the common idea of excuses doesn't have value - but I think the game theory view also has some truth to it. I also think the game theoretic view can be useful in cases where the common view fails. It can inform cases in law, international diplomacy, and politics where a tool somewhat stronger than the easily-muddled common view is helpful."
          },
          "voteCount": 614
        },
        {
          "name": "Fake Causality",
          "type": "post",
          "slug": "fake-causality",
          "_id": "RgkqLqkg8vLhsYpfh",
          "url": null,
          "title": "Fake Causality",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Causality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Phlogiston was the eighteenth century’s answer to the Elemental Fire of the Greek alchemists. Ignite wood, and let it burn. What is the orangey-bright “fire” stuff? Why does the wood transform into ash? To both questions, the eighteenth-century chemists answered, “phlogiston.”\n\n. . . and that was it, you see, that was their answer: “Phlogiston.”\n\nPhlogiston escaped from burning substances as visible fire. As the phlogiston escaped, the burning substances lost phlogiston and so became ash, the “true material.” Flames in enclosed containers went out because the air became saturated with phlogiston, and so could not hold any more. Charcoal left little residue upon burning because it was nearly pure phlogiston.\n\nOf course, one didn’t use phlogiston theory to *predict* the outcome of a chemical transformation. You looked at the result first, then you used phlogiston theory to *explain* it. It’s not that phlogiston theorists predicted a flame would extinguish in a closed container; rather they lit a flame in a container, watched it go out, and then said, “The air must have become saturated with phlogiston.” You couldn’t even use phlogiston theory to say what you ought *not* to see; it could explain everything.\n\nThis was an earlier age of science. For a long time, no one realized there was a problem. Fake explanations don’t *feel* fake. That’s what makes them dangerous.\n\nModern research suggests that humans think about cause and effect using something like the directed acyclic graphs (DAGs) of Bayes nets. Because it rained, the sidewalk is wet; because the sidewalk is wet, it is slippery:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8f32ffc1c38cd1e27bb9704309d0b2bf863bb7270952b46c.png)\n\nFrom this we can infer—or, in a Bayes net, rigorously calculate in probabilities—that when the sidewalk is slippery, it probably rained; but if we already know that the sidewalk is wet, learning that the sidewalk is slippery tells us nothing more about whether it rained.\n\nWhy is fire hot and bright when it burns?\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8ff736bb4279ee44938bc54a05d6849665b3433c7bc69ab2.png)\n\nIt *feels* like an explanation. It’s *represented* using the same cognitive data format. But the human mind does not automatically detect when a cause has an unconstraining arrow to its effect. Worse, thanks to hindsight bias, it may feel like the cause constrains the effect, when it was merely [fitted](https://www.lesswrong.com/rationality/conservation-of-expected-evidence) to the effect.\n\nInterestingly, our modern understanding of probabilistic reasoning about causality can describe precisely what the phlogiston theorists were doing wrong. One of the primary inspirations for Bayesian networks was noticing the problem of double-counting evidence if inference resonates between an effect and a cause. For example, let’s say that I get a bit of unreliable information that the sidewalk is wet. This should make me think it’s more likely to be raining. But, if it’s more likely to be raining, doesn’t that make it more likely that the sidewalk is wet? And wouldn’t *that* make it more likely that the sidewalk is slippery? But if the sidewalk is slippery, it’s probably wet; and then I should again raise my probability that it’s raining . . .\n\nJudea Pearl uses the metaphor of an algorithm for counting soldiers in a line. Suppose you’re in the line, and you see two soldiers next to you, one in front and one in back. That’s three soldiers, including you. So you ask the soldier behind you, “How many soldiers do *you* see?” They look around and say, “Three.” So that’s a total of six soldiers. This, obviously, is *not* how to do it.\n\nA smarter way is to ask the soldier in front of you, “How many soldiers forward of you?” and the soldier in back, “How many soldiers backward of you?” The question “How many soldiers forward?” can be passed on as a message without confusion. If I’m at the front of the line, I pass the message “1 soldier forward,” for myself. The person directly in back of me gets the message “1 soldier forward,” and passes on the message “2 soldiers forward” to the soldier behind them. At the same time, each soldier is also getting the message “N soldiers backward” from the soldier behind them, and passing it on as “N + 1 soldiers backward” to the soldier in front of them. How many soldiers in total? Add the two numbers you receive, plus one for yourself: that is the total number of soldiers in line.\n\nThe key idea is that every soldier must *separately* track the two messages, the forward-message and backward-message, and add them together only at the end. You never add any soldiers from the backward-message you receive to the forward-message you pass back. Indeed, the total number of soldiers is never passed as a message—no one ever says it aloud.\n\nAn analogous principle operates in rigorous probabilistic reasoning about causality. If you learn something about whether it’s raining, from some source *other* than observing the sidewalk to be wet, this will send a forward-message from \\[Rain\\] to \\[Sidewalk Wet\\] and raise our expectation of the sidewalk being wet. If you observe the sidewalk to be wet, this sends a backward-message to our belief that it is raining, and this message propagates from \\[Rain\\] to all neighboring nodes *except* the \\[Sidewalk Wet\\] node. We count each piece of evidence exactly once; no update message ever “bounces” back and forth. The exact algorithm may be found in Judea Pearl’s classic *Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference*.\n\nSo what went wrong in phlogiston theory? When we observe that fire is hot and bright, the \\[Fire Hot and Bright\\] node can send backward-evidence to the \\[Phlogiston\\] node, leading us to update our beliefs about phlogiston. But if so, we can’t count this as a successful forward-prediction of phlogiston theory. The message should go in only one direction, and not bounce back.\n\nAlas, human beings do not use a rigorous algorithm for updating belief networks. We learn about parent nodes from observing children, and predict child nodes from beliefs about parents. But we don’t keep rigorously separate books for the backward-message and forward-message. We just remember that phlogiston is hot, which *causes* fire to be hot. So it seems like phlogiston theory predicts the hotness of fire. Or, worse, it just feels like *phlogiston makes the fire hot.*\n\nUntil you notice that no *advance* predictions are being made, the non-constraining causal node is not labeled “fake.” It’s represented the same way as any other node in your belief network. It feels like a fact, like all the other facts you know: *Phlogiston makes the fire hot.*\n\nA properly designed AI would notice the problem instantly. This wouldn’t even require special-purpose code, just correct bookkeeping of the belief network. (Sadly, we humans can’t rewrite our own code, the way a properly designed AI could.)\n\nSpeaking of “hindsight bias” is just the nontechnical way of saying that humans do not rigorously separate forward and backward messages, allowing forward messages to be contaminated by backward ones.\n\nThose who long ago went down the path of phlogiston were not trying to be fools. No scientist deliberately wants to get stuck in a blind alley. Are there any fake explanations in *your* mind? If there are, I guarantee they’re not labeled “fake explanation,” so polling your thoughts for the “fake” keyword will not turn them up.\n\nThanks to hindsight bias, it’s also not enough to check how well your theory “predicts” facts you already know. You’ve got to predict for tomorrow, not yesterday. It’s the only way a messy human mind can be guaranteed of sending a pure forward message."
          },
          "voteCount": 89
        },
        {
          "name": "Chaotic Inversion",
          "type": "post",
          "slug": "chaotic-inversion",
          "_id": "NyFtHycJvkyNjXNsP",
          "url": null,
          "title": "Chaotic Inversion",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Productivity"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Akrasia"
            },
            {
              "name": "Willpower"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "I was recently having a conversation with some friends on the topic of hour-by-hour productivity and willpower maintenance—something I've struggled with my whole life.\n\nI can [avoid running away from a hard problem the first time I see it](/lw/un/on_doing_the_impossible/) (perseverance on a timescale of seconds), and I can stick to the same problem for years; but to keep working on a timescale of _hours_ is a constant battle for me.  It goes without saying that I've already read reams and reams of advice; and the most help I got from it was realizing that a sizable fraction other creative professionals had the same problem, and couldn't beat it either, no matter how reasonableall the advice sounds.\n\n\"What do you do when you can't work?\" my friends asked me.  (Conversation probably not accurate, this is a very loose gist.)\n\nAnd I replied that I usually browse random websites, or watch a short video.\n\n\"Well,\" they said, \"if you know you can't work for a while, you should watch a movie or something.\"\n\n\"Unfortunately,\" I replied, \"I have to do something whose time comes in short units, like browsing the Web or watching short videos, because I might become able to work again at any time, and I can't predict when—\"\n\nAnd then I stopped, because I'd just had a revelation.\n\nI'd always thought of my workcycle as something _chaotic,_ something _unpredictable._  I never used those words, but that was the way I _treated_ it.\n\nBut here my friends seemed to be implying—what a strange thought—that _other_ people could predict when they would become able to work again, and structure their time accordingly.\n\nAnd it occurred to me for the first time that I might have been committing that damned old chestnut the [Mind Projection Fallacy](/lw/oi/mind_projection_fallacy/), right out there in my ordinary everyday life instead of high abstraction.\n\nMaybe it wasn't that my productivity was _unusually_ _chaotic;_ maybe I was just _unusually_ _stupid_ with respect to predicting it.\n\nThat's what inverted stupidity looks like—chaos.  Something hard to handle, hard to grasp, hard to guess, something you can't do anything with.  It's not just an idiom for high abstract things like Artificial Intelligence.  It can apply in ordinary life too.\n\nAnd the reason we don't think of the alternative explanation \"I'm stupid\", is _not_—I suspect—that we think so highly of ourselves.  It's just that we don't think of ourselves at all.  We just see [a chaotic feature of the environment](/lw/oi/mind_projection_fallacy/).\n\nSo now it's occurred to me that my productivity problem may not be chaos, but my own stupidity.\n\nAnd that may or may not help anything.  It certainly doesn't fix the problem right away.  Saying \"I'm ignorant\" doesn't make you knowledgeable.\n\nBut it is, at least, a different path than saying \"it's too chaotic\"."
          },
          "voteCount": 73
        },
        {
          "name": "Measuring aversion and habit strength",
          "type": "post",
          "slug": "measuring-aversion-and-habit-strength",
          "_id": "Fxv4o3LGEkgR2Qsz7",
          "url": null,
          "title": "Measuring aversion and habit strength",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Habits"
            },
            {
              "name": "Akrasia"
            },
            {
              "name": "Introspection"
            },
            {
              "name": "Emotions"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Story",
                "anchor": "Story",
                "level": 1
              },
              {
                "title": "Some lessons to learn here:",
                "anchor": "Some_lessons_to_learn_here_",
                "level": 1
              },
              {
                "title": "Operationalizing aversion and propensity",
                "anchor": "Operationalizing_aversion_and_propensity",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "14 comments"
              }
            ],
            "headingsCount": 5
          },
          "contents": {
            "markdown": "![Not me!](http://i51.tinypic.com/mcq23t.jpg) tl;dr: _Strong aversions don't always originate from strong feelings (see [Ugh fields](/lw/21b/ugh_fields/)). It's useful to measure the strength of an aversion by **how effectively** it averts your thoughts/behavior instead of **how saliently** you can feel it, or even remember feeling it. If there's a low cost behaviour that you somehow always \"end up not doing\", there's evidence for a mechanism steering you away from it. Try to find it, and defy it._\n\n**Story**\n\nRight after writing [Break your habits: be more empirical](/lw/2sh/break_your_habits_be_more_empirical/), someone asked me to a live music show, and I declined, with some explanation about being busy. This [felt a little forced](/lw/if/your_strength_as_a_rationalist/), and I realized: _I always decline live music shows. This counts as a habit._ The interesting thing was that I declined them for many different, unrelated reasons. This was evidence for something more systemic, because it would be a coincidence if random, unrelated reasons always came up to prevent me from attending live music.\n\nSo I asked myself if I really disliked live music. Emotions returned: \"Not really. It's not awesome, but it's not terrible.\" Now, there was a time when I would have stopped thinking there. My time is valuable, and mediocrity is enough to stop me from doing anything, right?\n\nBut wait... is it? Is it enough to _always_ stop me? If it was only mediocre, and not terrible, than surely on _one_ of the many occasions I could have seen live music, there would have been sufficient justification to go... a particularly good composer, a particularly interesting group of people go with, a particular need to get out and do something different... but no, somehow I _always_ didn't go.\n\nAnd that's when I realized I probably had an _aversion_ to live music: some brain mechanism that consistently and effectively averted me from seeing it, and in this case, _not_ something I could feel. In particular, it wasn't accompanied by any sense of [\"Ugh\"](/lw/21b/ugh_fields/). So since I couldn't feel the aversion, I took an [outside view](http://wiki.lesswrong.com/wiki/Outside_view) to ask what could have caused it, if it indeed exists...\n\nMy first guess was the fact that I used to perform live music a lot, until I suffered a hand injury. Huh! That could totally have been emotional at some point.\n\nCould that be the cause of my current aversion? I instrospected on how I might have felt at live music show shortly after my injury, and the answer was \"excluded\". I felt disappointed that I wasn't, and couldn't be, one of the performers. I began to suspect my aversion _was_ conditioned from this \"ugh\" response that had lost salience many years ago.\n\nSolution: shortly afterward I went to the symphony with a very dear friend, and sat in the front row behind the brass section where I could read their music and feel like I was involved. It worked! The experience was very cathartic. I cried a little bit, thanked my friend, and have been to many more live music performances since.\n\n**Some lessons to learn here:**\n\nLook at the picture at the top of the post... this man's face doesn't show any strong emotion averting him from the many feminine hands that reach for him, yet he avoids them. If he does this a lot, and he's not gay or already taken, he should be curious about what aversion mechanism might be causing this behavior... because however it works, it's working! In general,\n\n1.  **Alarm bells** should go off when there's a small-cost option you _always_ avoid (or a small-gain option you _always_ pursue).\n2.  **Measure** the strength of the aversion (or propsensity) by how _effectively_ it averts (or attracts) your thoughts/behavior, not how _saliently_ you feel it.\n3.  **Search** for the underlying aversion or propensity mechanism.\n4.  **Combine** introspection with an outside view in your search, especially when you can't easily feel the mechanism, and don't forget,\n5.  **[Break](/lw/2sh/break_your_habits_be_more_empirical/)** the habit when it's not costly to do so!\n\n**Operationalizing aversion and propensity**\n\nI've been using 1-4 a lot more ever since I realized them explicitly back in October, and I must say it has been a helpful and eye-opening experience.\n\nAversions — habit mechanisms that steer you away from a thought or behavior — are more general than [ugh fields](/lw/21b/ugh_fields/) in that they don't have to originate from bad feelings, even though my example did. They can also originate from [anchoring](http://en.wikipedia.org/wiki/Anchoring), or other biases. That's why I want to promote an attitude of measuring aversion by effect instead of salience. Whatever the mechanism — an ugh field, an anchor, or pink elephants — we need an [operationalized](http://en.wikipedia.org/wiki/Operationalization) notion of aversion to search for and notice in our daily lives.\n\nThe same goes for _propensities_ (or _apetites_): habit mechanisms that steer you _toward_ a behavior. If you _always_ take the bus, even though it's only slightly shorter than walking, there's probably a very effective mechanism in place for it. Are you curious how it works?\n\nNow, I'm no behaviorist. The outside view should complement, not replace, the inside one. My introspection at the end of the story helped me finish the search for my aversion, but it was an outside view that made me notice it in the first place.\n\nSo, let us go forth to notice aversions to small costs, and propensities for small gains. And then defy them :)"
          },
          "voteCount": 84
        },
        {
          "name": "Intellectual Hipsters and Meta-Contrarianism",
          "type": "post",
          "slug": "intellectual-hipsters-and-meta-contrarianism",
          "_id": "9kcTNWopvXFncXgPy",
          "url": null,
          "title": "Intellectual Hipsters and Meta-Contrarianism",
          "author": "Scott Alexander",
          "question": false,
          "tags": [
            {
              "name": "Signaling"
            },
            {
              "name": "Social Status"
            },
            {
              "name": "Social & Cultural Dynamics"
            },
            {
              "name": "Contrarianism"
            },
            {
              "name": "World Modeling"
            },
            {
              "name": "Intellectual Fashion"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [Why Real Men Wear Pink](/lw/154/why_real_men_wear_pink/), [That Other Kind of Status](/lw/1kr/that_other_kind_of_status/), [Pretending to be Wise](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBIQFjAA&url=http%3A%2F%2Flesswrong.com%2Flw%2Fyp%2Fpretending_to_be_wise%2F&rct=j&q=Pretending%20to%20be%20wise&ei=Q5KOTPuoEdO4jAeauvnWBg&usg=AFQjCNGKvS__hFQHs2g5ra4dhSOaPE0DtQ&sig2=gstUbI-cNPhT7CfA5Zo8og&cad=rja), [The \"Outside The Box\" Box](/lw/k6/the_outside_the_box_box/)\n\n> _WARNING: Beware of things that are fun to argue -- Eliezer Yudkowsky_\n\nScience has inexplicably failed to come up with a precise definition of \"hipster\", but from my limited understanding a hipster is a person who deliberately uses unpopular, obsolete, or obscure styles and preferences in an attempt to be \"cooler\" than the mainstream. But why would being deliberately uncool be cooler than being cool?  \n  \nAs [previously discussed](/lw/154/why_real_men_wear_pink/), in certain situations refusing to signal can be a sign of high status. Thorstein Veblen invented the term \"conspicuous consumption\" to refer to the showy spending habits of the nouveau riche, who unlike the established money of his day took great pains to signal their wealth by buying fast cars, expensive clothes, and shiny jewelery. Why was such flashiness common among new money but not old? Because the old money was so secure in their position that it never even occurred to them that they might be confused with poor people, whereas new money, with their lack of aristocratic breeding, worried they might be mistaken for poor people if they didn't make it blatantly obvious that they had expensive things.  \n  \nThe old money might have started off not buying flashy things for pragmatic reasons - they didn't need to, so why waste the money? But if F. Scott Fitzgerald is to be believed, the old money actively cultivated an air of superiority to the nouveau riche and their conspicuous consumption; not buying flashy objects becomes a matter of principle. This makes sense: the nouveau riche need to differentiate themselves from the poor, but the old money need to differentiate themselves from the nouveau riche.  \n  \nThis process is called [countersignaling](http://en.wikipedia.org/wiki/Countersignaling), and one can find its telltale patterns in many walks of life. Those who study human romantic attraction warn men not to \"come on too strong\", and this has similarities to the nouveau riche example. A total loser might come up to a woman without a hint of romance, promise her nothing, and demand sex. A more sophisticated man might buy roses for a woman, write her love poetry, hover on her every wish, et cetera; this signifies that he is not a total loser. But the most desirable men may deliberately avoid doing nice things for women in an attempt to signal they are so high status that they don't need to. The average man tries to differentiate himself from the total loser by being nice; the extremely attractive man tries to differentiate himself from the average man by not being especially nice.  \n  \nIn all three examples, people at the top of the pyramid end up displaying characteristics similar to those at the bottom. Hipsters deliberately wear the same clothes uncool people wear. Families with old money don't wear much more jewelry than the middle class. And very attractive men approach women with the same lack of subtlety a total loser would use.^1^  \n  \nIf politics, philosophy, and religion are really about signaling, we should expect to find countersignaling there as well.\n\n  \n**Pretending To Be Wise**  \n  \nLet's go back to Less Wrong's long-running discussion on death. Ask any five year old child, and ey can tell you that death is bad. Death is bad because it kills you. There is nothing subtle about it, and there does not need to be. Death universally seems bad to pretty much everyone on first analysis, and what it seems, it is.  \n  \nBut as has been pointed out, along with the gigantic cost, death does have a few small benefits. It lowers overpopulation, it allows the new generation to develop free from interference by their elders, it provides motivation to get things done quickly. Precisely because these benefits are so much smaller than the cost, they are hard to notice. It takes a particularly subtle and clever mind to think them up. Any idiot can tell you why death is bad, but it takes a very particular sort of idiot to believe that death might be good.  \n  \nSo pointing out this contrarian position, that death has some benefits, is potentially a signal of high intelligence. It is not a very reliable signal, because once the first person brings it up everyone can just copy it, but it is a cheap signal. And to the sort of person who might not be clever enough to come up with the benefits of death themselves, and only notices that wise people seem to mention death can have benefits, it might seem super extra wise to say death has lots and lots of great benefits, and is really quite a good thing, and if other people should protest that death is bad, well, that's an opinion a five year old child could come up with, and so clearly that person is no smarter than a five year old child. Thus Eliezer's title for this mentality, \"Pretending To Be Wise\".  \n  \nIf dwelling on the benefits of a great evil is not your thing, you can also pretend to be wise by dwelling on the costs of a great good. All things considered, modern industrial civilization - with its advanced technology, its high standard of living, and its lack of typhoid fever -  is pretty neat. But modern industrial civilization also has many costs: alienation from nature, strains on the traditional family, the anonymity of big city life, pollution and overcrowding. These are real costs, and they are certainly worth taking seriously; nevertheless, the crowds of emigrants trying to get from the Third World to the First, and the lack of any crowd in the opposite direction, suggest the benefits outweigh the costs. But in my estimation - and speak up if you disagree - people spend a lot more time dwelling on the negatives than on the positives, and most people I meet coming back from a Third World country have to talk about how much more authentic their way of life is and how much we could learn from them. This sort of talk sounds Wise, whereas talk about how nice it is to have buses that don't break down every half mile sounds trivial and selfish..  \n  \nSo my hypothesis is that if a certain side of an issue has very obvious points in support of it, and the other side of an issue relies on much more subtle points that the average person might not be expected to grasp, then adopting the second side of the issue will become a signal for intelligence, even if that side of the argument is wrong.  \n  \nThis only works in issues which are so muddled to begin with that there is no fact of the matter, or where the fact of the matter is difficult to tease out: so no one tries to signal intelligence by saying that 1+1 equals 3 (although it would not surprise me to find a philosopher who says truth is relative and this equation is a legitimate form of discourse).  \n  \n**Meta-Contrarians Are Intellectual Hipsters**  \n  \nA person who is somewhat upper-class will conspicuously signal eir wealth by buying difficult-to-obtain goods. A person who is very upper-class will conspicuously signal that ey feels no need to conspicuously signal eir wealth, by deliberately not buying difficult-to-obtain goods.  \n  \nA person who is somewhat intelligent will conspicuously signal eir intelligence by holding difficult-to-understand opinions. A person who is very intelligent will conspicuously signal that ey feels no need to conspicuously signal eir intelligence, by deliberately not holding difficult-to-understand opinions.  \n  \nAccording to [the survey](/lw/fk/survey_results/), the average IQ on this site is around 145^2^. People on this site differ from the mainstream in that they are more willing to say death is bad, more willing to say that science, capitalism, and the like are good, and less willing to say that there's some deep philosophical sense in which 1+1 = 3. That suggests people around that level of intelligence have reached the point where they no longer feel it necessary to differentiate themselves from the sort of people who aren't smart enough to understand that there might be side benefits to death. Instead, they are at the level where they want to differentiate themselves from the somewhat smarter people who think the side benefits to death are great. They are, basically, meta-contrarians, who counter-signal by holding opinions contrary to those of the contrarians' signals. And in the case of death, this cannot but be a good thing.  \n  \nBut just as contrarians risk becoming too contrary, moving from \"actually, death has a few side benefits\" to \"DEATH IS GREAT!\", meta-contrarians are at risk of becoming too meta-contrary.  \n  \nAll the possible examples here are controversial, so I will just take the least controversial one I can think of and beg forgiveness. A naive person might think that industrial production is an absolute good thing. Someone smarter than that naive person might realize that global warming is a strong negative to industrial production and desperately needs to be stopped. Someone even smarter than that, to differentiate emself from the second person, might decide global warming wasn't such a big deal after all, or doesn't exist, or isn't man-made.  \n  \nIn this case, the contrarian position happened to be right (well, maybe), and the third person's meta-contrariness took em further from the truth. I do feel like there are more global warming skeptics among what Eliezer called \"the atheist/libertarian/technophile/sf-fan/early-adopter/programmer empirical cluster in personspace\" than among, say, college professors.  \n  \nIn fact, very often, the uneducated position of the five year old child may be deeply flawed and the contrarian position a necessary correction to those flaws. This makes meta-contrarianism a very dangerous business.  \n  \nRemember, most everyone hates hipsters.  \n  \nWithout meaning to imply anything about whether or not any of these positions are correct or not^3^, the following triads come to mind as connected to an uneducated/contrarian/meta-contrarian divide:  \n  \n\\- KKK-style racist / politically correct liberal / \"but there are scientifically proven genetic differences\"  \n\\- misogyny / women's rights movement / men's rights movement  \n\\- conservative / liberal / libertarian^4^  \n\\- herbal-spiritual-alternative medicine / conventional medicine / Robin Hanson  \n\\- don't care about Africa / give aid to Africa / don't give aid to Africa  \n\\- Obama is Muslim / Obama is obviously not Muslim, you idiot / [Patri](http://patrissimo.livejournal.com/1386151.html) [Friedman](http://patrissimo.livejournal.com/1386940.html)^5^  \n  \nWhat is interesting about these triads is not that people hold the positions (which could be expected by chance) but that people [get deep personal satisfaction from arguing the positions](/lw/181/solutions_to_political_problems_as_counterfactuals/14mh?c=1) even when their arguments are unlikely to change policy^6^ \\- and that people identify with these positions to the point where arguments about them can become personal.  \n  \nIf meta-contrarianism is a real tendency in over-intelligent people, it doesn't mean they should immediately abandon their beliefs; that would just be meta-meta-contrarianism. It means that they need to recognize the meta-contrarian tendency within themselves and so be extra suspicious and careful about a desire to believe something contrary to the prevailing contrarian wisdom, especially if they really enjoy doing so.\n\n  \n**Footnotes**  \n  \n1) But what's really interesting here is that people at each level of the pyramid don't just follow the customs of their level. They enjoy following the customs, it makes them feel good to talk about how they follow the customs, and they devote quite a bit of energy to insulting the people on the other levels. For example, old money call the nouveau riche \"crass\", and men who don't need to pursue women call those who do \"chumps\". Whenever holding a position makes you feel superior and is fun to talk about, that's a good sign that the position is not just practical, but signaling related.\n\n2) There is no need to point out just how unlikely it is that such a number is correct, nor how unscientific the survey was.\n\n3) One more time: _the fact that those beliefs are in an order does not mean some of them are good and others are bad_. For example, \"5 year old child / pro-death / transhumanist\" is a triad, and \"warming denier / warming believer / warming skeptic\" is a triad, but I personally support 1+3 in the first triad and 2 in the second. You can't evaluate the truth of a statement by its position in a signaling game; otherwise [you could use human psychology to figure out if global warming is real](/lw/2lr/the_importance_of_selfdoubt/2h40?c=1)!\n\n4) This is my solution to the eternal question of why libertarians are always more hostile toward liberals, even though they have just about as many points of real disagreement with the conservatives.\n\n5) To be fair to Patri, he admitted that those two posts were \"trolling\", but I think the fact that he derived so much enjoyment from trolling in that particular way is significant.\n\n6) Worth a footnote: I think in a lot of issues, the original uneducated position has disappeared, or been relegated to a few rednecks in some remote corner of the world, and so meta-contrarians simply look like contrarians. I think it's important to keep the terminology, because most contrarians retain a psychology of feeling like they are being contrarian, even after they are the new norm. But my only evidence for this is introspection, so it might be false."
          },
          "voteCount": 306
        },
        {
          "name": "Too busy to think about life",
          "type": "post",
          "slug": "too-busy-to-think-about-life",
          "_id": "4psQW7vRwt7PE5Pnj",
          "url": null,
          "title": "Too busy to think about life",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_Many adults maintain their intelligence through a dedication to study or hard work.  I suspect this is related to sub-optimal levels of careful introspection among intellectuals._\n\nIf someone asks you what you want for yourself in life, do you have the answer ready at hand?  How about what you want for others?  Human values are [complex](http://wiki.lesswrong.com/wiki/Complexity_of_value), which means your talents and technical knowledge should help you think about them.  Just as in your work, complexity shouldn't be a curiosity-stopper.  It means \"think\", not \"give up now.\"\n\nBut there are so many terrible excuses stopping you...\n\nToo busy studying?  Life is the exam you are always taking.  Are you studying for that?  Did you even write yourself a course outline?\n\nToo busy helping?  Decision-making is the skill you are aways using, or always lacking, as much when you help others as yourself.  Isn't something you use _constantly_ worth improving _on purpose_?\n\nToo busy thinking to learn about your brain?  That's like being too busy flying an airplane to learn where the engines are.  Yes, you've got passengers in real life, too: the people whose lives you affect.\n\nEmotions too irrational to think about them?  Irrational emotions are things you don't want to think _for you_, and therefore _are_ something you want to think _about_.  By analogy, children are often irrational, and no one sane concludes that we therefore shouldn't think about their welfare, or that they shouldn't exist.\n\nSo set aside a date.  Sometime soon.  Write yourself some notes.  Find that introspective friend of yours, and start solving for happiness.  Don't have one?  For the first time in history, you've got LessWrong.com!\n\nReasons to make the effort:\n\n**Happiness is a pairing between your situation and your disposition.** Truly optimizing your life requires adjusting both variables: what happens, _and_ how it affects you.\n\n**You are constantly changing your disposition.** The question is whether you'll do it with a purpose.  Your experiences change you, and you affect those, as well as how you think about them, which also changes you.  It's going to happen.  It's happening now.  Do you even know how it works?  Put your intelligence to work and figure it out!\n\n**The road to harm is paved with ignorance.** Using your capability to understand yourself and what you're doing is a matter of responsibility to others, too.  It makes you better able to be a better friend.\n\n**You're almost certainly suffering from [Ugh Fields](/lw/21b/ugh_fields/):** unconscious don't-think-about-it reflexes that form via Pavlovian conditioning.  The issues most in need of your attention are often ones you just happen not to think about for reasons undetectable to you.\n\nHow not to waste the effort:\n\n**Don't wait till you're sad.** Only thinking when you're sad gives you a skew perspective.  Don't infer that you can think better when you're sad just because that's the only time you try to be thoughtful.  Sadness often makes it _harder_ to think: you're farther from happiness, which can make it more difficult to empathize with and understand.  Nonethess we often _have_ to think when sad, because something bad may have happened that needs addressing.\n\n**Introspect carefully, not constantly.** Don't interrupt your work every 20 minutes to wonder whether it's your true purpose in life.  Respect that question as something that requires concentration, note-taking, and solid blocks of scheduled time.  In those times, check over your analysis by trying to confound it, so lingering doubts can be justifiably quieted by remembering how thorough you were.\n\n**Re-evaluate on an appropriate time-scale.** Try devoting a few days before each semester or work period to look at your life as a whole.  At these times you'll have accumulated experience data from the last period, ripe and ready for analysis.  You'll have more ideas per hour that way, and feel better about it.  Before starting something new is also the most natural and opportune time to affirm or change long term goals.  Then, barring large unexpecte d opportunities, stick to what you decide until the next period when you've gathered enough experience to warrant new reflection.\n\n([The absent minded driver](/lw/182/the_absentminded_driver/) is a mathematical example of how planning outperforms constant re-evaluation.  When not engaged in a deep and careful introspection, we're all absent minded drivers to a degree.)\n\nLost about where to start?  I think [Alicorn's story](/lw/20l/ureshiku_naritai/) is an inspiring one.  Learn to understand and defeat [procrastination/akrasia](http://wiki.lesswrong.com/wiki/Akrasia).  Overcome your [cached selves](/lw/4e/cached_selves/) so you can grow freely (definitely read their _possible strategies_ at the end).  Foster an everyday awareness that [you are a brain](/lw/fc/you_are_a_brain/), and in fact more like [two half-brains](/lw/20/the_apologist_and_the_revolutionary/).\n\nThese suggestions are among the top-rated LessWrong posts, so they'll be of interest to lots of intellectually-minded, rationalist-curious individuals.  But you have your own task ahead of you, that only you can fulfill.\n\nSo don't give up.  Don't procrastinate it.  If you haven't done it already, schedule a day and time _right now_ when you can realistically assess\n\n*   how you want your life to affect you and other people, and\n*   what you must change to better achieve this.\n\nEliezer has said _I want you to live_.  Let me say:\n\n_I want you to be better at your life._"
          },
          "voteCount": 118
        },
        {
          "name": "37 Ways That Words Can Be Wrong",
          "type": "post",
          "slug": "37-ways-that-words-can-be-wrong",
          "_id": "FaJaCgqBKphrDzDSj",
          "url": null,
          "title": "37 Ways That Words Can Be Wrong",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Philosophy of Language"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Some reader is bound to declare that a better title for this post would be \"37 Ways That You Can Use Words Unwisely\", or \"37 Ways That Suboptimal Use Of Categories Can Have Negative Side Effects On Your Cognition\".\n\nBut one of the primary lessons of this gigantic list is that saying \"There's no way my choice of X can be 'wrong'\" is nearly always an error in practice, whatever the theory.  You can always be wrong.  Even when it's theoretically impossible to be wrong, you can still be wrong.  There is never a Get-Out-Of-Jail-Free card for anything you do.  That's life.\n\nBesides, I can define the word \"wrong\" to mean anything I like - it's not like a word can be _wrong._\n\nPersonally, I think it quite justified to use the word \"wrong\" when:\n\n1.  _A word fails to connect to reality in the first place._  Is Socrates a framster?  Yes or no?  ([The Parable of the Dagger.](/lw/ne/the_parable_of_the_dagger/))\n2.  _Your argument, if it worked, could coerce reality to go a different way by choosing a different word definition._  Socrates is a human, and humans, by definition, are mortal.  So if you defined humans to not be mortal, would Socrates live forever?  ([The Parable of Hemlock](/lw/nf/the_parable_of_hemlock/).)\n3.  _You try to establish any sort of empirical proposition as being true \"by definition\"._  Socrates is a human, and humans, by definition, are mortal.  So is it a _logical truth_ if we empirically predict that Socrates should keel over if he drinks hemlock?  It seems like there are logically possible, non-self-contradictory worlds where Socrates doesn't keel over - where he's immune to hemlock by a quirk of biochemistry, say.  Logical truths are true in all possible worlds, and so never tell you _which_ possible world you live in - and anything you can establish \"by definition\" is a logical truth.  ([The Parable of Hemlock](/lw/nf/the_parable_of_hemlock/).)\n4.  _You unconsciously slap the conventional label on something, without actually using the verbal definition you just gave._  You know perfectly well that Bob is \"human\", even though, on your definition, you can never call Bob \"human\" without first observing him to be mortal.  ([The Parable of Hemlock](/lw/nf/the_parable_of_hemlock/).)\n5.  _The act of labeling something with a word, disguises a challengable inductive inference you are making._ If the last 11 egg-shaped objects drawn have been blue, and the last 8 cubes drawn have been red, it is a matter of induction to say this rule will hold in the future.  But if you call the blue eggs \"bleggs\" and the red cubes \"rubes\", you may reach into the barrel, feel an egg shape, and think \"Oh, a blegg.\"  ([Words as Hidden Inferences.](/lw/ng/words_as_hidden_inferences/))\n6.  _You try to define a word using words, in turn defined with ever-more-abstract words, without being able to point to an example._  \"What is red?\"  \"Red is a color.\"  \"What's a color?\"  \"It's a property of a thing?\"  \"What's a thing?  What's a property?\"  It never occurs to you to point to a stop sign and an apple.  ([Extensions and Intensions.](/lw/nh/extensions_and_intensions/))\n7.  _The extension doesn't match the intension._  We aren't consciously aware of our identification of a red light in the sky as \"Mars\", which will probably happen regardless of your attempt to define \"Mars\" as \"The God of War\".  ([Extensions and Intensions.](/lw/nh/extensions_and_intensions/))\n8.  _Your verbal definition doesn't capture more than a tiny fraction of the category's shared characteristics, but you try to reason as if it does._  When the philosophers of Plato's Academy claimed that the best definition of a human was a \"featherless biped\", Diogenes the Cynic is said to have exhibited a plucked chicken and declared \"Here is Plato's Man.\"  The Platonists promptly changed their definition to \"a featherless biped with broad nails\".  ([Similarity Clusters.](/lw/nj/similarity_clusters/))\n9.  _You try to treat category membership as all-or-nothing, ignoring the existence of more and less typical subclusters._  Ducks and penguins are less typical birds than robins and pigeons. Interestingly, a between-groups experiment showed that subjects thought a dis ease was more likely to spread from robins to ducks on an island, than from ducks to robins.  ([Typicality and Asymmetrical Similarity.](/lw/nk/typicality_and_asymmetrical_similarity/))\n10.  _A verbal definition works well enough in practice to point out the intended cluster of similar things, but you nitpick exceptions._ Not every human has ten fingers, or wears clothes, or uses language; but if you look for an empirical cluster of things which share these characteristics, you'll get enough information that the occasional nine-fingered human won't fool you.  ([The Cluster Structure of Thingspace](/lw/nl/the_cluster_structure_of_thingspace/).)\n11.  _You ask whether something \"is\" or \"is not\" a category member but can't name the question you really want answered._  What is a \"man\"?  Is Barney the Baby Boy a \"man\"?  The \"correct\" answer may depend considerably on whether the query you _really_ want answered is \"Would hemlock be a good thing to feed Barney?\" or \"Will Barney make a good husband?\"  ([Disguised Queries.](/lw/nm/disguised_queries/))\n12.  _You treat intuitively perceived hierarchical categories like the only correct way to parse the world, without realizing that other forms of statistical inference are possible even though your brain doesn't use them._  It's much easier _for a human_ to notice whether an object is a \"blegg\" or \"rube\"; than _for a human_ to notice that red objects never glow in the dark, but red furred objects have all the other characteristics of bleggs.  Other statistical algorithms work differently.  ([Neural Categories.](/lw/nn/neural_categories/))\n13.  _You talk about categories as if they are manna fallen from the Platonic Realm, rather than inferences implemented in a real brain._ The ancient philosophers said \"Socrates is a man\", not, \"My brain perceptually classifies Socrates as a match against the 'human' concept\".  ([How An Algorithm Feels From Inside.](/lw/no/how_an_algorithm_feels_from_inside/))\n14.  _You argue about a category membership even after screening off all questions that could possibly depend on a category-based inference._  After you observe that an object is blue, egg-shaped, furred, flexible, opaque, luminescent, and palladium-containing, what's _left_ to ask by arguing, \"Is it a blegg?\"  But if your brain's categorizing neural network contains a (metaphorical) central unit corresponding to the inference of blegg-ness, it may still _feel_ like there's a leftover question.  ([How An Algorithm Feels From Inside.](/lw/no/how_an_algorithm_feels_from_inside/))\n15.  _You allow an argument to slide into being about definitions, even though it isn't what you originally wanted to argue about._ If, before a dispute started about whether a tree falling in a deserted forest makes a \"sound\", you asked the two soon-to-be arguers whether they thought a \"sound\" should be defined as \"acoustic vibrations\" or \"auditory experiences\", they'd probably tell you to flip a coin.  Only after the argument starts does the definition of a word become politically charged.  ([Disputing Definitions.](/lw/np/disputing_definitions/))\n16.  _You think a word has a meaning, as a property of the word itself; rather than there being a label that your brain associates to a particular concept._  When someone shouts, \"Yikes!  A tiger!\", evolution would not favor an organism that thinks, \"Hm... I have just heard the syllables 'Tie' and 'Grr' which my fellow tribemembers associate with their internal analogues of my own _tiger_ concept and which _aiiieeee_ CRUNCH CRUNCH GULP.\"  So the brain takes a shortcut, and it seems that the meaning of tigerness is a property of the label itself.  People argue about the _correct meaning_ of a label like \"sound\". ([Feel the Meaning.](/lw/nq/feel_the_meaning/))\n17.  _You argue over the meanings of a word, even after all sides understand perfectly well what the other sides are trying to say._  The human ability to associate labels to concepts is a tool for communication.  When people _want_ to communicate, we're hard to stop; if we have no common language, we'll draw pictures in sand.  When you each understand what is in the other's mind, you are _done._ ([The Argument From Common Usage.](/lw/nr/the_argument_from_common_usage/))\n18.  _You pull out a dictionary in the middle of an empirical or moral argument._ Dictionary editors are historians of usage, not legislators of language.  If the common definition contains a problem - if \"Mars\" is defined as the God of War, or a \"dolphin\" is defined as a kind of fish, or \"Negroes\" are defined as a separate category from humans, the dictionary will reflect the standard mistake.  ([The Argument From Common Usage.](/lw/nr/the_argument_from_common_usage/))\n19.  _You pull out a dictionary in the middle of any argument ever._ Seriously, what the heck makes you think that dictionary editors are an authority on whether \"atheism\" is a \"religion\" or whatever?  If you have any substantive issue whatsoever at stake, do you really think dictionary editors have access to ultimate wisdom that settles the argument?  ([The Argument From Common Usage.](/lw/nr/the_argument_from_common_usage/))\n20.  _You defy common usage without a reason, making it gratuitously hard for others to understand you._  Fast stand up plutonium, with bagels without handle.  ([The Argument From Common Usage.](/lw/nr/the_argument_from_common_usage/))\n21.  _You use complex renamings to create the illusion of inference._ Is a \"human\" defined as a \"mortal featherless biped\"?  Then write:  \"All \\[mortal featherless bipeds\\] are mortal; Socrates is a \\[mortal featherless biped\\]; therefore, Socrates is mortal.\"  Looks less impressive that way, doesn't it?  ([Empty Labels.](/lw/ns/empty_labels/))\n22.  _You get into arguments that you could avoid if you just didn't use the word._ If Albert and Barry aren't allowed to use the word \"sound\", then Albert will have to say \"A tree falling in a deserted forest generates acoustic vibrations\", and Barry will say \"A tree falling in a deserted forest generates no auditory experiences\".  When a word poses a problem, the simplest solution is to eliminate the word and its synonyms.  ([Taboo Your Words.](/lw/nu/taboo_your_words/))\n23.  _The existence of a neat little word prevents you from seeing the details of the thing you're trying to think about._ What actually goes on in schools once you stop calling it \"education\"? What's a degree, once you stop calling it a \"degree\"?  If a coin lands \"heads\", what's its radial orientation?  What is \"truth\", if you can't say \"accurate\" or \"correct\" or \"represent\" or \"reflect\" or \"semantic\" or \"believe\" or \"knowledge\" or \"map\" or \"real\" or any other simple term?  ([Replace the Symbol with the Substance.](/lw/nv/replace_the_symbol_with_the_substance/))\n24.  _You have only one word, but there are two or more different things-in-reality, so that all the facts about them get dumped into a single undifferentiated mental bucket._  It's part of a detective's ordinary work to observe that Carol wore red last night, or that she has black hair; and it's part of a detective's ordinary work to wonder if maybe Carol dyes her hair.  But it takes a subtler detective to wonder if there are two Carols, so that the Carol who wore red is not the same as the Carol who had black hair.  ([Fallacies of Compression.](/lw/nw/fallacies_of_compression/))\n25.  _You see patterns where none exist, harvesting other characteristics from your definitions even when there is no similarity along that dimension._  In Japan, it is thought that people of blood type A are earnest and creative, blood type Bs are wild and cheerful, blood type Os are agreeable and sociable, and blood type ABs are cool and controlled.  ([Categorizing Has Consequences.](/lw/nx/categorizing_has_consequences/))\n26.  _You try to sneak in the connotations of a word, by arguing from a definition that doesn't include the connotations._ A \"wiggin\" is defined in the dictionary as a person with green eyes and black hair.  The word \"wiggin\" also carries the connotation of someone who commits crimes and launches cute baby squirrels, but that part isn't in the dictionary.  So you point to someone and say:  \"Green eyes?  Black hair?  See, told you he's a wiggin!  Watch, next he's going to steal the silverware.\"  ([Sneaking in Connotations.](/lw/ny/sneaking_in_connotations/))\n27.  _You claim \"X, by definition, is a Y!\"  On such occasions you're almost certainly trying to sneak in a connotation of Y that wasn't in your given definition._  You define \"human\" as a \"featherless biped\", and point to Socrates and say, \"No feathers - two legs - he must be human!\"  But what you _really_ care about is something else, like mortality.  If what was in dispute was Socrates's number of legs, the other fellow would just reply, \"Whaddaya mean, Socrates's got two legs?  That's what we're arguing about in the first place!\"  ([Arguing \"By Definition\".](/lw/nz/arguing_by_definition/))\n28.  _You claim \"Ps, by definition, are Qs!\"_  If you see Socrates out in the field with some biologists, gathering herbs that might confer resistance to hemlock, there's no point in arguing \"Men, by definition, are mortal!\"  The main time you feel the need to tighten the vise by insisting that something is true \"by definition\" is when there's other information that calls the default inference into doubt. ([Arguing \"By Definition\".](/lw/nz/arguing_by_definition/))\n29.  _You try to establish membership in an empirical cluster \"by definition\"._  You wouldn't feel the need to say, \"Hinduism, _by definition,_ is a religion!\" because, well, of course Hinduism is a religion.  It's not just a religion \"by definition\", it's, like, an _actual_ religion.  Atheism does not resemble the central members of the \"religion\" cluster, so if it wasn't for the fact that atheism is a religion _by definition,_ you might go around thinking that atheism _wasn't_ a religion.  That's why you've got to crush all opposition by pointing out that \"Atheism is a religion\" is true _by definition_, because it isn't true any other way.  ([Arguing \"By Definition\".](/lw/nz/arguing_by_definition/))\n30.  _Your definition draws a boundary around things that don't really belong together._  You can claim, if you like, that you are _defining_ the word \"fish\" to refer to salmon, guppies, sharks, dolphins, and trout, but not jellyfish or algae.  You can claim, if you like, that this is merely a list, and there is no way a list can be \"wrong\".  Or you can stop playing nitwit games and admit that you made a mistake and that dolphins don't belong on the fish list.  ([Where to Draw the Boundary?](/lw/o0/where_to_draw_the_boundary/))\n31.  _You use a short word for something that you won't need to describe often, or a long word for something you'll need to describe often.  This can result in inefficient thinking, or even misapplications of Occam's Razor, if your mind thinks that short sentences sound \"simpler\"._  Which sounds more plausible, \"God did a miracle\" or \"A supernatural universe-creating entity temporarily suspended the laws of physics\"?  ([Entropy, and Short Codes.](/lw/o1/entropy_and_short_codes/))\n32.  _You draw your boundary around a volume of space where there is no greater-than-usual density, meaning that the associated word does not correspond to any performable Bayesian inferences._  Since green-eyed people are not more likely to have black hair, or vice versa, and they don't share any other characteristics in common, why have a word for \"wiggin\"?  ([Mutual Information, and Density in Thingspace.](/lw/o2/mutual_information_and_density_in_thingspace/))\n33.  _You draw an unsimple boundary without any reason to do so._ The act of defining a word to refer to all humans, except black people, seems kind of suspicious.  If you don't present reasons to draw that particular boundary, trying to create an \"arbitrary\" word in that location is like a detective saying:  \"Well, I haven't the slightest shred of support one way or the other for who could've murdered those orphans... but have we considered John Q. Wiffleheim as a suspect?\"  ([Superexponential Conceptspace, and Simple Words.](/lw/o3/superexponential_conceptspace_and_simple_words/))\n34.  _You use categorization to make inferences about properties that don't have the appropriate empirical structure, namely, conditional independence given knowledge of the class, to be well-approximated by Naive Bayes._  No way am I trying to summarize this one.  Just read the blog post.  ([Conditional Independence, and Naive Bayes.](/lw/o8/conditional_independence_and_naive_bayes/))\n35.  _You think that words are like tiny little LISP symbols in your mind, rather than words being labels that act as handles to direct complex mental paintbrushes that can paint detailed pictures in your sensory workspace._  Visualize a \"triangular lightbulb\".  What did you see?  ([Words as Mental Paintbrush Handles.](/lw/o9/words_as_mental_paintbrush_handles/))\n36.  _You use a word that has different meanings in different places as though it meant the same thing on each occasion, possibly creating the illusion of something protean and shifting._  \"Martin told Bob the building was on his left.\"  But \"left\" is a function-word that evaluates with a speaker-dependent variable grabbed from the surrounding context.  Whose \"left\" is meant, Bob's or Martin's?  ([Variable Question Fallacies.](/lw/oc/variable_question_fallacies/))\n37.  _You think that definitions can't be \"wrong\", or that \"I can define a word any way I like!\"_ This kind of attitude teaches you to indignantly defend your past actions, instead of paying attention to their consequences, or fessing up to your mistakes.  ([37 Ways That Suboptimal Use Of Categories Can Have Negative Side Effects On Your Cognition.](/lw/od/37_ways_that_words_can_be_wrong/))\n\nEverything you do in the mind has an effect, and your brain races ahead unconsciously without your supervision.\n\nSaying \"Words are arbitrary; I can define a word any way I like\" makes around as much sense as driving a car over thin ice with the accelerator floored and saying, \"Looking at this steering wheel, I can't see why one radial angle is special - so I can turn the steering wheel any way I like.\"\n\nIf you're trying to go anywhere, or even just trying to _survive,_ you had better start paying attention to the three or six dozen optimality criteria that control how you use words, definitions, categories, classes, boundaries, labels, and concepts."
          },
          "voteCount": 137
        },
        {
          "name": "How to Be Happy",
          "type": "post",
          "slug": "how-to-be-happy",
          "_id": "ZbgCx2ntD5eu8Cno9",
          "url": null,
          "title": "How to Be Happy",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Well-being"
            },
            {
              "name": "Practical"
            },
            {
              "name": "Gratitude"
            },
            {
              "name": "Happiness"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "The correlates of happiness",
                "anchor": "The_correlates_of_happiness",
                "level": 2
              },
              {
                "title": "Happiness, personality, and skills",
                "anchor": "Happiness__personality__and_skills",
                "level": 2
              },
              {
                "title": "Happiness is subjective and relative",
                "anchor": "Happiness_is_subjective_and_relative",
                "level": 1
              },
              {
                "title": "Flow and mindfulness",
                "anchor": "Flow_and_mindfulness",
                "level": 1
              },
              {
                "title": "How to be happier",
                "anchor": "How_to_be_happier",
                "level": 1
              },
              {
                "title": "Notes",
                "anchor": "Notes",
                "level": 2
              },
              {
                "title": "References",
                "anchor": "References",
                "level": 2
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "207 comments"
              }
            ],
            "headingsCount": 9
          },
          "contents": {
            "markdown": "Part of the sequence: [The Science of Winning at Life](http://wiki.lesswrong.com/wiki/The_Science_of_Winning_at_Life)\n\nOne day a coworker said to me, \"Luke! You're, like, the happiest person I know! How come you're so happy all the time?\"\n\nIt was probably a rhetorical question, but I had a very long answer to give. See, I was _un_happy for most of my life,^1^ and even considered suicide a few times. Then I spent two years studying the science of happiness. Now, happiness is my natural state. I can't remember the last time I felt unhappy for longer than 20 minutes.\n\nThat kind of change won't happen for everyone, or even most people ([beware of other-optimizing](/lw/9v/beware_of_otheroptimizing/)), but it's worth a shot! \n\nWe all want to be happy, and happiness is useful for other things, too.^2^ For example, happiness improves physical health,^3^ improves creativity,^4^ and even enables you to make better decisions.^5^ (It's harder to be rational when you're unhappy.^6^) So, as part of a series on how to [win](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/) [at](/lw/3w3/how_to_beat_procrastination/) [life](/lw/1sm/akrasia_tactics_review/) with science and rationality, let's review **the science of happiness**.\n\n#### The correlates of happiness\n\n[Earlier](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge/), I noted that there is an abundance of research on factors that correlate with _subjective well-being_ (individuals' own assessments of their happiness and life satisfaction).\n\nFactors that _don't correlate_ much with happiness include: age,^7^ gender,^8^ parenthood,^9^ intelligence,^10^ physical attractiveness,^11^ and money^12^ (as long as you're above the poverty line). Factors that _correlate moderately_ with happiness include: health,^13^ social activity,^14^ and religiosity.^15^ Factors that _correlate strongly_ with happiness include: genetics,^16^ love and relationship satisfaction,^17 ^and work satisfaction.^18^\n\nBut correlation is not enough. We want to know what _causes_ happiness. And that is a trickier thing to measure. But we do know a _few_ things.\n\n#### Happiness, personality, and skills\n\nGenes account for about 50% of the variance in happiness.^19^ Even lottery winners and newly-made quadriplegics do not see as much of a change in happiness as you would expect.^20^ Presumably, genes shape your happiness by shaping your personality, which is known to be quite heritable.^21^\n\nSo which personality traits tend to correlate most with happiness? Extroversion is among the best predictors of happiness,^22^ as are conscientiousness, agreeableness, self-esteem, and optimism.^23^\n\nWhat if you don't have those traits? The first thing to say is that you might be capable of them without knowing it. Introversion, for example, can be exacerbated by _a lack of social skills_. If you decide to [learn](http://www.amazon.com/Social-Skills-Picture-School-Beyond/dp/1932565353/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) and [practice](http://reports.toastmasters.org/findaclub/) social skills, you might find that you are more extroverted than you thought! (That's what happened to me.) The same goes for [conscientiousness](http://www.amazon.com/dp/055380491X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20), [agreeableness](http://www.amazon.com/dp/1439167346/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20), [self-esteem](http://www.psychologicalselfhelp.org/Chapter14/chap14_5.html), and [optimism](/lw/3w3/how_to_beat_procrastination/#optimism) \\- these are only partly linked to personality. They are to some extent learnable skills, and learning these skills (or even \"acting as if\") can increase happiness.^24^\n\nThe second thing to say is that lacking some of these traits does not, of course, doom you to unhappiness.\n\n###   \n\n### Happiness is subjective and relative\n\nHappiness is not determined by objective factors, but by how you _feel_ about them.^25^\n\nHappiness is also relative^26^: you'll probably be happier making $25,000/yr in Costa Rica (where your neighbors are making $13,000/yr) than you will be making $80,000/yr in Beverly Hills (where your neighbors are making $130,000/yr).\n\nHappiness is relative in another sense, too: it is relative to your _expectations_.^27^ We are quite poor at predicting the strength of our emotional reactions to future events. We overestimate the misery we will experience after a romantic breakup, failure to get a promotion, or even contracting an illness. We also overestimate the _pleasure_ we will get from buying a nice car, getting a promotion, or moving to a lovely coastal city. So: lower your expectations about the pleasure you'll get from such expenditures.\n\n### Flow and mindfulness\n\nYou may have heard of the famous studies^28^ showing that people are happiest when they are in a state of \"[flow](http://en.wikipedia.org/wiki/Flow_(psychology)).\" Flow is the state you're in when you are fully engaged in a task that is interesting, challenging, and intrinsically rewarding to you. This is the experience of \"losing yourself in the moment\" or, as sports players say, \"being in the zone.\"\n\nFinding flow has largely to do with performing tasks that match your skill level. When a task is far beyond your skill level, you will feel defeated. When a task is too easy, you'll be bored. Only when a task is challenging but achievable will you feel good about doing it. I'm reminded of the state troopers in _[Super Troopers](http://www.youtube.com/watch?v=2-9D2qUHN-E)_, who devised strange games and challenges to make their boring jobs passable. [Myrtle Young](http://www.youtube.com/watch?v=EY3Lw_-bj5U) made her boring job at a potato chip factory more interesting and challenging by looking for potato chips that resembled celebrities, and pulling them off the conveyor belts for her collection.\n\nIf you're struggling with negative thoughts, achieving flow is probably the best medicine. Contrary to popular wisdom, forced positive thinking often makes things worse.^29^ Trying to _not_ think about Upsetting Thought X has the same effect as trying to not think about pink elephants: you can't help but think about pink elephants.\n\nWhile being \"lost in the moment\" may provide some of your happiest moments, research has also shown that when you're not in flow, taking a step outside the moment and practicing \"mindfulness\" - that is, paying attention to your situation, your actions, and your feelings - can reduce chronic pain and depression^30^, reduce stress and anxiety^31^, and produce a wide range of other positive effects.^32^ \n\n### How to be happier\n\nHappiness, then, is an enormously complex thing. Worse, we must remember the difference between [experienced happiness and remembered happiness](http://www.ted.com/talks/daniel_kahneman_the_riddle_of_experience_vs_memory.html). I can only scratch the surface of happiness research in this tiny post. In short, there is no simple fix for unhappiness; no straight path to bliss.\n\nMoreover, happiness will be achieved differently for different people. A person suffering from depression due to chemical imbalance may get more help from a pill than from learning better social skills. A healthy, extroverted, agreeable, conscientious woman can still be unhappy if she is trapped in a bad marriage. Some people were raised by parents whose parenting style did not encourage the development of healthy self-esteem,^33^ and they will need to devote significant energy to overcome this deficit. For some, the road to happiness is long. For others, it is short.\n\nBelow, I review a variety of methods for becoming happier. Some of them I discussed above; many, I did not.\n\nThese methods are ranked roughly in descending order of importance and effect, based on my own reading of the literature. You will need to think about who you are, what makes you happy, what makes you unhappy, and what you can achieve in order to determine which of the below methods should be attempted first. Also, engaging any of these methods may require that you first gain some [mastery over procrastination](/lw/3w3/how_to_beat_procrastination/).\n\nHere, then, are some methods for becoming happier^34^:\n\n1.  If you suffer from serious illness, depression, anxiety, paranoia, schizophrenia, or other serious problems, _seek professional help first_. [Here's how](http://www.liveyourlifewell.org/go/live-your-life-well/help).\n2.  Even if you don't need professional help, you may benefit from some self-exploration and _initial guidance_ from a reductionistic, naturalistic counselor like [Tom Clark](http://naturalism.org/consulting.htm).\n3.  Develop the skills and habits associated with _extroversion_. First, get some decent clothes and learn how to wear them properly. If you're a guy, [read](http://www.amazon.com/Details-Mens-Style-Manual-Ultimate/dp/159240328X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) [these](http://www.bradp.com/brads-fashion-bible) [books](http://www.amazon.com/Mens-Style-Thinking-Guide-Dress/dp/0312361653/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). If you're a girl, ask your girlfriends or try [these](http://www.amazon.com/What-Not-Wear-Trinny-Woodall/dp/B0042P5752/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) [books](http://www.amazon.com/Dress-Your-Best-Complete-Finding/dp/0307236714/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). Next, learn basic [social](http://www.amazon.com/How-Talk-Anyone-Success-Relationships/dp/007141858X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) [skills](http://www.amazon.com/Social-Skills-Picture-School-Beyond/dp/1932565353/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20), including [body](http://www.amazon.com/Winning-Body-Language-Conversation-Attention/dp/0071700579/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) [language](http://www.amazon.com/Definitive-Book-Body-Language/dp/0553804723/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20). If you're _really_ introverted, practice on [Chatroulette](http://en.wikipedia.org/wiki/Chatroulette) or [Omegle](http://www.omegle.com/) first. Next, spend more time with other people, making small talk. Go to [meetups](http://www.meetup.com/) and [CouchSurfing](http://www.couchsurfing.org/) group activities. Practice your skills until they become more natural, and you find yourself _enjoying_ being in the company of others. Learn [how](http://www.amazon.com/Comic-Toolbox-Funny-Even-Youre/dp/1879505215/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) to [be](http://www.amazon.com/Finding-Funny-Fast-Connect-Coworkers/dp/0984099905/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) [funny](http://www.amazon.com/How-Be-Funny-Discovering-Comic/dp/1573922064/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20) and practice that, too.\n4.  Improve your _self-esteem_ and _optimism_. This is tricky. First, too much self-esteem can lead to harmful narcissism.^35^ Second, it's not clear that a rationalist can endorse several standard methods for improving one's self esteem (self-serving bias, basking in reflected glory, self-handicapping)^36^ because they toy with self-deception and [anti-epistemology](http://wiki.lesswrong.com/wiki/Anti-epistemology). But there are a few safe ways to increase your self-esteem and optimism. Make use of success spirals, vicarious victory, and mental contrasting, as described [here](/lw/3w3/how_to_beat_procrastination/#optimism).\n5.  Improve your _agreeableness_. In simpler terms, this basically means: increase your empathy. Unfortunately, little is currently known (scientifically) about how to increase one's empathy.^37^ The usual advice about trying to see things from another's perspective, and thinking more about people less fortunate than oneself, will have to do for now. The organization [Roots of Empathy](http://www.rootsofempathy.org/) may have some good [advice](http://opinionator.blogs.nytimes.com/2010/11/08/fighting-bullying-with-babies/), too.\n6.  Improve your _conscientiousness_. Conscientiousness involves a variety of tendencies: useful organization, strong work ethic, reliability, planning ahead, etc. Each of these individual skills can be learned. The techniques for [overcoming procrastination](/lw/3w3/how_to_beat_procrastination/) are useful, here. Some people report that books like _[Getting Things Done](http://www.amazon.com/Getting-Things-Done-Stress-Free-Productivity/dp/0142000280/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_ have helped them become more organized and reliable.\n7.  Develop the _habit of gratitude_. Savor the good moments throughout each day.^38^ Spend time thinking about happy memories.^39^ And at the end of each day, write down 5 things you are grateful for: the roof over your head, your good fortune at being born in a wealthy country, the existence of _Less Wrong_, the taste of chocolate, the feel of orgasm... whatever. It sounds childish, but it works.^40^\n8.  Find your _purpose_ and live it. One benefit of religion may be that it gives people a sense of meaning and purpose. Without a magical deity to give you purpose, though, you'll have to find out for yourself what drives you. It may take a while to find it though, and you may have to dip your hands and mind into many fields. But once you find a path that strongly motivates you and fulfills you, take it. (Of course, you might not find one purpose but many.) Having a strong sense of meaning and purpose has a wide range of positive effects.^41^ The 'find a purpose' recommendation also offers an illustration of how methods may differ in importance for people. 'Find a purpose' is not always emphasized in happiness literature, but for my own brain chemistry I suspect that finding motivating purposes has made more difference in my life than anything else on this list.\n9.  Find a more _fulfilling job_. Few people do what they love for a living. Getting to that point can be difficult and complicated. You may find that doing 10 other things on this list _first_ is needed for you to have a good chance at getting a more fulfilling job. To figure out which career might be full of tasks that you love to do, a [RIASEC](http://www.bigjobportal.com/riasec/) personality test might help. In the USA, [O*NET](http://www.onetonline.org/) can help you find jobs that are in-demand and fit your personality.\n10.  Improve your relationship with your _romantic partner_, or find a different one. As with finding a more fulfilling job, this one is complicated, but can have major impact. If you know your relationship isn't going anywhere, you may want to drop it so you can spend more time developing yourself, which will improve future relationships. If you're pretty serious about your partner, there are many things you can do to improve the relationship. Despite being touted widely, \"active listening\" doesn't predict relationship success.^42^ Tested advice for improving the chances of relationship success and satisfaction include: (1) do novel and exciting things with your partner often^43^, (2) say positive things to and about your partner at least 5 times more often than you say negative things^44^, (3) spend each week writing about why your relationship is better than some others you know about^45^, (4) qualify every criticism of your partner with a review of one or two of their positive qualities^46^, and (5) stare into each other's eyes more often.^47^\n11.  _Go outside_ and move your body. This will improve your attention and well-being.^48^\n12.  Spend more time in _flow_. Drop impossible tasks in favor of tasks that are at the outer limits of your skillset. Make easy and boring tasks more engaging by turning them into games or adding challenges for yourself.\n13.  _Practice mindfulness_ regularly. When not in flow, step outside yourself and pay attention to how you are behaving, how your emotions are functioning, and how your current actions work toward your goals. [Meditation](http://www.wikihow.com/Meditate) may help.\n14.  _Avoid consumerism_. The things you own _do_ come to own you, in a sense. Consumerism leads to unhappiness.^49^ Unfortunately, you've probably been programmed from birth to see through the lens of consumerism. One way to start deprogramming is by watching [this documentary](http://en.wikipedia.org/wiki/The_Century_of_the_Self) about the deliberate invention of consumerism by [Edward Bernays](http://en.wikipedia.org/wiki/Edward_Bernays). After that, you may want to sell or give away many of your possessions and, more importantly, drastically change your purchasing patterns.\n\nNote that seeking happiness _as an end_ might be counterproductive. Many people report that constantly checking to see if they are happy actually decreases their happiness - a report that fits with the research on \"flow.\" It may be better to seek some of the above goals as ends, and happiness will be a side-effect.\n\nRemember: Happiness will not come from reading articles on the internet. Happiness will come when you _do_ the things research recommends.\n\nGood luck!\n\nNext post: [The Good News of Situationist Psychology](/lw/52g/the_good_news_of_situationist_psychology/)\n\nPrevious post: [How to Beat Procrastination](/lw/3w3/how_to_beat_procrastination/)\n\n#### Notes\n\n^1^ From a young age through my teenage years, I was known as the pessimist in my family. Of course, I would retort I was merely a _realist_. Making happiness work within me made me an optimist. These days I'm pessimistic about many things: For example I think there's about a 50/50 chance the human species will survive this century. But it's a kind of rationalistic, emotionally detached pessimism. It doesn't affect my mood.\n\n^2^ Lyubomirsky, King, & Diener (2005).\n\n^3^ Steptoe et al. (2005).\n\n^4^ Isen et al. (1987); Isen (2004); Fredrickson (1998).\n\n^5^ Isen (2002); Morris (1999).\n\n^6^ Beck (2008); Ellis (2001).\n\n^7^ Age and happiness are unrelated (Lykken 1999), age accounting for less than 1% of the variation in people's happiness (Inglehart 1990; Myers & Diener 1997).\n\n^8^ Despite being treated for depressive disorders twice as often as men (Nolen-Hoeksema 2002), women report just as high levels of well-being as men do (Myers 1992).\n\n^9^ Apparently, the joys and stresses of parenthood balance each other out, as people with and without children are equally happy (Argyle 2001).\n\n^10^ Both IQ and educational attainment appear to be unrelated to happiness (Diener et al. 2009; Ross & Van Willigen 1997).\n\n^11^ Good-looking people enjoy huge advantages, but do not report greater happiness than others (Diener et al. 1995).\n\n^12^ The correlation between income and happiness is surprisingly weak (Diener & Seligman 2004; Diener et al. 1993; Johnson & Krueger 2006). One problem may be that higher income contributes to greater materialism, which impedes happiness (Frey & Stutzer 2002; Kasser et al. 2004; Solberg et al. 2002; Kasser 2002; Van Boven 2005; Nickerson et al. 2003; Kahneman et al. 2006).\n\n^13^ Those with disabling health conditions are happier than you might think (Myers 1992; Riis et al. 2005; Argyle 1999).\n\n^14^ Those who are satisfied with their social life are moderately more happy than others (Diener & Seligman 2004; Myers 1999; Diener & Seligman 2002).\n\n^15^ Religiosity correlates with happiness (Abdel-Kahlek 2005; Myers 2008), though it may be religious attendance and not religious belief that matters (Chida et al. 2009).\n\n^16^ Past happiness is the best predictor of future happiness (Lucas & Diener 2008). Happiness is surprisingly unmoved by external factors (Lykken & Tellegen 1996), because genes accounts for about 50% of the variance in happiness (Lyubomirsky et al. 2005; Stubbe et al. 2005).\n\n^17^ Married people are happier than those who are single or divorced (Myers & Diener 1995; Diener et al. 2000), and marital satisfaction predicts happiness (Proulx et al. 2007).\n\n^18^ Unemployment makes people very unhappy (Argyle 2001), and job satisfaction is strongly correlated with happiness (Judge & Klinger 2008; Warr 1999).\n\n^19^ Lyubomirsky et al. (2005); Stubbe et al. (2005).\n\n^20^ Brickman et al. (1978).\n\n^21^ Weiss et al. (2008).\n\n^22^ Lucas & Diener (2008); Fleeson et al. (2002).\n\n^23^ Lucas (2008) and Lyubomirsky et al. (2006).\n\n^24^ On the learnability of extroversion, see Fleeson et al. (2002); Bouchard & Loehlin (2001); McNeil & Fleeson (2006). On the learnability of agreeableness, see Graziano & Tobin (2009). On the learnability of conscientiousness, see Roberts et al. (2009). On the learnability of self-esteem, see Barrett et al. (1999); Borras et al. (2009). On the learnability of optimism, see Lindsley et al. (1995); Hans (2000); Feldman & Matjasko (2005). On the learnability of character traits in general, see Peterson & Seligman (2004).\n\n^25^ Schwarz & Strack (1999).\n\n^26^ Argyle (1999); Hagerty (2000).\n\n^27^ Gilbert (2006), Hsee & Hastie (2005), Wilson & Gilbert (2005).\n\n^28^ Csikszentmihalyi (1990, 1998); Gardner, Csikszentmihalyi & Damon (2002); Nakamura & Csikszentmihalyi (2009).\n\n^29^ Wegner (1989).\n\n^30^ Kabat-Zinn (1982).\n\n^31^ Shapiro et al. (1998); Chang et al. (2004).\n\n^32^ Grossman et al. (2004).\n\n^33^ Felson (1989); Harter (1998); Furnham & Cheng (2000); Wissink et al. (2006).\n\n^34^ There are several disputed and uncertain methods I did not mention. One example is \"expressive writing.\" Compare Lepore & Smyth (2002) and Spera et al. (1994) to Seery et al. (2008). Moreover, talking with a others about bad experiences may help, but maybe not: see Zech & Rimé (2005). Another disputed method is that of improving mood by thinking quicker and more varied thoughts: see Pronin & Jacobs (2008). I'm waiting for more research to come in on that one. The results of \"affectionate writing\" are mixed: see Floyd et al. (2009). The effects of household plants are also mixed: see Bringslimark et al. (2009). There remains [debate](http://mentalhealthnews.org/a-genuine-smile-found-to-improve-health-happiness/84834/) on whether forced smiles and laughter improve happiness. Finally, see the review of literature in Helliwell (2011).\n\n^35^ Crocker & Park (2004); Bushman & Baumeister (1998); Bushman & Baumeister (2002).\n\n^36^ Self-serving bias is the tendency to attribute success to internal causes (oneself), but attribute failure to external causes. Basking in reflected glory is an attempt to enhance one's image by announcing and [displaying](/lw/i7/belief_as_attire/) association with a well-perceived group or individual. Self-handicapping is a way of saving face by sabotaging one's performance in order to provide an excuse for the failure.\n\n^37^ See, for example: Stepien & Baernstein (2006); de Vignemont & Singer (2006); Heln & Singer (2008).\n\n^38^ Bryant & Veroff (2007).\n\n^39^ Burton & King (2004).\n\n^40^ Emmons & McCullough (2003); Lyubomirsky et al. (2005); Peterson (2006).\n\n^41^ Park & Folkman (1997); Bauer et al. (2008); Lee et al. (2006); Reker et al. (1987); Ulmer et al. (1991); Langer & Rodin (1976).\n\n^42^ Gottman et al. (1998); Hahlweg et al. (1984); Jacobson et al. (1987).\n\n^43^ Aron et al. (2000); Aron et al. (2003).\n\n^44^ Gottman (1984).\n\n^45^ Buunk et al. (2001).\n\n^46^ Murray & Holmes (1999).\n\n^47^ Aron et al. (2000). As for how to find, attract, and keep a great romantic partner in the first place, well: that will have to wait for another article. And of course, perhaps you're not looking for a _long term_ romantic relationship at all. That's another article, too.\n\n^48^ Berto (2005); Hartig et al. (2003); Kaplan (1993, 2001); Price (2008); Berman et al. (2008); Tennessen & Cimprich (1995).\n\n^49^ Frey & Stutzer (2002); Kasser et al. (2004); Solberg et al. (2002); Kasser (2002); Van Boven (2005); Nickerson et al. (2003); Kahneman et al. (2006).\n\n#### References\n\nArgyle (1999). Causes and correlates of happiness. In Kahneman, Diener, & Schwartz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Sage.\n\nArgyle (2001). _The Psychology of Happiness_ (2nd ed.). New York: Routledge.\n\nAron, Norman, Aron, McKenna, & Heyman (2000). Couples shared participation in novel and arousing activities and experienced relationship quality. _Journal of Personality and Social Psychology, 78_: 273-283.\n\nAron, Norman, Aron, & Lewandowski (2003). Shared participation in self- expanding activities: Positive effects on experienced marital quality. In Noller & Feeney (Eds.), _Marital interaction_ (pp. 177-196). Cambridge University Press.\n\nBarrett, Webster, Wallis (1999). Adolescent self-esteem and cognitive skills training: a school-based intervention. _Journal of Child and Family Studies 8(2)_: 217-227.\n\nBauer, McAdams, & Pals (2008). Narrative identity and eudaimonic well-being. _Journal of Happiness Studies, 9_: 81-104.\n\nBeck (2008). [The evolution of the cognitive model of depression and its neurobiological correlates](http://commonsenseatheism.com/wp-content/uploads/2011/03/Beck-The-Evolution-of-the-Cognitive-Model-of-Depression-and-Its-Neurobiological-Correlates.pdf). _American Journal of Psychiatry, 165_: 969-977.\n\nBerman, Jonides, & Kaplan (2008). [The cognitive benefits of interacting with nature](http://commonsenseatheism.com/wp-content/uploads/2011/03/Berman-The-cognitive-benefits-of-interacting-with-nature.pdf). _Psychological Science, 19_: 1207-1212.\n\nBerto (2005). Exposure to restorative environments helps restore attentional capacity. _Journal of Environmental Psychology, 25_: 249-259.\n\nBrickman, Coates, & Janoff-Bulman (1978). Lottery winners and accident victims: Is happiness relative? _Journal of Personality and Social Psychology, 36_: 917-927.\n\nBringslimark, Hartig, & Patil (2009). The psychological benefits of indoor plants: A critical review of the experimental literature. Journal of _Environmental Psychology, 29(4)_: 422-433.\n\nBryant & Veroff (2006)._ [Savoring: A new model of positive experience](http://www.amazon.com/Savoring-New-Model-Positive-Experience/dp/0805851208/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. Mahwah, NJ: Erlbaum.\n\nBorras, Boucherie, Mohr, Lecomte, Perroud, & Huguelet (2009). Increasing self-esteem: Efficacy for a group intervention for individuals with severe mental disorders. _European Psychiatry, 24_: 307-316.\n\nBouchard & Loehlin (2001). Genes, evolution, and personality. _Behavior Genetics, 31_: 243–273.\n\nBurton & King (2004). The health benefits of writing about intensely positive experiences. _Journal of Research in Personality, 38_: 150-163.\n\nBushman & Baumeister (1998). [Threatened egotism, narcissism, self-esteem, and direct and displaced aggression: Does self-love or self-hate lead to violence?](http://commonsenseatheism.com/wp-content/uploads/2011/03/Bushman-Baumeister-Threatened-egotism-narcissism-self-esteem-and-direct-and-displaced-aggression.pdf) _Journal of Personality and Social Psychology, 75(1)_: 219-229.\n\nBushman & Baumeister (2002). Does self-love or self-hate lead to violence? _Journal of Research in Personality, 36(6)_: 543-545.\n\nBuunk, Oldersma, & de Dreu (2001). Enhancing satisfaction through downward comparison: The role of relational discontent and individual differences in social comparison orientation. _Journal of Experimental Social Psychology, 37_: 452-467.\n\nChang, Palesh, Caldwell, Glasgow, Abramson, Luskin, Gill, Burke, & Koopman (2004). The effects of a mindfulness-based stress reduction program on stress, mindfulness self-efficacy, and positive states of mind. _Stress and Health, 20(3)_: 141-147.\n\nChida, Steptoe, & Powell (2009). [Religiosity/Spirituality and Mortality](http://commonsenseatheism.com/wp-content/uploads/2011/01/Chida-Religiosity-Spirituality-and-Mortality.pdf). _Psychotherapy and Psychosomatics_, 78(2): 81-90.\n\nCrocker & Park (2004). [The costly pursuit of self-esteem](http://commonsenseatheism.com/wp-content/uploads/2011/03/Crocker-Park-The-costly-pursuit-of-self-esteem.pdf). _Psychological Bulletin, 130_: 392-414.\n\nCsikszentmihalyi (1990). _[Flow: The Psychology of Optimal Experience](http://www.amazon.com/dp/0061339202/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: Harper and Row.\n\nCsikszentmihalyi (1998). _[Finding Flow: The Psychology of Engagement With Everyday Life](http://www.amazon.com/dp/0465024114/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. Basic Books.\n\nDiener, Wolsic, & Fujita (1995). [Physical attractiveness and subjective well-being](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Physical-Attractiveness-and-Subjective-Well-Being.pdf). _Journal of Personality and Social Psychology_, 69: 120-129.\n\nDiener, Gohm, Suh, & Oishi (2000). [Similarity of the relations between marital status and subjective well-being across cultures](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Similarity-of-the-relations-between-marital-status-and-subjective-well-being-across-cultures.pdf). _Journal of Cross-Cultural Psychology_, 31: 419-436.\n\nDiener & Seligman (2002). [Very happy people](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Very-Happy-People.pdf). _Psychological Science_, 13: 80-83.\n\nDiener & Seligman (2004). [Beyond money: Toward an economy of well-being](http://commonsenseatheism.com/wp-content/uploads/2011/01/Diener-Beyond-money.pdf). _Psychological Science in the Public Interest_, 5(1): 1-31.\n\nDiener, Kesebir, & Tov (2009). Happiness. In Leary & Hoyle (Eds.), _Handbook of Individual Differences in Social Behavior_ (pp. 147-160). New York: Guilford.\n\nEllis (2001). _[Overcoming destructive beliefs, feelings, and behaviors: New directions for Rational Emotive Behavior Therapy](http://www.amazon.com/dp/1573928798/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. Amherst, NY: Prometheus Books.\n\nEmmons & McCullough (2003). [Counting blessings versus burdens: An experimental investigation of gratitude and subjective well-being in daily life](http://commonsenseatheism.com/wp-content/uploads/2011/03/Emmons-McCullough-Counting-blessings-versus-burdens.pdf). _Journal of Personality and Social Psychology, 84_: 377-389.\n\nFeldman & Matjasko (2005). [The role of school-based extracurricular activities in adolescent development: A comprehensive review and future directions](http://commonsenseatheism.com/wp-content/uploads/2011/02/Feldman-The-role-of-school-based-extracurricular-activities-in-adolescent-development.pdf). _Review of Educational Research, 75(2)_, 159-210.\n\nFelson (1989). Parents and the reflected appraisal process: A longitudinal analysis. _Journal of Personality and Social Psychology, 56_: 965-971.\n\nFleeson, Malanos, & Achille (2002). [An intraindividual process approach to the relationship between extraversion and positive affect: is acting extraverted as \"good\" as being extraverted?](http://commonsenseatheism.com/wp-content/uploads/2011/03/Fleeson-An-intraindividual-process-approach-to-the-relationship-between-extraversion-and-positive-affect.pdf) _Journal of Personality and Social Psychology, 83(6)_: 1409-1422.\n\nFloyd, Hesse, & Pauley (2009). Writing affectionate letters reduces stress: replication and extension. Paper presented at the annual meeting of the NCA 95th annual convention, Chicago, IL, Nov. 11, 2009.\n\nFredrickson (1998). [What good are positive emotions?](http://commonsenseatheism.com/wp-content/uploads/2011/03/Fredrickson-What-good-are-positive-emotions.pdf) _Review of General Psychology, 2_: 300-319.\n\nFrey & Stutzer (2002). [What can economists learn from happiness research?](http://commonsenseatheism.com/wp-content/uploads/2011/01/Frey-What-can-economists-learn-from-happiness-research.pdf) _Journal of Economic Literature_, 40: 402-435.\n\nFurnham & Cheng (2000). Perceived parental behavior, self-esteem and happiness. _Social Psychiatry and Psychiatric Epidemiology, 35(10)_: 463-470.\n\nGardner, Csikszentmihalyi, & Damon (2002). _[Good Business: Leadership, Flow, and the Making of Meaning](http://www.amazon.com/Good-Business-Leadership-Making-Meaning/dp/014200409X/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. Basic Books.\n\nGilbert (2006). _[Stumbling on happiness](http://www.amazon.com/dp/1400077427/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: Knopf.\n\nGottman (1984). _[Why marriages succeed or fail](http://www.amazon.com/Why-Marriages-Succeed-Fail-Yours/dp/0684802414/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: Simon & Schuster.\n\nGottman, Coan, Carrere, & Swanson (1998). Predicting marital happiness and stability from newlywed interactions. _Journal of Marriage and the Family, 60_: 5-22.\n\nGraziano & Tobin (2009). Agreeableness. In Leary & Hoyle (Eds.), _Handbook of individual differences in social behavior_ (pp. 46-61). New York: Guilford.\n\nGrossman, Niemann, Schmidt, & Walach (2004). [Mindfulness-based stress reduction and health benefits: A meta-analysis](http://commonsenseatheism.com/wp-content/uploads/2011/03/Grossman-Mindfulness-based-stress-reduction-and-health-benefits.pdf). _Journal of Psychosomatic Research, 57_: 35-43.\n\nHagerty (2000). Social comparisons of income in one's community: Evidence from national surveys of income and happiness. _Journal of Personality and Social Psychology, 78_: 746-771.\n\nHahlweg, Schindler, Revensdorf, & Brengelmann (1984). The Munich marital therapy study. In Hahlweg & Jacobson (Eds.), _Marital interaction: Analysis and modification_ (pp. 3-26). New York: Guilford Press.\n\nHans (2000). [A meta-analysis of the effects of adventure programming on locus of control](http://commonsenseatheism.com/wp-content/uploads/2011/02/Hans-A-meta-analysis-of-the-effects-of-adventure-programming-on-locus-of-control.pdf). _Journal of Contemporary Psychotherapy, 30(1)_: 33-60.\n\nHarter (1998). The development of self-representations. In Eisenberg (Ed.), _Handbook of child psychology: Vol. 3. Social, emotional, and personality development_. New York: Wiley.\n\nHartig, Evans, Jamner, Davis, & Garling (2003). [Tracking restoration in natural and urban field settings](http://commonsenseatheism.com/wp-content/uploads/2011/03/Hartig-Tracking-restoration-in-natural-and-urban-field-settings.pdf). _Journal of Environmental Psychology, 23_: 109-123.\n\nHelliwell (2011). [How can subjective well-being be improved?](http://www.csls.ca/events/2011/helliwell.pdf)\n\nHeln & Singer (2008). I feel how you feel but not always: the empathic brain and its modulation. _Current Opinion in Neurobiology, 18(2)_: 153-158.\n\nHsee & Hastie (2005). [Decision and experience: Why don't we choose what makes us happy?](http://commonsenseatheism.com/wp-content/uploads/2011/03/Hsee-Hastie-Decision-and-experience-why-dont-we-choose-what-makes-us-happy.pdf) _Trends in Cognitive Sciences, 10(1)_: 31-37.\n\nInglehart (1990). _Culture shift in advanced industrial society_. Princeton, NJ: Princeton University Press.\n\nIsen (2002). [A role for neuropsychology in understanding the facilitating influence of positive affect on social behavior and cognitive processes](http://commonsenseatheism.com/wp-content/uploads/2011/03/Isen-A-role-for-neuropsychology-in-understanding-the-facilitating-influence-of-positive-affect.pdf). In Snyder & Lopez (Eds.), _Handbook of positive psychology_ (pp. 528-540). New York: Oxford University Press.\n\nIsen (2004). Some perspectives on positive feelings and emotions: Positive affect facilitates thinking and problem solving. In Manstead, Frijda, & Fischer (Eds.), _Feelings and emotions: The Amsterdam symposium_ (pp. 263-281). New York: Cambridge University Press.\n\nIsen, Daubman, & Nowicki (1987). Positive affect facilitates creative problemsolving. _Journal of Personality and Social Psychology, 52_: 1122–1131.\n\nJacobson, Schmaling, & Holtzworth-Monroe (1987). Component analysis of behavioral marital therapy: 2-year follow-up and prediction of relapse. _Journal of Marital and Family Therapy, 13_: 187-195.\n\nJohnson & Krueger (2006). \"How money buys happiness: Genetic and environmental processes linking finances and life satisfaction.\" _Journal of Personality and Social Psychology_, 90: 680-691.\n\nJudge & Klinger (2008). Job satisfaction: Subjective well-being at work. In Eid & Larsen (Eds.), _The science of subjective well-being_ (pp. 393-413). New York: Guilford.\n\nKabat-Zinn (1982). [An outpatient program in behavioral medicine for chronic pain patients based on the practice of mindfulness meditation: Theoretical considerations and preliminary results](http://commonsenseatheism.com/wp-content/uploads/2011/03/Kabat-Zinn-An-outpatient-program-in-behavioral-medicine-for-chronic-pain-patients-based-on-the-practice-of-mindfulness-meditation.pdf). _General Hospital Psychiatry, 4_: 33-47.\n\nKahneman, Krueger, Schkade, Schwarz, & Stone (2006). \"[Would you be happier if you were richer? A focusing illusion](http://commonsenseatheism.com/wp-content/uploads/2011/01/Kahneman-Would-you-be-happier-if-you-were-richer-A-focusing-illusion.pdf).\"_Science_, 312: 1908-1910.\n\nKaplan (1993). [The role of nature in the context of the workplace](http://commonsenseatheism.com/wp-content/uploads/2011/03/Kaplan-The-role-of-nature-in-the-context-of-the-workplace.pdf). _Landscaping and Urban Planning, 26_: 193-201.\n\nKaplan (2001). The nature of the view from home: Psychological benefits. _Environment and Behavior, 33(4)_: 507-542.\n\nKasser (2002). _The high prices of materialism_. Cambridge, MA: MIT Press.\n\nKasser, Ryan, Couchman, & Sheldon (2004). Materialistic values: Their causes and consequences. In Kasser & Kanner (Eds.), _Psychology and consumer culture: The struggle for a good life in a materialistic world_. Washington DC: American Psychological Association.\n\nLanger & Rodin (1976). The effects of choice and enhanced personal responsibility for the aged: A field experiment in an institutional setting. _Journal of Personality and Social Psychology, Vol 34(2)_: 191-198.\n\nLee, Cohen, Edgar, Laizner, & Gagnon (2006). Meaning-making intervention during breast or colorectal cancer treatment improves self-esteem, optimism, and self-efficacy. _Social Science & Medicine, 62(12)_: 3133-3145.\n\nLepore & Smyth, eds. (2002). _The writing cure: How expressive writing promotes health and emotional well-being_. Washington, DC: American Psychological Association.\n\nLindsley, Brass, & Thomas (1995). [Efficacy-performance spirals: A multilevel perspective](http://commonsenseatheism.com/wp-content/uploads/2011/02/Lindsley-Efficacy-performance-spirals-A-multilevel-perspective.pdf). _Academy of Management Review, 20(3)_: 645-678.\n\nLucas (2008). Personality and subjective well-being. In Eid & Larsen (Eds.), _The science of subjective well-being_ (pp. 171-194). New York: Guilford.\n\nLucas & Diener (2008). [Personality and subjective well-being](http://commonsenseatheism.com/wp-content/uploads/2011/03/Lucas-Diener-Personality-and-subjective-well-being.pdf). In John, Robins, & Pervin (Eds.), _Handbook of personality: Theory and research, 3rd ed._ (pp. 795-814). New York: Guilford.\n\nLyubomirsky, Sheldon, & Schkade (2005). [Pursuing happiness: The architecture of sustainable change](http://commonsenseatheism.com/wp-content/uploads/2011/01/Lyubomirsky-Pursuing-happiness-The-architecture-of-sustainable-change.pdf). _Review of General Psychology_, 9(2), 111-131.\n\nLyubomirsky, Tkach, & DiMatteo (2006). [What are the differences between happiness and self-esteem?](http://commonsenseatheism.com/wp-content/uploads/2011/03/Lyubomirsky-What-are-the-differences-between-happiness-and-self-esteem.pdf) _Social Indicators Research, 78_: 363-404.\n\nLyubomirsky, King, & Diener (2005). [The benefits of frequent positive affect: Does happiness lead to success?](http://commonsenseatheism.com/wp-content/uploads/2011/03/Lyubomirsky-The-benefits-of-frequent-positive-affect.pdf) _Psychological Bulletin, 131_: 803-855.\n\nLykken & Tellegen (1996). [Happiness is a stochastic phenomenon](http://commonsenseatheism.com/wp-content/uploads/2011/01/Lykken-Happiness-Is-a-Stochastic-Phenomenon.pdf). _Psychological Science_, 7: 186-189.\n\nLykken (1999). _[Happiness: The nature and nurture of joy and contentment](http://www.amazon.com/dp/0312263333/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: St. Martin's.\n\nMcNeil & Fleeson (2006). The causal effect of extraversion on positive affect and neuroticism on negative affect: Manipulating state extraversion and state neuroticism in an experimental approach. _Journal of Research in Personality, 40_: 529-550.\n\nMorris (1999). The mood system. In Kahneman, Diener, & Schwatrz (Eds.), _Well-being: The foundations of hedonic psychology_ (pp. 169-189). New York: Russell Sage Foundation.\n\nMurray & Holmes (1999). The (mental) ties that bind: Cognitive structures that predict relationship resilience. _Journal of Personality and Social Psychology, 77_: 1228-1244.\n\nMyers (1992). _The pursuit of happiness: Who is happy, and why_. New York: Morrow.\n\nMyers (1999). Close relationships and quality of life. In Kahnemann, Diener, & Schwarz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Sage.\n\nMyers & Diener (1995). [Who is happy?](http://commonsenseatheism.com/wp-content/uploads/2011/01/Myers-who-is-happy.pdf) _Psychological Science_, 6: 10-19.\n\nMyers & Diener (1997). The pursuit of happiness. _Scientific American, Special Issue 7_: 40-43.\n\nNakamura & Csikszentmihalyi (2009). Flow theory and research. In Lopez & Snyder (Eds.), _Oxford handbook of positive psychology_ (2nd ed., pp. 195-206). New York: Oxford.\n\nNickerson, Schwartz, Diener, & Kahnemann (2003). [Zeroing in on the dark side of the American dream: A closer look at the negative consequences of the goal for financial success](http://commonsenseatheism.com/wp-content/uploads/2011/01/Nickerson-Zeroing-in-on-the-dark-side-of-the-american-dream.pdf). _Psychological Science_, 14(6): 531-536.\n\nPark & Folkman (1997). Meaning in the context of stress and coping. _Review of General Psychology, 1_: 115-144.\n\nPeterson (2006)._ [A primer on positive psychology](http://www.amazon.com/Primer-Positive-Psychology-Christopher-Peterson/dp/0195188330/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: Oxford University Press.\n\nPeterson & Seligman (2004). _[Character strengths and virtues: A Handbook of classification](http://www.amazon.com/Character-Strengths-Virtues-Handbook-Classification/dp/0195167015/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: Oxford University Press.\n\nPrice (2008). Research roundup: Get out of town. _gradPSYCH, 6(3)_: 10.\n\nPronin & Jacobs (2008). [Thought speed, mood, and the experience of mental motion](http://commonsenseatheism.com/wp-content/uploads/2011/03/Pronin-Jacobs-Thought-speed-mood-and-the-experience-of-mental-motion.pdf). _Perspectives on Psychological Science, 3_: 461-485.\n\nProulx, Helms, & Cheryl (2007). [Marital quality and personal well-being: A meta-analysis](http://commonsenseatheism.com/wp-content/uploads/2011/01/Proulx-Marital-Quality-and-Personal-Well‐Being-A-Meta‐Analysis.pdf). _Journal of Marriage and Family_, 69: 576-593.\n\nReker, Peacock, & Wong (1987). Meaning and purpose in life and well-being: a life-span perspective. _The Journal of Gerontology, 42(1)_: 44-49.\n\nRiis, Loewenstein, Baron, Jepson, Fagerlin, & Ubel (2005). Ignorance of hedonic adaptation to hemodialysis: A study using ecological momentary assessment. _Journal of Experimental Psychology: General_, 134: 3-9.\n\nRoberts, Jackson, Fayard, Edmonds, & Meints (2009). Conscientiousness. In Leary & Hoyle (Eds.), _Handbook of individual differences in social behavior_ (pp. 369-381). New York: Guilford.\n\nRoss & Van Willigen (1997). [Education and the subjective quality of life](http://commonsenseatheism.com/wp-content/uploads/2011/01/Ross-Education-and-the-subjective-quality-of-life.pdf). _Journal of Health & Social Behavior_, 38: 275-297.\n\nSchwartz & Strack (1999). Reports of subjective well-being: Judgmental processes and their methodological implications. In Kahneman, Diener, & Schwartz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Russell Sage Foundation.\n\nSeery, Silver, Holman, Ence, & Chu (2008). Expressing thoughts and feelings following a collective trauma: Immediate responses to 9/11 predict negative outcomes in a national sample. _Journal of Consulting and Clinical Psychology, 76(4)_: 657-667.\n\nShapiro, Schwartz, & Bonner (1998). The effects of mindfulness-based stress reduction on medical and pre-medical students. _Journal of Behavioral Medicine, 21_: 581-599.\n\nSolberg, Diener, Wirtz, Lucas, & Oishi (2002). [Wanting, having, and satisfaction: Examining the role of desire discrepancies in satisfaction with income](http://commonsenseatheism.com/wp-content/uploads/2011/01/Solberg-Wanting-Having-and-Satisfaction-Examining-the-Role-of-Desire-Discrepancies-in-Satisfaction-With-Income.pdf). _Journal of Personality and Social Psychology_, 83(3): 725-734.\n\nSpera, Buhrfeind, & Pennebaker (1994). [Expressive writing and coping with job loss](http://commonsenseatheism.com/wp-content/uploads/2011/03/Spera-Expressive-writing-and-coping-with-job-loss.pdf). _Academy of Management Journal, 3_: 72-733.\n\nStepien & Baernstein (2006). Educating for empaty: a review. _Journal of General Internal Medicine, 21(5)_: 524-530.\n\nSteptoe, Wardle, & Marmot (2005). [Positive affect and health-related neuroendocrine, cardiovascular, and inflammatory processes](http://commonsenseatheism.com/wp-content/uploads/2011/03/Steptoe-Positive-affect-and-health-related-neuroendocrine-cardiovascular-and-inflammatory-processes.pdf). _Proceedings of the National Academy of Sciences of the United States of America, 102(18)_: 6508-6512.\n\nStubbe, Posthuma, Boomsa, & De Geus (2005). [Heritability and life satisfaction in adults: A twin-family study](http://commonsenseatheism.com/wp-content/uploads/2011/01/Stubbe-Heritability-of-life-satisfaction-in-adults-A-twin-family-study.pdf). _Psychological Medicine_, 35: 1581-1588.\n\nTennessen & Cimprich (1995). Views to nature: Effects on attention. _Journal of Environmental Psychology, 15_: 77-85.\n\nUlmer, Range, & Smith (1991). Purpose in life: a moderator of recovery from bereavement. _Journal of Death and Dying, 23(4)_: 279-289.\n\nVan Boven (2005). Experientialism, materialism, and the pursuit of happiness. _Review of General Psychology_, 9(2): 132-142.\n\nde Vignemont & Singer (2006). The empathic brain: how, when, and why? _Trends in Cognitive Sciences, 10(10)_: 435-441.\n\nWarr (1999). Well-being and the workplace. In Kahneman, Diener, & Schwartz (Eds.), _Well-being: The foundations of hedonic psychology_. New York: Sage.\n\nWegner (1989). _[White bears and other unwanted thoughts: Suppression, obsession, and the psychology of mental control](http://www.amazon.com/White-Bears-Other-Unwanted-Thoughts/dp/0898622239/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0131103628&linkCode=as2&tag=lesswrong-20)_. New York: Viking.\n\nWeiss, Bates, & Luciano (2008). [Happiness is a personal(ity) thing](http://commonsenseatheism.com/wp-content/uploads/2011/03/Weiss-Happiness-is-a-personality-thing.pdf). _Psychology Science, 19_: 205-210.\n\nWilson & Gilbert (2005). Affective forecasting: Knowing what to want. _Current Directions in Psychological Science, 14_: 131-134.\n\nWissink, Dekovic, & Meijer (2006). Parenting behavior, quality of the parent-adolescent relationship, and adolescent functioning in four ethnic groups. _Journal of Early Adolescence, 26_: 133-159.\n\nZech & Rimé (2005). Is talking about an emotional experience helpful? Effects on emotional recovery and perceived benefits. _Clinical Psychology and Psychotehrapy, 12_: 270-287."
          },
          "voteCount": 192
        },
        {
          "name": "Righting a Wrong Question",
          "type": "post",
          "slug": "righting-a-wrong-question",
          "_id": "rQEwySCcLtdKHkrHp",
          "url": null,
          "title": "Righting a Wrong Question",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Introspection"
            },
            {
              "name": "Causality"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "When you are faced with an _unanswerable_ question—a question to which it seems impossible to even _imagine_ an answer—there is a simple trick which can turn the question solvable.\n\nCompare:\n\n*   \"Why do I have free will?\"\n*   \"Why do I think I have free will?\"\n\nThe nice thing about the second question is that it is _guaranteed_ to have a real answer, _whether or not_ there is any such thing as free will.  Asking \"Why do I have free will?\" or \"Do I have free will?\" sends you off thinking about tiny details of the laws of physics, so distant from the macroscopic level that you couldn't begin to see them with the naked eye.  And you're asking \"Why is X the case?\" where X may not be _coherent,_ let alone the case.\n\n\"Why do I _think_ I have free will?\", in contrast, is guaranteed answerable.  You do, in fact, believe you have free will.  This belief seems far more solid and graspable than the ephemerality of free will.  And there is, _in fact,_ some nice solid chain of cognitive cause and effect leading up to this belief.\n\nIf you've already outgrown free will, choose one of these substitutes:\n\n*   \"Why does time move forward instead of backward?\" versus \"Why do I think time moves forward instead of backward?\"\n*   \"Why was I born as myself rather than someone else?\" versus \"Why do I think I was born as myself rather than someone else?\"\n*   \"Why am I conscious?\" versus \"Why do I think I'm conscious?\"\n*   \"Why does reality exist?\" versus \"Why do I think reality exists?\"\n\nThe beauty of this method is that it works _whether or not_ the question is confused.  As I type this, I am wearing socks.  I could ask \"Why am I wearing socks?\" or \"Why do I believe I'm wearing socks?\"  Let's say I ask the second question.  Tracing back the chain of causality, I find:\n\n*   I believe I'm wearing socks, because I can see socks on my feet.\n*   I see socks on my feet, because my retina is sending sock signals to my visual cortex.\n*   My retina is sending sock signals, because sock-shaped light is impinging on my retina.\n*   Sock-shaped light impinges on my retina, because it reflects from the socks I'm wearing.\n*   It reflects from the socks I'm wearing, because I'm wearing socks.\n*   I'm wearing socks because I put them on.\n*   I put socks on because I believed that otherwise my feet would get cold.\n*   &c.\n\nTracing back the chain of causality, step by step, I discover that my belief that I'm wearing socks is fully explained by the fact that I'm wearing socks.  This is right and proper, as [you cannot gain information about something without interacting with it](/lw/o6/perpetual_motion_beliefs/).\n\nOn the other hand, if I see a mirage of a lake in a desert, the correct causal explanation of my vision does not involve the fact of any actual lake in the desert.  In this case, my belief in the lake is not just _explained,_ but _explained away._\n\nBut _either way,_ the belief itself is a real phenomenon taking place in the real universe—psychological events are events—and its causal history can be traced back.\n\n\"Why is there a lake in the middle of the desert?\" may fail if there is no lake to be explained.  But \"Why do I _perceive_ a lake in the middle of the desert?\" always has a causal explanation, one way or the other.\n\nPerhaps someone will see an opportunity to be clever, and say:  \"Okay.  I believe in free will because I have free will.  There, I'm done.\"  Of course it's not that easy.\n\nMy perception of socks on my feet, is an event in the visual cortex.  The workings of the visual cortex can be investigated by cognitive science, should they be confusing.\n\nMy retina receiving light is not a mystical sensing procedure, a magical sock detector that lights in the presence of socks for no explicable reason; there are mechanisms that can be understood in terms of biology.  The photons entering the retina can be understood in terms of optics.  The shoe's surface reflectance can be understood in terms of electromagnetism and chemistry.  My feet getting cold can be understood in terms of thermodynamics.\n\nSo it's not as easy as saying, \"I believe I have free will because I have it—there, I'm done!\"  You have to be able to break the causal chain into smaller steps, and explain the steps in terms of elements not themselves confusing.\n\nThe mechanical interaction of my retina with my socks is quite clear, and can be described in terms of non-confusing components like photons and electrons.  Where's the free-will-sensor in your brain, and how does it detect the presence or absence of free will?  How does the sensor interact with the sensed event, and what are the mechanical details of the interaction?\n\nIf your belief does derive from valid observation of a real phenomenon, we will eventually reach that fact, if we start tracing the causal chain backward from your belief.\n\nIf what you are really seeing is your own confusion, tracing back the chain of causality will find an algorithm that [runs skew to reality](/lw/of/dissolving_the_question/).\n\nEither way, the question is guaranteed to have an answer.  You even have a nice, concrete place to begin tracing—your belief, sitting there solidly in your mind.\n\nCognitive science may not seem so lofty and glorious as metaphysics.  But at least questions of cognitive science are _solvable._  Finding an answer may not be _easy,_ but at least an answer _exists._\n\nOh, and also: the idea that cognitive science is not so lofty and glorious as metaphysics is simply wrong.  Some readers are beginning to notice this, I hope."
          },
          "voteCount": 90
        },
        {
          "name": "Self-fulfilling correlations",
          "type": "post",
          "slug": "self-fulfilling-correlations",
          "_id": "XuyRMxky6G8gq7a69",
          "url": null,
          "title": "Self-fulfilling correlations",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Causality"
            },
            {
              "name": "World Modeling"
            },
            {
              "name": "AI"
            },
            {
              "name": "Self Fulfilling/Refuting Prophecies"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Perceived causation causes correlation",
                "anchor": "Perceived_causation_causes_correlation",
                "level": 1
              },
              {
                "title": "Conditions under which this occurs",
                "anchor": "Conditions_under_which_this_occurs",
                "level": 1
              },
              {
                "title": "Application to machine learning and smart people",
                "anchor": "Application_to_machine_learning_and_smart_people",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "50 comments"
              }
            ],
            "headingsCount": 5
          },
          "contents": {
            "markdown": "Correlation does not imply causation.  Sometimes corr(X,Y) means X=>Y; sometimes it means Y=>X; sometimes it means W=>X, W=>Y.  And sometimes it's an artifact of people's beliefs about corr(X, Y).  With intelligent agents, perceived causation causes correlation.\n\nVolvos are believed by many people to be safe.  Volvo has an excellent record of being concerned with safety; they introduced 3-point seat belts, crumple zones, laminated windshields, and safety cages, among other things.  But how would you evaluate the claim that Volvos are safer than other cars?\n\nPresumably, you'd look at the accident rate for Volvos compared to the accident rate for similar cars driven by a similar demographic, as reflected, for instance in insurance rates.  (My google-fu did not find accident rates posted on the internet, but [insurance rates](http://www.leaseguide.com/articles/autoinsurance_bestcars.htm) don't come out especially pro-Volvo.)  But suppose the results showed that Volvos had only 3/4 as many accidents as similar cars driven by similar people.  Would that prove Volvos are safer?\n\nPerceived causation causes correlation\n--------------------------------------\n\nNo.  Besides having a reputation for safety, Volvos also have a reputation for being overpriced and ugly.  Mostly people who are concerned about safety buy Volvos.  Once the reputation exists, even if it's not true, a cycle begins that feeds on itself:  Cautious drivers buy Volvos, have fewer accidents, resulting in better statistics, leading more cautious drivers to buy Volvos.\n\nDo Montessori schools or home-schooling result in better scores on standardized tests?  I'd bet that they do.  Again, my google-fu is not strong enough to find any actual reports on, say, average SAT-score increases for students in Montessori schools vs. public schools.  But the largest observable factor determining student test scores, last I heard, is participation by the parents.  Any new education method will show increases in student test scores if people believe it results in increases in student test scores, because only interested parents will sign up for that method.  The crazier, more-expensive, and more-difficult the method is, the more improvement it should show; craziness should filter out less-committed parents.\n\nAre vegetarian diets or yoga healthy for you?  Does using the phone while driving increase accident rates?  Yes, probably; but there is a self-fulfilling component in the data that is difficult to factor out.\n\nConditions under which this occurs  \n\n-------------------------------------\n\nIf you believe X helps you achieve Y, and so you use X when you are most-motivated to achieve Y _and your motivation has some bearing on the outcome_, you will observe a correlation between X and Y.\n\nThis won't happen if your motivation or attitude has no bearing on the outcome (beyond your choice of X).  If passengers prefer one airline based on their perception of its safety, that won't make its safety record improve.\n\nHowever, this is different from either confidence or the placebo effect.  I'm not talking about the PUA mantra that \"if you believe a pickup line will work, it will work\".  And I'm not talking about feeling better when you take a pill that you think will help you feel better.  This is a _sample-selection bias_.  A person is more likely to choose X when they are motivated to achieve Y _relative to other possible positive_ outcomes of X, and hence more inclined to make many other little trade-offs to achieve Y which will not be visible in the data set.\n\nIt's also not the effect people are guarding against with double-blind experiments.  That's guarding against the experimenter favoring one method over another.  This is, rather, an effect guarded against with random assignment to different groups.\n\nNor should it happen in cases where the outcome being studied is the _only_ outcome people consider.  If a Montessori school cost the same, and was just as convenient for the parents, as every other school, and all factors other than test score were equal, and Montessori schools were believed to increase test scores, then any parent who cared at all would choose the Montessori school.  The filtering effect would vanish, and so would the portion of the test-score increase caused by it.  Same story if one choice improves all the outcomes under consideration:  Aluminum tennis racquets are better than wooden racquets in weight, sweet spot size, bounce, strength, air resistance, longevity, time between restrings, and cost.  You need not suspect a self-fulfilling correlation.\n\nIt may be cancelled by a balancing effect, when you are more highly-motivated to achieve Y when you are less likely to achieve Y.  In sports, if you wear your lucky undershirt only for tough games, you'll find it appears to be unlucky, because you're more likely to lose tough games.  Another balancing effect is if your choice of X makes you feel so confident of attaining Y that you act less concerned about Y; an example is (IIRC) research showing that people wearing seat-belts are more likely to get into accidents.\n\nApplication to machine learning and smart people  \n\n---------------------------------------------------\n\nBack in the late 1980s, neural networks were hot; and evaluations usually indicated that they outperformed other methods of classification.  In the early 1990s, genetic algorithms were hot; and evaluations usually indicated that they outperformed other methods of classification.  Today, support vector machines (SVMs) are hot; and evaluations usually indicate that they outperform other methods of classifications.  Neural networks and genetic algorithms no longer outperform older methods.  (I write this from memory, so you shouldn't take it as gospel.)\n\nThere is a publication bias:  When a new technology appears, publications indicating it performs well are interesting.  Once it's established, publications indicating it performs poorly are interesting.  But there's also a selection bias.  People strongly motivated to make their systems work well on difficult problems are strongly motivated to try new techniques; and also to fiddle with the parameters until they work well.\n\nFads can create self-fulfilling correlations.  If neural networks are hot, the smartest people tend to work on neural networks.  When you compare their results to other results, it can be difficult to look at neural networks vs., say, logistic regression; and factor out the smartest people vs. pretty smart people effect.\n\n(The attention of smart people is a proxy for effectiveness, which often misleads other smart people - e.g., the popularity of communism among academics in America in the 1930s.  But that's yet another separate issue.)"
          },
          "voteCount": 117
        },
        {
          "name": "Why Truth?",
          "type": "post",
          "slug": "why-truth",
          "_id": "YshRbqZHYFoEMqFAu",
          "url": null,
          "title": "Why Truth?",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Truth, Semantics, & Meaning"
            },
            {
              "name": "Emotions"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The goal of instrumental rationality mostly speaks for itself. Some commenters have wondered, on the other hand, why rationalists care about truth. Which invites a few different answers, depending on who you ask; and these different answers have differing characters, which can shape the search for truth in different ways.\n\nYou might hold the view that pursuing truth is inherently noble, important, and worthwhile. In which case your priorities will be determined by your ideals about which truths are most important, or about when truthseeking is most virtuous.\n\nThis motivation tends to have a moral character to it. If you think it your duty to look behind the curtain, you are a lot more likely to believe that someone *else* should look behind the curtain too, or castigate them if they deliberately close their eyes.\n\nI tend to be suspicious of morality as a motivation for rationality, *not* because I reject the moral ideal, but because it invites certain kinds of trouble. It is too easy to acquire, as learned moral duties, modes of thinking that are dreadful missteps in the dance.\n\nConsider Spock, the naive archetype of rationality. Spock's affect is always set to “calm,” even when wildly inappropriate. He often gives many significant digits for probabilities that are grossly uncalibrated.^1^ Yet this popular image is how many people conceive of the duty to be “rational”—small wonder that they do not embrace it wholeheartedly.\n\nTo make rationality into a moral duty is to give it all the dreadful degrees of freedom of an arbitrary tribal custom. People arrive at the wrong answer, and then indignantly protest that they acted with propriety, rather than learning from their mistake.\n\nWhat other motives are there?\n\nWell, you might want to accomplish some specific real-world goal, like building an airplane, and therefore you need to know some specific truth about aerodynamics. Or more mundanely, you want chocolate milk, and therefore you want to know whether the local grocery has chocolate milk, so you can choose whether to walk there or somewhere else.\n\nIf this is the reason you want truth, then the priority you assign to your questions will reflect the expected utility of their information—how much the possible answers influence your choices, how much your choices matter, and how much you expect to find an answer that changes your choice from its default.\n\nTo seek truth merely for its instrumental value may seem impure—should we not desire the truth for its own sake?—but such investigations are extremely important because they create an outside criterion of verification: if your airplane drops out of the sky, or if you get to the store and find no chocolate milk, its a hint that you did something wrong. You get back feedback on which modes of thinking work, and which don't.\n\nAnother possibility: you might care about whats true because, damn it, you're *curious*.\n\nAs a reason to seek truth, curiosity has a special and admirable purity. If your motive is curiosity, you will assign priority to questions according to how the questions, themselves, tickle your aesthetic sense. A trickier challenge, with a greater probability of failure, may be worth more effort than a simpler one, just because it's more fun.\n\nCuriosity and morality can both attach an intrinsic value to truth. Yet being curious about whats behind the curtain is a very different state of mind from believing that you have a moral duty to look there. If you're curious, your priorities will be determined by which truths you find most intriguing, not most important or most useful.\n\nAlthough pure curiosity is a wonderful thing, it may not linger too long on verifying its answers, once the attractive mystery is gone. Curiosity, as a human emotion, has been around since long before the ancient Greeks. But what set humanity firmly on the path of Science was noticing that certain modes of thinking uncovered beliefs that let us *manipulate the world*—truth as an instrument. As far as sheer curiosity goes, spinning campfire tales of gods and heroes satisfied that desire just as well, and no one realized that anything was wrong with that.\n\nAt the same time, if we're going to improve our skills of rationality, go beyond the standards of performance set by hunter-gatherers, we'll need deliberate beliefs about how to think—things that look like norms of rationalist “propriety.” When we write new mental programs for ourselves, they start out as explicit injunctions, and are only slowly (if ever) trained into the neural circuitry that underlies our core motivations and habits.\n\nCuriosity, pragmatism, and quasi-moral injunctions are all key to the rationalist project. Yet if you were to ask me which of these is most foundational, I would say: “curiosity.” I have my principles, and I have my plans, which may well tell me to look behind the curtain. But then, I also just really want to know. What will I see? The world has handed me a puzzle, and a solution feels tantalizingly close.\n\n* * *\n\n^1^ E.g., “Captain, if you steer the Enterprise directly into that black hole, our probability of surviving is only 2.234%.” Yet nine times out of ten the *Enterprise* is not destroyed. What kind of tragic fool gives four significant digits for a figure that is off by two orders of magnitude?"
          },
          "voteCount": 124
        },
        {
          "name": "You Only Live Twice",
          "type": "post",
          "slug": "you-only-live-twice",
          "_id": "yKXKcyoBzWtECzXrE",
          "url": null,
          "title": "You Only Live Twice",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Cryonics"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "\"It just so happens that your friend here is only _mostly_ dead.  There's a big difference between _mostly_ dead and _all_ dead.\"  \n        \\-\\- _The Princess Bride_\n\nMy co-blogger Robin and I may disagree on how fast an AI can improve itself, but we agree on an issue that seems much simpler to us than that:  **At the point where the current legal and medical system gives up on a patient, they aren't really dead.**\n\nRobin has [already said](http://www.overcomingbias.com/2008/12/we-agree-get-froze.html) much of what needs saying, but a few more points:\n\n• [Ben Best's Cryonics FAQ](http://www.benbest.com/cryonics/CryoFAQ.html), [Alcor's FAQ](http://www.alcor.org/FAQs/index.html), [Alcor FAQ for scientists](http://www.alcor.org/sciencefaq.htm), [Scientists' Open Letter on Cryonics](http://www.imminst.org/cryonics_letter/)\n\n• I know more people who are planning to sign up for cryonics Real Soon Now than people who have actually signed up.  **I expect that more people have _died while cryocrastinating_ than have _actually been cryopreserved_.**  If you've _already decided_ this is a good idea, but you \"haven't gotten around to it\", sign up for cryonics NOW.  I mean _RIGHT NOW._  Go to the website of [Alcor](http://www.alcor.org/BecomeMember/index.html) or the [Cryonics Institute](http://cryonics.org/become.html) and follow the instructions.\n\n• Cryonics is usually funded through life insurance.  The following conversation from an Overcoming Bias meetup is worth quoting:\n\nHim:  I've been thinking about signing up for cryonics when I've got enough money.\n\nMe:  Um... it doesn't take all that much money.\n\nHim:  It doesn't?\n\nMe:  Alcor is the high-priced high-quality organization, which is something like $500-$1000 in annual fees for the organization, I'm not sure how much.  I'm young, so I'm signed up with the Cryonics Institute, which is $120/year for the membership.  I pay $180/year for more insurance than I need - it'd be enough for Alcor too.\n\nHim:  That's ridiculous.\n\nMe:  Yes.\n\nHim:  No, really, that's _ridiculous._  If that's true then my decision isn't just determined, it's overdetermined.\n\nMe:  Yes.  And there's around a thousand people worldwide \\[actually 1400\\] who are signed up for cryonics.  Figure that at most a quarter of those did it for systematically rational reasons.  That's a high upper bound on the number of people on Earth who can reliably reach the right conclusion on massively overdetermined issues.\n\n• Cryonics is not marketed well - or at all, really.  There's no salespeople who get commissions.  There is _no one to hold your hand through signing up_, so you're going to have to get the papers signed and notarized yourself.  The closest thing out there might be [Rudi Hoffman](http://www.rudihoffman.com/), who sells life insurance with cryonics-friendly insurance providers (I went through him).\n\n• If you want to _securely_ erase a hard drive, it's not as easy as writing it over with zeroes.  Sure, an \"erased\" hard drive like this won't boot up your computer if you just plug it in again.  But if the drive falls into the hands of a specialist with a scanning tunneling microscope, they can tell the difference between \"this was a 0, overwritten by a 0\" and \"this was a 1, overwritten by a 0\".\n\nThere are programs advertised to \"securely erase\" hard drives using many overwrites of 0s, 1s, and random data.  But if you want to keep the secret on your hard drive secure against _all possible future technologies that might ever be developed,_ then cover it with [thermite](http://www.youtube.com/watch?v=WrCWLpRc1yM) and set it on fire.  It's the only way to be sure.\n\n_Pumping someone full of cryoprotectant and gradually lowering their temperature until they can be stored in liquid nitrogen_ is not a secure way to erase a person.\n\nSee also the [information-theoretic criterion of death](http://en.wikipedia.org/wiki/Information_theoretical_death).\n\n• You don't have to buy what's usually called the \"patternist\" philosophy of identity, to sign up for cryonics.  After reading all the information off the brain, you could put the \"same atoms\" back into their old places.\n\n• \"Same atoms\" is in scare quotes because our current physics _prohibits_ particles from possessing individual identities.  It's a much stronger statement than \"we can't tell the particles apart with current measurements\" and has to do with the notion of configuration spaces in quantum mechanics.  This is a standard idea in QM, _not_ an unusual woo-woo one - see [this sequence on Overcoming Bias](http://www.overcomingbias.com/2008/06/qm-and-identity.html) for a gentle introduction.  Although patternism is not _necessary_ to the cryonics thesis, we happen to live in a universe where \"the same atoms\" is physical nonsense.\n\nThere's a number of intuitions we have in our brains for processing a world of distinct physical objects, built in from a very young age.  These intuitions, which may say things like \"If an object disappears, and then comes back, it isn't the same object\", are tuned to our macroscopic world and generally don't match up well with _fundamental_ physics.  Your identity is not like a little billiard ball that follows you around - there aren't _actually_ any billiard balls down there.\n\n_Separately and convergently,_ more abstract reasoning strongly suggests that \"identity\" should not be epiphenomenal; that is, you should not be able to change someone's identity without changing any observable fact about them.\n\nIf you go through [the aforementioned Overcoming Bias sequence](http://www.overcomingbias.com/2008/06/qm-and-identity.html), you should actually be able to _see intuitively_ that successful cryonics preserves anything about you that is preserved by going to sleep at night and waking up the next morning.\n\n• Cryonics, to me, makes two statements.\n\nThe first statement is about [systematically valuing human life](http://intelligence.org/blog/2007/06/16/transhumanism-as-simplified-humanism/).  It's bad when a pretty young white girl goes missing somewhere in America.  But when 800,000 Africans get murdered in Rwanda, that gets 1/134 the media coverage of the Michael Jackson trial.  It's sad, to be sure, but no cause for emotional alarm.  When brown people die, that's _all part of the plan_ \\- as a smiling man once said.\n\nCryonicists are people who've decided that their deaths, and the deaths of their friends and family and the rest of the human species, are [not part of the plan](http://yudkowsky.net/other/yehuda).\n\nI've met one or two Randian-type \"selfish\" cryonicists, but they aren't a majority.  Most people who sign up for cryonics wish that everyone would sign up for cryonics.\n\nThe second statement is that you have at least a _little_ hope in the future.  Not faith, not blind hope, not irrational hope - just, any hope at all.\n\nI was once at a table with Ralph Merkle, talking about how to market cryonics if anyone ever gets around to marketing it, and Ralph suggested a group of people in a restaurant, having a party; and the camera pulls back, and moves outside the window, and the restaurant is on the Moon.  Tagline:  \"Wouldn't you want to be there?\"\n\nIf you look back at, say, the Middle Ages, things were worse then.  I'd rather live here then there.  I have hope that humanity will move forward _further_, and that's something that I want to see.\n\nAnd I hope that the idea that people are disposable, and that their deaths are part of the plan, is something that fades out of the Future.\n\nOnce upon a time, infant deaths were part of the plan, and now they're not.  Once upon a time, slavery was part of the plan, and now it's not.  Once upon a time, dying at thirty was part of the plan, and now it's not.  That's a psychological shift, not just an increase in living standards.  Our era doesn't value human life with perfect consistency - but the value of human life is higher than it once was.\n\nWe have a concept of what a medieval peasant _should_ have had, the dignity with which they _should_ have been treated, that is higher than what they would have thought to ask for themselves.\n\nIf no one in the future cares enough to save people who can be saved... well.  In cryonics there is an element of taking responsibility for the Future.  You may be around to reap what your era has sown.  It is not just my _hope_ that the Future be a better place; it is my _responsibility._  If I thought that we were on track to a Future where no one cares about human life, and lives that could easily be saved are just thrown away - then I would try to change that.  Not everything worth doing is easy.\n\n_Not_ signing up for cryonics - what does that say?  That you've lost hope in the future.  That you've lost your will to live.  That you've stopped believing that human life, and your own life, is something of value.\n\nThis can be a painful world we live in, and the media is always telling us how much worse it will get.  If you spend enough time not looking forward to the next day, it damages you, after a while.  You lose your ability to hope.  Try telling someone already grown old to sign up for cryonics, and they'll tell you that they don't want to be old forever - that they're tired.  If you try to explain to someone already grown old, that the nanotechnology to revive a cryonics patient is sufficiently advanced that reversing aging is almost trivial by comparison... then it's not something they can imagine on an emotional level, no matter what they believe or don't believe about future technology.  They can't imagine not being tired.  I think that's true of a lot of people in this world.  If you've been hurt enough, you can no longer imagine healing.\n\nBut things really were a lot worse in the Middle Ages.  And they really are a lot better now.  Maybe humanity _isn't_ doomed.  The Future could be something that's worth seeing, worth living in.  And it may have a concept of sentient dignity that values your life more than you dare to value yourself.\n\nOn behalf of the Future, then - please ask for a little more for yourself.  More than death.  It really... isn't being selfish.  _I_ want you to live.  I think that the Future will want you to live.  That if you let yourself die, people who aren't even born yet will be sad for the irreplaceable thing that was lost.\n\nSo please, live.\n\nMy brother didn't.  My grandparents won't.  But everything we can hold back from the Reaper, even a single life, is precious.\n\nIf other people want you to live, then it's not just you doing something selfish and unforgivable, right?\n\nSo I'm saying it to you.\n\nI want you to live."
          },
          "voteCount": 131
        },
        {
          "name": "Less Wrong NYC: Case Study of a Successful Rationalist Chapter",
          "type": "post",
          "slug": "less-wrong-nyc-case-study-of-a-successful-rationalist",
          "_id": "CsKboswS3z5iaiutC",
          "url": null,
          "title": "Less Wrong NYC: Case Study of a Successful Rationalist Chapter",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Community"
            },
            {
              "name": "Meetups & Local Communities (topic)"
            },
            {
              "name": "Practical"
            },
            {
              "name": "Group Rationality"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Genesis, or a Brief History of Nearly Everything",
                "anchor": "Genesis__or_a_Brief_History_of_Nearly_Everything",
                "level": 1
              },
              {
                "title": "Lessons",
                "anchor": "Lessons",
                "level": 1
              },
              {
                "title": "Building a Community",
                "anchor": "Building_a_Community",
                "level": 2
              },
              {
                "title": "Meetup topics",
                "anchor": "Meetup_topics",
                "level": 3
              },
              {
                "title": "Group Rationality",
                "anchor": "Group_Rationality",
                "level": 2
              },
              {
                "title": "The Road Ahead",
                "anchor": "The_Road_Ahead",
                "level": 1
              },
              {
                "title": "Call to Assemble",
                "anchor": "Call_to_Assemble",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "172 comments"
              }
            ],
            "headingsCount": 9
          },
          "contents": {
            "markdown": "It is perhaps the best-kept secret on Less Wrong that the [New York City community](http://wiki.lesswrong.com/wiki/NYC_meetup_group) has been meeting regularly for almost two years. For nearly a year we've been meeting weekly or more.  The rest of this post is going to be a practical guide to the benefits of group rationality, but first I will do something that is still too rare on this blog: [make it clear how strongly I feel about this](/lw/hp/feeling_rational). Before this community took off, _I did not believe that life could be this much fun or that I could possibly achieve such a sustained level of happiness._\n\nBeing rational in an irrational world is incredibly lonely. Every interaction reveals that our thought processes differ widely from those around us, and I had accepted that such a divide would always exist. For the first time in my life I have dozens of people with whom I can act freely and _revel in the joy of rationality_ without any social concern - hell, it's actively rewarded! Until the NYC Less Wrong community formed, I didn't realize that I was a forager lost without a tribe...\n\nRationalists are still human, and we still have basic human needs. lukeprog [summarizes the literature on subjective well-being](/lw/3nn/scientific_selfhelp_the_state_of_our_knowledge), and the only factors which correlate to any degree are genetics, health, work satisfaction and social life - which actually gets listed _three separate times_ as social activity, relationship satisfaction and religiosity. Rationalists tend to be [less socially adept](/lw/28l/do_you_have_highfunctioning_aspergers_syndrome) [on average](/lw/2am/aspergers_survey_reresults), and this can make it difficult to obtain the full rewards of social interaction. However, once rationalists learn to socialize with each other, they also become increasingly social towards everyone more generally. This improves your life. _A lot._\n\nWe are a group of friends to enjoy life alongside, while we [try miracle fruit](http://en.wikipedia.org/wiki/Synsepalum_dulcificum), dance ecstatically until sunrise, actively embarrass ourselves at karaoke, get lost in the woods, and jump off waterfalls.  Poker, paintball, parties, go-karts, concerts, camping... I have a community where I can live in truth and be accepted as I am, where I can give and receive feedback and get help [becoming stronger](http://wiki.lesswrong.com/wiki/Tsuyoku_naritai). I am immensely grateful to have all of these people in my life, and I look forward to every moment I spend with them. To love and be loved is an unparalleled experience in this world, once you actually try it.\n\nSo, you ask, how did all of this get started...?\n\nGenesis, or a Brief History of Nearly Everything\n------------------------------------------------\n\nThe origin of the NYC chapter was the [April 24th, 2009 meetup](http://www.overcomingbias.com/2009/04/nyc-meetup-friday-7pm.html) that Robin Hanson organized when he came to the city for a prediction markets conference.  Approximately 15 people attended over the course of the night, and we all agreed that we had way too much fun together not to do this on a regular basis. I handed out my business cards to everyone there, told them to e-mail me, and I would create a mailing list. Thus [Overcoming Bias NYC](http://groups.google.com/group/overcomingbiasnyc) was born.\n\nIt was clear from the very beginning that Jasen Murray was the person most interested in seeing this happen, so he became the organizer of the group for the first year of its existence. At first the times and locations were impromptu, but in August Jasen made the brilliant move of precommitting to be at a specific time and place for a minimum of two hours twice per month. Because enough of us liked Jasen and wanted to hang out with him anyway, several people began showing up every time and a regular meetup was established. Going forward we tried a combination of social meetups, focused discussions and game nights. Jasen also attempted to shift coordination from the mailing list to the [Meetup group](http://www.meetup.com/Less-Wrong-Overcoming-Bias-NYC/), but Meetup is not a great mailing list and people were loathe to use multiple services. That now serves as our public face.\n\nIn April 2010, Jasen departed to run the Visiting Fellows program at SIAI, and I became the group's organizer. We immediately agreed on a number of changes: weekly meetups (with game nights every other week), focused discussions addressing specific problems instead of general theory, and a temporary taboo on discussion of AGI/FAI. We also moved the majority of our meetups from a public diner to a private residence, which avoided a lot of hassles with loud crowds, ordering of food, etc.  These changes marked our transition to a social group that focused on practical life benefits. June brought two more key changes: we started holding strategy sessions on request to help members optimize their lives, and I started hugging people, which began a cascade of increasing physical contact. That summer brought an increased interest in skill sharing, a reduced game night frequency, and meetups focused around specific topics. That fall we began using the group more for discussions, sharing social events of mutual interest, and coordinating activities together outside of the weekly meetups.\n\nThen, in October, things began to accelerate. I told everyone on the list to respond or be removed, to get an idea of numbers and to galvanize the core membership. Several members broke off old relationships and some of them entered new ones within the group. More women started attending; we had previously been almost all male. We began having more contact with the west coast rationalists, including visits by Jasen and Michael Vassar and an extended stay by [Divia](http://meaningandmagic.com/?c=1), which brought valuable new memes to our community. Self-reported levels of fun and happiness began to radically increase. Mailing list discussions turned towards asking for practical advice. The meetups took on a self-improvement focus, with weekly goal-setting and accountability. Andrew Rettek began a [public lecture series](http://www.meetup.com/Less-Wrong-Overcoming-Bias-NYC/events/16332288/) presenting the Sequences. Demand for more-than-weekly meetups grew...\n\nLessons\n-------\n\nNYC has pioneered creating [rationalist communities](/lw/5v/church_vs_taskforce). While we have largely proceeded via trial and error, the rest of you who are going to become organizers can learn from our experiments and avoid a lot of mistakes. The lessons largely fall under two categories: how to build a group, and what to do with a group once you have one. I hope that you find this advice helpful in your own efforts to establish rationalist communities.\n\n### Building a Community\n\n**Communities need heroes:** Until we have a cadre of paid community organizers, LW meetups will have to run on hero power. Most members are going to be passively attending, a few will actively contribute ideas and activities on the mailing list, but someone needs to be willing to step up as a leader and begin organizing people. Do you want a community badly enough to build one yourself?\n\n**Commitment _works_:** We started having regular meetings because Jasen committed to showing up at a specific time and place and staying for a minimum length of time, regardless of other attendance. Enough folks wanted to hang out that this resulted in successful meetups.\n\n**Schedule events first, get feedback later:** Trying to ask everyone to state their preferences in order to accommodate them all rarely works and can result in prolonged indecision. Just schedule a time and place and topic; people who want to come but can't will speak up and tell you why. With enough iterations you can settle on something approximately utility maximizing.\n\n**We are a group of friends:** This is the true secret of our success, we are not just a meetup group. We started off as a bunch of people who enjoyed talking about rationality enough that we kept doing it regularly until we became a part of each other's lives. You can tell because we greet each other with hugs instead of handshakes. That physical contact has a profound psychological impact. Furthermore, almost the entire growth of our group has come through friend-referral, not through increased Less Wrong readership. Rationality per se is not the core selling point of the group - people genuinely like hanging out with us, and they tell other people to come hang out with us too.\n\n**Gender ratio matters:** It is no secret that rationality suffers from a [paucity of women](/lw/ap/of_gender_and_rationality), which makes it difficult to start a group with any women at all. There is no easy answer here, but it is important to address this factor as early as possible. Simply put, if you're winning at life and having enough fun women will want to join you, and a balanced gender ratio encourages more people of _both genders_ to attend.  Work hard to find interested women, and be careful in the presence of newcomers when trying to sanely explicitly discuss hot-button gender topics.  In case the argument for more women is not sufficiently clear, gender-balanced meetups are _a lot more fun_, and it provides a unique perspective on ideas and group dynamics.\n\n**The mailing list is for more than just meetups:** While scheduling meetups is an obvious function of a group mailing list, it can be used for all manner of discussions and coordination between group members. Given our significant overlapping interests, one function of the list is for people to invite others to join them on their adventures, be that going to conferences, parties, sous-vide steak dinners, rock climbing, or whatever else people feel like doing.  Another very important use is to ask the group for advice on a particular subject, like [optimizing OKCupid profiles](/lw/2tw/love_and_rationality_less_wrongers_on_okcupid), learning programming languages, alleviating bad moods, and more! Last but not least, mailing lists make large group discussions on serious questions feasible.\n\n**Interact with outside rationalists as much as possible:** Just as division of labor exists within the group, it also exists among groups. This allows a steady flow of new memes to try out, and an external evaluation of the current group memes. SIAI and the NYC community have been working on different projects and have different perspectives, and it has been extremely helpful to both groups to have more collaboration between them. NYC is also a major city, so we get a lot of visiting rationalists passing through, and people have traveled from neighboring states to attend our events. This provides constant perspective and growth.\n\n**Meetup topics**\n\n*   **Social/unfocused discussions:** Attendance is usually poor, members replied that hanging out is harder to justify than having a specific purpose.\n*   **Discussion topics:** Reliably good attendance and fun. The topics can vary widely, everything from TDT to making money. Note that large group discussions rarely lead to progress/insight on a question, but breaking into smaller sub-groups can work.\n*   **Presentation/skill share:** Depends on the topic, draws specialized crowds, but usually high interest.\n*   **Game nights:** Good for social bonding, regulars reliably show up. Poker, [Nomic](http://en.wikipedia.org/wiki/Nomic), [German-style games](http://en.wikipedia.org/wiki/German-style_board_game) popular. Games also represent a very stylized domain within which we can practice optimizing - rationalists should reliably win more on average or we're doing something wrong.  Note that even folks not playing the game still show up to socialize.\n*   **Group planning/meta:** Draws only core members, so low attendance, but that is actually useful in this context. Worth doing occasionally for feedback and direction if no other avenues exist.\n*   **Structured exercises:** Attendance varies but exercises tend to be highly engaging, we will likely explore with this format more in the future. Our [recent fun with cognitive biases](/lw/4fp/fun_and_games_with_cognitive_biases/) is a good example\n*   **Bacchanalia:** Because sometimes, you just really need to party.\n\n### Group Rationality\n\n**Spend time with each other:** The biggest benefit of having the community is _having the community_.  Hold meetups often, and use the mailing list to arrange activities outside the meetups as well. Do the things you like doing... together. Get to know other people in the group, figure out who your closest friends are and hang out with them.  This is incredibly fun, promotes well-being, and encourages the spread of knowledge. When everyone is feeling good, the positive mood contagion can be overwhelmingly powerful.\n\n**Epistemic privilege and meme-sharing:** The most powerful aspect of a group of rationalists is that you have an entire class of people whose reasoning you trust. Division of labor arises naturally as each member has different interests, they all pursue a variety of skills and areas of expertise, which they can then bring back to the group. Even the lowest-level rationalists in the group can rapidly upgrade themselves by adopting winning heuristics from other group members. I cannot overstate the power of epistemic privilege. We have rapidly spread knowledge about metabolism, exercise, neuroscience, meditation, hypnosis, several systems of therapy... and don't forget the Dark Arts.\n\n**Ask the group for help:** There is a reason we identify as _aspiring rationalists_, rather than just plain rationalists. Despite our best efforts we are not perfect Bayesians, but at least we know [the importance of saying oops](/lw/i9/the_importance_of_saying_oops). One of the biggest advantages of a group of rationalists is that any of the individual members can ask the group for help when they are feeling indecisive or they think their logic is compromised. When everyone else in the group unanimously agrees with each other and disagrees with us, that's evidence strong enough not to ignore. For the record, the only thing that drives rationalists crazier than loneliness is being in a relationship.\n\n**Be honest with each other:** Maybe this should go without saying, but it bears worth repeating. One of a rationalist's strengths is not identifying with our beliefs, which allows us to [surrender our old attire](/lw/i7/belief_as_attire), [update on new evidence](http://yudkowsky.net/rational/bayes), [and actually change our minds](http://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind). It is difficult for others to identify errors in our data or reasoning if that entire process is a black box - and by symmetry, if others wish to improve as well, they need to be willing to hear us and we need to be willing to tell them unpleasant truths. Most rationalists I have encountered also tend not to be very judgmental, and this quality makes this kind of communication drastically easier because everyone feels safe. Make your community a place where everyone can give and receive feedback and share their best knowledge of the map without fear.\n\n**Learn to be social, and go forth into the world:** To be frank, many of us are not very good at social interaction, which can definitely be painful, and, when socializing is an important part of our life or job, debilitating. Fortunately, rationalists have a major hack: we can start socializing with each other in a non-judgmental environment. Once some of the benefits of regular social interaction settle in, and people become happier and more comfortable in groups, it becomes increasingly easy to socialize with other people outside the group. There has been a very clear trend towards increased sociability and as a result good social outcomes.\n\n**Most progress is accomplished in small groups:** There is strong consensus that group discussions rarely result in updating, even if they are fun. Conversations of 2 or 3 (maybe 4 at the most), seem to produce the most useful insights. This is why spending time together bilaterally is incredibly important to group development. When a handful of people are all interested in a particular topic and practice it together, they form a de facto working group which allows them to iterate rapidly and then teach it to the rest of the members.\n\n**Set goals and hold each other accountable:** This has been a recent, but powerful, addition to the group. [Humans are not automatically strategic](/lw/2p5/humans_are_not_automatically_strategic), but we have each other to remind us of this fact. The vast majority of people don't even reach the first step of having explicit goals! Not only that, but being a social group allows us to leverage that social pressure on each other - it is legitimately challenging to stand in front of the group and admit that you have not achieved your goal for the week. These goals should either be focused on the most important step that would change your life, or radically push you outside your comfort zone.\n\nThe Road Ahead\n--------------\n\nThe NYC community continues to change and grow, and every week brings something new. The problem of optimizing group rationality is far from solved, and I hope to share insights with Less Wrong as we continue to have them. Our current biggest challenge is that we are outgrowing our usual meetup location as there has been demand for more meetups on a wide variety of topics. Given that our biggest strengths are social in nature, we are beginning to hit [fundamental limits on group size](/lw/x9/dunbars_function) above which coordination begins to break down.\n\nThe solution we are currently implementing is [creating multiple groups](http://wiki.lesswrong.com/mediawiki/index.php?title=NYC_meetup_group#Regular_Meetups), each meeting weekly and focused on a different topic. Andrew Rettek is creating a group at Columbia University, focusing on outreach/education and specifically teaching rationality through cognitive biases. My own group is focusing on self-development, which involves goal-setting, skill-sharing, and creating tools to correct errors in reason and emotion - in short, instrumental rationality. Zvi Mowshowitz is running a third group sticking to the core meetups like discussions and game nights, and trying experimental formats as well.  Members may attend any meetups they wish during the week, with the goal of decreasing total attendance at each one to keep numbers reasonable - and we will keep creating more groups if these ones get full.\n\nMost importantly, however, we want to [make everything we have done here and everything we have learned _reliably reproducible_](/lw/c4/go_forth_and_create_the_art).  This post is one example of an attempt to codify what steps we have taken to get here from there as a community so that others can begin following our lead, and I fully intend to flesh out each of these in more exact detail. We have also stumbled on a number of useful memes and heuristics, all of which I seek to turn into explicit knowledge: step by step instructions that anyone could follow to achieve similar benefits. Given that much of this knowledge will likely contain implicit components, instructors of these skills should be able to earn profits teaching them to others. _Making more money_ seems to be one of the biggest metrics on which rationalists do not yet perform exceptionally, but if we are truly creating value in the world we should learn how to capture it.\n\nCall to Assemble\n----------------\n\nYou have now heard my case for group rationality, and it rests upon the individual benefits it incurs: you will be _drastically more happy_, and you will level up _a lot more quickly_. Armed with this knowledge, what should you do?\n\nFirst of all, if you live in an area which already has a [critical mass of rationalists](/lw/43s/starting_a_lw_meetup_is_easy/3gbz) you should take these lessons and **create a community of your own**, so that you and everyone else can reap the rewards. [It is up to you to be the hero](/lw/9m/collective_apathy_and_the_internet) \\- [yes, _you_](/lw/9j/bystander_apathy).  One common piece of feedback we get from new members is that Less Wrong discussions are intimidating, and they don't feel qualified to even talk about these topics (much less contribute or become an organizer).  They are invariably wrong.\n\nIf you find yourself having to move for any reason, then you should make every attempt you can to **congregate in an area with more people**.  Note that in-person interaction requires minimal effective distance between people. There is a strong case to pick NYC: it is a major urban area with a lot of different job opportunities, the unusually good subway system shortens effective distance, and we are creating a model which can scale with additional rationalists. Two alternatives are suburban areas with good highways, or to move within walking distance of other rationalists. Taken to the limit you can share housing with other rationalists, as in the case of the Visiting Fellows program. As the NYC community grows we are naturally clustering around different parts of the city, and we hope to build an intentional community where many of us live together in shared housing.\n\nYou may have had [a sense that more was possible](/lw/2c/a_sense_that_more_is_possible), and if you did then you were correct: groups of rationalists _have more fun and win at life_, and it's time to scale up the awesome. Whether you decide to make your own home or come join ours, the NYC community will always welcome you with open arms."
          },
          "voteCount": 160
        },
        {
          "name": "Outside the Laboratory",
          "type": "post",
          "slug": "outside-the-laboratory",
          "_id": "N2pENnTPB75sfc9kb",
          "url": null,
          "title": "Outside the Laboratory",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Law-Thinking"
            },
            {
              "name": "Practice & Philosophy of Science"
            },
            {
              "name": "Religion"
            },
            {
              "name": "Compartmentalization"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "\"Outside the laboratory, scientists are no wiser than anyone else.\"  Sometimes this proverb is spoken by scientists, humbly, sadly, to remind themselves of their own fallibility.  Sometimes this proverb is said for rather less praiseworthy reasons, to devalue unwanted expert advice.  Is the proverb true?  Probably not in an absolute sense.  It seems much too pessimistic to say that scientists are literally _no_ wiser than average, that there is literally _zero_ correlation.\n\nBut the proverb does appear true to some degree, and I propose that we should be very disturbed by this fact.  We should not sigh, and shake our heads sadly.  Rather we should sit bolt upright in alarm.  Why?  Well, suppose that an apprentice shepherd is laboriously trained to count sheep, as they pass in and out of a fold.  Thus the shepherd knows when all the sheep have left, and when all the sheep have returned.  Then you give the shepherd a few apples, and say:  \"How many apples?\"  But the shepherd stares at you blankly, because they weren't trained to count apples - just sheep.  You would probably suspect that the shepherd didn't understand counting very well.\n\nNow suppose we discover that a Ph.D. economist buys a lottery ticket every week.  We have to ask ourselves:  Does this person really _understand_ expected utility, on a gut level?  Or have they just been trained to perform certain algebra tricks?\n\nOne thinks of Richard Feynman's account of a failing physics education program:\n\n> \"The students had memorized everything, but they didn't know what anything meant.  When they heard 'light that is reflected from a medium with an index', they didn't know that it meant a material _such as water._  They didn't know that the 'direction of the light' is the direction in which you _see_ something when you're looking at it, and so on.  Everything was entirely memorized, yet nothing had been translated into meaningful words.  So if I asked, 'What is Brewster's Angle?' I'm going into the computer with the right keywords.  But if I say, 'Look at the water,' nothing happens - they don't have anything under 'Look at the water'!\"\n\nSuppose we have an apparently competent scientist, who knows how to design an experiment on N subjects; the N subjects will receive a randomized treatment; blinded judges will classify the subject outcomes; and then we'll run the results through a computer and see if the results are significant at the 0.05 confidence level.  Now this is not just a ritualized tradition.  This is not a point of arbitrary etiquette like using the correct fork for salad.  It is a ritualized tradition for _testing hypotheses experimentally._  Why should you test your hypothesis experimentally?  Because you know the journal will demand so before it publishes your paper?  Because you were trained to do it in college?  Because everyone else says in unison that it's important to do the experiment, and they'll look at you funny if you say otherwise?\n\nNo: because, in order to map a territory, you have to go out and look at the territory.  It isn't possible to produce an accurate map of a city while sitting in your living room with your eyes closed, thinking pleasant thoughts about what you wish the city was like.  You have to go out, walk through the city, and write lines on paper that correspond to what you see.  It happens, in miniature, every time you look down at your shoes to see if your shoelaces are untied.  Photons arrive from the Sun, bounce off your shoelaces, strike your retina, are transduced into neural firing frequences, and are reconstructed by your visual cortex into an activation pattern that is strongly correlated with the current shape of your shoelaces.  To gain new information about the territory, you have to interact with the territory.  There has to be some real, physical process whereby your brain state ends up correlated to the state of the environment.  Reasoning processes aren't magic; you can give causal descriptions of how they work.  Which all goes to say that, to find things out, you've got to go look_._\n\nNow what are we to think of a scientist who seems competent inside the laboratory, but who, outside the laboratory, believes in a spirit world?  We ask why, and the scientist says something along the lines of:  \"Well, no one really knows, and I admit that I don't have any evidence - it's a religious belief, it can't be disproven one way or another by observation.\"  I cannot but conclude that this person _literally doesn't know why you have to look at things._  They may have been taught a certain ritual of experimentation, but they don't understand the _reason_ for it - that to map a territory, you have to look at it - that to gain information about the environment, you have to undergo a causal process whereby you interact with the environment and end up correlated to it.  This applies just as much to a double-blind experimental design that gathers information about the efficacy of a new medical device, as it does to your eyes gathering information about your shoelaces.\n\nMaybe our spiritual scientist says:  \"But it's not a matter for experiment.  The spirits spoke to me in my heart.\"  Well, if we really suppose that spirits are speaking in any fashion whatsoever, that is a causal interaction and it counts as an observation.  Probability theory still applies.  If you propose that some personal experience of \"spirit voices\" is evidence for actual spirits, you must propose that there is a favorable likelihood ratio for spirits causing \"spirit voices\", as compared to other explanations for \"spirit voices\", which is sufficient to overcome the prior improbability of a complex belief with many parts.  Failing to realize that \"the spirits spoke to me in my heart\" is an instance of \"causal interaction\", is analogous to a physics student not realizing that a \"medium with an index\" means a material such as water.\n\nIt is easy to be fooled, perhaps, by the fact that people wearing lab coats use the phrase \"causal interaction\" and that people wearing gaudy jewelry use the phrase \"spirits speaking\".  Discussants wearing different clothing, as we all know, demarcate independent spheres of existence - \"separate magisteria\", in Stephen J. Gould's immortal blunder of a phrase.  Actually, \"causal interaction\" is just a fancy way of saying, \"Something that makes something else happen\", and probability theory doesn't care what clothes you wear.\n\nIn modern society there is a prevalent notion that spiritual matters can't be settled by logic or observation, and therefore you can have whatever religious beliefs you like.  If a scientist falls for this, and decides to live their extralaboratorial life accordingly, then this, to me, says that they only understand the experimental principle as a _social convention_.  They know when they are _expected_ to do experiments and test the results for statistical significance.  But put them in a context where it is _socially conventional_ to make up wacky beliefs without looking, and they just as happily do that instead.\n\nThe apprentice shepherd is told that if \"seven\" sheep go out, and \"eight\" sheep go out, then \"fifteen\" sheep had better come back in.  Why \"fifteen\" instead of \"fourteen\" or \"three\"?  Because otherwise you'll get no dinner tonight, that's why!  So that's professional training of a kind, and it works after a fashion - but if social convention is the only reason why seven sheep plus eight sheep equals fifteen sheep, then maybe seven apples plus eight apples equals three apples.  Who's to say that the rules shouldn't be different for apples?\n\nBut if you know _why_ the rules work, you can see that addition is the same for sheep and for apples.  Isaac Newton is justly revered, not for his outdated theory of gravity, but for discovering that - amazingly, surprisingly - the celestial planets, in the glorious heavens, obeyed just the same rules as falling apples.  In the macroscopic world - the everyday ancestral environment - different trees bear different fruits, different customs hold for different people at different times.  A genuinely unified universe, with stationary universal laws, is a highly counterintuitive notion to humans!  It is only scientists who really believe it, though some religions may talk a good game about the \"unity of all things\".\n\nAs Richard Feynman put it:\n\n> \"If we look at a glass closely enough we see the entire universe. There are the things of physics: the twisting liquid which evaporates depending on the wind and weather, the reflections in the glass, and our imaginations adds the atoms. The glass is a distillation of the Earth's rocks, and in its composition we see the secret of the universe's age, and the evolution of the stars. What strange array of chemicals are there in the wine? How did they come to be? There are the ferments, the enzymes, the substrates, and the products. There in wine is found the great generalization: all life is fermentation. Nobody can discover the chemistry of wine without discovering, as did Louis Pasteur, the cause of much disease. How vivid is the claret, pressing its existence into the consciousness that watches it! If our small minds, for some convenience, divide this glass of wine, this universe, into parts — physics, biology, geology, astronomy, psychology, and so on — remember that Nature does not know it! So let us put it all back together, not forgetting ultimately what it is for. Let it give us one more final pleasure: drink it and forget it all!\"\n\nA few religions, especially the ones invented or refurbished after Isaac Newton, may profess that \"everything is connected to everything else\".  (Since there is a trivial isomorphism between graphs and their complements, this profound wisdom conveys exactly the same useful information as a graph with no edges.)  But when it comes to the actual meat of the religion, prophets and priests follow the ancient human practice of making everything up as they go along.  And they make up one rule for women under twelve, another rule for men over thirteen; one rule for the Sabbath and another rule for weekdays; one rule for science and another rule for sorcery...\n\nReality, we have learned to our shock, is _not_ a collection of separate magisteria, but a single unified process governed by mathematically simple low-level rules.  Different buildings on a university campus do not belong to different universes, though it may sometimes seem that way.  The universe is not divided into mind and matter, or life and nonlife; the atoms in our heads interact seamlessly with the atoms of the surrounding air.  Nor is Bayes's Theorem different from one place to another.\n\nIf, outside of their specialist field, some particular scientist is just as susceptible as anyone else to wacky ideas, then they probably never did understand _why_ the scientific rules work.  Maybe they can parrot back a bit of Popperian falsificationism; but they don't understand on a deep level, the algebraic level of probability theory, the causal level of cognition-as-machinery. They've been trained to behave a certain way in the laboratory, but they don't _like_ to be constrained by evidence; when they go home, they take off the lab coat and relax with some comfortable nonsense.  And yes, that does make me wonder if I can trust that scientist's opinions even in their own field - especially when it comes to any controversial issue, any open question, anything that isn't already nailed down by massive evidence and social convention.\n\nMaybe we _can_ beat the proverb - be rational in our personal lives, not just our professional lives.  We shouldn't let a mere proverb stop us:  \"A witty saying proves nothing,\" as Voltaire said.  Maybe we can do better, if we study enough probability theory to know _why_ the rules work, and enough experimental psychology to see how they apply in real-world cases - if we can learn to look at the water.  An ambition like that lacks the [comfortable modesty](/lw/gq/the_proper_use_of_humility/) of being able to confess that, outside your specialty, you're no better than anyone else.  But if our theories of rationality don't generalize to everyday life, we're doing something wrong.  It's not a different universe inside and outside the laboratory.\n\n**Addendum:**  If you think that (a) science is purely logical and therefore opposed to emotion, or (b) that we shouldn't bother to seek truth in everyday life, see \"[Why Truth?](/lw/go/why_truth_and/)\"  For new readers, I also recommend \"[Twelve Virtues of Rationality.](http://yudkowsky.net/virtues/)\""
          },
          "voteCount": 100
        },
        {
          "name": "Raising the Sanity Waterline",
          "type": "post",
          "slug": "raising-the-sanity-waterline",
          "_id": "XqmjdBKa4ZaXJtNmf",
          "url": null,
          "title": "Raising the Sanity Waterline",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Community"
            },
            {
              "name": "Public Discourse"
            },
            {
              "name": "Social & Cultural Dynamics"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "To [paraphrase](http://www.acceleratingfuture.com/steven/?p=3) the Black Belt Bayesian:  Behind every exciting, dramatic failure, there is a more important story about a larger and less dramatic failure that made the first failure possible.\n\nIf every trace of religion was magically eliminated from the world tomorrow, then—however much improved the lives of many people would be—we would not even have come close to solving the larger failures of sanity that made religion possible in the first place.\n\nWe have good cause to spend some of our efforts on trying to eliminate religion directly, because it _is_ a direct problem.  But religion also serves the function of an asphyxiated canary in a coal mine—religion is a sign, a symptom, of larger problems that don't go away just because someone loses their religion.\n\nConsider this thought experiment—what could you teach people that is not _directly_ about religion, which is true and useful as a _general_ method of rationality, which would cause them to lose their religions?  In fact—imagine that we're going to go and survey all your students five years later, and see how many of them have lost their religions compared to a control group; if you make the slightest move at fighting religion _directly,_ you will invalidate the experiment.  You may not make a single mention of religion or any religious belief in your classroom, you may not even hint at it in any obvious way.  All your examples must center about real-world cases that have nothing to do with religion.\n\nIf you can't fight religion _directly,_ what do you teach that raises the _general waterline of sanity_ to the point that religion goes underwater?\n\nHere are some such topics I've already covered—_not_ avoiding all mention of religion, but it could be done:\n\n*   [Affective Death Spirals](http://www.overcomingbias.com/2007/12/affective-death.html)—plenty of [non-supernaturalist](http://www.overcomingbias.com/2007/12/ayn-rand.html) examples.\n*   How to avoid [cached thoughts](http://www.overcomingbias.com/2007/10/cached-thoughts.html) and [fake wisdom](http://www.overcomingbias.com/2007/10/how-to-seem-and.html); the pressure of [conformity](http://www.overcomingbias.com/2007/12/aschs-conformit.html).\n*   [Evidence](http://www.overcomingbias.com/2007/09/what-is-evidenc.html) and [Occam's Razor](http://www.overcomingbias.com/2007/09/occams-razor.html)—the rules of probability.\n*   [The Bottom Line](http://www.overcomingbias.com/2007/09/the-bottom-line.html) / [Engines of Cognition](http://www.overcomingbias.com/2008/02/second-law.html)—the causal reasons why Reason works.\n*   [Mysterious Answers to Mysterious Questions](http://www.overcomingbias.com/2007/08/mysterious-answ.html)—and the whole associated sequence, like [making beliefs pay rent](http://www.overcomingbias.com/2007/07/making-beliefs-.html) and [curiosity-stoppers](http://www.overcomingbias.com/2007/08/semantic-stopsi.html)—have excellent historical examples in vitalism and [phlogiston](http://www.overcomingbias.com/2007/08/fake-causality.html).\n*   [Non-existence of ontologically fundamental mental things](http://www.overcomingbias.com/2008/09/excluding-the-s.html)—apply the [Mind Projection Fallacy](http://www.overcomingbias.com/2008/03/mind-projection.html) to [probability](http://www.overcomingbias.com/2008/03/mind-probabilit.html), move on to [reductionism](http://www.overcomingbias.com/2008/03/reductionism.html) versus [holism](http://www.overcomingbias.com/2007/08/the-futility-of.html), then [brains](http://www.overcomingbias.com/2008/04/brain-breakthro.html) and [cognitive science](http://www.overcomingbias.com/2008/03/angry-atoms.html).\n*   The many sub-arts of [Crisis of Faith](http://www.overcomingbias.com/2008/10/got-crisis.html)—though you'd better find something else to call this ultimate high master-level technique of _actually updating on evidence_.\n*   [Dark Side Epistemology](http://www.overcomingbias.com/2008/10/the-dark-side.html)—teaching this with no mention of religion would be hard, but perhaps you could videotape the interrogation of some snake-oil sales agent as your real-world example.\n*   [Fun Theory](http://www.overcomingbias.com/2009/01/fun-theory-sequence.html)—teach as a literary theory of utopian fiction, without the direct application to [theodicy](http://www.overcomingbias.com/2009/01/the-uses-of-fun-theory.html).\n*   [Joy in the Merely Real](http://www.overcomingbias.com/2008/03/joy-in-the-real.html), [naturalistic metaethics](http://www.overcomingbias.com/2008/08/rightness-redux.html), etcetera etcetera etcetera and so on.\n\nBut to look at it another way—\n\nSuppose we have [a scientist who's still religious](http://www.overcomingbias.com/2007/01/outside_the_lab.html), either full-blown scriptural-religion, or in the sense of tossing around vague casual endorsements of \"spirituality\".\n\nWe now know this person is not applying any _technical, explicit_ understanding of...\n\n*   ...what constitutes evidence and why;\n*   ...Occam's Razor;\n*   ...how the above two rules derive from the lawful and causal operation of minds as mapping engines, and do not switch off when you talk about tooth fairies;\n*   ...how to tell the difference between a real answer and a curiosity-stopper;\n*   ...how to rethink matters for themselves instead of just repeating things they heard;\n*   ...certain general trends of science over the last three thousand years;\n*   ...the difficult arts of actually updating on new evidence and relinquishing old beliefs;\n*   ...epistemology 101;\n*   ...self-honesty 201;\n*   ...etcetera etcetera etcetera and so on.\n\nWhen you consider it—these are all rather _basic_ matters of study, as such things go.  A quick introduction to _all_ of them (well, except [naturalistic metaethics](http://www.overcomingbias.com/2009/01/fragile-value.html)) would be... a four-credit undergraduate course with no prerequisites?\n\nBut there are Nobel laureates who haven't taken that course!  [Richard Smalley](http://www.overcomingbias.com/2007/02/what_evidence_i.html?cid=61577404#comment-61577404) if you're looking for a cheap shot, or [Robert Aumann](http://www.overcomingbias.com/2008/05/changing-the-de.html) if you're looking for a scary shot.\n\nAnd they can't be isolated exceptions.  If all of their professional compatriots had taken that course, then Smalley or Aumann would either have been corrected (as their colleagues kindly took them aside and explained the bare fundamentals) or else regarded with too much pity and concern to win a Nobel Prize.  Could you—_realistically_ speaking, regardless of fairness—win a Nobel while advocating the existence of Santa Claus?\n\nThat's what the dead canary, religion, is telling us: that the general sanity waterline is currently _really ridiculously low._  Even in the highest halls of science.\n\nIf we throw out that dead and rotting canary, then our mine may stink a bit less, but the sanity waterline may not rise much higher.\n\nThis is not to criticize the neo-atheist movement.  The harm done by religion is clear and present danger, or rather, current and ongoing disaster.  Fighting religion's directly harmful effects takes precedence over its use as a canary or experimental indicator.  But even if Dawkins, and Dennett, and Harris, and Hitchens should somehow win utterly and absolutely to the last corner of the human sphere, the real work of rationalists will be only just beginning."
          },
          "voteCount": 186
        }
      ]
    }
  ]
}