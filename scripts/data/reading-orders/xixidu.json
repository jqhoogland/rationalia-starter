{
  "author": "XiXiDu",
  "chapters": [
    {
      "title": "Introduction",
      "children": [
        {
          "name": "Twelve Virtues of Rationality",
          "type": "post",
          "slug": "twelve-virtues-of-rationality",
          "_id": "7ZqGiPHTpiDMwqMN2",
          "url": null,
          "title": "Twelve Virtues of Rationality",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Virtues"
            },
            {
              "name": "Rationality"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The first virtue is curiosity. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance.\n\nThere is a time to confess your ignorance and a time to relinquish your ignorance. The second virtue is relinquishment. P. C. Hodgell said: “That which can be destroyed by the truth should be.”\\[1\\] Do not flinch from experiences that might destroy your beliefs. The thought you cannot think controls you more than thoughts you speak aloud. Submit yourself to ordeals and test yourself in fire. Relinquish the emotion which rests upon a mistaken belief, and seek to feel fully that emotion which fits the facts. If the iron approaches your face, and you believe it is hot, and it is cool, the Way opposes your fear. If the iron approaches your face, and you believe it is cool, and it is hot, the Way opposes your calm. Evaluate your beliefs first and then arrive at your emotions. Let yourself say: “If the iron is hot, I desire to believe it is hot, and if it is cool, I desire to believe it is cool.” Beware lest you become attached to beliefs you may not want.\n\nThe third virtue is lightness. Let the winds of evidence blow you about as though you are a leaf, with no direction of your own. Beware lest you fight a rearguard retreat against the evidence, grudgingly conceding each foot of ground only when forced, feeling cheated. Surrender to the truth as quickly as you can. Do this the instant you realize what you are resisting, the instant you can see from which quarter the winds of evidence are blowing against you. Be faithless to your cause and betray it to a stronger enemy. If you regard evidence as a constraint and seek to free yourself, you sell yourself into the chains of your whims. For you cannot make a true map of a city by sitting in your bedroom with your eyes shut and drawing lines upon paper according to impulse. You must walk through the city and draw lines on paper that correspond to what you see. If, seeing the city unclearly, you think that you can shift a line just a little to the right, just a little to the left, according to your caprice, this is just the same mistake.\n\nThe fourth virtue is evenness. One who wishes to believe says, “Does the evidence permit me to believe?” One who wishes to disbelieve asks, “Does the evidence force me to believe?” Beware lest you place huge burdens of proof only on propositions you dislike, and then defend yourself by saying: “But it is good to be skeptical.” If you attend only to favorable evidence, picking and choosing from your gathered data, then the more data you gather, the less you know. If you are selective about which arguments you inspect for flaws, or how hard you inspect for flaws, then every flaw you learn how to detect makes you that much stupider. If you first write at the bottom of a sheet of paper “And therefore, the sky is green!” it does not matter what arguments you write above it afterward; the conclusion is already written, and it is already correct or already wrong. To be clever in argument is not rationality but rationalization. Intelligence, to be useful, must be used for something other than defeating itself. Listen to hypotheses as they plead their cases before you, but remember that you are not a hypothesis; you are the judge. Therefore do not seek to argue for one side or another, for if you knew your destination, you would already be there.\n\nThe fifth virtue is argument. Those who wish to fail must first prevent their friends from helping them. Those who smile wisely and say “I will not argue” remove themselves from help and withdraw from the communal effort. In argument strive for exact honesty, for the sake of others and also yourself: the part of yourself that distorts what you say to others also distorts your own thoughts. Do not believe you do others a favor if you accept their arguments; the favor is to you. Do not think that fairness to all sides means balancing yourself evenly between positions; truth is not handed out in equal portions before the start of a debate. You cannot move forward on factual questions by fighting with fists or insults. Seek a test that lets reality judge between you.\n\nThe sixth virtue is empiricism. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes,” and one saying “No,” the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.”\\[2\\] Do not be blinded by words. When words are subtracted, anticipation remains.\n\nThe seventh virtue is simplicity. Antoine de Saint-Exupéry said: “Perfection is achieved not when there is nothing left to add, but when there is nothing left to take away.”\\[3\\] Simplicity is virtuous in belief, design, planning, and justification. When you profess a huge belief with many details, each additional detail is another chance for the belief to be wrong. Each specification adds to your burden; if you can lighten your burden you must do so. There is no straw that lacks the power to break your back. Of artifacts it is said: The most reliable gear is the one that is designed out of the machine. Of plans: A tangled web breaks. A chain of a thousand links will arrive at a correct conclusion if every step is correct, but if one step is wrong it may carry you anywhere. In mathematics a mountain of good deeds cannot atone for a single sin. Therefore, be careful on every step.\n\nThe eighth virtue is humility. To be humble is to take specific actions in anticipation of your own errors. To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty. Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans. Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.\n\nThe ninth virtue is perfectionism. The more errors you correct in yourself, the more you notice. As your mind becomes more silent, you hear more noise. When you notice an error in yourself, this signals your readiness to seek advancement to the next level. If you tolerate the error rather than correcting it, you will not advance to the next level and you will not gain the skill to notice new errors. In every art, if you do not seek perfection you will halt before taking your first steps. If perfection is impossible that is no excuse for not trying. Hold yourself to the highest standard you can imagine, and look for one still higher. Do not be content with the answer that is almost right; seek one that is exactly right.\n\nThe tenth virtue is precision. One comes and says: The quantity is between 1 and 100. Another says: The quantity is between 40 and 50. If the quantity is 42 they are both correct, but the second prediction was more useful and exposed itself to a stricter test. What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world. The narrowest statements slice deepest, the cutting edge of the blade. As with the map, so too with the art of mapmaking: The Way is a precise Art. Do not walk to the truth, but dance. On each and every step of that dance your foot comes down in exactly the right spot. Each piece of evidence shifts your beliefs by exactly the right amount, neither more nor less. What is exactly the right amount? To calculate this you must study probability theory. Even if you cannot do the math, knowing that the math exists tells you that the dance step is precise and has no room in it for your whims.\n\nThe eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. It is especially important to eat math and science which impinge upon rationality: evolutionary psychology, heuristics and biases, social psychology, probability theory, decision theory. But these cannot be the only fields you study. The Art must have a purpose other than itself, or it collapses into infinite recursion.\n\nBefore these eleven virtues is a virtue which is nameless.\n\nMiyamoto Musashi wrote, in _The Book of Five Rings_:\\[4\\]\n\n> The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy’s cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him. More than anything, you must be thinking of carrying your movement through to cutting him.\n\nEvery step of your reasoning must cut through to the correct answer in the same movement. More than anything, you must think of carrying your map through to reflecting the territory.\n\nIf you fail to achieve a correct answer, it is futile to protest that you acted with propriety.\n\nHow can you improve your conception of rationality? Not by saying to yourself, “It is my duty to be rational.” By this you only enshrine your mistaken conception. Perhaps your conception of rationality is that it is rational to believe the words of the Great Teacher, and the Great Teacher says, “The sky is green,” and you look up at the sky and see blue. If you think, “It may look like the sky is blue, but rationality is to believe the words of the Great Teacher,” you lose a chance to discover your mistake.\n\nDo not ask whether it is “the Way” to do this or that. Ask whether the sky is blue or green. If you speak overmuch of the Way you will not attain it. You may try to name the highest principle with names such as “the map that reflects the territory” or “experience of success and failure” or “Bayesian decision theory.” But perhaps you describe incorrectly the nameless virtue. How will you discover your mistake? Not by comparing your description to itself, but by comparing it to that which you did not name.\n\nIf for many years you practice the techniques and submit yourself to strict constraints, it may be that you will glimpse the center. Then you will see how all techniques are one technique, and you will move correctly without feeling constrained. Musashi wrote: “When you appreciate the power of nature, knowing the rhythm of any situation, you will be able to hit the enemy naturally and strike naturally. All this is the Way of the Void.”\n\nThese then are twelve virtues of rationality:\n\nCuriosity, relinquishment, lightness, evenness, argument, empiricism, simplicity, humility, perfectionism, precision, scholarship, and the void.\n\n* * *\n\n###### 1\\. Patricia C. Hodgell, _Seeker’s Mask_ (Meisha Merlin Publishing, Inc., 2001).\n\n###### 2\\. Cleaver, _Immediate Fiction: A Complete Writing Course_.\n\n###### 3\\. Antoine de Saint-Exupéry, _Terre des Hommes_ (Paris: Gallimard, 1939).\n\n###### 4\\. Musashi, _Book of Five Rings_.\n\n* * *\n\n_The first publication of this post is_ [_here_](http://yudkowsky.net/rational/virtues/)_._"
          },
          "voteCount": 86
        },
        {
          "name": "Interview with Eliezer Yudkowsky (Part 1)",
          "href": "http://johncarlosbaez.wordpress.com/2011/03/07/this-weeks-finds-week-311/",
          "type": "post"
        },
        {
          "name": "Interview with Eliezer Yudkowsky (Part 2)",
          "href": "http://johncarlosbaez.wordpress.com/2011/03/14/this-weeks-finds-week-312/",
          "type": "post"
        },
        {
          "name": "Interview with Eliezer Yudkowsky (Part 3)",
          "href": "http://johncarlosbaez.wordpress.com/2011/03/25/this-weeks-finds-week-313/",
          "type": "post"
        }
      ]
    },
    {
      "title": "Wisdom & Insight",
      "children": [
        {
          "name": "Diseased thinking: dissolving questions about disease",
          "type": "post",
          "slug": "diseased-thinking-dissolving-questions-about-disease",
          "_id": "895quRDaK6gR2rM82",
          "url": null,
          "title": "Diseased thinking: dissolving questions about disease",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Health / Medicine / Disease"
            },
            {
              "name": "Philosophy of Language"
            },
            {
              "name": "Carving / Clustering Reality"
            },
            {
              "name": "Reversal Test"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "What is Disease?",
                "anchor": "What_is_Disease_",
                "level": 1
              },
              {
                "title": "Hidden Inferences From Disease Concept",
                "anchor": "Hidden_Inferences_From_Disease_Concept",
                "level": 1
              },
              {
                "title": "Sympathy or Condemnation?",
                "anchor": "Sympathy_or_Condemnation_",
                "level": 1
              },
              {
                "title": "The Ethics of Treating Marginal Conditions",
                "anchor": "The_Ethics_of_Treating_Marginal_Conditions",
                "level": 1
              },
              {
                "title": "Summary",
                "anchor": "Summary",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "354 comments"
              }
            ],
            "headingsCount": 7
          },
          "contents": {
            "markdown": "**Related to:** [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/), [Words as Hidden Inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/), [Dissolving the Question](https://www.lesswrong.com/lw/of/dissolving_the_question/), [Eight Short Studies on Excuses](https://www.lesswrong.com/lw/24o/eight_short_studies_on_excuses/)\n\n> _Today's therapeutic ethos, which celebrates curing and disparages judging, expresses the liberal disposition to assume that crime and other problematic behaviors reflect social or biological causation. While this absolves the individual of responsibility, it also strips the individual of personhood, and moral dignity_\n\n_\\-\\- George Will, [townhall.com](http://townhall.com/Common/PrintPage.aspx?g=761ecc84-473b-4123-bf28-c4fc179a9d3f&t=c)_\n\nSandy is a morbidly obese woman looking for advice.\n\nHer husband has no sympathy for her, and tells her she obviously needs to stop eating like a pig, and would it kill her to go to the gym once in a while?\n\nHer doctor tells her that obesity is primarily genetic, and recommends the diet pill orlistat and a consultation with a surgeon about gastric bypass.\n\nHer sister tells her that obesity is a perfectly valid lifestyle choice, and that fat-ism, equivalent to racism, is society's way of keeping her down.\n\nWhen she tells each of her friends about the opinions of the others, things really start to heat up.\n\nHer husband accuses her doctor and sister of absolving her of personal responsibility with feel-good platitudes that in the end will only prevent her from getting the willpower she needs to start a real diet.\n\nHer doctor accuses her husband of ignorance of the real causes of obesity and of the most effective treatments, and accuses her sister of legitimizing a dangerous health risk that could end with Sandy in hospital or even dead.\n\nHer sister accuses her husband of being a jerk, and her doctor of trying to medicalize her behavior in order to turn it into a \"condition\" that will keep her on pills for life and make lots of money for Big Pharma.\n\nSandy is fictional, but similar conversations happen every day, not only about obesity but about a host of other marginal conditions that some consider character flaws, others diseases, and still others normal variation in the human condition. Attention deficit disorder, internet addiction, social anxiety disorder (as one skeptic said, didn't we used to call this \"shyness\"?), alcoholism, chronic fatigue, oppositional defiant disorder (\"didn't we used to call this being a teenager?\"), compulsive gambling, homosexuality, Aspergers' syndrome, antisocial personality, even depression have all been placed in two or more of these categories by different people.\n\n  \n\nSandy's sister may have a point, but this post will concentrate on the debate between her husband and her doctor, with the understanding that the same techniques will apply to evaluating her sister's opinion. The disagreement between Sandy's husband and doctor centers around the idea of \"disease\". If obesity, depression, alcoholism, and the like are diseases, most people default to the doctor's point of view; if they are not diseases, they tend to agree with the husband.\n\nThe debate over such marginal conditions is in many ways a debate over whether or not they are \"real\" diseases. The usual surface level arguments trotted out in favor of or against the proposition are generally inconclusive, but this post will apply a host of techniques previously discussed on Less Wrong to illuminate the issue.\n\n  \n\n**What is Disease?**\n\n  \n\nIn [Disguised Queries](https://www.lesswrong.com/lw/nm/disguised_queries/) , Eliezer demonstrates how a word refers to a cluster of objects related upon multiple axes. For example, in a company that sorts red smooth translucent cubes full of vanadium from blue furry opaque eggs full of palladium, you might invent the word \"rube\" to designate the red cubes, and another \"blegg\", to designate the blue eggs. Both words are useful because they \"carve reality at the joints\" - they refer to two completely separate classes of things which it's practically useful to keep in separate categories. Calling something a \"blegg\" is a quick and easy way to describe its color, shape, opacity, texture, and chemical composition. It may be that the odd blegg might be purple rather than blue, but in general the characteristics of a blegg remain sufficiently correlated that \"blegg\" is a useful word. If they weren't so correlated - if blue objects were equally likely to be palladium-containing-cubes as vanadium-containing-eggs, then the word \"blegg\" would be a waste of breath; the characteristics of the object would remain just as mysterious to your partner after you said \"blegg\" as they were before.\n\n\"Disease\", like \"blegg\", suggests that certain characteristics always come together. A rough sketch of some of the characteristics we expect in a disease might include:\n\n1\\. Something caused by the sorts of thing you study in biology: proteins, bacteria, ions, viruses, genes.\n\n2\\. Something involuntary and completely immune to the operations of free will\n\n3\\. Something rare; the vast majority of people don't have it\n\n4\\. Something unpleasant; when you have it, you want to get rid of it\n\n5\\. Something discrete; a graph would show two widely separate populations, one with the disease and one without, and not a normal distribution.\n\n6\\. Something commonly treated with science-y interventions like chemicals and radiation.\n\nCancer satisfies every one of these criteria, and so we have no qualms whatsoever about classifying it as a disease. It's a type specimen, [the sparrow as opposed to the ostrich](https://www.lesswrong.com/lw/nl/the_cluster_structure_of_thingspace/). The same is true of heart attack, the flu, diabetes, and many more.\n\nSome conditions satisfy a few of the criteria, but not others. Dwarfism seems to fail (5), and it might get its status as a disease only after studies show that the supposed dwarf falls way out of normal human height variation. Despite the best efforts of transhumanists, it's hard to convince people that aging is a disease, partly because it fails (3). Calling homosexuality a disease is a poor choice for many reasons, but one of them is certainly (4): it's not necessarily unpleasant.\n\nThe marginal conditions mentioned above are also in this category. Obesity arguably sort-of-satisfies criteria (1), (4), and (6), but it would be pretty hard to make a case for (2), (3), and (5).\n\nSo, is obesity really a disease? Well, is Pluto really a planet? Once we state that obesity satisfies some of the criteria but not others, it is meaningless to talk about an additional fact of whether it \"really deserves to be a disease\" or not.\n\nIf it weren't for those pesky [hidden inferences](https://www.lesswrong.com/lw/ng/words_as_hidden_inferences/#more)...\n\n**Hidden Inferences From Disease Concept**\n\nThe state of the disease node, meaningless in itself, is used to predict several other nodes with non-empirical content. In English: we make value decisions based on whether we call something a \"disease\" or not.\n\nIf something is a real disease, the patient deserves our sympathy and support; for example, [cancer sufferers must universally be described as \"brave\"](http://www.theonion.com/articles/loved-ones-recall-local-mans-cowardly-battle-with,772/). If it is not a real disease, people are more likely to get our condemnation; for example Sandy's husband who calls her a \"pig\" for her inability to control her eating habits. The difference between \"shyness\" and \"social anxiety disorder\" is that people with the first get called \"weird\" and told to man up, and people with the second get special privileges and the sympathy of those around them.\n\nAnd if something is a real disease, it is socially acceptable (maybe even mandated) to seek medical treatment for it. If it's not a disease, medical treatment gets derided as a \"quick fix\" or an \"abdication of personal responsibility\". I have talked to several doctors who are uncomfortable suggesting gastric bypass surgery, even in people for whom it is medically indicated, because they believe it is morally wrong to turn to medicine to solve a character issue.\n\n![](http://res.cloudinary.com/lesswrong-2-0/image/upload/v1531709899/disease_first_tlnfio.gif)\n\nWhile a condition's status as a \"real disease\" ought to be meaningless as a \"hanging node\" after the status of all other nodes have been determined, it has acquired political and philosophical implications because of its role in determining whether patients receive sympathy and whether they are permitted to seek medical treatment.\n\nIf we can determine whether a person should get sympathy, and whether they should be allowed to seek medical treatment, independently of the central node \"disease\" or of the criteria that feed into it, we will have successfully unasked the question \"are these marginal conditions real diseases\" and cleared up the confusion.\n\n**Sympathy or Condemnation?**\n\nOur attitudes toward people with marginal conditions mainly reflect a deontologist libertarian (libertarian as in \"free will\", not as in \"against government\") model of blame. In this concept, people make decisions using their free will, a spiritual entity operating free from biology or circumstance. People who make good decisions are intrinsically good people and deserve good treatment; people who make bad decisions are intrinsically bad people and deserve bad treatment. But people who make bad decisions for reasons that are outside of their free will may not be intrinsically bad people, and may therefore be absolved from deserving bad treatment. For example, if a normally peaceful person has a brain tumor that affects areas involved in fear and aggression, they go on a crazy killing spree, and then they have their brain tumor removed and become a peaceful person again, many people would be willing to accept that the killing spree does not reflect negatively on them or open them up to deserving bad treatment, since it had biological and not spiritual causes.\n\nUnder this model, deciding whether a condition is biological or spiritual becomes very important, and the rationale for worrying over whether something \"is a real disease\" or not is plain to see. Without figuring out this extremely difficult question, we are at risk of either blaming people for things they don't deserve, or else letting them off the hook when they commit a sin, both of which, to libertarian deontologists, would be terrible things. But determining whether marginal conditions like depression have a spiritual or biological cause is difficult, and no one knows how to do it reliably.\n\nDeterminist consequentialists can do better. We believe it's biology all the way down. Separating spiritual from biological illnesses is impossible and unnecessary. Every condition, from brain tumors to poor taste in music, is \"biological\" insofar as it is encoded in things like cells and proteins and follows laws based on their structure.\n\nBut determinists don't just ignore the very important differences between brain tumors and poor taste in music. Some biological phenomena, like poor taste in music, are encoded in such a way that they are extremely vulnerable to what we can call social influences: praise, condemnation, introspection, and the like. Other biological phenomena, like brain tumors, are completely immune to such influences. This allows us to develop a more useful model of blame.\n\nThe consequentialist model of blame is very different from the deontological model. Because all actions are biologically determined, none are more or less metaphysically blameworthy than others, and none can mark anyone with the metaphysical status of \"bad person\" and make them \"deserve\" bad treatment. Consequentialists don't on a primary level want anyone to be treated badly, full stop; thus [is it written](http://yudkowsky.net/obsolete/tmol-faq.html#theo_free): \"Saddam Hussein doesn't deserve so much as a stubbed toe.\" But if consequentialists don't believe in punishment for its own sake, they do believe in punishment for the sake of, well, consequences. Hurting bank robbers may not be a good in and of itself, but it will prevent banks from being robbed in the future. And, one might infer, although alcoholics may not deserve condemnation, societal condemnation of alcoholics makes alcoholism a less attractive option.\n\nSo here, at last, is a rule for which diseases we offer sympathy, and which we offer condemnation: if giving condemnation instead of sympathy decreases the incidence of the disease enough to be worth the hurt feelings, condemn; otherwise, sympathize. Though the rule is based on philosophy that the majority of the human race would disavow, it leads to intuitively correct consequences. Yelling at a cancer patient, shouting \"How dare you allow your cells to divide in an uncontrolled manner like this; is that the way your mother raised you??!\" will probably make the patient feel pretty awful, but it's not going to cure the cancer. Telling a lazy person \"Get up and do some work, you worthless bum,\" very well might cure the laziness. The cancer is a biological condition immune to social influences; the laziness is a biological condition susceptible to social influences, so we try to socially influence the laziness and not the cancer.\n\nThe question \"Do the obese deserve our sympathy or our condemnation,\" then, is asking whether condemnation is such a useful treatment for obesity that its utility outweights the disutility of hurting obese people's feelings. This question may have different answers depending on the particular obese person involved, the particular person doing the condemning, and the availability of other methods for treating the obesity, which brings us to...\n\n**The Ethics of Treating Marginal Conditions**\n\nIf a condition is susceptible to social intervention, but an effective biological therapy for it also exists, is it okay for people to use the biological therapy instead of figuring out a social solution? My gut answer is \"Of course, why wouldn't it be?\", but apparently lots of people find this controversial for some reason.\n\nIn a libertarian deontological system, throwing biological solutions at spiritual problems might be disrespectful or dehumanizing, or a band-aid that doesn't affect the deeper problem. To someone who believes it's biology all the way down, this is much less of a concern.\n\nOthers complain that the existence of an easy medical solution prevents people from learning personal responsibility. But here [we see the status-quo bias at work, and so can apply a preference reversal test](http://www.nickbostrom.com/ethics/statusquo.pdf). If people really believe learning personal responsibility is more important than being not addicted to heroin, we would expect these people to support deliberately addicting schoolchildren to heroin so they can develop personal responsibility by coming off of it. Anyone who disagrees with this somewhat shocking proposal must believe, on some level, that having people who are not addicted to heroin is more important than having people develop whatever measure of personal responsibility comes from kicking their heroin habit the old-fashioned way.\n\nBut the most convincing explanation I have read for why so many people are opposed to medical solutions for social conditions is a signaling explanation by Robin Hans...wait! no!...by Katja Grace. On [her blog](http://meteuphoric.wordpress.com/2009/09/21/why-do-animal-lovers-want-animals-to-feel-pain/), she says:\n\n> _...the situation reminds me of a pattern in similar cases I have noticed before. It goes like this. Some people make personal sacrifices, supposedly toward solving problems that don’t threaten them personally. They sort recycling, buy free range eggs, buy fair trade, campaign for wealth redistribution etc. Their actions are seen as virtuous. They see those who don’t join them as uncaring and immoral. A more efficient solution to the problem is suggested. It does not require personal sacrifice. People who have not previously sacrificed support it. Those who have previously sacrificed object on grounds that it is an excuse for people to get out of making the sacrifice. The supposed instrumental action, as the visible sign of caring, has become virtuous in its own right. Solving the problem effectively is an attack on the moral people._\n\nA case in which some people eat less enjoyable foods and exercise hard to avoid becoming obese, and then campaign against a pill that makes avoiding obesity easy demonstrates some of the same principles.\n\nThere are several very reasonable objections to treating any condition with drugs, whether it be a classical disease like cancer or a marginal condition like alcoholism. The drugs can have side effects. They can be expensive. They can build dependence. They may later be found to be placebos whose efficacy was overhyped by dishonest pharmaceutical advertising.. They may raise ethical issues with children, the mentally incapacitated, and other people who cannot decide for themselves whether or not to take them. But these issues do not magically become more dangerous in conditions typically regarded as \"character flaws\" rather than \"diseases\", and the same good-enough solutions that work for cancer or heart disease will work for alcoholism and other such conditions (but see [here](https://www.lesswrong.com/lw/2as/diseased_thinking_dissolving_questions_about/230p)).\n\nI see no reason why people who want effective treatment for a condition should be denied it or stigmatized for seeking it, whether it is traditionally considered \"medical\" or not.\n\n**Summary**\n\nPeople commonly debate whether social and mental conditions are real diseases. This masquerades as a medical question, but its implications are mainly social and ethical. We use the concept of disease to decide who gets sympathy, who gets blame, and who gets treatment.\n\nInstead of continuing the fruitless \"disease\" argument, we should address these questions directly. Taking a determinist consequentialist position allows us to do so more effectively. We should blame and stigmatize people for conditions where blame and stigma are the most useful methods for curing or preventing the condition, and we should allow patients to seek treatment whenever it is available and effective."
          },
          "voteCount": 367
        },
        {
          "name": "Self-fulfilling correlations",
          "type": "post",
          "slug": "self-fulfilling-correlations",
          "_id": "XuyRMxky6G8gq7a69",
          "url": null,
          "title": "Self-fulfilling correlations",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Causality"
            },
            {
              "name": "World Modeling"
            },
            {
              "name": "AI"
            },
            {
              "name": "Self Fulfilling/Refuting Prophecies"
            }
          ],
          "tableOfContents": {
            "sections": [
              {
                "title": "Perceived causation causes correlation",
                "anchor": "Perceived_causation_causes_correlation",
                "level": 1
              },
              {
                "title": "Conditions under which this occurs",
                "anchor": "Conditions_under_which_this_occurs",
                "level": 1
              },
              {
                "title": "Application to machine learning and smart people",
                "anchor": "Application_to_machine_learning_and_smart_people",
                "level": 1
              },
              {
                "divider": true,
                "level": 0,
                "anchor": "postHeadingsDivider"
              },
              {
                "anchor": "comments",
                "level": 0,
                "title": "50 comments"
              }
            ],
            "headingsCount": 5
          },
          "contents": {
            "markdown": "Correlation does not imply causation.  Sometimes corr(X,Y) means X=>Y; sometimes it means Y=>X; sometimes it means W=>X, W=>Y.  And sometimes it's an artifact of people's beliefs about corr(X, Y).  With intelligent agents, perceived causation causes correlation.\n\nVolvos are believed by many people to be safe.  Volvo has an excellent record of being concerned with safety; they introduced 3-point seat belts, crumple zones, laminated windshields, and safety cages, among other things.  But how would you evaluate the claim that Volvos are safer than other cars?\n\nPresumably, you'd look at the accident rate for Volvos compared to the accident rate for similar cars driven by a similar demographic, as reflected, for instance in insurance rates.  (My google-fu did not find accident rates posted on the internet, but [insurance rates](http://www.leaseguide.com/articles/autoinsurance_bestcars.htm) don't come out especially pro-Volvo.)  But suppose the results showed that Volvos had only 3/4 as many accidents as similar cars driven by similar people.  Would that prove Volvos are safer?\n\nPerceived causation causes correlation\n--------------------------------------\n\nNo.  Besides having a reputation for safety, Volvos also have a reputation for being overpriced and ugly.  Mostly people who are concerned about safety buy Volvos.  Once the reputation exists, even if it's not true, a cycle begins that feeds on itself:  Cautious drivers buy Volvos, have fewer accidents, resulting in better statistics, leading more cautious drivers to buy Volvos.\n\nDo Montessori schools or home-schooling result in better scores on standardized tests?  I'd bet that they do.  Again, my google-fu is not strong enough to find any actual reports on, say, average SAT-score increases for students in Montessori schools vs. public schools.  But the largest observable factor determining student test scores, last I heard, is participation by the parents.  Any new education method will show increases in student test scores if people believe it results in increases in student test scores, because only interested parents will sign up for that method.  The crazier, more-expensive, and more-difficult the method is, the more improvement it should show; craziness should filter out less-committed parents.\n\nAre vegetarian diets or yoga healthy for you?  Does using the phone while driving increase accident rates?  Yes, probably; but there is a self-fulfilling component in the data that is difficult to factor out.\n\nConditions under which this occurs  \n\n-------------------------------------\n\nIf you believe X helps you achieve Y, and so you use X when you are most-motivated to achieve Y _and your motivation has some bearing on the outcome_, you will observe a correlation between X and Y.\n\nThis won't happen if your motivation or attitude has no bearing on the outcome (beyond your choice of X).  If passengers prefer one airline based on their perception of its safety, that won't make its safety record improve.\n\nHowever, this is different from either confidence or the placebo effect.  I'm not talking about the PUA mantra that \"if you believe a pickup line will work, it will work\".  And I'm not talking about feeling better when you take a pill that you think will help you feel better.  This is a _sample-selection bias_.  A person is more likely to choose X when they are motivated to achieve Y _relative to other possible positive_ outcomes of X, and hence more inclined to make many other little trade-offs to achieve Y which will not be visible in the data set.\n\nIt's also not the effect people are guarding against with double-blind experiments.  That's guarding against the experimenter favoring one method over another.  This is, rather, an effect guarded against with random assignment to different groups.\n\nNor should it happen in cases where the outcome being studied is the _only_ outcome people consider.  If a Montessori school cost the same, and was just as convenient for the parents, as every other school, and all factors other than test score were equal, and Montessori schools were believed to increase test scores, then any parent who cared at all would choose the Montessori school.  The filtering effect would vanish, and so would the portion of the test-score increase caused by it.  Same story if one choice improves all the outcomes under consideration:  Aluminum tennis racquets are better than wooden racquets in weight, sweet spot size, bounce, strength, air resistance, longevity, time between restrings, and cost.  You need not suspect a self-fulfilling correlation.\n\nIt may be cancelled by a balancing effect, when you are more highly-motivated to achieve Y when you are less likely to achieve Y.  In sports, if you wear your lucky undershirt only for tough games, you'll find it appears to be unlucky, because you're more likely to lose tough games.  Another balancing effect is if your choice of X makes you feel so confident of attaining Y that you act less concerned about Y; an example is (IIRC) research showing that people wearing seat-belts are more likely to get into accidents.\n\nApplication to machine learning and smart people  \n\n---------------------------------------------------\n\nBack in the late 1980s, neural networks were hot; and evaluations usually indicated that they outperformed other methods of classification.  In the early 1990s, genetic algorithms were hot; and evaluations usually indicated that they outperformed other methods of classification.  Today, support vector machines (SVMs) are hot; and evaluations usually indicate that they outperform other methods of classifications.  Neural networks and genetic algorithms no longer outperform older methods.  (I write this from memory, so you shouldn't take it as gospel.)\n\nThere is a publication bias:  When a new technology appears, publications indicating it performs well are interesting.  Once it's established, publications indicating it performs poorly are interesting.  But there's also a selection bias.  People strongly motivated to make their systems work well on difficult problems are strongly motivated to try new techniques; and also to fiddle with the parameters until they work well.\n\nFads can create self-fulfilling correlations.  If neural networks are hot, the smartest people tend to work on neural networks.  When you compare their results to other results, it can be difficult to look at neural networks vs., say, logistic regression; and factor out the smartest people vs. pretty smart people effect.\n\n(The attention of smart people is a proxy for effectiveness, which often misleads other smart people - e.g., the popularity of communism among academics in America in the 1930s.  But that's yet another separate issue.)"
          },
          "voteCount": 117
        },
        {
          "name": "Probability is in the Mind",
          "type": "post",
          "slug": "probability-is-in-the-mind",
          "_id": "f6ZLxEWaankRZ2Crv",
          "url": null,
          "title": "Probability is in the Mind",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Mind Projection Fallacy"
            },
            {
              "name": "Bayes' Theorem"
            },
            {
              "name": "Map and Territory"
            },
            {
              "name": "Bayesianism"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "[![Monsterwithgirl_2](/static/imported/2007/08/10/monsterwithgirl_2.jpg \"Monsterwithgirl_2\")](/static/imported/2007/08/10/monsterwithgirl_2.jpg)\n\nYesterday I spoke of the Mind Projection Fallacy, giving the example of the alien monster who carries off a girl in a torn dress for intended ravishing—a mistake which I imputed to the artist's tendency to think that a woman's sexiness is a property of the woman herself, woman.sexiness, rather than something that exists in the mind of an observer, and probably wouldn't exist in an alien mind.\n\nThe term \"Mind Projection Fallacy\" was coined by the late great Bayesian Master, E. T. Jaynes, as part of his long and hard-fought battle against the accursèd frequentists.  Jaynes was of the opinion that probabilities were in the mind, not in the environment—that probabilities express ignorance, states of partial information; and if I am ignorant of a phenomenon, that is a fact about my state of mind, not a fact about the phenomenon.\n\nI cannot do justice to this ancient war in a few words—but the classic example of the argument runs thus:\n\nYou have a coin.  \nThe coin is biased.  \nYou don't know which way it's biased or how much it's biased.  Someone just told you, \"The coin is biased\" and that's all they said.  \nThis is all the information you have, and the only information you have.\n\nYou draw the coin forth, flip it, and slap it down.\n\nNow—before you remove your hand and look at the result—are you willing to say that you assign a 0.5 probability to the coin having come up heads?\n\nThe frequentist says, \"No.  Saying 'probability 0.5' means that the coin has an inherent propensity to come up heads as often as tails, so that if we flipped the coin infinitely many times, the ratio of heads to tails would approach 1:1.  But we know that the coin is biased, so it can have any probability of coming up heads _except_ 0.5.\"\n\nThe Bayesian says, \"Uncertainty exists in the map, not in the territory.  In the real world, the coin has either come up heads, or come up tails.  Any talk of 'probability' must refer to the _information_ that I have about the coin—my state of partial ignorance and partial knowledge—not just the coin itself.  Furthermore, I have all sorts of theorems showing that if I don't treat my partial knowledge a [certain way](http://www.overcomingbias.com/2008/01/something-to-pr.html), I'll make stupid bets.  If I've got to plan, I'll plan for a 50/50 state of uncertainty, where I don't weigh outcomes conditional on heads any more heavily in my mind than outcomes conditional on tails.  You can call that number whatever you like, but it has to obey the probability laws on pain of stupidity.  So I don't have the slightest hesitation about calling my outcome-weighting a probability.\"\n\nI side with the Bayesians.  You may have noticed that about me.\n\nEven before a fair coin is tossed, the notion that it has an _inherent_ 50% probability of coming up heads may be just plain wrong.  Maybe you're holding the coin in such a way that it's just about guaranteed to come up heads, or tails, given the force at which you flip it, and the air currents around you.  But, if you don't know which way the coin is biased on this one occasion, so what?\n\nI believe there was a lawsuit where someone alleged that the draft lottery was unfair, because the slips with names on them were not being mixed thoroughly enough; and the judge replied, \"To whom is it unfair?\"\n\nTo make the coinflip experiment repeatable, as frequentists are wont to demand, we could build an automated coinflipper, and verify that the results were 50% heads and 50% tails.  But maybe a robot with extra-sensitive eyes and a good grasp of physics, watching the autoflipper prepare to flip, could predict the coin's fall in advance—not with certainty, but with 90% accuracy.  Then what would the _real_ probability be?\n\nThere is no \"real probability\".  The robot has one state of partial information.  You have a different state of partial information.  The coin itself has no mind, and doesn't assign a probability to anything; it just flips into the air, rotates a few times, bounces off some air molecules, and lands either heads or tails.\n\nSo that is the Bayesian view of things, and I would now like to point out a couple of classic brainteasers that derive their brain-_teasing_ ability from the tendency to think of probabilities as inherent properties of objects.\n\nLet's take the old classic:  You meet a mathematician on the street, and she happens to mention that she has given birth to two children on two separate occasions.  You ask:  \"Is at least one of your children a boy?\"  The mathematician says, \"Yes, he is.\"\n\nWhat is the probability that she has two boys?  If you assume that the prior probability of a child being a boy is 1/2, then the probability that she has two boys, on the information given, is 1/3.  The prior probabilities were:  1/4 two boys, 1/2 one boy one girl, 1/4 two girls.  The mathematician's \"Yes\" response has probability ~1 in the first two cases, and probability ~0 in the third.  Renormalizing leaves us with a 1/3 probability of two boys, and a 2/3 probability of one boy one girl.\n\nBut suppose that instead you had asked, \"Is your eldest child a boy?\" and the mathematician had answered \"Yes.\"  Then the probability of the mathematician having two boys would be 1/2.  Since the eldest child is a boy, and the younger child can be anything it pleases.\n\nLikewise if you'd asked \"Is your youngest child a boy?\"  The probability of their being both boys would, again, be 1/2.\n\nNow, if at least one child is a boy, it must be either the oldest child who is a boy, or the youngest child who is a boy.  So how can the answer in the first case be different from the answer in the latter two?\n\nOr here's a very similar problem:  Let's say I have four cards, the ace of hearts, the ace of spades, the two of hearts, and the two of spades.  I draw two cards at random.  You ask me, \"Are you holding at least one ace?\" and I reply \"Yes.\"  What is the probability that I am holding a pair of aces?  It is 1/5.  There are six possible combinations of two cards, with equal prior probability, and you have just eliminated the possibility that I am holding a pair of twos.  Of the five remaining combinations, only one combination is a pair of aces.  So 1/5.\n\nNow suppose that instead you asked me, \"Are you holding the ace of spades?\"  If I reply \"Yes\", the probability that the other card is the ace of hearts is 1/3.  (You know I'm holding the ace of spades, and there are three possibilities for the other card, only one of which is the ace of hearts.)  Likewise, if you ask me \"Are you holding the ace of hearts?\" and I reply \"Yes\", the probability I'm holding a pair of aces is 1/3.\n\nBut then how can it be that if you ask me, \"Are you holding at least one ace?\" and I say \"Yes\", the probability I have a pair is 1/5?  Either I must be holding the ace of spades or the ace of hearts, as you know; and either way, the probability that I'm holding a pair of aces is 1/3.\n\nHow can this be?  Have I miscalculated one or more of these probabilities?\n\nIf you want to figure it out for yourself, do so now, because I'm about to reveal...\n\nThat all stated calculations are correct.\n\nAs for the paradox, there isn't one.  The _appearance_ of paradox comes from thinking that the probabilities must be properties of the cards themselves.  The ace I'm holding has to be either hearts or spades; but that doesn't mean that your _knowledge about_ my cards must be the same as if you _knew_ I was holding hearts, or _knew_ I was holding spades.\n\nIt may help to think of Bayes's Theorem:\n\n> P(H|E) = P(E|H)P(H) / P(E)\n\nThat last term, where you divide by P(E), is the part where you throw out all the possibilities that have been eliminated, and renormalize your probabilities over what remains.\n\nNow let's say that you ask me, \"Are you holding at least one ace?\"  _Before_ I answer, your probability that I say \"Yes\" should be 5/6.\n\nBut if you ask me \"Are you holding the ace of spades?\", your prior probability that I say \"Yes\" is just 1/2.\n\nSo right away you can see that you're _learning_ something very different in the two cases.  You're going to be eliminating some different possibilities, and renormalizing using a different P(E).  If you learn two different items of evidence, you shouldn't be surprised at ending up in two different states of partial information.\n\nSimilarly, if I ask the mathematician, \"Is at least one of your two children a boy?\" I expect to hear \"Yes\" with probability 3/4, but if I ask \"Is your eldest child a boy?\" I expect to hear \"Yes\" with probability 1/2.  So it shouldn't be surprising that I end up in a different state of partial knowledge, depending on which of the two questions I ask.\n\nThe only reason for seeing a \"paradox\" is thinking as though the probability of holding a pair of aces is _a property of cards_ that have at least one ace, or a property _of cards_ that happen to contain the ace of spades.  In which case, it would be paradoxical for card-sets containing at least one ace to have an inherent pair-probability of 1/5, while card-sets containing the ace of spades had an inherent pair-probability of 1/3, and card-sets containing the ace of hearts had an inherent pair-probability of 1/3.\n\nSimilarly, if you think a 1/3 probability of being both boys is an _inherent property_ of child-sets that include at least one boy, then that is not consistent with child-sets of which the eldest is male having an _inherent_ probability of 1/2 of being both boys, and child-sets of which the youngest is male having an inherent 1/2 probability of being both boys.  It would be like saying, \"All green apples weigh a pound, and all red apples weigh a pound, and all apples that are green or red weigh half a pound.\"\n\nThat's what happens when you start thinking as if probabilities are _in_ things, rather than probabilities being states of partial information _about_ things.\n\nProbabilities express uncertainty, and it is only agents who can be uncertain.  A blank map does not correspond to a blank territory.  Ignorance is in the mind."
          },
          "voteCount": 95
        },
        {
          "name": "You're Entitled to Arguments, But Not (That Particular) Proof",
          "type": "post",
          "slug": "you-re-entitled-to-arguments-but-not-that-particular-proof",
          "_id": "vqbieD9PHG8RRJddu",
          "url": null,
          "title": "You're Entitled to Arguments, But Not (That Particular) Proof",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Motivated Reasoning"
            },
            {
              "name": "Evolution"
            },
            {
              "name": "Religion"
            },
            {
              "name": "Climate Change"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Followup to**:  [Logical Rudeness](/lw/1p1/logical_rudeness/)\n\n> \"Modern man is so committed to empirical knowledge, that he sets the standard for evidence higher than either side in his disputes can attain, thus suffering his disputes to be settled by philosophical arguments as to which party must be crushed under the burden of proof.\"  \n>         \\-\\- [Alan Crowe](http://www.overcomingbias.com/2008/09/beware-high-sta.html#comment-396465)\n\nThere's a story - in accordance with [Poe's Law](http://rationalwiki.com/wiki/Poe%27s_Law), I have no idea whether it's a joke or it actually happened - about a creationist who was trying to claim a \"gap\" in the fossil record, two species without an intermediate fossil having been discovered.  When an intermediate species _was_ discovered, the creationist responded, \"Aha!  Now there are _two_ gaps.\"\n\nSince I'm not a professional evolutionary biologist, I couldn't begin to rattle off all the ways that we know evolution is true; true facts tend to [leave traces](/lw/uw/entangled_truths_contagious_lies/) of themselves behind, and [evolution](http://wiki.lesswrong.com/wiki/Evolution) is the hugest fact in all of biology.  My specialty is the cognitive sciences, so I can tell you [of my own knowledge](/lw/la/truly_part_of_you/) that the human brain looks just like we'd expect it to look if it had evolved, and not at all like you'd think it would look if it'd been intelligently designed.  And I'm not really going to say much more on that subject.  As I once said to someone who questioned whether humans were really related to apes:  \"That question might have made sense when Darwin first came up with the hypothesis, but this is the twenty-first century.  We can _read the genes._  Human beings and chimpanzees have _95% shared genetic material._  It's _over._\"\n\nWell, it's over, unless you're crazy like a human (ironically, more evidence that the human brain was fashioned by [a sloppy and alien god](/lw/kr/an_alien_god/)).  If you're crazy like a human, you will engage in [motivated cognition](http://wiki.lesswrong.com/wiki/Motivated_cognition); and instead of focusing on the unthinkably huge heaps of evidence in favor of evolution, the innumerable signs by which the fact of evolution has left its heavy footprints on all of reality, the uncounted observations that discriminate between the world we'd expect to see if intelligent design ruled and the world we'd expect to see if evolution were true...\n\n...instead you search your mind, and you pick out one form of proof that you think evolutionary biologists _can't_ provide; and you demand, you insist upon that one form of proof; and when it is not provided, you take that as a refutation.\n\nYou say, \"Have you ever _seen_ an ape species evolving into a human species?\"  You insist on videotapes - on that _particular_ proof.\n\nAnd that _particular proof_ is one we couldn't possibly be expected to have on hand; it's a form of evidence we couldn't possibly be expected to be able to provide, _even given that evolution is true._\n\nYet it follows illogically that if a video tape would provide definite proof, then, likewise, the absence of a videotape must constitute definite disproof.  Or perhaps just render all other arguments void and turn the issue into a mere matter of personal opinion, with no one's opinion being better than anyone else's.\n\nSo far as I can tell, the position of human-caused global warming (anthropogenic global warming aka AGW) has the ball.  I get the impression there's a lot of evidence piled up, a lot of people trying and failing to poke holes, and so I have no reason to play contrarian here.  It's now heavily politicized science, which means that I take the assertions with a grain of skepticism and worry - well, to be honest I _don't_ spend a whole lot of time worrying about it, because (a) there are worse global catastrophic risks and (b) lots of other people are worrying about AGW already, so there are much better places to invest the next marginal minute of worry.\n\nBut if I pretend for a moment to live in the mainstream mental universe in which there is [nothing scarier to worry about](http://www.nickbostrom.com/existential/risks.html) than global warming, and a 6 °C (11 °F) rise in global temperatures by 2100 seems like a top issue for the care and feeding of humanity's future...\n\nThen I must shake a disapproving finger at anyone who claims the state of evidence on AGW is indefinite.\n\nSure, if we waited until 2100 to see how much global temperatures increased and how high the seas rose, we would have definite proof.  We would have definite proof in 2100, however, and that sounds just a little bit way the hell too late.  If there are cost-effective things we can do to mitigate global warming - and by this I don't mean ethanol-from-corn or cap-and-trade, more along the lines of standardizing on a liquid fluoride thorium reactor design and building 10,000 of them - if there's something we can do about AGW, we need to do it _now,_ not in a hundred years.\n\nWhen the hypothesis at hand makes _time valuable_ \\- when the proposition at hand, conditional on its being true, means there are certain things we should be doing NOW - then you've got to do your best to figure things out with the evidence that we have.  Sure, if we had annual data on global temperatures and CO2 going back to 100 million years ago, we would know more than we do right now.  But we don't have that time-series data - not because global-warming advocates destroyed it, or because they were neglectful in gathering it, but because they couldn't possibly be expected to provide it in the first place.  And so we've got to look among the observations we _can_ perform, to find those that discriminate between \"the way the world could be expected to look if AGW is true / a big problem\", and \"the way the world would be expected to look if AGW is false / a small problem\".  If, for example, we discover large deposits of [frozen methane clathrates](http://en.wikipedia.org/wiki/Arctic_methane_release) that are released with rising temperatures, this at least _seems_ like \"the sort of observation\" we might be making if we live in the sort of world where AGW is a big problem.  It's not a necessary connection, it's not sufficient on its own, it's something we _could_ potentially also observe in a world where AGW is _not_ a big problem - but unlike the perfect data we can never obtain, it's something we can actually find out, and in fact _have_ found out.\n\nYes, we've never actually experimented to observe the results over 50 years of artificially adding a large amount of carbon dioxide to the atmosphere.  But we know from physics that it's a greenhouse gas.  It's not a [privileged hypothesis](http://wiki.lesswrong.com/wiki/Privileging_the_hypothesis) we're pulling out of nowhere.  It's not like saying \"You can't _prove_ there's no invisible pink unicorn in my garage!\"  AGW is, _ceteris paribus,_ what we should expect to happen if the other things we believe are true.  We don't have any experimental results on what will happen 50 years from now, and so you can't grant the proposition the special, super-strong status of something that has been [scientifically confirmed](/lw/in/scientific_evidence_legal_evidence_rational/) by a replicable experiment.  But as I point out in \"[Scientific Evidence, Legal Evidence, Rational Evidence](/lw/in/scientific_evidence_legal_evidence_rational/)\", if science couldn't say anything about that which has not already been observed, we couldn't ever make scientific _predictions_ by which the theories could be _confirmed._  Extrapolating from the science we _do_ know, global warming _should_ be occurring; you would need specific experimental evidence to _contradict_ that.\n\nWe are, I think, dealing with that old problem of motivated cognition.  As Gilovich says:  \"Conclusions a person does not want to believe are held to a higher standard than conclusions a person wants to believe.  In the former case, the person asks if the evidence _compels_ one to accept the conclusion, whereas in the latter case, the person asks instead if the evidence _allows_ one to accept the conclusion.\"  People [map the domain of belief onto the social domain of authority](/lw/mn/absolute_authority/), with a qualitative difference between absolute and nonabsolute demands:  If a teacher tells you certain things, and you have to believe them, and you have to recite them back on the test.  But when a student makes a suggestion in class, you don't have to go along with it - you're free to agree or disagree (it seems) and no one will punish you.\n\nAnd so the implicit emotional theory is that if something is not _proven_ \\- better yet, proven using a _particular_ piece of evidence that isn't available and that you're pretty sure is never going to become available - then you are allowed to disbelieve; it's like something a student says, not like something a teacher says.\n\nYou demand particular proof P; and if proof P is not available, then you're _allowed to disbelieve._\n\nAnd this is flatly wrong as probability theory.\n\nIf the hypothesis at hand is H, and we have access to pieces of evidence E1, E2, and E3, but we do _not_ have access to proof X one way or the other, then the rational probability estimate is the result of the [Bayesian](http://wiki.lesswrong.com/wiki/Bayes%27s_Theorem) [update](http://wiki.lesswrong.com/wiki/Updating) P(H|E1,E2,E3).  You do not get to say, \"Well, we don't know whether X or ~X, so I'm going to throw E1, E2, and E3 out the window until you tell me about X.\"  I cannot begin to describe how much that is not the way the laws of probability theory work.  You do not get to [screen off](/lw/lx/argument_screens_off_authority/) E1, E2, and E3 based on your _ignorance_ of X!\n\nNor do you get to ignore the arguments that influence the prior probability of H - the standard science by which, _ceteris paribus_ and without anything unknown at work, carbon dioxide is a greenhouse gas and _ought_ to make the Earth hotter.\n\nNor can you hold up the nonobservation of your particular proof X as a triumphant refutation.  If we _had_ time cameras and could look into the past, then indeed, the fact that no one had ever \"seen with their own eyes\" primates evolving into humans would refute the hypothesis.  But, _given_ that time cameras don't exist, then _assuming evolution to be true_ we don't expect anyone to have witnessed humans evolving from apes with our own eyes, for the laws of natural selection _require_ that this have happened far in the distant past.  And so, once you have updated on the fact that time cameras don't exist - computed P(_Evolution_|_~Camera_) \\- and the fact that time cameras don't exist hardly seems to refute the theory of evolution - then you obtain no further evidence by observing _~Video,_ i.e., P(_Evolution_|_~Video_,_~Camera_) = P(_Evolution_|_~Camera_).  In slogan-form, \"The absence of unobtainable proof is not even weak evidence of absence.\"  See appendix for details.\n\n(And while we're on the subject, yes, the laws of probability theory are laws, rather than suggestions.  It is like something the teacher tells you, okay?  If you're going to ignore the Bayesian update you logically have to perform when you see a new piece of evidence, you might as well ignore outright mathematical proofs.  I see no reason why it's any less epistemically sinful to ignore probabilities than to ignore certainties.)\n\nThrowing E1, E2 and E3 out the window, and ignoring the prior probability of H, because you haven't seen unobtainable proof x; or holding up the nonobservation of X as a triumphant refutation, when you couldn't reasonably expect to see X even given that the underlying theory is true; all this is more than just a formal probability-theoretic mistake.  It is _[logically rude](/lw/1p1/logical_rudeness/)._\n\nAfter all - in the absence of your unobtainable particular proof, there may be plenty of other arguments by which you can hope to figure out whether you live in a world where the hypothesis of interest is true, or alternatively false.  _It takes work to provide you with those arguments._  It takes work to provide you with extrapolations of existing knowledge to prior probabilities, and items of evidence with which to update those prior probabilities, to form a prediction about the unseen.  _Someone who does the work to provide those arguments is doing the best they can by you; throwing the arguments out the window is not just irrational, but logically rude._\n\nAnd I emphasize this, because it seems to me that the underlying metaphor of demanding particular proof is to say as if, \"You are supposed to provide me with a video of apes evolving into humans, I am entitled to see it with my own eyes, and it is your responsibility to make that happen; and if you do not provide me with that particular proof, you are deficient in your duties of argument, and I have no obligation to believe you.\"  And this is, in the first place, bad math as probability theory.  And it is, in the second place, an attitude of trying to be _defensible_ rather than _accurate,_ the attitude of someone who wants to be allowed to retain the beliefs they have, and not the attitude of someone who is [honestly curious](/lw/jz/the_meditation_on_curiosity/) and trying to figure out which possible world they live in, by whatever signs _are_ available.  But if these considerations do not move you, then even in terms of the original and flawed metaphor, you are in the wrong: you are _entitled to arguments, but not that particular proof._\n\nIgnoring someone's hard work to _provide_ you with the arguments you need - the extrapolations from existing knowledge to make predictions about events not yet observed, the items of evidence that are suggestive even if not definite and that fit some possible worlds better than others - and instead demanding proof they can't possibly give you, proof they couldn't be expected to provide _even if they were right_ \\- _that_ is logically rude.  It is invalid as probability theory, foolish on the face of it, and logically rude.\n\nAnd of course if you go so far as to _act smug_ about the absence of an unobtainable proof, or chide the other for their credulity, then you have crossed the line into outright ordinary rudeness as well.\n\nIt is likewise a madness of decision theory to hold off pending positive proof until it's too late to do anything; the whole point of decision theory is to choose under conditions of uncertainty, and that is not how the expected value of information is likely to work out.  Or in terms of plain common sense:  There are signs and portents, smoke alarms and hot doorknobs, by which you can hope to determine whether your house is on fire _before_ your face melts off your skull; and to delay leaving the house until _after_ your face melts off, because only this is the positive and particular proof that you demand, is decision-theoretical insanity.  It doesn't matter if you cloak your demand for that unobtainable proof under the heading of scientific procedure, saying, \"These are the proofs you could not obtain even if you were right, which I know you will not be able to obtain until the time for action has long passed, which surely any scientist would demand before confirming your proposition as a [scientific truth](/lw/in/scientific_evidence_legal_evidence_rational/).\"  It's still nuts.\n\n* * *\n\n_Since this post has already gotten long, I've moved some details of probability theory, the subtext on cryonics, the sub-subtext on molecular nanotechnology, and the sub-sub-subtext on Artificial Intelligence, into:_\n\n**[Demands for Particular Proof:  Appendices](/lw/1rv/demands_for_particular_proof_appendices/)**."
          },
          "voteCount": 68
        },
        {
          "name": "Efficient Charity: Do Unto Others...",
          "type": "post",
          "slug": "efficient-charity-do-unto-others",
          "_id": "pC47ZTsPNAkjavkXs",
          "url": null,
          "title": "Efficient Charity: Do Unto Others...",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Altruism"
            },
            {
              "name": "World Optimization"
            },
            {
              "name": "Cause Prioritization"
            },
            {
              "name": "Motivational Intro Posts"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_This was originally posted as part of the [efficient charity](/lw/35o/100_for_the_best_article_on_efficient_charity/) contest back in November. Thanks to Roko, multifoliaterose, Louie, jmmcd, jsalvatier, and others I forget for help, corrections, encouragement, and bothering me until I finally remembered to post this here._  \n  \nImagine you are setting out on a dangerous expedition through the Arctic on a limited budget. The grizzled old prospector at the general store shakes his head sadly: you can't afford everything you need; you'll just have to purchase the bare essentials and hope you get lucky. But what is essential? Should you buy the warmest parka, if it means you can't afford a sleeping bag? Should you bring an extra week's food, just in case, even if it means going without a rifle? Or can you buy the rifle, leave the food, and hunt for your dinner?  \n  \nAnd how about the field guide to Arctic flowers? You like flowers, and you'd hate to feel like you're failing to appreciate the harsh yet delicate environment around you. And a digital camera, of course - if you make it back alive, you'll have to put the Arctic expedition pics up on Facebook. And a hand-crafted scarf with authentic Inuit tribal patterns woven from organic fibres! Wicked!  \n  \n...but of course buying any of those items would be insane. The problem is what economists call opportunity costs: buying one thing costs money that could be used to buy others. A hand-crafted designer scarf might have some value in the Arctic, but it would cost so much it would prevent you from buying much more important things. And when your life is on the line, things like impressing your friends and buying organic pale in comparison. You have one goal - staying alive - and your only problem is how to distribute your resources to keep your chances as high as possible. These sorts of economics concepts are natural enough when faced with a journey through the freezing tundra.\n\n  \nBut they are decidedly not natural when facing a decision about charitable giving. Most donors say they want to \"help people\". If that's true, they should try to distribute their resources to help people as much as possible. Most people don't. In the [\"Buy A Brushstroke\"](http://www.artfund.org/savebluerigi/Introduction.html) campaign, eleven thousand British donors gave a total of £550,000 to keep the famous painting \"Blue Rigi\" in a UK museum. If they had given that £550,000 to buy better sanitation systems in African villages instead, the latest statistics suggest it would have saved the lives of about one thousand two hundred people from disease. Each individual $50 donation could have given a year of normal life back to a Third Worlder afflicted with a disabling condition like blindness or limb deformity..  \n  \nMost of those 11,000 donors genuinely wanted to help people by preserving access to the original canvas of a beautiful painting. And most of those 11,000 donors, if you asked, would say that a thousand people's lives are more important than a beautiful painting, original or no. But these people didn't have the proper mental habits to realize that was the choice before them, and so a beautiful painting remains in a British museum and somewhere in the Third World a thousand people are dead.  \n  \nIf you are to \"love your neighbor as yourself\", then you should be as careful in maximizing the benefit to others when donating to charity as you would be in maximizing the benefit to yourself when choosing purchases for a polar trek. And if you wouldn't buy a pretty picture to hang on your sled in preference to a parka, you should consider not helping save a famous painting in preference to helping save a thousand lives.  \n  \nNot all charitable choices are as simple as that one, but many charitable choices do have right answers. GiveWell.org, a site which collects and interprets data on the effectiveness of charities, predicts that antimalarial drugs save one child from malaria per $5,000 worth of medicine, but insecticide-treated bed nets save one child from malaria per $500 worth of netting. If you want to save children, donating bed nets instead of antimalarial drugs is the objectively right answer, the same way buying a $500 TV instead of an identical TV that costs $5,000 is the right answer. And since saving a child from diarrheal disease costs $5,000, donating to an organization fighting malaria instead of an organization fighting diarrhea is the right answer, unless you are donating based on some criteria other than whether you're helping children or not.  \n  \nSay all of the best Arctic explorers agree that the three most important things for surviving in the Arctic are good boots, a good coat, and good food. Perhaps they have run highly unethical studies in which they release thousands of people into the Arctic with different combination of gear, and consistently find that only the ones with good boots, coats, and food survive. Then there is only one best answer to the question \"What gear do I buy if I want to survive\" - good boots, good food, and a good coat. Your preferences are irrelevant; you may choose to go with alternate gear, but only if you don't mind dying.  \n  \nAnd likewise, there is only one best charity: the one that helps the most people the greatest amount per dollar. This is vague, and it is up to you to decide whether a charity that raises forty children's marks by one letter grade for $100 helps people more or less than one that prevents one fatal case of tuberculosis per $100 or one that saves twenty acres of rainforest per $100. But you cannot abdicate the decision, or you risk ending up like the 11,000 people who accidentally decided that a pretty picture was worth more than a thousand people's lives.  \n  \nDeciding which charity is the best is hard. It may be straightforward to say that one form of antimalarial therapy is more effective than another. But how do both compare to financing medical research that might or might not develop a \"magic bullet\" cure for malaria? Or financing development of a new kind of supercomputer that might speed up all medical research? There is no easy answer, but the question has to be asked.  \n  \nWhat about just comparing charities on overhead costs, the one easy-to-find statistic that's universally applicable across all organizations? This solution is simple, elegant, and wrong. High overhead costs are only one possible failure mode for a charity. Consider again the Arctic explorer, trying to decide between a $200 parka and a $200 digital camera. Perhaps a parka only cost $100 to make and the manufacturer takes $100 profit, but the camera cost $200 to make and the manufacturer is selling it at cost. This speaks in favor of the moral qualities of the camera manufacturer, but given the choice the explorer should still buy the parka. The camera does something useless very efficiently, the parka does something vital inefficiently. A parka sold at cost would be best, but in its absence the explorer shouldn't hesitate to choose the the parka over the camera. The same applies to charity. An antimalarial net charity that saves one life per $500 with 50% overhead is better than an antidiarrheal drug charity that saves one life per $5000 with 0% overhead: $10,000 donated to the high-overhead charity will save ten lives; $10,000 to the lower-overhead will only save two. Here the right answer is to donate to the antimalarial charity while encouraging it to find ways to lower its overhead. In any case, [examining the financial practices of a charity](http://www.charitynavigator.org/) is helpful but not enough to answer the \"which is the best charity?\" question.  \n  \nJust as there is only one best charity, there is only one best way to donate to that charity. Whether you [volunteer versus donate money](/lw/65/money_the_unit_of_caring/) versus raise awareness is your own choice, but that choice has consequences. If a high-powered lawyer who makes $1,000 an hour chooses to take an hour off to help clean up litter on the beach, he's wasted the opportunity to work overtime that day, make $1,000, donate to a charity that will hire a hundred poor people for $10/hour to clean up litter, and end up with a hundred times more litter removed. If he went to the beach because he wanted the sunlight and the fresh air and the warm feeling of personally contributing to something, that's fine. If he actually wanted to help people by beautifying the beach, he's chosen an objectively wrong way to go about it. And if he wanted to help people, period, he's chosen a very wrong way to go about it, since that $1,000 could save two people from malaria. Unless the litter he removed is really worth more than two people's lives to him, he's erring even according to his own value system.  \n  \n...and the same is true if his philanthropy leads him to work full-time at a nonprofit instead of going to law school to become a lawyer who makes $1,000 / hour in the first place. Unless it's one HELL of a nonprofit.  \n  \nThe Roman historian Sallust said of Cato \"He preferred to be good, rather than to seem so\". The lawyer who quits a high-powered law firm to work at a nonprofit organization certainly seems like a good person. But if we define \"good\" as helping people, then the lawyer who stays at his law firm but donates the profit to charity is taking Cato's path of maximizing how much good he does, rather than how good he looks.  \n  \nAnd this dichotomy between being and seeming good applies not only to looking good to others, but to ourselves. When we donate to charity, one incentive is the [warm glow of a job well done](/lw/6z/purchase_fuzzies_and_utilons_separately/). A lawyer who spends his day picking up litter will feel a sense of personal connection to his sacrifice and relive the memory of how nice he is every time he and his friends return to that beach. A lawyer who works overtime and donates the money online to starving orphans in Romania may never get that same warm glow. But concern with a warm glow is, at root, concern about seeming good rather than being good - albeit seeming good to yourself rather than to others. There's nothing wrong with donating to charity as a form of entertainment if it's what you want - giving money to the Art Fund may well be a quicker way to give yourself a warm feeling than seeing a romantic comedy at the cinema - but charity given by people who genuinely want to be good and not just to feel that way requires more forethought.  \n  \nIt is important to be rational about charity for the same reason it is important to be rational about Arctic exploration: it requires the same awareness of opportunity costs and the same hard-headed commitment to investigating efficient use of resources, and it may well be a matter of life and death. Consider going to [www.GiveWell.org](http://www.givewell.org) and making use of the excellent resources on effective charity they have available."
          },
          "voteCount": 158
        },
        {
          "name": "Newcomb's Problem and Regret of Rationality",
          "type": "post",
          "slug": "newcomb-s-problem-and-regret-of-rationality",
          "_id": "6ddcsdA2c2XpNpE5x",
          "url": null,
          "title": "Newcomb's Problem and Regret of Rationality",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Rationality"
            },
            {
              "name": "Decision Theory"
            },
            {
              "name": "Newcomb's Problem"
            },
            {
              "name": "Pre-Commitment"
            },
            {
              "name": "One-Boxing"
            },
            {
              "name": "Conditional Consistency"
            },
            {
              "name": "Two-Boxing"
            },
            {
              "name": "Bayesianism"
            },
            {
              "name": "Something To Protect"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The following may well be the most controversial dilemma in the history of decision theory:\n\n> A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game.  In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.\n> \n> Box A is transparent and contains a thousand dollars.  \n> Box B is opaque, and contains either a million dollars, or nothing.\n> \n> You can take both boxes, or take only box B.\n> \n> And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.\n> \n> Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars.  (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)\n> \n> Before you make your choice, Omega has flown off and moved on to its next game.  Box B is already empty or already full.\n> \n> Omega drops two boxes on the ground in front of you and flies off.\n> \n> Do you take both boxes, or only box B?\n\nAnd the standard philosophical conversation runs thusly:\n\n> One-boxer:  \"I take only box B, of course.  I'd rather have a million than a thousand.\"\n> \n> Two-boxer:  \"Omega has already left.  Either box B is already full or already empty.  If box B is already empty, then taking both boxes nets me $1000, taking only box B nets me $0.  If box B is already full, then taking both boxes nets $1,001,000, taking only box B nets $1,000,000.  In either case I do better by taking both boxes, and worse by leaving a thousand dollars on the table - so I will be rational, and take both boxes.\"\n> \n> One-boxer:  \"If you're so rational, why ain'cha rich?\"\n> \n> Two-boxer:  \"It's not my fault Omega chooses to reward only people with irrational dispositions, but it's already too late for me to do anything about that.\"\n\nThere is a _large_ literature on the topic of Newcomblike problems - especially if you consider the Prisoner's Dilemma as a special case, which it is generally held to be.  \"Paradoxes of Rationality and Cooperation\" is an edited volume that includes Newcomb's original essay.  For those who read only online material, [this PhD thesis](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.724&rep=rep1&type=pdf) summarizes the major standard positions.\n\nI'm not going to go into the whole literature, but the dominant consensus in modern decision theory is that one should two-box, and Omega is just rewarding agents with irrational dispositions.  This dominant view goes by the name of \"causal decision theory\".\n\nAs you know, the primary [reason I'm blogging](/lw/jf/why_im_blooking/) is that I am an incredibly slow writer when I try to work in any other format.  So I'm not going to try to present my own analysis here.  Way too long a story, even by my standards.\n\nBut it is agreed even among causal decision theorists that if you have the power to precommit yourself to take one box, in Newcomb's Problem, then you should do so.  If you can precommit yourself before Omega examines you; then you are directly causing box B to be filled.\n\nNow in my field - which, in case you have forgotten, is self-modifying AI - this works out to saying that if you build an AI that two-boxes on Newcomb's Problem, it will self-modify to one-box on Newcomb's Problem, if the AI considers in advance that it might face such a situation.  Agents with free access to their own source code have access to a cheap method of precommitment.\n\nWhat if you expect that you might, in general, face a Newcomblike problem, without knowing the exact form of the problem?  Then you would have to modify yourself into a sort of agent whose disposition was such that it would generally receive high rewards on Newcomblike problems.\n\nBut what does an agent with a disposition generally-well-suited to Newcomblike problems look like?  Can this be formally specified?\n\nYes, but when I tried to write it up, I realized that I was starting to write a small book.  And it wasn't the most important book I had to write, so I shelved it.  My slow writing speed really is the bane of my existence.  The theory I worked out seems, to me, to have many nice properties besides being well-suited to Newcomblike problems.  It would make a nice PhD thesis, if I could get someone to accept it as my PhD thesis.  But that's pretty much what it would take to make me unshelve the project.  Otherwise I can't justify the time expenditure, not at the speed I currently write books.\n\nI say all this, because there's a common attitude that \"Verbal arguments for one-boxing are easy to come by, what's hard is developing a good decision theory that one-boxes\" - coherent math which one-boxes on Newcomb's Problem without producing absurd results elsewhere.  So I do understand that, and I did set out to develop such a theory, but my writing speed on big papers is so slow that I can't publish it.  Believe it or not, it's true.\n\nNonetheless, I would like to present some of my _motivations_ on Newcomb's Problem - the reasons I felt impelled to seek a new theory - because they illustrate my source-attitudes toward rationality.  Even if I can't present the theory that these motivations motivate...\n\nFirst, foremost, fundamentally, above all else:\n\nRational agents should WIN.\n\nDon't mistake me, and think that I'm talking about the Hollywood Rationality stereotype that rationalists should be selfish or shortsighted.  If your utility function has a term in it for others, then win their happiness.  If your utility function has a term in it for a million years hence, then win the eon.\n\nBut at any rate, _WIN_.  Don't lose reasonably, **_WIN_**.\n\nNow there are defenders of causal decision theory who argue that the two-boxers are doing their best to win, and cannot help it if they have been cursed by a Predictor who favors irrationalists.  I will talk about this defense in a moment.  But first, I want to draw a distinction between causal decision theorists who believe that two-boxers are genuinely doing their best to win; versus someone who thinks that two-boxing is the _reasonable_ or the _rational_ thing to do, but that the reasonable move just happens to predictably lose, in this case.  There are a _lot_ of people out there who think that rationality predictably loses on various problems - that, too, is part of the Hollywood Rationality stereotype, that Kirk is predictably superior to Spock.\n\nNext, let's turn to the charge that Omega favors irrationalists.  I can conceive of a superbeing who rewards only people born with a particular gene, _regardless of their choices._  I can conceive of a superbeing who rewards people whose brains inscribe the _particular algorithm_ of \"Describe your options in English and choose the last option when ordered alphabetically,\" but who does not reward anyone who chooses the same option for a different reason.  But Omega rewards people who choose to take only box B, _regardless of which algorithm they use to arrive at this decision,_ and this is why I don't buy the charge that Omega is rewarding the irrational.  Omega doesn't care whether or not you follow some particular ritual of cognition; Omega only cares about your predicted _decision_.\n\nWe can choose whatever reasoning algorithm we like, and will be rewarded or punished only according to that algorithm's choices, with no other dependency - Omega just cares where we go, not how we got there.\n\nIt is precisely the notion that Nature does not care about our _algorithm,_ which frees us up to pursue the winning Way - without attachment to any particular ritual of cognition, apart from our belief that it wins.  Every rule is up for grabs, _except_ the rule of winning.\n\nAs Miyamoto Musashi said - it's really worth repeating:\n\n> \"You can win with a long weapon, and yet you can also win with a short weapon.  In short, the Way of the Ichi school is the spirit of winning, whatever the weapon and whatever its size.\"\n\n(Another example:  It was [argued by McGee](/lw/na/trust_in_bayes/) that we must adopt bounded utility functions or be subject to \"Dutch books\" over infinite times.  But:  _The utility function is not up for grabs._  I love life [without limit or upper bound:](http://yudkowsky.net/singularity/simplified/)  There is no finite amount of life lived N where I would prefer a 80.0001% probability of living N years to an 0.0001% chance of living a googolplex years and an 80% chance of living forever.  This is a sufficient condition to imply that my utility function is unbounded.  So I just have to figure out how to optimize _for that morality._  You can't tell me, first, that above all I must conform to a particular ritual of cognition, and then that, if I conform to that ritual, I must change my morality to avoid being Dutch-booked.  Toss out the losing ritual; don't change the definition of winning.  That's like deciding to prefer $1000 to $1,000,000 so that Newcomb's Problem doesn't make your preferred ritual of cognition look bad.)\n\n\"But,\" says the causal decision theorist, \"to take only one box, you must somehow believe that your choice can affect whether box B is empty or full - and that's _unreasonable!_  Omega has already left!  It's physically impossible!\"\n\nUnreasonable?  I am a rationalist: what do I care about being unreasonable?  I don't have to conform to a particular ritual of cognition.  I don't have to take only box B _because I believe my choice affects the box, even though Omega has already left._  I can just... take only box B.\n\nI do have a proposed alternative ritual of cognition which computes this decision, which this margin is too small to contain; but I shouldn't need to show this to you.  The point is not to have an elegant theory of winning - the point is to win; elegance is a side effect.\n\nOr to look at it another way:  Rather than starting with a concept of what is the reasonable decision, and then asking whether \"reasonable\" agents leave with a lot of money, start by looking at the agents who leave with a lot of money, develop a theory of which agents tend to leave with the most money, and from this theory, try to figure out what is \"reasonable\".  \"Reasonable\" may just refer to decisions in conformance with our current ritual of cognition - what else would determine whether something seems \"reasonable\" or not?\n\nFrom James Joyce (no relation), _Foundations of Causal Decision Theory:_\n\n> Rachel has a perfectly good answer to the \"Why ain't you rich?\" question.  \"I am not rich,\" she will say, \"because I am not the kind of person the psychologist thinks will refuse the money.  I'm just not like you, Irene.  Given that I know that I am the type who takes the money, and given that the psychologist knows that I am this type, it was reasonable of me to think that the $1,000,000 was not in my account.  The $1,000 was the most I was going to get no matter what I did.  So the only reasonable thing for me to do was to take it.\"\n> \n> Irene may want to press the point here by asking, \"But don't you wish you were like me, Rachel?  Don't you wish that you were the refusing type?\"  There is a tendency to think that Rachel, a committed causal decision theorist, must answer this question in the negative, which seems obviously wrong (given that being like Irene would have made her rich).  This is not the case.  Rachel can and should admit that she does wish she were more like Irene.  \"It would have been better for me,\" she might concede, \"had I been the refusing type.\"  At this point Irene will exclaim, \"You've admitted it!  It wasn't so smart to take the money after all.\"  Unfortunately for Irene, her conclusion does not follow from Rachel's premise.  Rachel will patiently explain that wishing to be a refuser in a Newcomb problem is not inconsistent with thinking that one should take the $1,000 _whatever type one is._  When Rachel wishes she was Irene's type she is wishing _for Irene's options,_ not sanctioning her choice.\n\nIt is, I would say, a general principle of rationality - indeed, part of how I _define_ rationality - that you never end up envying someone else's mere _choices._  You might envy someone their genes, if Omega rewards genes, or if the genes give you a generally happier disposition.  But Rachel, above, envies Irene her choice, and _only_ her choice, irrespective of what algorithm Irene used to make it.  Rachel wishes _just_ that she had a disposition to choose differently.\n\nYou shouldn't claim to be more rational than someone and simultaneously envy them their choice - _only_ their choice.  Just _do_ the act you envy.\n\nI keep trying to say that rationality is the winning-Way, but causal decision theorists insist that taking both boxes is what _really_ wins, because you _can't possibly do better_ by leaving $1000 on the table... even though the single-boxers leave the experiment with more money.  Be careful of this sort of argument, any time you find yourself defining the \"winner\" as someone other than the agent who is currently smiling from on top of a giant heap of utility.\n\nYes, there are various thought experiments in which some agents start out with an advantage - but if the task is to, say, decide whether to jump off a cliff, you want to be careful not to define cliff-refraining agents as having an unfair prior advantage over cliff-jumping agents, by virtue of their unfair refusal to jump off cliffs.  At this point you have covertly redefined \"winning\" as conformance to a particular ritual of cognition.  _Pay attention to the money!_\n\nOr here's another way of looking at it:  Faced with Newcomb's Problem, would you want to look really hard for a reason to believe that it was perfectly reasonable and rational to take only box B; because, if such a line of argument existed, you would take only box B and find it full of money?  Would you spend an extra hour thinking it through, if you were confident that, at the end of the hour, you would be able to convince yourself that box B was the rational choice?  This too is a rather odd position to be in.  Ordinarily, the work of rationality goes into figuring out which choice is the best - not finding a reason to believe that a particular choice is the best.\n\nMaybe it's too easy to say that you \"ought to\" two-box on Newcomb's Problem, that this is the \"reasonable\" thing to do, so long as the money isn't actually in front of you.  Maybe you're just numb to philosophical dilemmas, at this point.  What if your daughter had a 90% fatal disease, and box A contained a serum with a 20% chance of curing her, and box B might contain a serum with a 95% chance of curing her?  What if there was an asteroid rushing toward Earth, and box A contained an asteroid deflector that worked 10% of the time, and box B might contain an asteroid deflector that worked 100% of the time?\n\nWould you, at that point, find yourself _tempted to make an unreasonable choice?_\n\nIf the stake in box B was [something you _could not_ leave behind](/lw/nb/something_to_protect/)?  Something overwhelmingly more important to you than being reasonable?  If you absolutely _had to_ win - _really_ win, not just be defined as winning?\n\nWould you _wish with all your power_ that the \"reasonable\" decision was to take only box B?\n\nThen maybe it's time to update your definition of reasonableness.\n\nAlleged rationalists should not find themselves envying the mere decisions of alleged nonrationalists, because your decision can be whatever you like.  When you find yourself in a position like this, you shouldn't chide the other person for failing to conform to your concepts of reasonableness.  You should realize you got the Way wrong.\n\nSo, too, if you ever find yourself keeping separate track of the \"reasonable\" belief, versus the belief that seems likely to be actually _true._  Either you have misunderstood reasonableness, or your second intuition is just wrong.\n\nNow one can't simultaneously _define_ \"rationality\" as the winning Way, and _define_ \"rationality\" as Bayesian probability theory and decision theory.  But it is the argument that I am putting forth, and the moral of my advice to [Trust In Bayes](/lw/na/trust_in_bayes/), that the laws governing winning have indeed proven to be [math](/lw/mt/beautiful_probability/).  If it ever turns out that Bayes fails - receives systematically lower rewards on some problem, relative to a superior alternative, in virtue of its mere decisions - then Bayes has to go _out the window._ \"Rationality\" is just the label I use for my beliefs about the winning Way - the Way of the agent smiling from on top of the giant heap of utility.  _Currently,_ that label refers to Bayescraft.\n\nI realize that this is not a knockdown criticism of causal decision theory - that would take the actual book and/or PhD thesis - but I hope it illustrates some of my underlying attitude toward this notion of \"rationality\".\n\nYou shouldn't find yourself distinguishing the winning choice from the reasonable choice.  Nor should you find yourself distinguishing the reasonable belief from the belief that is most likely to be true.\n\nThat is why I use the word \"rational\" to denote my beliefs about accuracy and winning - _not_ to denote [verbal](/lw/go/why_truth_and/) reasoning, or strategies which yield [certain](/lw/mm/the_fallacy_of_gray/) success, or that which is [logically](/lw/k2/a_priori/) provable, or that which is [publicly demonstrable](/lw/in/scientific_evidence_legal_evidence_rational/), or that which is reasonable.\n\nAs Miyamoto Musashi said:\n\n> \"The primary thing when you take a sword in your hands is your intention to cut the enemy, whatever the means. Whenever you parry, hit, spring, strike or touch the enemy's cutting sword, you must cut the enemy in the same movement. It is essential to attain this. If you think only of hitting, springing, striking or touching the enemy, you will not be able actually to cut him.\""
          },
          "voteCount": 110
        },
        {
          "name": "Occam's Razor",
          "type": "post",
          "slug": "occam-s-razor",
          "_id": "f4txACqDWithRi7hs",
          "url": null,
          "title": "Occam's Razor",
          "author": "Eliezer Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Occam's Razor"
            },
            {
              "name": "Principles"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "The more complex an explanation is, the more evidence you need just to find it in belief-space. (In Traditional Rationality this is often phrased misleadingly, as “The more complex a proposition is, the more evidence is required to argue for it.”) How can we measure the complexity of an explanation? How can we determine how much evidence is required?\n\nOccam’s Razor is often phrased as “The simplest explanation that fits the facts.” Robert Heinlein replied that the simplest explanation is “The lady down the street is a witch; she did it.”\n\nOne observes that the length of an English sentence is not a good way to measure “complexity.” And “fitting” the facts by merely *failing to prohibit* them is insufficient.\n\nWhy, exactly, is the length of an English sentence a poor measure of complexity? Because when you speak a sentence aloud, you are using *labels* for concepts that the listener shares—the receiver has already stored the complexity in them. Suppose we abbreviated Heinlein’s whole sentence as “Tldtsiawsdi!” so that the entire explanation can be conveyed in one word; better yet, we’ll give it a short arbitrary label like “Fnord!” Does this reduce the complexity? No, because you have to tell the listener in advance that “Tldtsiawsdi!” stands for “The lady down the street is a witch; she did it.” “Witch,” itself, is a label for some extraordinary assertions—just because we all know what it means doesn’t mean the concept is simple.\n\nAn enormous bolt of electricity comes out of the sky and hits something, and the Norse tribesfolk say, “Maybe a really powerful agent was angry and threw a lightning bolt.” The human brain is the most complex artifact in the known universe. If *anger* seems simple, it’s because we don’t see all the neural circuitry that’s implementing the emotion. (Imagine trying to explain why *Saturday Night Live* is funny, to an alien species with no sense of humor. But don’t feel superior; you yourself have no sense of fnord.) The complexity of anger, and indeed the complexity of intelligence, was glossed over by the humans who hypothesized Thor the thunder-agent.\n\n*To a human,* Maxwell’s equations take much longer to explain than Thor. Humans don’t have a built-in vocabulary for calculus the way we have a built-in vocabulary for anger. You’ve got to explain your language, and the language behind the language, and the very concept of mathematics, before you can start on electricity.\n\nAnd yet it seems that there should be some sense in which Maxwell’s equations are *simpler* than a human brain, or Thor the thunder-agent.\n\nThere is. It’s *enormously* easier (as it turns out) to write a computer program that simulates Maxwell’s equations, compared to a computer program that simulates an intelligent emotional mind like Thor.\n\nThe formalism of Solomonoff induction measures the “complexity of a description” by the length of the shortest computer program which produces that description as an output. To talk about the “shortest computer program” that does something, you need to specify a space of computer programs, which requires a language and interpreter. Solomonoff induction uses Turing machines, or rather, bitstrings that specify Turing machines. What if you don’t like Turing machines? Then there’s only a constant complexity penalty to design your own universal Turing machine that interprets whatever code you give it in whatever programming language you like. Different inductive formalisms are penalized by a worst-case constant factor relative to each other, corresponding to the size of a universal interpreter for that formalism.\n\nIn the better (in my humble opinion) versions of Solomonoff induction, the computer program does not produce a deterministic prediction, but assigns probabilities to strings. For example, we could write a program to explain a fair coin by writing a program that assigns equal probabilities to all 2^N^ strings of length N. This is Solomonoff induction’s approach to *fitting* the observed data. The higher the probability a program assigns to the observed data, the better that program *fits* the data. And probabilities must sum to 1, so for a program to better “fit” one possibility, it must steal probability mass from some other possibility which will then “fit” much more poorly. There is no superfair coin that assigns 100% probability to heads and 100% probability to tails.\n\nHow do we trade off the fit to the data, against the complexity of the program? If you ignore complexity penalties, and think *only* about fit, then you will always prefer programs that claim to deterministically predict the data, assign it 100% probability. If the coin shows HTTHHT, then the program that claims that the coin was fixed to show HTTHHT fits the observed data 64 times better than the program which claims the coin is fair. Conversely, if you ignore fit, and consider *only* complexity, then the “fair coin” hypothesis will always seem simpler than any other hypothesis. Even if the coin turns up HTHHTHHHTHHHHTHHHHHT  . . .\n\nIndeed, the fair coin *is* simpler and it fits this data exactly as well as it fits any other string of 20 coinflips—no more, no less—but we see another hypothesis, seeming not too complicated, that fits the data much better.\n\nIf you let a program store one more binary bit of information, it will be able to cut down a space of possibilities by half, and hence assign twice as much probability to all the points in the remaining space. This suggests that one bit of program complexity should cost *at least* a “factor of two gain” in the fit. If you try to design a computer program that explicitly stores an outcome like HTTHHT, the six bits that you lose in complexity must destroy all plausibility gained by a 64-fold improvement in fit. Otherwise, you will sooner or later decide that all fair coins are fixed.\n\nUnless your program is being smart, and *compressing* the data, it should do no good just to move one bit from the data into the program description.\n\nThe way Solomonoff induction works to predict sequences is that you sum up over all allowed computer programs—if every program is allowed, Solomonoff induction becomes uncomputable—with each program having a prior probability of 1/2 to the power of its code length in bits, and each program is further weighted by its fit to all data observed so far. This gives you a weighted mixture of experts that can predict future bits.\n\nThe Minimum Message Length formalism is nearly equivalent to Solomonoff induction. You send a string describing a code, and then you send a string describing the data in that code. Whichever explanation leads to the shortest *total* message is the best. If you think of the set of allowable codes as a space of computer programs, and the code description language as a universal machine, then Minimum Message Length is nearly equivalent to Solomonoff induction.[^1^](#fn1x26)\n\nThis lets us see clearly the problem with using “The lady down the street is a witch; she did it” to explain the pattern in the sequence 0101010101. If you’re sending a message to a friend, trying to describe the sequence you observed, you would have to say: “The lady down the street is a witch; she made the sequence come out 0101010101.” Your accusation of witchcraft wouldn’t let you *shorten* the rest of the message; you would still have to describe, in full detail, the data which her witchery caused.\n\nWitchcraft may fit our observations in the sense of qualitatively *permitting* them; but this is because witchcraft permits *everything* , like saying “Phlogiston!” So, even after you say “witch,” you still have to describe all the observed data in full detail. You have not *compressed the total length of the message describing your observations* by transmitting the message about witchcraft; you have simply added a useless prologue, increasing the total length.\n\nThe real sneakiness was concealed in the word “it” of “A witch did it.” A witch did *what*?\n\nOf course, thanks to [hindsight bias](https://lesswrong.com/lw/il/hindsight_bias/) and [anchoring](https://www.lesswrong.com/rationality/anchoring-and-adjustment) and [fake explanations](https://www.lesswrong.com/rationality/fake-explanations) and [fake causality](https://www.lesswrong.com/rationality/fake-causality) and [positive bias](https://www.lesswrong.com/rationality/positive-bias-look-into-the-dark) and [motivated cognition](https://www.lesswrong.com/rationality/knowing-about-biases-can-hurt-people), it may seem all too obvious that if a woman is a witch, of *course* she would make the coin come up 0101010101. But I’ll get to that soon enough. . .\n\n* * *\n\n^1^ Nearly, because it chooses the *shortest* program, rather than summing up over all programs."
          },
          "voteCount": 79
        },
        {
          "name": "Confidence levels inside and outside an argument",
          "type": "post",
          "slug": "confidence-levels-inside-and-outside-an-argument",
          "_id": "GrtbTAPfkJa4D6jjH",
          "url": null,
          "title": "Confidence levels inside and outside an argument",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Probability & Statistics"
            },
            {
              "name": "Forecasting & Prediction"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Inside/Outside View"
            },
            {
              "name": "Distinctions"
            },
            {
              "name": "Explicit Reasoning"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "**Related to:** [Infinite Certainty](/lw/mo/infinite_certainty/)\n\nSuppose the people at [FiveThirtyEight](http://fivethirtyeight.blogs.nytimes.com/) have created a model to predict the results of an important election. After crunching poll data, area demographics, and all the usual things one crunches in such a situation, their model returns a greater than 999,999,999 in a billion chance that the incumbent wins the election. Suppose further that the results of this model are your only data and you know nothing else about the election. What is your confidence level that the incumbent wins the election?  \n  \nMine would be significantly less than 999,999,999 in a billion.\n\nWhen an argument gives a probability of 999,999,999 in a billion for an event, then probably the majority of the probability of the event is no longer in \"But that still leaves a one in a billion chance, right?\". The majority of the probability is in \"That argument is flawed\". Even if you have no particular reason to believe the argument is flawed, the background chance of an argument being flawed is still greater than one in a billion.\n\n  \nMore than one in a billion times a political scientist writes a model, ey will get completely confused and write something with no relation to reality. More than one in a billion times a programmer writes a program to crunch political statistics, there will be a bug that completely invalidates the results. More than one in a billion times a staffer at a website publishes the results of a political calculation online, ey will accidentally switch which candidate goes with which chance of winning.  \n  \nSo one must distinguish between levels of confidence internal and external to a specific model or argument. Here the model's internal level of confidence is 999,999,999/billion. But my external level of confidence should be lower, even if the model is my only evidence, by an amount proportional to my trust in the model.\n\n  \n  \n**Is That Really True?**\n\nOne might be tempted to respond \"But there's an equal chance that the false model is too high, versus that it is too low.\" Maybe there was a bug in the computer program, but it prevented it from giving the incumbent's real chances of 999,999,999,999 out of a _trillion_.  \n  \nThe prior probability of a candidate winning an election is 50%^1^. We need information to push us away from this probability in either direction. To push significantly away from this probability, we need strong information. Any weakness in the information weakens its ability to push away from the prior. If there's a flaw in FiveThirtyEight's model, that takes us away from their probability of 999,999,999 in of a billion, and back closer to the prior probability of 50%  \n  \nWe can confirm this with a quick sanity check. Suppose we know nothing about the election (ie we still think it's 50-50) until an insane person reports a hallucination that an angel has declared the incumbent to have a 999,999,999/billion chance. We would not be tempted to accept this figure on the grounds that it is equally likely to be too high as too low.  \n  \nA second objection covers situations such as a lottery. I would like to say the chance that Bob wins a lottery with one billion players is 1/1 billion. Do I have to adjust this upward to cover the possibility that my model for how lotteries work is somehow flawed? No. Even if I am misunderstanding the lottery, I have not departed from my prior. Here, new information really does have an equal chance of going against Bob as of going in his favor. For example, the lottery may be fixed (meaning my original model of how to determine lottery winners is fatally flawed), but there is no greater reason to believe it is fixed in favor of Bob than anyone else.^2^  \n  \n**Spotted in the Wild**  \n  \nThe recent Pascal's Mugging thread spawned a discussion of the Large Hadron Collider destroying the universe, which also got continued on an older LHC thread from a few years ago. Everyone involved agreed the chances of the LHC destroying the world were less than one in a million, but several people gave extraordinarily low chances based on cosmic ray collisions. The argument was that since cosmic rays have been performing particle collisions similar to the LHC's zillions of times per year, the chance that the LHC will destroy the world is either literally zero, or else a number related to the probability that there's some chance of a cosmic ray destroying the world so miniscule that it hasn't gotten actualized in zillions of cosmic ray collisions. Of the commenters mentioning this argument, one gave a probability of 1/3*10^22, another suggested 1/10^25, both of which may be good numbers for the internal confidence of this argument.  \n  \nBut the connection between this argument and the general LHC argument flows through statements like \"collisions produced by cosmic rays will be exactly like those produced by the LHC\", \"our understanding of the properties of cosmic rays is largely correct\", and \"I'm not high on drugs right now, staring at a package of M&Ms and mistaking it for a really intelligent argument that bears on the LHC question\", all of which are probably more likely than 1/10^20. So instead of saying \"the probability of an LHC apocalypse is now 1/10^20\", say \"I have an argument that has an internal probability of an LHC apocalypse as 1/10^20, which lowers my probability a bit depending on how much I trust that argument\".  \n  \nIn fact, the argument has a potential flaw: according to Giddings and Mangano, the physicists officially tasked with investigating LHC risks, black holes from cosmic rays [might have enough momentum](http://arxiv.org/ftp/arxiv/papers/0912/0912.5480.pdf) to fly through Earth without harming it, and black holes from the LHC might not^3^. This was predictable: this was a simple argument in a complex area trying to prove a negative, and it would have been presumptous to believe with greater than 99% probability that it was flawless. If you can only give 99% probability to the argument being sound, then it can only reduce your probability in the conclusion by a factor of a hundred, not a factor of 10^20.  \n  \nBut it's hard for me to be properly outraged about this, since the LHC did not destroy the world. A better example might be the following, taken from an online [discussion of creationism](http://www.sciforums.com/Scientific-Reasons-for-God-t-44465.html)^4^ and apparently based off of something by Fred Hoyle:\n\n> In order for a single cell to live, all of the parts of the cell must be assembled before life starts. This involves 60,000 proteins that are assembled in roughly 100 different combinations. The probability that these complex groupings of proteins could have happened just by chance is extremely small. It is about 1 chance in 10 to the 4,478,296 power. The probability of a living cell being assembled just by chance is so small, that you may as well consider it to be impossible. This means that the probability that the living cell is created by an intelligent creator, that designed it, is extremely large. The probability that God created the living cell is 10 to the 4,478,296 power to 1.\n\nNote that someone just gave a confidence level of 10^4478296 to one and was wrong. This is the sort of thing that should _never ever happen_. This is possibly the _most wrong anyone has ever been_.  \n  \nIt is hard to say in words exactly how wrong this is. Saying \"This person would be willing to bet the entire world GDP for a thousand years if evolution were true against a one in one million chance of receiving a single penny if creationism were true\" doesn't even begin to cover it: a mere 1/10^25 would suffice there. Saying \"This person believes he could make one statement about an issue as difficult as the origin of cellular life per Planck interval, every Planck interval from the Big Bang to the present day, and not be wrong even once\" only brings us to 1/10^61 or so. If the chance of getting [Ganser's Syndrome](http://en.wikipedia.org/wiki/Ganser%27s_syndrome), the extraordinarily rare psychiatric condition that manifests in a compulsion to say false statements, is one in a hundred million, and the world's top hundred thousand biologists all agree that evolution is true, then this person should preferentially believe it is more likely that all hundred thousand have simultaneously come down with Ganser's Syndrome than that they are doing good biology^5^  \n  \nThis creationist's flaw wasn't mathematical; the math probably does return that number. The flaw was confusing the internal probability (that complex life would form completely at random in a way that can be represented with this particular algorithm) with the external probability (that life could form without God). He should have added a term representing the chance that his knockdown argument just didn't apply.  \n  \nFinally, consider the question of whether you can assign 100% certainty to a mathematical theorem for which a proof exists. Eliezer [has already examined this issue](/lw/mo/infinite_certainty/ ) and come out against it (citing as an example [this story of Peter de Blanc's](http://www.spaceandgames.com/?p=27)). In fact, this is just the specific case of differentiating internal versus external probability when internal probability is equal to 100%. Now your probability that the theorem is false is entirely based on the probability that you've made some mistake.  \n  \nThe many [mathematical proofs](http://mathoverflow.net/questions/35468) [that were later overturned](http://en.wikipedia.org/wiki/List_of_published_incomplete_proofs ) provide practical justification for this mindset.  \n  \nThis is not a fully general argument against giving very high levels of confidence: very complex situations and situations with many exclusive possible outcomes (like the lottery example) may still make it to the 1/10^20 level, albeit probably not the 1/10^4478296. But in other sorts of cases, giving a very high level of confidence requires a check that you're not confusing the probability inside one argument with the probability of the question as a whole.\n\n**Footnotes**\n\n**1.** Although technically we know we're talking about an incumbent, who typically has a much higher chance, around 90% in Congress.  \n  \n**2.** A particularly devious objection might be \"What if the lottery commissioner, in a fit of political correctness, decides that \"everyone is a winner\" and splits the jackpot a billion ways? If this would satisfy your criteria for \"winning the lottery\", then this mere possibility should indeed move your probability upward. In fact, since there is probably greater than a one in one billion chance of this happening, the majority of your probability for Bob winning the lottery should concentrate here!  \n  \n**3.** Giddings and Mangano then go on to re-prove the original \"won't cause an apocalypse\" argument using a more complicated method involving white dwarf stars.  \n  \n**4.** While searching creationist websites for the half-remembered argument I was looking for, I [found](http://www.christiananswers.net/q-eden/origin-of-life-ref.html) what may be my new favorite quote: \"Mathematicians generally agree that, statistically, any odds beyond 1 in 10 to the 50th have a zero probability of ever happening.\"   \n  \n**5.** I'm a little worried that five years from now I'll see this quoted on some creationist website as an actual argument."
          },
          "voteCount": 174
        }
      ]
    },
    {
      "title": "Education",
      "children": [
        {
          "name": "Bayes' rule: Guide",
          "type": "post",
          "comment": "The original post is deprecated in favor of this explanation.",
          "href": "https://arbital.com/p/bayes_rule/?l=1zq"
        }
      ]
    },
    {
      "title": "Excitement, Novelty, Fun",
      "children": [
        {
          "name": "The Apologist and the Revolutionary",
          "type": "post",
          "slug": "the-apologist-and-the-revolutionary",
          "_id": "ZiQqsgGX6a42Sfpii",
          "url": null,
          "title": "The Apologist and the Revolutionary",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Rationalization"
            },
            {
              "name": "Rationality"
            },
            {
              "name": "Psychiatry"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Rationalists complain that most people are too willing to [make excuses](http://www.overcomingbias.com/2007/10/fake-justificat.html) for their positions, and too unwilling to abandon those positions for ones that better fit the evidence. And most people really _are_ pretty bad at this. But certain stroke victims called anosognosiacs are much, much worse.  \n  \n[Anosognosia](http://en.wikipedia.org/wiki/Anosognosia) is the condition of not being aware of your own disabilities. To be clear, we're not talking minor disabilities here, the sort that only show up during a comprehensive clinical exam. We're talking paralysis or even blindness^1^. Things that should be pretty hard to miss.  \n  \nTake the example of the woman discussed in Lishman's [Organic Psychiatry](http://books.google.ie/books?id=Jsq-O12Ydo8C&pg=PA69&lpg=PA69&dq=anosognosia+shoulder+ring&source=bl&ots=_JDmzV0NJe&sig=_8rVlu85GNnsSkYEDRmAaUhWL1Q&hl=en&ei=Utu2SfjDL5SIjAfQpeSlCQ&sa=X&oi=book_result&resnum=1&ct=result). After a right-hemisphere stroke, she lost movement in her left arm but continuously denied it. When the doctor asked her to move her arm, and she observed it not moving, she claimed that it wasn't actually her arm, it was her daughter's. Why was her daughter's arm attached to her shoulder? The patient claimed her daughter had been there in the bed with her all week. Why was her wedding ring on her daughter's hand? The patient said her daughter had borrowed it. Where was the patient's arm? The patient \"turned her head and searched in a bemused way over her left shoulder\".  \n  \nWhy won't these patients admit they're paralyzed, and what are the implications for neurotypical humans? Dr. [Vilayanur Ramachandran](http://en.wikipedia.org/wiki/Vilayanur_S._Ramachandran#Scientific_career), leading neuroscientist and current holder of the world land-speed record for hypothesis generation, has a theory.\n\nOne immediately plausible hypothesis: the patient is unable to cope psychologically with the possibility of being paralyzed, so he responds with denial. Plausible, but according to Dr. Ramachandran, wrong. He notes that patients with left-side strokes almost never suffer anosognosia, even though the left side controls the right half of the body in about the same way the right side controls the left half. There must be something special about the right hemisphere.  \n  \nAnother plausible hypothesis: the part of the brain responsible for thinking about the affected area was damaged in the stroke. Therefore, the patient has lost access to the area, so to speak. Dr. Ramachandran doesn't like this idea either. The lack of right-sided anosognosia in left-hemisphere stroke victims argues against it as well. But how can we disconfirm it?  \n  \nDr. Ramachandran performed an experiment^2^ where he \"paralyzed\" an anosognosiac's good right arm. He placed it in a clever system of mirrors that caused a research assistant's arm to look as if it was attached to the patient's shoulder. Ramachandran told the patient to move his own right arm, and the false arm didn't move. What happened? The patient claimed he could see the arm moving - a classic anosognosiac response. This suggests that the anosognosia is not specifically a deficit of the brain's left-arm monitoring system, but rather some sort of failure of rationality.\n\nSays Dr. Ramachandran:\n\n> The reason anosognosia is so puzzling is that we have come to regard the 'intellect' as primarily propositional in character and one ordinarily expects propositional logic to be internally consistent. To listen to a patient deny ownership of her arm and yet, in the same breath, admit that it is attached to her shoulder is one of the most perplexing phenomena that one can encounter as a neurologist.\n\nSo what's Dr. Ramachandran's solution? He [posits two different reasoning modules](http://psych.utoronto.ca/~peterson/psy430s2001/Ramachandran%20VS%20Evolution%20of%20self-deception%20Med%20Hypoth%201996.pdf) located in the two different hemispheres. The left brain tries to fit the data to the theory to preserve a coherent internal narrative and prevent a person from jumping back and forth between conclusions upon each new data point. It is primarily an apologist, there to explain why any experience is exactly what its own theory would have predicted. The right brain is the seat of the [second virtue](http://yudkowsky.net/rational/virtues). When it's had enough of the left-brain's confabulating, it initiates a Kuhnian paradigm shift to a completely new narrative. Ramachandran describes it as \"a left-wing revolutionary\".  \n  \nNormally these two systems work in balance. But if a stroke takes the revolutionary offline, the brain loses its ability to change its mind about anything significant. If your left arm was working before your stroke, the little voice that ought to tell you it might be time to reject the \"left arm works fine\" theory goes silent. The only one left is the poor apologist, who must tirelessly invent stranger and stranger excuses for why all the facts really fit the \"left arm works fine\" theory perfectly well.  \n  \nIt gets weirder. For some reason, [squirting cold water into the left ear canal](http://www.neurology.org/cgi/content/abstract/65/8/1278) wakes up the revolutionary. Maybe the intense sensory input from an unexpected source makes the right hemisphere unusually aroused. Maybe distoring the balance sense causes the eyes to move rapidly, activating a latent system for inter-hemisphere co-ordination usually restricted to REM sleep^3^. In any case, a patient who has been denying paralysis for weeks or months will, upon having cold water placed in the ear, admit to paralysis, admit to having been paralyzed the past few weeks or months, and express bewilderment at having ever denied such an obvious fact. And then the effect wears off, and the patient not only denies the paralysis but denies ever having admitted to it.  \n  \nThis divorce between the apologist and the revolutionary might also explain some of the odd behavior of [split-brain](http://en.wikipedia.org/wiki/Split-brain) patients. Consider [the following experiment](http://books.google.ie/books?id=_rkKxbevFZEC&pg=PA10&lpg=PA10&dq=split-brain+chicken+shovel&source=bl&ots=9gVX7xBkJq&sig=yKnpOKg1jdzifungp7VgIXMLMcA&hl=en&ei=muu2SZKXA-LBjAeupOCvCQ&sa=X&oi=book_result&resnum=8&ct=result): a split-brain patient was shown two images, one in each visual field. The left hemisphere received the image of a chicken claw, and the right hemisphere received the image of a snowed-in house. The patient was asked verbally to describe what he saw, activating the left (more verbal) hemisphere. The patient said he saw a chicken claw, as expected. Then the patient was asked to point with his left hand (controlled by the right hemisphere) to a picture related to the scene. Among the pictures available were a shovel and a chicken. He pointed to the shovel. So far, no crazier than what we've come to expect from neuroscience.  \n  \nNow the doctor verbally asked the patient to describe why he just pointed to the shovel. The patient verbally (left hemisphere!) answered that he saw a chicken claw, and of course shovels are necessary to clean out chicken sheds, so he pointed to the shovel to indicate chickens. The apologist in the left-brain is helpless to do anything besides explain why the data fits its own theory, and its own theory is that whatever happened had something to do with chickens, dammit!  \n  \nThe logical follow-up experiment would be to ask the right hemisphere to explain the left hemisphere's actions. Unfortunately, the right hemisphere is either non-linguistic or as close as to make no difference. Whatever its thoughts, it's keeping them to itself.  \n  \n...you know, my mouth is _still_ agape at that whole cold-water-in-the-ear trick. I have this fantasy of gathering all the leading creationists together and squirting ice cold water in each of their left ears. All of a sudden, one and all, they admit their mistakes, and express bafflement at ever having believed such nonsense. And then ten minutes later the effect wears off, and they're all back to talking about irreducible complexity or whatever. I don't mind. I've already run off to upload the video to YouTube.  \n  \nThis is surely so great an exaggeration of Dr. Ramachandran's theory as to be a parody of it. And in any case I don't know how much to believe all this about different reasoning modules, or how closely the intuitive understanding of it I take from his paper matches the way a neuroscientist would think of it. Are the apologist and the revolutionary active in normal thought? Do anosognosiacs demonstrate the same pathological inability to change their mind on issues other than their disabilities? What of the argument that [confabulation](http://books.google.ie/books?id=_rkKxbevFZEC&dq=brain+fiction+confabulation&printsec=frontcover&source=bl&ots=9gVX7xFiMo&sig=ecS9mLduiZePctwU8lly9DejYjo&hl=en&ei=jvq2SYigB9nHjAefq62dCQ&sa=X&oi=book_result&resnum=1&ct=result#PPP11,M1) is a rather common failure mode of the brain, shared by some conditions that have little to do with right-hemisphere failure? Why does the effect of the cold water wear off so quickly? I've yet to see any really satisfying answers to any of these questions.\n\nBut whether Ramachandran is right or wrong, I give him enormous credit for doing serious research into the neural correlates of human rationality. I can think of few other fields that offer so many potential benefits.\n\n**Footnotes**\n\n1: See [Anton-Babinski syndrome](http://en.wikipedia.org/wiki/Anton%27s_syndrome)\n\n2: See Ramachandran's \"The Evolutionary Biology of Self-Deception\", the link from \"posits two different reasoning modules\" in this article.\n\n3: For Ramachandran's thoughts on REM, again see \"The Evolutionary Biology of Self Deception\""
          },
          "voteCount": 196
        },
        {
          "name": "Beyond the Reach of God",
          "type": "post",
          "slug": "beyond-the-reach-of-god",
          "_id": "sYgv4eYH82JEsTD34",
          "url": null,
          "title": "Beyond the Reach of God",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "World Optimization"
            },
            {
              "name": "Religion"
            },
            {
              "name": "Courage"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Today's post is a tad gloomier than usual, as I measure such things.  It deals with a thought experiment I invented to smash my own optimism, after [I realized that optimism had misled me](http://www.overcomingbias.com/2008/09/it-was-pretty-b.html).  Those readers sympathetic to arguments like, \"It's important to keep our biases because they help us stay happy,\" should consider not reading.  (Unless they have [something to protect](http://www.overcomingbias.com/2008/01/something-to-pr.html), including their own life.)\n\nSo!  Looking back on the magnitude of my own folly, I realized that at the root of it had been a disbelief in the Future's vulnerability—a reluctance to accept that things could _really_ turn out wrong.  Not as the result of any explicit propositional verbal belief.  More like something inside that persisted in believing, even in the face of adversity, that everything would be all right in the end.\n\nSome would account this a virtue (_zettai daijobu da yo_), and others would say that it's a thing necessary for mental health.\n\nBut we don't live in that world.  We live in the world beyond the reach of God.\n\nIt's been a long, long time since I believed in God.  Growing up in an Orthodox Jewish family, I can recall the last remembered time I asked God for something, though I don't remember how old I was.  I was putting in some request on behalf of the next-door-neighboring boy, I forget what exactly—something along the lines of, \"I hope things turn out all right for him,\" or maybe \"I hope he becomes Jewish.\"\n\nI remember what it was like to have some higher authority to appeal to, to take care of things I couldn't handle myself.  I didn't think of it as \"warm\", because I had no alternative to compare it to.  I just took it for granted.\n\nStill I recall, though only from distant childhood, what it's like to live in the [conceptually impossible possible world](http://www.overcomingbias.com/2008/09/excluding-the-s.html) where God exists.  _Really_ exists, in the way that children and rationalists take all their [beliefs](http://www.overcomingbias.com/2007/07/belief-in-belie.html) at face value.\n\nIn the world where God exists, does God intervene to optimize _everything?_  Regardless of what rabbis assert about the fundamental nature of reality, the take-it-seriously operational answer to this question is obviously \"No\".  You can't ask God to bring you a lemonade from the refrigerator instead of getting one yourself.  When I believed in God after the serious fashion of a child, so very long ago, I didn't believe that.\n\nPostulating that particular divine inaction doesn't provoke a full-blown theological crisis.  If you said to me, \"I have constructed a benevolent superintelligent nanotech-user\", and I said \"Give me a banana,\" and no banana appeared, this would not _yet_ disprove your statement.  Human parents don't always do everything their children ask.  There are some decent fun-theoretic arguments—I even believe them myself—against the idea that the _best_ kind of help you can offer someone, is to always immediately give them everything they want.  I don't think that eudaimonia is formulating goals and having them instantly fulfilled; I don't _want_ to become a simple wanting-thing that never has to plan or act or think.\n\nSo it's not necessarily an attempt to avoid falsification, to say that God does not grant all prayers.  Even a Friendly AI might not respond to every request.\n\nBut clearly, there exists _some_ threshold of horror awful enough that God will intervene.  I remember that being true, when I believed after the fashion of a child.\n\nThe God who does not intervene _at all_, no matter how bad things get—_that's_ an obvious attempt to avoid falsification, to protect a [belief-in-belief](http://www.overcomingbias.com/2007/07/belief-in-belie.html).  Sufficiently young children don't have the deep-down knowledge that God doesn't really exist.  They really expect to see [a dragon in their garage](http://www.overcomingbias.com/2007/07/belief-in-belie.html).  They have no reason to imagine a loving God who never acts.  Where exactly is the boundary of sufficient awfulness?  Even a child can imagine arguing over the precise threshold.  But of course God will draw the line somewhere.  Few indeed are the loving parents who, desiring their child to grow up strong and self-reliant, would let their toddler be run over by a car.\n\nThe obvious example of a horror so great that God cannot tolerate it, is death—true death, mind-annihilation.  I don't think that even Buddhism allows that.  So long as there is a God in the classic sense—full-blown, ontologically fundamental, _the_ God—we can rest assured that no _sufficiently_ awful event will ever, ever happen.  There is no soul anywhere that need fear true annihilation; God will prevent it.\n\nWhat if you build your own simulated universe?  The classic example of a simulated universe is Conway's Game of Life.  I do urge you to [investigate](http://en.wikipedia.org/wiki/Conway%27s_Game_of_Life) Life if you've never played it—it's important for comprehending the notion of \"physical law\".  Conway's Life has been proven Turing-complete, so it would be possible to build a sentient being in the Life universe, albeit it might be rather fragile and awkward.  Other cellular automata would make it simpler.\n\nCould you, by creating a simulated universe, escape the reach of God?  Could you simulate a Game of Life containing sentient entities, and torture the beings therein?  But if God is watching everywhere, then trying to build an unfair Life just results in _the_ God stepping in to modify your computer's transistors.  If the physics you set up in your computer program calls for a sentient Life-entity to be endlessly tortured for no particular reason, _the_ God will intervene.  God being omnipresent, there is no refuge _anywhere_ for true horror:  Life is fair.\n\nBut suppose that instead you ask the question:\n\n_Given_ such-and-such initial conditions, and _given_ such-and-such cellular automaton rules, what _would be_ the mathematical result?\n\nNot even God can modify the answer to this question, unless you believe that God can implement logical impossibilities.  Even as a very young child, I don't remember believing that.  (And why would you need to believe it, if God can modify anything that _actually_ exists?)\n\nWhat does Life look like, in this imaginary world where every step follows _only_ from its immediate predecessor?  Where things _only_ ever happen, or don't happen, because of the cellular automaton rules?  Where the initial conditions and rules _don't_ describe any God that checks over each state?  What does it look like, the world beyond the reach of God?\n\nThat world wouldn't be fair.  If the initial state contained the seeds of something that could self-replicate, natural selection might or might not take place, and complex life might or might not evolve, and that life might or might not become sentient, with no God to guide the evolution.  That world might evolve the equivalent of conscious cows, or conscious dolphins, that lacked hands to improve their condition; maybe they would be eaten by conscious wolves who never thought that they were doing wrong, or cared.\n\nIf in a vast plethora of worlds, something like humans evolved, then they would suffer from diseases—not to teach them any lessons, but only because viruses happened to evolve as well, under the cellular automaton rules.\n\nIf the people of that world are happy, or unhappy, the causes of their happiness or unhappiness may have nothing to do with good or bad choices they made.  Nothing to do with free will or lessons learned.  In the what-if world where every step follows only from the cellular automaton rules, the equivalent of Genghis Khan can murder a million people, and laugh, and be rich, and never be punished, and live his life much happier than the average.  Who prevents it?  God would prevent it from ever _actually_ happening, of course; He would at the very least visit some shade of gloom in the Khan's heart.  But in the mathematical answer to the question _What if?_ there is no God in the axioms.  So if the cellular automaton rules say that the Khan is happy, that, simply, is the whole and only answer to the what-if question.  There is nothing, absolutely nothing, to prevent it.\n\nAnd if the Khan tortures people horribly to death over the course of days, for his own amusement perhaps?  They will call out for help, perhaps imagining a God.  And if you _really_ wrote that cellular automaton, God would intervene in your program, of course.  But in the what-if question, what the cellular automaton _would_ do under the mathematical rules, there isn't any God in the system.  Since the physical laws contain no specification of a utility function—in particular, no prohibition against torture—then the victims will be saved only if the right cells happen to be 0 or 1.  And it's not likely that anyone will defy the Khan; if they did, someone would strike them with a sword, and the sword would disrupt their organs and they would die, and that would be the end of that.  So the victims die, screaming, and no one helps them; that is the answer to the what-if question.\n\nCould the victims be completely innocent?  Why not, in the what-if world?  If you look at the rules for Conway's Game of Life (which is Turing-complete, so we can embed arbitrary computable physics in there), then the rules are really very simple.  Cells with three living neighbors stay alive; cells with two neighbors stay the same, all other cells die.  There isn't anything in there about only innocent people not being horribly tortured for indefinite periods.\n\nIs this world starting to sound familiar?\n\nBelief in a fair universe often manifests in more subtle ways than thinking that horrors should be outright prohibited:  Would the twentieth century have gone differently, if Klara Pölzl and Alois Hitler had made love one hour earlier, and a different sperm fertilized the egg, on the night that Adolf Hitler was conceived?\n\nFor so many lives and so much loss to turn on a single event, seems _disproportionate._  The Divine Plan ought to make more _sense_ than that.  You can believe in a Divine Plan without believing in God—Karl Marx surely did.  You shouldn't have millions of lives depending on a casual choice, an hour's timing, the speed of a microscopic flagellum.  It ought not to be allowed.  It's _too_ disproportionate.  Therefore, if Adolf Hitler had been able to go to high school and become an architect, there would have been someone else to take his role, and World War II would have happened the same as before.\n\nBut in the world beyond the reach of God, there isn't any clause in the physical axioms which says \"things have to make sense\" or \"big effects need big causes\" or \"history runs on reasons too important to be so fragile\".  There is no God to _impose_ that order, which is so severely violated by having the lives and deaths of millions depend on one small molecular event.\n\nThe point of the thought experiment is to lay out the God-universe and the Nature-universe side by side, so that we can recognize what kind of thinking belongs to the God-universe.  Many who are atheists, still think as if certain things are _not allowed_.  They would lay out arguments for why World War II was inevitable and would have happened in more or less the same way, even if Hitler had become an architect.  But in sober historical fact, this is an unreasonable belief; I chose the example of World War II because from my reading, it seems that events were mostly driven by Hitler's personality, often in defiance of his generals and advisors.  There is no particular empirical justification that I happen to have heard of, for doubting this.  The main reason to doubt would be _refusal to accept_ that the universe could make so little sense—that horrible things could happen so _lightly,_ for no more reason than a roll of the dice.\n\nBut why not?  What prohibits it?\n\nIn the God-universe, God prohibits it.  To recognize this is to recognize that we don't live in that universe.  We live in the what-if universe beyond the reach of God, driven by the mathematical laws and nothing else.  Whatever physics says will happen, will happen.  Absolutely _anything,_ good or bad, will happen.  And there is nothing in the laws of physics to lift this rule even for the _really extreme_ cases, where you might expect Nature to be a little more reasonable.\n\nReading William Shirer's _The Rise and Fall of the Third Reich,_ listening to him describe the disbelief that he and others felt upon discovering the full scope of Nazi atrocities, I thought of what a strange thing it was, to read all that, and know, already, that there wasn't a single protection against it.  To just read through the whole book and accept it; horrified, but not at all disbelieving, because I'd already understood what kind of world I lived in.\n\nOnce upon a time, I believed that the extinction of humanity was not allowed.  And others who call themselves rationalists, may yet have things they trust.  They might be called \"positive-sum games\", or \"democracy\", or \"technology\", but they are sacred.  The mark of this sacredness is that the trustworthy thing can't lead to anything _really_ bad; or they can't be _permanently_ defaced, at least not without a compensatory silver lining.  In that sense they can be trusted, even if a few bad things happen here and there.\n\nThe unfolding history of Earth can't ever turn from its positive-sum trend to a negative-sum trend; that is not allowed.  [Democracies](http://www.overcomingbias.com/2007/09/applause-lights.html)—_modern_ _liberal_ democracies, anyway—won't ever legalize torture.  [Technology](http://www.overcomingbias.com/2008/09/raised-in-sf.html) has done so much good up until now, that there can't possibly be a Black Swan technology that breaks the trend and does more harm than all the good up until this point.\n\nThere are all sorts of clever arguments why such things can't possibly happen.  But the source of these arguments is a much deeper belief that such things are _not allowed_.  Yet who prohibits?  Who prevents it from happening?  If you can't visualize at least one lawful universe where physics say that such dreadful things happen—and so they _do_ happen, there being nowhere to appeal the verdict—then you aren't yet ready to argue _probabilities_.\n\nCould it really be that sentient beings have died absolutely for thousands or millions of years, with no soul and no afterlife—and _not_ as part of any grand plan of Nature—_not_ to teach any great lesson about the meaningfulness or meaninglessness of life—not even to teach any profound lesson about what is impossible—so that a trick as simple and stupid-sounding as [vitrifying people in liquid nitrogen](http://www.overcomingbias.com/2008/06/timeless-identi.html) can save them from total annihilation—and a 10-second rejection of the silly idea can destroy someone's soul?  Can it be that a computer programmer who signs a few papers and buys a life-insurance policy continues into the far future, while Einstein rots in a grave?  We can be sure of one thing:  God wouldn't allow it.  Anything that ridiculous and disproportionate would be ruled out.  It would make a mockery of the Divine Plan—a mockery of the _strong reasons_ why things must be the way they are.\n\nYou can have secular rationalizations for things being _not allowed_.  So it helps to imagine that there _is_ a God, benevolent as you understand goodness—a God who enforces throughout Reality a _minimum_ of fairness and justice—whose plans make sense and depend proportionally on people's choices—who will never permit absolute horror—who does not always intervene, but who at least prohibits universes wrenched _completely_ off their track... to imagine all this, but also imagine that _you,_ yourself, live in a what-if world of pure mathematics—a world beyond the reach of God, an utterly unprotected world where anything at all can happen.\n\nIf there's any reader still reading this, who thinks that being happy counts for more than anything in life, then maybe they _shouldn't_ spend much time pondering the unprotectedness of their existence.  Maybe think of it _just_ long enough to sign up themselves and their family for cryonics, and/or write a check to an existential-risk-mitigation agency now and then.  And wear a seatbelt and get health insurance and all those other dreary necessary things that can destroy your life if you miss that one step... but aside from that, if you want to be happy, meditating on the fragility of life isn't going to help.\n\nBut this post was written for those who have [something to protect](http://www.overcomingbias.com/2008/01/something-to-pr.html).\n\nWhat can a twelfth-century peasant do to save themselves from annihilation?  Nothing.  Nature's little challenges aren't always fair.  When you run into a challenge that's too difficult, you suffer the penalty; when you run into a lethal penalty, you die.  That's how it is for people, and it isn't any different for planets.  Someone who wants to dance the deadly dance with Nature, does need to understand what they're up against:  Absolute, utter, exceptionless neutrality.\n\nKnowing this won't always save you.  It wouldn't save a twelfth-century peasant, even if they knew.  If you think that a rationalist who fully understands the mess they're in, must _surely_ be able to find a way out—then you [trust rationality](http://www.overcomingbias.com/2008/05/no-defenses.html), enough said.\n\nSome commenter is bound to castigate me for putting too dark a tone on all this, and in response they will list out all the reasons why it's lovely to live in a neutral universe.  Life is allowed to be a _little_ dark, after all; but not darker than a certain point, unless there's a silver lining.\n\nStill, because I don't want to create _needless_ despair, I will say a few hopeful words at this point:\n\nIf humanity's future unfolds in the right way, we might be able to make our future light cone fair(er).  We can't modify fundamental physics, but on a higher level of organization we could build some guardrails and put down some padding; organize the particles into a pattern that does some internal checks against catastrophe.  There's a lot of stuff out there that we can't touch—but it may help to consider everything that isn't in our future light cone, as being part of the \"generalized past\".  As if it had all already happened.  There's at least the _prospect_ of defeating neutrality, in the only future we can touch—the only world that it accomplishes something to care about.\n\nSomeday, maybe, immature minds will reliably be sheltered.  Even if children go through the equivalent of not getting a lollipop, or even burning a finger, they won't ever be run over by cars.\n\nAnd the adults wouldn't be in so much danger.  A superintelligence—a mind that could think a trillion thoughts without a misstep—would not be intimidated by a challenge where death is the price of a single failure.  The raw universe wouldn't seem so harsh, would be only another problem to be solved.\n\nThe problem is that building an adult is itself an adult challenge.  That's what I finally realized, years ago.\n\nIf there is a fair(er) universe, we have to get there starting from _this_ world—the neutral world, the world of hard concrete with no padding, the world where challenges are not calibrated to your skills.\n\nNot every child needs to stare Nature in the eyes.  Buckling a seatbelt, or writing a check, is not that complicated or deadly.  I don't say that every rationalist should meditate on neutrality.  I don't say that every rationalist should think all these unpleasant thoughts.  But anyone who plans on confronting an uncalibratedchallenge of instant death, must not avoid them.\n\nWhat does a child need to do—what rules should they follow, how should they behave—to solve an adult problem?"
          },
          "voteCount": 133
        },
        {
          "name": "The mathematical universe: the map that is the territory",
          "type": "post",
          "slug": "the-mathematical-universe-the-map-that-is-the-territory",
          "_id": "fZJRxYLtNNzpbWZAA",
          "url": null,
          "title": "The mathematical universe: the map that is the territory",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Physics"
            },
            {
              "name": "Logic & Mathematics "
            },
            {
              "name": "Simulation Hypothesis"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "_This post is for people who are not familiar with the Level IV Multiverse/Ultimate Ensemble/Mathematical Universe Hypothesis, people who are not convinced that there’s any reason to believe it, and people to whom it appears believable or useful but not satisfactory as an actual explanation for anything._\n\n_I’ve found that while it’s fairly easy to understand what this idea asserts, it is more difficult to get to the point where it actually seems convincing and intuitively correct, until you independently invent it for yourself. Doing so can be fun, but for those who want to skip that part, I’ve tried to write this post as a kind of [intuition pump](http://en.wikipedia.org/wiki/Intuition_pump) (of the variety, I hope, that deserves the non-derogatory use of that term) with the goal of leading you along the same line of thinking that I followed, but in a few minutes rather than a few years._\n\n* * *\n\nOnce upon a time, I was reading some Wikipedia articles on physics, [clicking links aimlessly](http://xkcd.com/214/), when I happened upon a page then titled “Ultimate Ensemble”. It described a multiverse of all internally-consistent mathematical structures, thereby allegedly explaining our own universe — it’s mathematically possible, so it exists along with every other possible structure.\n\nNow, I was certainly interested in the question it was attempting to answer. It’s one that most young aspiring deep thinkers (and many very successful deep thinkers) end up at eventually: why is there a universe at all? A friend of mine calls himself an agnostic because, he says, “Who created God?” and “What caused the Big Bang?” are the same question. Of course, they’re not _quite_ the same, but the fundamental point is valid: although nothing happened “before” the Big Bang (as a more naïve version of this query might ask), saying that it caused the universe to exist still requires us to explain what brought about the laws and circumstances allowing the Big Bang to happen. There are some hypotheses that try to explain this universe in terms of a more general multiverse, but all of them seemed to lead to another question: “Okay, fine, then what caused _that_ to be the case?”\n\nThe Ultimate Ensemble, although interesting, looked like yet another one of those non-explanations to me. “Alright, so every mathematical structure ‘exists’. Why? Where? If there are all these mathematical structures floating around in some multiverse, what are the laws of this multiverse, and what caused _those_ laws? What’s the evidence for it?” It seemed like every explanation would lead to an infinite regress of multiverses to explain, or a [stopsign](/lw/it/semantic_stopsigns/) like “God did it” or “it just exists because it exists and that’s the end of it” (I’ve seen that from several atheists trying to convince themselves or others that this is a non-issue) or “science can never know what lies beyond this point” or “here be dragons”. This was deeply vexing to my 15-year-old self, and after a completely secular upbringing, I suffered a mild bout of spirituality over the following year or so. Fortunately I made a full recovery, but I gave in and decided that Stephen Hawking was right that “Why does the universe bother to exist?” would remain permanently unanswerable.\n\nLast year, I found myself thinking about this question again — but only after unexpectedly making my way back to it while thinking about the idea of an AI being conscious. And the path I took actually suggested an answer this time. As I worked on writing it up, I noticed that it sounded familiar. After I remembered what that Wikipedia article was called, and after actually looking up Max Tegmark’s papers on it this time, I confirmed that it was indeed the same essential idea. (Don’t you hate/love it when you find out that your big amazing groundbreaking idea has already been advocated by someone smarter and more important than you? It’s so disappointing/validating.) One of the papers briefly explores reasoning similar to that which I had accidentally used to convince myself of it, but it’s an argument that I haven’t seen emphasized in any discussions of it hereabouts, and it’s one which seems inescapable with no assumptions outside of ordinary materialism and reductionism.\n\nI shall now get to the point.\n\n* * *\n\nSuppose this universe is a computer simulation.\n\nIt isn’t, but we’ll imagine for the next few paragraphs that it is.\n\nSuppose everything we see — and all of the Many Worlds that we don’t see, and everything in this World that is too distant for us to ever see — is the product of a precise simulation being performed by some amazing supercomputer. Let’s call it the Grand Order Deducer, or G.O.D. for short.\n\nActually, let’s say that G.O.D. is not an amazing supercomputer, but a 386 with an insanely large hard drive. Obviously, we wouldn’t notice the slowness from the inside, any more than the characters in a movie would notice that your DVD player is being choppy.\n\nClearly, then, if G.O.D. were turned off for a billion years, and then reactivated at the point where it left off, we wouldn’t notice anything either. How about if the state of the simulation were copied to a very different kind of computer (say, a prototypical tape-based universal Turing machine, or an immortal person doing lambda calculus operations by hand) and continued? If our universe’s physics turns out to be fundamentally time-symmetrical, then if G.O.D. started from the end of the universe and simulated backwards, would we experience our lives backwards? If it saved a copy of the universe at the beginning of your life and repeatedly ran the simulation from there until your death (if any), would it mean anything to say that you are experiencing your life multiple times? If the state of the simulation were copied onto a million identical computers, and continued thence on all of them, would we feel a million times as real (or would there be a million “more” of each of us in any meaningful sense), and would the implausibly humanlike agent who hypothetically created this simulation feel a million times more culpable for any suffering taking place within it? It would be hard to argue that any of this should be the case without resorting to some truly ridiculous metaphysics. Every computer is calculating the same thing, even the ones that don’t seem plausible as universe-containers under our intuitions about what a simulation would look like.\n\nBut what, then, makes us feel real? What if, after G.O.D. has been turned off for a billion years… it stays off? If we can feel real while being simulated by a hundred computers, and no less real while being simulated by [one computer](/lw/1hg/the_moral_status_of_independent_identical_copies/), how about if we’re being simulated by zero computers? More concretely, and perhaps more disturbingly, if [torturing a million identical simulations](/lw/1pz/the_ai_in_a_box_boxes_you/) is the same thing as torturing one (I’d argue that it is), is torturing one the same as torturing zero?\n\n2 + 2 will always be 4 whether somebody is computing it or not. (No Platonism is necessary here; only the Simple Truth that taking the string “2 + 2” and applying certain rules of inference to it always results in the string “4”.) Similarly, even if this universe is nothing but a hypothetical, not being computed by anyone, not existing in anything larger, there are certain things that are necessarily true _about_ the hypothetical, including facts about the subjective mental states of us self-aware substructures. Nothing magical happens when a simulation runs. Most of us agree that consciousness is probably purely mechanistic, and that we could therefore create a conscious AI or emulate an uploaded brain, and that it would be just as conscious as we are; that if we could simulate Descartes, we’d hear him make the usual arguments about the duality of the material body and the extraphysical mind, and if we could simulate Chalmers, he’d come to the same familiar nonsensical conclusions about qualia and zombies. But the fact remains that it’s just a computer doing what computers always do, with no special EXIST or FEEL opcodes added to its instruction set. If a mind, from the outside, can be a self-contained and timeless structure, and the full structure can be calculated (within given finite limits) from some initial state by a normal computer, then its consciousness is a property of the structure itself, not of the computer or the program — the program is not causing it, it’s just letting someone notice it. So deep runs the dualist intuition that even when we have reduced spirits and consciousness and free will to normal physical causality, there’s still sometimes a tendency to think as though turning on a sufficiently advanced calculator causes something to mysteriously blink into existence or awareness, when all it is doing is reporting facts about some very large numbers that would be true one way or the other.\n\nG.O.D. is doing the very same thing, just with numbers that are even more unimaginably huge: a universe instead of an individual mind. The distilled and generalized argument is thus: _If we can feel real inside a non-magical computer simulation, then our feeling of reality must be due to necessary properties of the information being computed, because such properties do not exist in the abstract process of computing, and those properties will not cease to be true about the underlying information if the simulation is stopped or is never created in the first place._ This is identically true about every other possible reality.\n\nBy Occam’s Razor, I conclude that if a universe _can_ exist in this way — as one giant subjunctive — then we must accept that that is how and why our universe _does_ exist; even if we are being simulated on a computer in some outer universe, or if we were created by an actual deity (which, from a non-intervening deity’s perspective, would probably look about the same as running a simulation anyway), or if there is some other explanation for this particular universe, we now see that this would not actually be the cause of our existence. Existence is what mathematical possibility feels like from the inside. Turn off G.O.D., and we’ll go on with our lives, not noticing that anything has changed. Because the only thing that _has_ changed is that the people who were running the simulation won’t get to find out what happens next.\n\n* * *\n\nTegmark has described this as a “theory of everything”. I’d discourage that use, merely as a matter of consistency with common usage; conventionally, “theory of everything” refers to the underlying laws that define the regularities of _this_ universe, and whatever heroic physicists eventually discover those laws should retain the honour of having their theory known as such. As a metaphysical theory (less arbitrary than conventional metaphysics, but metaphysical nonetheless), this does not fit that description; it gives us almost no useful information about our own universe. It is a theory of more than everything, and a theory of nothing (in the same way that a program that prints out every possible bit string will eventually print out any given piece of information, while its actual information content is near zero).\n\nThat said, this theory and the argument I presented are not _entirely_ free of implications about and practical applications within this particular universe. Here are some of them.\n\n*   The [simulation argument](http://www.simulation-argument.com/) is dissolved. At this point, the idea of “living in a computer simulation” is meaningless. Simulating a universe should properly be viewed as comparable more to looking in a window than building the house. (Most of Robin Hanson’s [thoughts about metaethics and self-preservation within a simulation](http://www.jetpress.org/volume7/simulation.htm) are similarly dissolved, since a reality doesn’t pop out of existence when people stop simulating it; the only relevant part is the section about “If our descendants sometimes play parts in their simulations”, and this doesn’t seem to be the case anyway.)\n    \n*   As I mentioned, this significantly changes the dynamics of thought experiments like [The AI In A Box Boxes You](/lw/1pz/the_ai_in_a_box_boxes_you/). Torturing a thousand identical simulations is the same as torturing one, and torturing one is the same as torturing zero — _if and only if_ the structure within the simulation(s) is not being causally influenced by any ongoing circumstances in _this_ universe. If it is, then the two realities are entangled to the point where they are essentially different parts of the same structure, and it is worth thinking about how much we should care about each one.\n    \n*   That leads me to a more general point about metaethics: although there are other realities out there where there are very sentient and very intelligent beings experiencing suffering literally 3^^^3 times greater than anything we can imagine, and others where there are beings experiencing bliss in the same proportions, we must resist the urge to feel (respectively) sorry for them or jealous of them. Your intuitive sense of what “really exists” should remain limited to this universe.\n    \n    Perhaps this caution only applies to me in the first place. I am, admittedly, the only person I know who has to leave the room when people are playing _The Sims_ because I can’t stand to watch those little nowhere-near-sentient structures being put in torturous or even merely uncomfortable situations, so maybe it’s only my own empathy that’s a bit overactive. However, when we’re talking about sentient, sapient structures, we really do need to think about where to draw the line. I’d draw it at the point where a simulation starts to interact with this universe, in both directions — of course it will affect our universe if we are observing the simulation and reacting based on it, but we should only start caring about its feelings if we have designed the software such that _it_ is affected by _our actions_ beyond our choices for its initial conditions. That’s what I referred to as entanglement earlier. Once there’s that bilateral feedback, it’s no longer one structure observing another; they are both part of the same reality. (Take that as a practicality, not as a statement of an alleged metaphysical law. We’re trying to eliminate the need for metaphysical laws here.)\n    \n*   This theory results in a variation on the [Boltzmann brain scenario](/lw/17d/forcing_anthropics_boltzmann_brains/): regardless of _this_ universe’s ability to create Boltzmann brains, there’s also the possibility (and, therefore, necessity) of disembodied mind-structures hallucinating their own realities. My best guess as to the solution to this problem (if we’re to take it as a problem) is that any mind-structure that contains enough information to reliably hallucinate an orderly, mechanistic reality must be isomorphic to that reality.\n    \n*   It raises other strange anthropic questions too. The one that comes most immediately to my mind is this: If every possible mathematical structure is real in the same way that this universe is, then isn’t there only an infinitesimal probability that this universe will turn out to be ruled entirely by simple regularities? Given a universe governed by a small set of uniformly applied laws, there will be an infinity of universes governed by the same laws plus arbitrary variations, possibly affecting the internally observable structure only at very specific points in space and time. This results in a sort of anti–Occam’s Razor (Macco’s Rozar? Occam’s Beard Tonic?), where the larger the irregularity, the more likely it becomes over the space of all possible universes, because there are that many more ways for it to happen. (For example, there is a universe — actually, a huge number (possibly infinity) of barely different universes — identical to this one, except that, for no reason explainable by the usual laws of quantum mechanics, but not ruled out as a logically possible law unto itself, your head will explode as soon as you finish reading this post. I hope that possibility does not dissuade you from doing so, but I accept no responsibility if this _does_ turn out to be one of those universes.)\n    \n    From the outside, this would appear to be a non-issue. Consider people in some other reality simulating this one (assuming that this one really _is_ as simple and consistent as it appears). By some extraordinary luck, they’ve zoomed in on this exact planet in this exact Everett branch, and from there, they’ve even zoomed in on me writing this. “What does this guy mean,” they ask themselves, “wondering what the probability is that this particular reality will have the laws that it does? It’s not like anyone had any choice in the matter.” Yes, there will be versions of the universe that really are that orderly, and if this is one of them, than that would be why this universe’s version of me is wondering about the apparent astronomical unlikelihood of being in this universe. But from the inside, this seems terribly unsatisfying — if these slightly-irregular universes are possible, then we don’t know for sure what kind we’re in, so _should_ we expect to find such irregularities? Perhaps such exceptions would constitute such a departure from quantum mechanics that they couldn’t be made consistent with it even as a special case. (Tegmark makes a related point in one paper: the hypothesis “does certainly _not_ imply that all imaginable universes exist. We humans can imagine many things that are mathematically undefined and hence do not correspond to mathematical structures.”) Or perhaps the infinity of universes where such irregularities exist in places we’ll never observe (outside our light cone, in vast areas of empty space, etc.) is a much larger infinity (in probability density, not cardinality) than that of those universes where any of those irregularities will actually affect us. I’m leaning toward that explanation, but maybe a simpler one is that I’m reasoning about this incorrectly — after a conversation about this with Justin Shovelain, I’m reconsidering whether it’s actually correct to use probabilities to reason about an infinite space of apparently equally-likely items — or maybe this reasoning is correct and it observationally refutes the hypothesis. We’ll see.\n    \n\n* * *\n\nOne last comment: some people I’ve discussed this with have actually taken it as a _reductio ad absurdum_ against the idea that a being within a simulation _could_ feel real. As we say, one person’s _modus ponens_ is another person’s _modus tollens_. Since the conclusion I’m arguing for is merely unusual, not inconsistent (as far as I can tell), that takes out the _absurdum_; therefore, in the apparent absence of any specific alternatives at all, you can weigh the probability of this hypothesis against the stand-in alternatives that there _is_ something extraphysical about our own existence, something noncomputable about consciousness, or something metaphysically significant about processes equivalent to universal computation (or any other alternatives that I’ve neglected to think of).\n\nFinally, as I mentioned, the main goal of this post was to serve as an intuition pump for the Level IV Multiverse idea (and to point out some of the rationality-related questions it raises, so we’ll have something apropos to discuss here), not to explore it in depth. So if this was your first exposure to it, you should probably read Max Tegmark’s [The Mathematical Universe](http://arxiv.org/abs/0704.0646) now."
          },
          "voteCount": 96
        },
        {
          "name": "That Alien Message",
          "type": "post",
          "slug": "that-alien-message",
          "_id": "5wMcKNAwB6X4mp9og",
          "url": null,
          "title": "That Alien Message",
          "author": "Eliezer_Yudkowsky",
          "question": false,
          "tags": [
            {
              "name": "Fiction"
            },
            {
              "name": "AI Boxing (Containment)"
            },
            {
              "name": "AI"
            },
            {
              "name": "Simulation Hypothesis"
            },
            {
              "name": "Bayesianism"
            },
            {
              "name": "Parables & Fables"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "Imagine a world much like this one, in which, thanks to gene-selection technologies, the average IQ is 140 (on our scale).  Potential Einsteins are one-in-a-thousand, not one-in-a-million; and they grow up in a school system suited, if not to them personally, then at least to bright kids.  Calculus is routinely taught in sixth grade.  Albert Einstein, himself, still lived and still made approximately the same discoveries, but his work no longer seems _exceptional._  Several modern top-flight physicists have made equivalent breakthroughs, and are still around to talk.\n\n(No, this is not the world [Brennan](/lw/p1/initiation_ceremony/) lives in.)\n\nOne day, the stars in the night sky begin to change.\n\nSome grow brighter.  Some grow dimmer.  Most remain the same.  Astronomical telescopes capture it all, moment by moment.  The stars that change, change their luminosity one at a time, distinctly so; the luminosity change occurs over the course of a microsecond, but a whole second separates each change.\n\nIt is clear, from the first instant anyone realizes that more than one star is changing, that the process seems to center around Earth particularly. The arrival of the light from the events, at many stars scattered around the galaxy, has been precisely timed to Earth in its orbit.  Soon, confirmation comes in from high-orbiting telescopes (they have those) that the astronomical miracles do _not_ seem as synchronized from outside Earth.  Only Earth's telescopes see one star changing every second (1005 milliseconds, actually).\n\nAlmost the entire combined brainpower of Earth turns to analysis.\n\nIt quickly becomes clear that the stars that jump in luminosity, all jump by a factor of exactly 256; those that diminish in luminosity, diminish by a factor of exactly 256.  There is no apparent pattern in the stellar coordinates.  This leaves, simply, a pattern of BRIGHT-dim-BRIGHT-BRIGHT...\n\n\"A binary message!\" is everyone's first thought.\n\nBut in this world there are careful thinkers, of great prestige as well, and they are not so sure.  \"There are easier ways to send a message,\" they post to their blogs, \"if you can make stars flicker, and if you want to communicate.  _Something_ is happening.  It appears, _prima facie,_ to focus on Earth in particular.  To call it a 'message' presumes a great deal more about the cause behind it.  There might be some kind of evolutionary process among, um, things that can make stars flicker, that ends up sensitive to intelligence somehow...  Yeah, there's probably something like 'intelligence' behind it, but try to appreciate how wide a range of possibilities that really implies.  We don't know this is a message, or that it was sent from the same kind of motivations that might move us.  I mean, _we_ would just signal using a big flashlight, we wouldn't mess up a whole galaxy.\"\n\nBy this time, someone has started to collate the astronomical data and post it to the Internet.  Early suggestions that the data might be harmful, have been... not ignored, but not obeyed, either.  If anything this powerful wants to hurt you, you're pretty much dead (people reason).\n\nMultiple research groups are looking for patterns in the stellar coordinates—or fractional arrival times of the changes, relative to the center of the Earth—or exact durations of the luminosity shift—or any tiny variance in the magnitude shift—or any other fact that might be known about the stars before they changed.  But _most_ people are turning their attention to the pattern of BRIGHTS and dims.\n\nIt becomes clear almost instantly that the pattern sent is highly redundant.  Of the first 16 bits, 12 are BRIGHTS and 4 are dims.  The first 32 bits received align with the second 32 bits received, with only 7 out of 32 bits different, and then the next 32 bits received have only 9 out of 32 bits different from the second (and 4 of them are bits that changed before).  From the first 96 bits, then, it becomes clear that this pattern is not an optimal, compressed encoding of anything.  The obvious thought is that the sequence is meant to convey instructions for decoding a compressed message to follow...\n\n\"But,\" say the careful thinkers, \"anyone who cared about _efficiency,_ with enough power to mess with stars, could maybe have just signaled us with a big flashlight, and sent us a DVD?\"\n\nThere also seems to be structure within the 32-bit groups; some 8-bit subgroups occur with higher frequency than others, and this structure only appears along the natural alignments (32 = 8 + 8 + 8 + 8).\n\nAfter the first five hours at one bit per second, an additional redundancy becomes clear:  The message has started approximately repeating itself at the 16,385th bit.\n\nBreaking up the message into groups of 32, there are 7 bits of difference between the 1st group and the 2nd group, and 6 bits of difference between the 1st group and the 513th group.\n\n\"A 2D picture!\" everyone thinks.  \"And the four 8-bit groups are colors; they're tetrachromats!\"\n\nBut it soon becomes clear that there is a horizontal/vertical asymmetry:  Fewer bits change, on average, between (N, N+1) versus (N, N+512).  Which you wouldn't expect if the message was a 2D picture projected onto a symmetrical grid.  Then you would expect the average bitwise distance between two 32-bit groups to go as the 2-norm of the grid separation: √(h^2^ \\+ v^2^).\n\nThere also forms a general consensus that a certain binary encoding from 8-groups onto integers between -64 and 191—not the binary encoding that seems obvious to us, but still highly regular—minimizes the average distance between neighboring cells.  This continues to be borne out by incoming bits.\n\nThe statisticians and cryptographers and physicists and computer scientists go to work.  There is structure here; it needs only to be unraveled.  The masters of causality search for conditional independence, screening-off and Markov neighborhoods, among bits and groups of bits.  The so-called \"color\" appears to play a role in neighborhoods and screening, so it's not just the equivalent of surface reflectivity.  People search for simple equations, simple cellular automata, simple decision trees, that can predict or compress the message.  Physicists invent entire new theories of physics that might describe universes projected onto the grid—for it seems quite plausible that a message such as this is being sent from beyond the Matrix.\n\nAfter receiving 32 * 512 * 256 = 4,194,304 bits, around one and a half months, the stars stop flickering.\n\nTheoretical work continues.  Physicists and cryptographers roll up their sleeves and _seriously_ go to work.  They have cracked problems with far less data than this.  Physicists have tested entire theory-edifices with small differences of particle mass; cryptographers have unraveled shorter messages deliberately obscured.\n\nYears pass.\n\nTwo dominant models have survived, in academia, in the scrutiny of the public eye, and in the scrutiny of those scientists who once did Einstein-like work.  There is a theory that the grid is a projection from objects in a 5-dimensional space, with an asymmetry between 3 and 2 of the spatial dimensions.  There is also a theory that the grid is meant to encode a cellular automaton—arguably, the grid has several fortunate properties for such.  Codes have been devised that give interesting behaviors; but so far, running the corresponding automata on the largest available computers, has failed to produce any decodable result.  The run continues.\n\nEvery now and then, someone takes a group of especially brilliant young students who've never looked at the detailed binary sequence.  These students are then shown only the first 32 rows (of 512 columns each), to see if they can form new models, and how well those new models do at predicting the next 224.  Both the 3+2 dimensional model, and the cellular-automaton model, have been well duplicated by such students; they have yet to do better.  There are complex models finely fit to the whole sequence—but those, everyone knows, are probably worthless.\n\nTen years later, the stars begin flickering again. \n\nWithin the reception of the first 128 bits, it becomes clear that the Second Grid _can_ fit to small motions in the inferred 3+2 dimensional space, but does _not_ look anything like the successor state of any of the dominant cellular automaton theories.  Much rejoicing follows, and the physicists go to work on inducing what kind of dynamical physics might govern the objects seen in the 3+2 dimensional space.  Much work along these lines has already been done, just by speculating on what type of _balanced_ forces might give rise to the objects in the First Grid, if those objects were static—but now it seems not all the objects are static.  As most physicists guessed—statically balanced theories seemed contrived.\n\nMany neat equations are formulated to describe the dynamical objects in the 3+2 dimensional space being projected onto the First and Second Grids.  Some equations are more elegant than others; some are more precisely predictive (in retrospect, alas) of the Second Grid.  One group of brilliant physicists, who carefully isolated themselves and looked only at the first 32 rows of the Second Grid, produces equations that seem elegant to them—and the equations also do well on predicting the next 224 rows.  This becomes the dominant guess.\n\nBut these equations are underspecified; they don't seem to be enough to make a universe.  A small cottage industry arises in trying to guess what kind of laws might complete the ones thus guessed.\n\nWhen the Third Grid arrives, ten years after the Second Grid, it provides information about second derivatives, forcing a major modification of the \"incomplete but good\" theory.  But the theory doesn't do too badly out of it, all things considered.\n\nThe Fourth Grid doesn't add much to the picture.  Third derivatives don't seem important to the 3+2 physics inferred from the Grids.\n\nThe Fifth Grid looks almost exactly like it is expected to look.\n\nAnd the Sixth Grid, and the Seventh Grid.\n\n(Oh, and every time someone in this world tries to build a really powerful AI, the computing hardware spontaneously melts.  This isn't really important to the story, but I need to postulate this in order to have human people sticking around, in the flesh, for seventy years.)\n\n_My moral?_\n\nThat even Einstein did not come within a million light-years of making _efficient use of sensory data_.\n\nRiemann invented his geometries before Einstein had a use for them; the physics of our universe is not that complicated in an absolute sense.  A Bayesian superintelligence, hooked up to a webcam, would invent General Relativity as a hypothesis—perhaps not the _dominant_ hypothesis, compared to Newtonian mechanics, but still a hypothesis under direct consideration—by the time it had seen the third frame of a falling apple.  It might guess it from the first frame, if it saw the statics of a bent blade of grass.\n\n_We_ would think of it.  Our civilization, that is, given ten years to analyze each frame.  Certainly if the average IQ was 140 and Einsteins were common, we would.\n\nEven if we were human-level intelligences in a different sort of physics—minds who had never seen a 3D space projected onto a 2D grid—we would still think of the 3D->2D hypothesis.  Our mathematicians would still have invented vector spaces, and projections.\n\nEven if we'd never seen an accelerating billiard ball, our mathematicians would have invented calculus (e.g. for optimization problems).\n\nHeck, think of some of the crazy math that's been invented here on _our_ Earth.\n\nI occasionally run into people who say something like, \"There's a theoretical limit on how much you can deduce about the outside world, given a finite amount of sensory data.\"\n\nYes.  There is.  The theoretical limit is that every time you see 1 additional bit, it cannot be expected to eliminate more than half of the remaining hypotheses (half the remaining probability mass, rather).  And that a redundant message, cannot convey more information than the compressed version of itself.  Nor can a bit convey any information about a quantity, with which it has correlation _exactly zero,_ across the probable worlds you imagine.\n\nBut nothing I've depicted this human civilization doing, even _begins_ to approach the theoretical limits set by the formalism of Solomonoff induction.  It doesn't approach the picture you could get if you could search through _every single computable hypothesis_, weighted by their simplicity, and do Bayesian updates on _all_ of them.\n\nTo see the _theoretical_ limit on extractable information, imagine that you have infinite computing power, and you simulate all possible universes with simple physics, looking for universes that contain Earths embedded in them—perhaps inside a simulation—where some process makes the stars flicker in the order observed.  Any bit in the message—or any order of selection of stars, for that matter—that contains the tiniest correlation (across all possible computable universes, weighted by simplicity) to any element of the environment, gives you information about the environment.\n\nSolomonoff induction, taken literally, would create countably infinitely many sentient beings, trapped inside the computations.  All possible computable sentient beings, in fact.  Which scarcely seems ethical.  So let us be glad this is only a formalism.\n\nBut my point is that the \"theoretical limit on how much information you can extract from sensory data\" is _far_ above what I have depicted as the triumph of a civilization of physicists and cryptographers.\n\nIt certainly is not anything like a human looking at an apple falling down, and thinking, \"Dur, I wonder why that happened?\"\n\nPeople seem to make a leap from \"This is 'bounded'\" to \"The bound must be a reasonable-looking quantity on the scale I'm used to.\"  The power output of a supernova is 'bounded', but I wouldn't advise trying to shield yourself from one with a flame-retardant Nomex jumpsuit.\n\nNo one—not even a Bayesian superintelligence—will ever come remotely close to making efficient use of their sensory information...\n\n...is what I would like to say, but I don't trust my ability to set limits on the abilities of Bayesian superintelligences.\n\n(Though I'd bet money on it, if there were some way to judge the bet.  Just not at very extreme odds.)\n\n_The story continues:_\n\nMillennia later, frame after frame, it has become clear that some of the objects in the depiction are extending tentacles to move around other objects, and carefully configuring other tentacles to make particular signs.  They're trying to teach us to say \"rock\".\n\nIt seems the senders of the message have vastly underestimated our intelligence.  From which we might guess that the aliens themselves are not all that bright.  And these awkward children can shift the luminosity of our stars?  That much power and that much stupidity seems like a dangerous combination.\n\nOur evolutionary psychologists begin extrapolating possible courses of evolution that could produce such aliens.  A strong case is made for them having evolved asexually, with occasional exchanges of genetic material and brain content; this seems like the most plausible route whereby creatures that stupid could still manage to build a technological civilization.  Their Einsteins may be our undergrads, but they could still collect enough scientific data to get the job done _eventually_, in tens of their millennia perhaps.\n\nThe inferred physics of the 3+2 universe is not fully known, at this point; but it seems sure to allow for computers far more powerful than our quantum ones.  We are reasonably certain that our own universe is running as a simulation on such a computer.  Humanity decides not to probe for bugs in the simulation; we wouldn't want to shut ourselves down accidentally.\n\nOur evolutionary psychologists begin to guess at the aliens' psychology, and plan out how we could persuade them to [let us out of the box](http://yudkowsky.net/essays/aibox.html).  It's not difficult in an absolute sense—they aren't very bright—but we've got to be very careful...\n\nWe've got to pretend to be stupid, too; we don't want them to catch on to their mistake.\n\nIt's not until a million years later, though, that they get around to telling us how to signal back.\n\nAt this point, most of the human species is in cryonic suspension, at liquid helium temperatures, beneath radiation shielding.  Every time we try to build an AI, or a nanotechnological device, it melts down.  So humanity waits, and sleeps.  Earth is run by a skeleton crew of nine supergeniuses.  Clones, known to work well together, under the supervision of certain computer safeguards.\n\nAn additional hundred million human beings are born into that skeleton crew, and age, and enter cryonic suspension, before they get a chance to slowly begin to implement plans made eons ago...\n\nFrom the aliens' perspective, it took us thirty of their minute-equivalents to oh-so-innocently learn about their psychology, oh-so-carefully persuade them to give us Internet access, followed by five minutes to innocently discover their network protocols, then some trivial cracking whose only difficulty was an innocent-looking disguise.  We read a tiny handful of physics papers (bit by slow bit) from their equivalent of arXiv, learning far more from their experiments than they had.  (Earth's skeleton team spawned an extra twenty Einsteins, that generation.)\n\nThen we cracked their equivalent of the protein folding problem over a century or so, and did some simulated engineering in their simulated physics.  We sent messages (steganographically encoded until our cracked servers decoded it) to labs that did their equivalent of DNA sequencing and protein synthesis.  We found some unsuspecting schmuck, and gave it a plausible story and the equivalent of a million dollars of cracked computational monopoly money, and told it to mix together some vials it got in the mail.  Protein-equivalents that self-assembled into the first-stage nanomachines, that built the second-stage nanomachines, that built the third-stage nanomachines... and then we could finally begin to do things at a reasonable speed.\n\nThree of their days, all told, since they began speaking to us.  Half a billion years, for us.\n\nThey never suspected a thing.  They weren't very smart, you see, even before taking into account their slower rate of time.  Their primitive equivalents of rationalists went around saying things like, \"There's a bound to how much information you can extract from sensory data.\"  And they never quite realized what it meant, that we were smarter than them, and thought faster."
          },
          "voteCount": 228
        },
        {
          "name": "A Much Better Life?",
          "type": "post",
          "slug": "a-much-better-life",
          "_id": "5Qvvi23WT2unNCoS9",
          "url": null,
          "title": "A Much Better Life?",
          "author": null,
          "question": false,
          "tags": [
            {
              "name": "Wireheading"
            },
            {
              "name": "Fiction"
            },
            {
              "name": "Humor"
            }
          ],
          "tableOfContents": null,
          "contents": {
            "markdown": "(Response to: [You cannot be mistaken about (not) wanting to wirehead](/lw/1oc/you_cannot_be_mistaken_about_not_wanting_to),  [Welcome to Heaven)](/lw/1o9/welcome_to_heaven)\n\nThe Omega Corporation  \nInternal Memorandum  \nTo: Omega, CEO  \nFrom: Gamma, Vice President, Hedonic Maximization\n\nSir, this concerns the newest product of our Hedonic Maximization Department, the Much-Better-Life Simulator. This revolutionary device allows our customers to essentially plug into the Matrix, except that instead of providing robots with power in flagrant disregard for the basic laws of thermodynamics, they experience a life that has been determined by rigorously tested algorithms to be the most enjoyable life they could ever experience. The MBLS even eliminates all memories of being placed in a simulator, generating a seamless transition into a life of realistic perfection.\n\nOur department is baffled. Orders for the MBLS are significantly lower than estimated. We cannot fathom why every customer who could afford one has not already bought it. It is simply impossible to have a better life otherwise. Literally. Our customers' best possible real life has already been modeled and improved upon many times over by our programming. Yet, many customers have failed to make the transition. Some are even expressing shock and outrage over this product, and condemning its purchasers.\n\nExtensive market research has succeeded only at baffling our researchers. People have even refused free trials of the device. Our researchers explained to them in perfectly clear terms that their current position is misinformed, and that once they tried the MBLS, they would never want to return to their own lives again. Several survey takers went so far as to specify _that statement_ as their reason for refusing the free trial! They _know_ that the MBLS will make their life so much better that they won't want to live without it, and they refuse to try it _for that reason!_ Some cited their \"utility\" and claimed that they valued \"reality\" and \"actually accomplishing something\" over \"mere hedonic experience.\" Somehow these organisms are incapable of comprehending that, inside the MBLS simulator, they will be able to experience the feeling of actually accomplishing feats far greater than they could ever accomplish in real life. Frankly, it's remarkable such people amassed enough credits to be able to afford our products in the first place!\n\nYou may recall that a Beta version had an off switch, enabling users to deactivate the simulation after a specified amount of time, or could be terminated externally with an appropriate code. These features received somewhat positive reviews from early focus groups, but were ultimately eliminated. No agent could reasonably want a device that could allow for the interruption of its perfect life. Accounting has suggested we respond to slack demand by releasing the earlier version at a discount; we await your input on this idea.\n\nProfits aside, the greater good is at stake here. We feel that we should find every customer with sufficient credit to purchase this device,  forcibly install them in it, and bill their accounts. They will immediately forget our coercion, and they will be many, many times happier. To do anything less than this seems criminal. Indeed, our ethics department is currently determining if we can justify delaying putting such a plan into action. Again, your input would be invaluable.\n\nI can't help but worry there's something we're just not getting."
          },
          "voteCount": 75
        }
      ]
    },
    {
      "title": "Other",
      "children": [
        {
          "name": "List of all articles from Less Wrong",
          "type": "post",
          "href": "https://www.lesswrong.com/allPosts"
        },
        {
          "name": "References & Resources for LessWrong",
          "href": "https://www.lesswrong.com/posts/TNHQLZK5pHbxdnz4e/references-and-resources-for-lesswrong",
          "type": "post"
        }
      ]
    }
  ]
}