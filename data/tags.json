[
  {
    "_id": "c42eTtBCXyJmtpqwZ",
    "name": "Using AI to solve Alignment",
    "core": false,
    "slug": "using-ai-to-solve-alignment",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">Not obviously the best name for this tag, but maybe good to explore/rename. Wiki-tags are publicly editable!</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "Not obviously the best name for this tag, but maybe good to explore/rename. Wiki-tags are publicly editable!"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zuwsLCbxbugqB7FQY",
    "name": "Shard Theory of Human Values",
    "core": false,
    "slug": "shard-theory-of-human-values",
    "tableOfContents": {
      "html": "<p><span class=\"by_nRknKQuPzoG2Wuyyi\">The Shard Theory of human values states that values are ultimately made by shards in the brain. Shards are reinforced behavior based on predicted reward, and when morality is involved, shards in the brain negotiate or fight it out until one coalition of shards wins out, resulting in a decision or shying away from a decision. It's a theory that tries to explain why humans have the values they have, and why value is complex, as well as why the brain is outer-aligned and doesn't do terrible things like X-risks thanks to Instrumental Convergence and the Orthogonality Thesis.</span></p><span class=\"by_nRknKQuPzoG2Wuyyi\">\n</span>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "The Shard Theory of human values states that values are ultimately made by shards in the brain. Shards are reinforced behavior based on predicted reward, and when morality is involved, shards in the brain negotiate or fight it out until one coalition of shards wins out, resulting in a decision or shying away from a decision. It's a theory that tries to explain why humans have the values they have, and why value is complex, as well as why the brain is outer-aligned and doesn't do terrible things like X-risks thanks to Instrumental Convergence and the Orthogonality Thesis."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GKffzsk2bKssZGRva",
    "name": "Improving the LessWrong Wiki",
    "core": false,
    "slug": "improving-the-lesswrong-wiki",
    "tableOfContents": {
      "html": "<ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">This is a deliberate bastardisation and subversion of how the wiki is used</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">In order that people don't think I'm a defector, I'm making clear what I'm doing as I'm doing it. Tell me what you think</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Anyone should still edit this document as if it were theirs</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">The wiki would be better if it had a range of features I discuss below</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">If you disagree, edit it! Sometimes edit wars are good</span></li></ul><h1 id=\"Wiki_features_and_how_valuable_they_would_be\"><span class=\"by_cJnvyeYrotgZgfG8W\">Wiki features and how valuable they would be</span></h1><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Comments on wikis</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">The current tech exists and it would make it much cleaner to write things I intend to work on in future.</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Comments are hidden unless you click show. You can toggle them to be permanently shown</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Double bracket search and tagging&nbsp;</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">When you use [[, it opens up a search window to all Lesswrong tags. You type in what you want to reference. If there isn't one. it creates it eg &nbsp;[[Solipsism]]</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Disputed changes sit beside text, like comments</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">A process for requesting a number -&gt; getting an accurate forecast</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">There is a description in the text</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">A user flags it for quantification</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Other users collaboratively write a forecasting question</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Users give estimates, weighted by LW karma score (I hate this but it would be better than nothing, hard to abuse etc)</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Ideally, eventually this gets replaced by a manifold market or similar</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Toggle hide/unhide sections</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Gwern-style floating windows which appear when you hover over text in Lesswrong</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Embeddable manifold markets</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Embeddable squiggle estimation</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Taking variables from manifold markets and squiggle and displaying them within the text</span></li></ul><h1 id=\"Some_pages_I_have_edited_to_be_the_style_of_what_I_think_should_be_all_pages\"><span class=\"by_cJnvyeYrotgZgfG8W\">Some pages I have edited to be the style of what I think should be all pages</span></h1><ul><li><a href=\"https://www.lesswrong.com/tag/effective-altruism\"><span class=\"by_cJnvyeYrotgZgfG8W\">https://www.lesswrong.com/tag/effective-altruism</span></a><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">It now has a section on funding use, impact and criticims</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">With a little more work I think it could be the best single page introduction to EA</span></li></ul></li><li><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_cJnvyeYrotgZgfG8W\">https://www.lesswrong.com/tag/forecasting-and-prediction</span></a><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">A better overview of the topic</span></li></ul></li></ul>",
      "sections": [
        {
          "title": "Wiki features and how valuable they would be",
          "anchor": "Wiki_features_and_how_valuable_they_would_be",
          "level": 1
        },
        {
          "title": "Some pages I have edited to be the style of what I think should be all pages",
          "anchor": "Some_pages_I_have_edited_to_be_the_style_of_what_I_think_should_be_all_pages",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": null,
    "description": {
      "markdown": "*   This is a deliberate bastardisation and subversion of how the wiki is used\n*   In order that people don't think I'm a defector, I'm making clear what I'm doing as I'm doing it. Tell me what you think\n*   Anyone should still edit this document as if it were theirs\n*   The wiki would be better if it had a range of features I discuss below\n*   If you disagree, edit it! Sometimes edit wars are good\n\nWiki features and how valuable they would be\n============================================\n\n*   Comments on wikis\n    *   The current tech exists and it would make it much cleaner to write things I intend to work on in future.\n    *   Comments are hidden unless you click show. You can toggle them to be permanently shown\n*   Double bracket search and tagging \n    *   When you use \\[\\[, it opens up a search window to all Lesswrong tags. You type in what you want to reference. If there isn't one. it creates it eg  \\[\\[Solipsism\\]\\]\n*   Disputed changes sit beside text, like comments\n*   A process for requesting a number -> getting an accurate forecast\n    *   There is a description in the text\n    *   A user flags it for quantification\n    *   Other users collaboratively write a forecasting question\n    *   Users give estimates, weighted by LW karma score (I hate this but it would be better than nothing, hard to abuse etc)\n    *   Ideally, eventually this gets replaced by a manifold market or similar\n*   Toggle hide/unhide sections\n*   Gwern-style floating windows which appear when you hover over text in Lesswrong\n*   Embeddable manifold markets\n*   Embeddable squiggle estimation\n*   Taking variables from manifold markets and squiggle and displaying them within the text\n\nSome pages I have edited to be the style of what I think should be all pages\n============================================================================\n\n*   [https://www.lesswrong.com/tag/effective-altruism](https://www.lesswrong.com/tag/effective-altruism)\n    *   It now has a section on funding use, impact and criticims\n    *   With a little more work I think it could be the best single page introduction to EA\n*   [https://www.lesswrong.com/tag/forecasting-and-prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)\n    *   A better overview of the topic"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "siW3hDgPwAKSDmwgG",
    "name": "Refine",
    "core": false,
    "slug": "refine",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">Refine is a conceptual research incubator hosted by Conjecture.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "Refine is a conceptual research incubator hosted by Conjecture."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E4wsWbWrqjovpMrYd",
    "name": "Research Taste",
    "core": false,
    "slug": "research-taste",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Research Taste </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is the intuitions that guide researchers towards productive lines of inquiry.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Research Taste** is the intuitions that guide researchers towards productive lines of inquiry."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "n7qCAKXJhop8kEYxh",
    "name": "PIBBSS",
    "core": false,
    "slug": "pibbss",
    "tableOfContents": {
      "html": "<p><i><span class=\"by_THnLZeup4L7R4Rqkw\">Principles for Intelligent Behaviour in Biological and Social Systems </span></i><span class=\"by_THnLZeup4L7R4Rqkw\">(short: </span><a href=\"https://www.pibbss.ai/\"><span class=\"by_THnLZeup4L7R4Rqkw\">PIBBSS</span></a><span class=\"by_THnLZeup4L7R4Rqkw\">) is an initiative aimed at facilitating knowledge transfer towards AI alignment from fields studying intelligent behaviour in natural systems.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": "*Principles for Intelligent Behaviour in Biological and Social Systems* (short: [PIBBSS](https://www.pibbss.ai/)) is an initiative aimed at facilitating knowledge transfer towards AI alignment from fields studying intelligent behaviour in natural systems."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HsiovoPND7mWbW5D9",
    "name": "Distributional Shifts (was: Out of Distribution)",
    "core": false,
    "slug": "distributional-shifts-was-out-of-distribution",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">Not the best name, but likely a tag for something something distribution shift in AI would be useful. Not sure.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": {
      "markdown": "Not the best name, but likely a tag for something something distribution shift in AI would be useful. Not sure."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E4TozA6yTzee5oPMa",
    "name": "Meetups (specific examples)",
    "core": false,
    "slug": "meetups-specific-examples",
    "tableOfContents": {
      "html": "<p><span class=\"by_88BHmY89XnzyJnCGH\">This tag collects specific meetups that an organizer can run or things community members can do when they come together. It does include instructions or at least an inspiring outline for running a particular activity, exercise, discussion or game with a group. It can also include guidance for events such as Winter Solstice or Petrov Day that a community might come together for. It does not include general advice for running meetups or discussions on the purpose of meetups. It also does not include announcements of meetups that will be held at a particular time and place, which would be under </span><a href=\"https://www.lesswrong.com/tag/events-community\"><span class=\"by_88BHmY89XnzyJnCGH\">Events</span></a><span class=\"by_88BHmY89XnzyJnCGH\">. The intended purpose of this tag is to be something a meetup organizer can browse, thinking \"what should I run for my group next meeting?\"</span></p><p><strong><span class=\"by_88BHmY89XnzyJnCGH\">Related Pages:</span></strong><span class=\"by_88BHmY89XnzyJnCGH\"> </span><a href=\"https://www.lesswrong.com/tag/exercises-problem-sets\"><span class=\"by_88BHmY89XnzyJnCGH\">Exercises / Problem-sets</span></a><span class=\"by_88BHmY89XnzyJnCGH\"> and </span><a href=\"https://www.lesswrong.com/tag/games-posts-describing\"><span class=\"by_88BHmY89XnzyJnCGH\">Games (posts describing)</span></a><span class=\"by_88BHmY89XnzyJnCGH\">&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "This tag collects specific meetups that an organizer can run or things community members can do when they come together. It does include instructions or at least an inspiring outline for running a particular activity, exercise, discussion or game with a group. It can also include guidance for events such as Winter Solstice or Petrov Day that a community might come together for. It does not include general advice for running meetups or discussions on the purpose of meetups. It also does not include announcements of meetups that will be held at a particular time and place, which would be under [Events](https://www.lesswrong.com/tag/events-community). The intended purpose of this tag is to be something a meetup organizer can browse, thinking \"what should I run for my group next meeting?\"\n\n**Related Pages:** [Exercises / Problem-sets](https://www.lesswrong.com/tag/exercises-problem-sets) and [Games (posts describing)](https://www.lesswrong.com/tag/games-posts-describing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LdhpgZDCR967hWcFt",
    "name": "Encultured AI (org)",
    "core": false,
    "slug": "encultured-ai-org",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hwHxSdrxafFTuaCJE",
    "name": "Prompt Engineering",
    "core": false,
    "slug": "prompt-engineering",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Prompt Engineering </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is the practice of designing inputs to go into an ML system (often a language model), to get it to produce a particular output.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Prompt Engineering** is the practice of designing inputs to go into an ML system (often a language model), to get it to produce a particular output."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ESkc5kecgkLF235dj",
    "name": "Acute Risk Period",
    "core": false,
    "slug": "acute-risk-period",
    "tableOfContents": {
      "html": "<p><span class=\"by_r38pkCm7wF4M44MDQ\">The Acute Risk Period is the period of human history wherein it is possible to destroy all of civilization. (i.e. we destroy ourselves in nuclear war, or build an AGI that quickly bootstraps to be much more powerful than the rest of humanity combined).</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">It's an important strategic consideration in the field of Existential Risk – it's important to steer humanity safely through this period to a point where we are technologically and civilizationally mature, and have spread across the stars such that no one act could destroy everyone.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">See also Existential Risk, AI Risk, and the Most Important Century.</span></p><p><i><span class=\"by_r38pkCm7wF4M44MDQ\">This is a stub wiki post I hope gets fleshed out soon. I thought probably this was already written up on Arbital or something but I can't find it.</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": null,
    "description": {
      "markdown": "The Acute Risk Period is the period of human history wherein it is possible to destroy all of civilization. (i.e. we destroy ourselves in nuclear war, or build an AGI that quickly bootstraps to be much more powerful than the rest of humanity combined).\n\nIt's an important strategic consideration in the field of Existential Risk – it's important to steer humanity safely through this period to a point where we are technologically and civilizationally mature, and have spread across the stars such that no one act could destroy everyone.\n\nSee also Existential Risk, AI Risk, and the Most Important Century.\n\n*This is a stub wiki post I hope gets fleshed out soon. I thought probably this was already written up on Arbital or something but I can't find it.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "woeLa6nvmhroCh3Fi",
    "name": "General Alignment Properties",
    "core": false,
    "slug": "general-alignment-properties",
    "tableOfContents": {
      "html": "<p><span class=\"by_pgi5MqvGrtvQozEH8\">The AI alignment properties of agents which would be interesting to a range of principals trying to solve AI alignment. For example:</span></p><ul><li><span class=\"by_pgi5MqvGrtvQozEH8\">Does the AI \"care\" about reality, or just about its sensory observations?</span></li><li><span class=\"by_pgi5MqvGrtvQozEH8\">Does the AI properly navigate </span><a href=\"https://arbital.com/p/ontology_identification/\"><span class=\"by_pgi5MqvGrtvQozEH8\">ontological shifts</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">?</span></li><li><span class=\"by_pgi5MqvGrtvQozEH8\">Does the AI reason about itself as embedded in its environment?</span></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": "The AI alignment properties of agents which would be interesting to a range of principals trying to solve AI alignment. For example:\n\n*   Does the AI \"care\" about reality, or just about its sensory observations?\n*   Does the AI properly navigate [ontological shifts](https://arbital.com/p/ontology_identification/)?\n*   Does the AI reason about itself as embedded in its environment?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MbhipAhtifhaDEAcp",
    "name": "Squiggle",
    "core": false,
    "slug": "squiggle",
    "tableOfContents": {
      "html": "<p><a href=\"https://quantifieduncertainty.org/careers\"><u><span class=\"by_HR9tYBLXJxruiT72j\">Squiggle</span></u></a><span class=\"by_HR9tYBLXJxruiT72j\">&nbsp;is a special-purpose programming language for probabilistic estimation. Think: </span><i><span class=\"by_HR9tYBLXJxruiT72j\">\"</span></i><a href=\"https://getguesstimate.com/\"><i><u><span class=\"by_HR9tYBLXJxruiT72j\">Guesstimate</span></u></i></a><i><span class=\"by_HR9tYBLXJxruiT72j\"> as a programming language.</span></i><span class=\"by_HR9tYBLXJxruiT72j\">\" Squiggle is free and&nbsp;</span><a href=\"https://github.com/quantified-uncertainty/squiggle\"><span class=\"by_HR9tYBLXJxruiT72j\">open-source</span></a><span class=\"by_HR9tYBLXJxruiT72j\">.</span></p><p><strong id=\"https___www_lesswrong_com_posts_mv6nuN7nhnNpj9MLr_announcing_squiggle_early_access\"><span class=\"by_HR9tYBLXJxruiT72j\">https://www.lesswrong.com/posts/mv6nuN7nhnNpj9MLr/announcing-squiggle-early-access</span></strong></p>",
      "sections": [
        {
          "title": "https://www.lesswrong.com/posts/mv6nuN7nhnNpj9MLr/announcing-squiggle-early-access",
          "anchor": "https___www_lesswrong_com_posts_mv6nuN7nhnNpj9MLr_announcing_squiggle_early_access",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 6,
    "description": {
      "markdown": "[Squiggle](https://quantifieduncertainty.org/careers) is a special-purpose programming language for probabilistic estimation. Think: *\"*[*Guesstimate*](https://getguesstimate.com/) *as a programming language.*\" Squiggle is free and [open-source](https://github.com/quantified-uncertainty/squiggle).\n\n**https://www.lesswrong.com/posts/mv6nuN7nhnNpj9MLr/announcing-squiggle-early-access**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7jaBCxPHRDfJppYws",
    "name": "AI Sentience",
    "core": false,
    "slug": "ai-sentience",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yoLz9Ritmc7cve9a5",
    "name": "Addiction",
    "core": false,
    "slug": "addiction",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KRSY6MhAcgtBCC9FH",
    "name": "2017-2019 AI Alignment Prize",
    "core": false,
    "slug": "2017-2019-ai-alignment-prize",
    "tableOfContents": {
      "html": "<p><span class=\"by_mwyNSdFndg8SGhNZz\">The AI Alignment Prize was a contest carried out during 2017-2019, meant to encourage better work in the area.&nbsp;</span></p><p><span class=\"by_mwyNSdFndg8SGhNZz\">Throughout its existent, the alignment price paid $50k ($15k in the first iteration, $15k in the second, $10k in the third and $20k in the fourth and last).&nbsp;</span></p><p><span class=\"by_mwyNSdFndg8SGhNZz\">The successive rounds received 40, 37, 12 and 10 entries, and awarded 6, 5, 2 and 4 people, respectively. Towards the end of the prize's existence, numbers declined, as the organizers note and reflect about in the body and comments of the last </span><a href=\"https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg/announcement-ai-alignment-prize-round-4-winners#Moving_on\"><span class=\"by_mwyNSdFndg8SGhNZz\">post</span></a><span class=\"by_mwyNSdFndg8SGhNZz\">. Two bottlenecks to participation and impact noted were a) lack of prestige, and b) the time requirements for the organizers.</span></p><p><span class=\"by_mwyNSdFndg8SGhNZz\">Since then, there have been other prizes hosted on LessWrong, some of which can be found in the </span><a href=\"https://www.lesswrong.com/tag/bounties-and-prizes-active\"><span class=\"by_mwyNSdFndg8SGhNZz\">bounties and prizes</span></a><span class=\"by_mwyNSdFndg8SGhNZz\"> tag, though older articles might not be tagged. One such related prize is </span><a href=\"https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals\"><span class=\"by_mwyNSdFndg8SGhNZz\">the ELK prize</span></a><span class=\"by_mwyNSdFndg8SGhNZz\">, which attracted 197 entries, selected 32 winners, and </span><a href=\"https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals\"><span class=\"by_mwyNSdFndg8SGhNZz\">gave out</span></a><span class=\"by_mwyNSdFndg8SGhNZz\"> $274,000 in prizes.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "The AI Alignment Prize was a contest carried out during 2017-2019, meant to encourage better work in the area. \n\nThroughout its existent, the alignment price paid $50k ($15k in the first iteration, $15k in the second, $10k in the third and $20k in the fourth and last). \n\nThe successive rounds received 40, 37, 12 and 10 entries, and awarded 6, 5, 2 and 4 people, respectively. Towards the end of the prize's existence, numbers declined, as the organizers note and reflect about in the body and comments of the last [post](https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg/announcement-ai-alignment-prize-round-4-winners#Moving_on). Two bottlenecks to participation and impact noted were a) lack of prestige, and b) the time requirements for the organizers.\n\nSince then, there have been other prizes hosted on LessWrong, some of which can be found in the [bounties and prizes](https://www.lesswrong.com/tag/bounties-and-prizes-active) tag, though older articles might not be tagged. One such related prize is [the ELK prize](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals), which attracted 197 entries, selected 32 winners, and [gave out](https://www.lesswrong.com/posts/QEYWkRoCn4fZxXQAY/prizes-for-elk-proposals) $274,000 in prizes."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XGEumKCYarMaZh8iu",
    "name": "Air Conditioning",
    "core": false,
    "slug": "air-conditioning",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "haiwnEEx3vhrkfmAP",
    "name": "AI Robustness",
    "core": false,
    "slug": "ai-robustness",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YYFBmLCzeFsyd27rd",
    "name": "SERI MATS",
    "core": false,
    "slug": "seri-mats",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Stanford Existential Risks Initiative ML Alignment Theory Scholars</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> program.</span></p><p><a href=\"https://www.serimats.org/\"><span class=\"by_HoGziwmhpMGqGeWZy\">https://www.serimats.org/</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 30,
    "description": {
      "markdown": "The **Stanford Existential Risks Initiative ML Alignment Theory Scholars** program.\n\n[https://www.serimats.org/](https://www.serimats.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "a78XXmCuD3eFLYzph",
    "name": "Verification",
    "core": false,
    "slug": "verification",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LSruxwjPrufsD8swY",
    "name": "Fashion",
    "core": false,
    "slug": "fashion",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Fashion</span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\"> is a form of self-expression </span><span class=\"by_C6fyCDFwpQPgchX8A\">which involves</span><span class=\"by_r38pkCm7wF4M44MDQ\"> clothing, footwear, lifestyle, accessories, makeup, hairstyle, and body posture.</span><span class=\"by_C6fyCDFwpQPgchX8A\"> It can be used for </span></span><a href=\"https://www.lesswrong.com/tag/signaling\"><span class=\"by_C6fyCDFwpQPgchX8A\">signalling</span></a><span class=\"by_C6fyCDFwpQPgchX8A\">, and what is \"fashionable\" changes across history, cultures, and contexts.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "**Fashion** is a form of self-expression which involves clothing, footwear, lifestyle, accessories, makeup, hairstyle, and body posture. It can be used for [signalling](https://www.lesswrong.com/tag/signaling), and what is \"fashionable\" changes across history, cultures, and contexts."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xKz23YJ7h5JZijGFs",
    "name": "Spurious Counterfactuals",
    "core": false,
    "slug": "spurious-counterfactuals",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Spurious Counterfactuals</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> are spuriously low evaluations of the quality of a potential action, which are only provable because they are self-fulfilling (usually due to Lob's theorem). For example, if I know that I go left, then it is logically true that if I went right, I would get -10 utility (because in classical logic, false statements imply any statement). This suggests that if I fully believed that I went left, then I would indeed go left. By Lob's theorem, I indeed go left.&nbsp;</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Building agents who avoid this line of reasoning, despite having full access to their own source code and the ability to logically reason about their own behavior, is one goal of </span><a href=\"https://www.lesswrong.com/tag/embedded-agency\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Embedded Agency</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Spurious Counterfactuals** are spuriously low evaluations of the quality of a potential action, which are only provable because they are self-fulfilling (usually due to Lob's theorem). For example, if I know that I go left, then it is logically true that if I went right, I would get -10 utility (because in classical logic, false statements imply any statement). This suggests that if I fully believed that I went left, then I would indeed go left. By Lob's theorem, I indeed go left. \n\nBuilding agents who avoid this line of reasoning, despite having full access to their own source code and the ability to logically reason about their own behavior, is one goal of [Embedded Agency](https://www.lesswrong.com/tag/embedded-agency)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4PDaoDFQGem3ruHbN",
    "name": "Tensor Networks",
    "core": false,
    "slug": "tensor-networks",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rCChqJHkgiGxccrNw",
    "name": "NSFW",
    "core": false,
    "slug": "nsfw",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">Posts that are </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Not Safe For Work</span></strong></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": "Posts that are **Not Safe For Work**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AGgktgYb72PPjET9r",
    "name": "Multipolar Scenarios",
    "core": false,
    "slug": "multipolar-scenarios",
    "tableOfContents": {
      "html": "<p><span class=\"by_r38pkCm7wF4M44MDQ\">A </span><strong><span class=\"by_r38pkCm7wF4M44MDQ\">multipolar scenario</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is one where no single AI or agent takes over the world.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Featured in the book \"Superintelligence\" by Nick Bostrom.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "A **multipolar scenario** is one where no single AI or agent takes over the world.\n\nFeatured in the book \"Superintelligence\" by Nick Bostrom."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YkPxg2tFDQNdaEZDJ",
    "name": "AI Risk Concrete Stories",
    "core": false,
    "slug": "ai-risk-concrete-stories",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6zBEfFYJxhSEcchbR",
    "name": "AI Alignment Fieldbuilding",
    "core": false,
    "slug": "ai-alignment-fieldbuilding",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">AI Alignment Fieldbuilding</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is the effort to improve the alignment ecosystem. Some priorities include introducing new people to the importance of AI risk, on-boarding them by connecting them with key resources and ideas, educating them on existing literature and methods for generating new and valuable research, supporting people who are contributing, and maintaining and improving the funding systems.</span></p><p><span class=\"by_Sp5wM4aRAhNERd4oY\">There is an invite-only Slack for people working on the alignment ecosystem. If you'd like to join message </span><a href=\"https://www.lesswrong.com/users/ete\"><span class=\"by_Sp5wM4aRAhNERd4oY\">plex</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> with an overview of your involvement.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 22,
    "description": {
      "markdown": "**AI Alignment Fieldbuilding** is the effort to improve the alignment ecosystem. Some priorities include introducing new people to the importance of AI risk, on-boarding them by connecting them with key resources and ideas, educating them on existing literature and methods for generating new and valuable research, supporting people who are contributing, and maintaining and improving the funding systems.\n\nThere is an invite-only Slack for people working on the alignment ecosystem. If you'd like to join message [plex](https://www.lesswrong.com/users/ete) with an overview of your involvement."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cawGhjgJmnoLLHXMC",
    "name": "Adversarial Training",
    "core": false,
    "slug": "adversarial-training",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uiWL6tGbeZbxNwpz7",
    "name": "Global Poverty",
    "core": false,
    "slug": "global-poverty",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cj4oev4dKKkmrpXty",
    "name": "Boltzmann's brains",
    "core": false,
    "slug": "boltzmann-s-brains",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rZCxz9XBonCFLdwja",
    "name": "Poverty",
    "core": false,
    "slug": "poverty",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dNNwmxdmvtxMGMFgX",
    "name": "Air Quality",
    "core": false,
    "slug": "air-quality",
    "tableOfContents": {
      "html": "<p><span class=\"by_6NixjQa89fiBZxKS4\">Poor </span><strong><span class=\"by_6NixjQa89fiBZxKS4\">Air Quality</span></strong><span class=\"by_6NixjQa89fiBZxKS4\"> can reduce cognitive functioning</span><sup class=\"footnote-ref\"><a href=\"#fn-wdiYZ7FEBvFx2tzGn-1\" id=\"fnref-wdiYZ7FEBvFx2tzGn-1\"><span class=\"by_6NixjQa89fiBZxKS4\">[1]</span></a></sup><span class=\"by_6NixjQa89fiBZxKS4\">, lifespans</span><sup class=\"footnote-ref\"><a href=\"#fn-wdiYZ7FEBvFx2tzGn-2\" id=\"fnref-wdiYZ7FEBvFx2tzGn-2\"><span class=\"by_6NixjQa89fiBZxKS4\">[2]</span></a></sup><span class=\"by_6NixjQa89fiBZxKS4\"> and the techniques to improve air quality are also useful for getting rid of aerosolized respiratory pathogens. Improving air quality can be an impactful global health intervention.</span><sup class=\"footnote-ref\"><a href=\"#fn-wdiYZ7FEBvFx2tzGn-3\" id=\"fnref-wdiYZ7FEBvFx2tzGn-3\"><span class=\"by_6NixjQa89fiBZxKS4\">[3]</span></a></sup><span class=\"by_6NixjQa89fiBZxKS4\"> Many members of the LessWrong community have also put effort into improving the air quality of their own homes or offices, as an implication of instrumental rationality.</span></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><p><span class=\"by_6NixjQa89fiBZxKS4\">Newer Green buildings are infamous among those who care about this topic for being excellently sealed, meaning that they have less interchange with outside air. This is good for energy efficiency, but bad for indoor air quality.</span></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><h2 id=\"The_Carbon_Dioxide_Debate\"><span class=\"by_6NixjQa89fiBZxKS4\">The Carbon Dioxide Debate</span></h2><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><p><span class=\"by_6NixjQa89fiBZxKS4\">Mostly when people talk about air quality, they're talking about particulates and Volatile Organic Compounds (VOCs). However, some studies have tried to look at </span><a href=\"https://www.lesswrong.com/posts/pPZ27eZdBXtGuLqZC/what-is-up-with-carbon-dioxide-and-cognition-an-offer\"><span class=\"by_6NixjQa89fiBZxKS4\">carbon dioxide alone</span></a><span class=\"by_6NixjQa89fiBZxKS4\">, and have found large effects on cognition. It is this wiki author's belief that better studies have failed to find anything close to the size of the original effect, if anything.</span><sup class=\"footnote-ref\"><a href=\"#fn-wdiYZ7FEBvFx2tzGn-2\" id=\"fnref-wdiYZ7FEBvFx2tzGn-2:1\"><span class=\"by_6NixjQa89fiBZxKS4\">[2:1]</span></a></sup><sup class=\"footnote-ref\"><a href=\"#fn-wdiYZ7FEBvFx2tzGn-4\" id=\"fnref-wdiYZ7FEBvFx2tzGn-4\"><span class=\"by_6NixjQa89fiBZxKS4\">[4]</span></a></sup></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><h2 id=\"External_Links\"><span class=\"by_ezbRa3dntKWQ5995r\">External Links</span></h2><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><ul><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><li><a href=\"https://forum.effectivealtruism.org/tag/air-pollution\"><span class=\"by_ezbRa3dntKWQ5995r\">Air Pollution</span></a><span class=\"by_ezbRa3dntKWQ5995r\"> on the Effective Altruism Forum</span></li><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><li><a href=\"https://patrickcollison.com/pollution\"><span class=\"by_6NixjQa89fiBZxKS4\">Collection of studies</span></a><span class=\"by_6NixjQa89fiBZxKS4\"> by Patrick Collison on air pollution and cognition</span></li><span class=\"by_ezbRa3dntKWQ5995r\">\n</span></ul><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><hr class=\"footnotes-sep\"><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><section class=\"footnotes\"><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><ol class=\"footnotes-list\"><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><li id=\"fn-wdiYZ7FEBvFx2tzGn-1\" class=\"footnote-item\"><p><a href=\"https://www.iza.org/publications/dp/12632/indoor-air-quality-and-cognitive-performance\"><span class=\"by_6NixjQa89fiBZxKS4\">Künn et. al</span></a><span class=\"by_6NixjQa89fiBZxKS4\"> </span><a href=\"#fnref-wdiYZ7FEBvFx2tzGn-1\" class=\"footnote-backref\"><span class=\"by_6NixjQa89fiBZxKS4\">↩︎</span></a></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span></li><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><li id=\"fn-wdiYZ7FEBvFx2tzGn-2\" class=\"footnote-item\"><p><a href=\"https://www.nature.com/articles/s41598-021-01802-5\"><span class=\"by_6NixjQa89fiBZxKS4\">Juginovic et. al</span></a><span class=\"by_6NixjQa89fiBZxKS4\"> </span><a href=\"#fnref-wdiYZ7FEBvFx2tzGn-2\" class=\"footnote-backref\"><span class=\"by_6NixjQa89fiBZxKS4\">↩︎</span></a><span class=\"by_6NixjQa89fiBZxKS4\"> </span><a href=\"#fnref-wdiYZ7FEBvFx2tzGn-2:1\" class=\"footnote-backref\"><span class=\"by_6NixjQa89fiBZxKS4\">↩︎</span></a></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span></li><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><li id=\"fn-wdiYZ7FEBvFx2tzGn-3\" class=\"footnote-item\"><p><a href=\"https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/#south-asian-air-quality-021112\"><span class=\"by_6NixjQa89fiBZxKS4\">Alexander Berger on the 80,000 Hours Podcast</span></a><span class=\"by_6NixjQa89fiBZxKS4\"> (link goes to transcript or audio) </span><a href=\"#fnref-wdiYZ7FEBvFx2tzGn-3\" class=\"footnote-backref\"><span class=\"by_6NixjQa89fiBZxKS4\">↩︎</span></a></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span></li><span class=\"by_6NixjQa89fiBZxKS4\">\n</span><li id=\"fn-wdiYZ7FEBvFx2tzGn-4\" class=\"footnote-item\"><p><a href=\"https://www.lesswrong.com/posts/kxW6q5YdTGWh5sWby/eight-hundred-slightly-poisoned-word-games\"><span class=\"by_6NixjQa89fiBZxKS4\">Eight Hundred Slight Poisoned Word Games</span></a><span class=\"by_6NixjQa89fiBZxKS4\"> by Scott Alexander </span><a href=\"#fnref-wdiYZ7FEBvFx2tzGn-4\" class=\"footnote-backref\"><span class=\"by_6NixjQa89fiBZxKS4\">↩︎</span></a></p><span class=\"by_6NixjQa89fiBZxKS4\">\n</span></li><span class=\"by_6NixjQa89fiBZxKS4\">\n</span></ol><span class=\"by_6NixjQa89fiBZxKS4\">\n</span></section><span class=\"by_6NixjQa89fiBZxKS4\">\n</span>",
      "sections": [
        {
          "title": "The Carbon Dioxide Debate",
          "anchor": "The_Carbon_Dioxide_Debate",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 12,
    "description": {
      "markdown": "Poor **Air Quality** can reduce cognitive functioning[^1], lifespans[^2] and the techniques to improve air quality are also useful for getting rid of aerosolized respiratory pathogens. Improving air quality can be an impactful global health intervention.[^3] Many members of the LessWrong community have also put effort into improving the air quality of their own homes or offices, as an implication of instrumental rationality.\n\nNewer Green buildings are infamous among those who care about this topic for being excellently sealed, meaning that they have less interchange with outside air. This is good for energy efficiency, but bad for indoor air quality.\n\n## The Carbon Dioxide Debate\n\nMostly when people talk about air quality, they're talking about particulates and Volatile Organic Compounds (VOCs). However, some studies have tried to look at [carbon dioxide alone](https://www.lesswrong.com/posts/pPZ27eZdBXtGuLqZC/what-is-up-with-carbon-dioxide-and-cognition-an-offer), and have found large effects on cognition. It is this wiki author's belief that better studies have failed to find anything close to the size of the original effect, if anything.[^2][^4]\n\n## External Links\n\n* [Air Pollution](https://forum.effectivealtruism.org/tag/air-pollution) on the Effective Altruism Forum\n* [Collection of studies](https://patrickcollison.com/pollution) by Patrick Collison on air pollution and cognition\n\n[^1]: [Künn et. al](https://www.iza.org/publications/dp/12632/indoor-air-quality-and-cognitive-performance)\n[^2]: [Juginovic et. al](https://www.nature.com/articles/s41598-021-01802-5)\n[^3]: [Alexander Berger on the 80,000 Hours Podcast](https://80000hours.org/podcast/episodes/alexander-berger-improving-global-health-wellbeing-clear-direct-ways/#south-asian-air-quality-021112) (link goes to transcript or audio)\n[^4]: [Eight Hundred Slight Poisoned Word Games](https://www.lesswrong.com/posts/kxW6q5YdTGWh5sWby/eight-hundred-slightly-poisoned-word-games) by Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uDqJKemEHHsyWPm4m",
    "name": "Market making (AI safety technique) ",
    "core": false,
    "slug": "market-making-ai-safety-technique",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "L2cuwYAisTL3QF4TA",
    "name": "DALL-E",
    "core": false,
    "slug": "dall-e",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">DALL-E</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a family of </span><a href=\"machine-learning\"><span class=\"by_HoGziwmhpMGqGeWZy\">machine learning</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> models created by </span><a href=\"openai\"><span class=\"by_HoGziwmhpMGqGeWZy\">OpenAI</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> that generate images from text descriptions.</span></p><p><a href=\"https://openai.com/dall-e-2/\"><span class=\"by_HoGziwmhpMGqGeWZy\">DALL-E 2 official website</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "**DALL-E** is a family of [machine learning](machine-learning) models created by [OpenAI](openai) that generate images from text descriptions.\n\n[DALL-E 2 official website](https://openai.com/dall-e-2/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bQZAkiFgtbEcr5h6f",
    "name": "AI Persuasion",
    "core": false,
    "slug": "ai-persuasion",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">AI which is highly capable of persuading people might have significant effects on humanity.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "AI which is highly capable of persuading people might have significant effects on humanity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vxsbTxQGYTeNDAZXb",
    "name": "Conjecture (org)",
    "core": false,
    "slug": "conjecture-org",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fCA878kj7zunQHnp9",
    "name": "Flashcards",
    "core": false,
    "slug": "flashcards",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "emPKooKGELngMG7sP",
    "name": "Truthful AI",
    "core": false,
    "slug": "truthful-ai",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Cx4ACNdBaGALjhCJP",
    "name": "PaLM",
    "core": false,
    "slug": "palm",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EqakYxCsxXwpJpJG9\">PaLM</span></strong><span class=\"by_EqakYxCsxXwpJpJG9\"> is a Transformer language model created by Google in April 2022. &nbsp;Google claims that the model exhibits discontinuous jumps in capabilities as it scales. The original paper announcing PaLM can be found here: </span><a href=\"https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf\"><span class=\"by_EqakYxCsxXwpJpJG9\">https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf</span></a><span class=\"by_EqakYxCsxXwpJpJG9\">&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**PaLM** is a Transformer language model created by Google in April 2022.  Google claims that the model exhibits discontinuous jumps in capabilities as it scales. The original paper announcing PaLM can be found here: [https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf](https://storage.googleapis.com/pathways-language-model/PaLM-paper.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bt2e3HEcZmuHo3xf7",
    "name": "Modularity",
    "core": false,
    "slug": "modularity",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Kkruzub8DmrhZmjLa\">Modularity</span></strong><span><span class=\"by_Kkruzub8DmrhZmjLa\"> is the extent to which a system can be </span><span class=\"by_5fmpviAHeBpoTuukx\">divided into clusters.</span><span class=\"by_Kkruzub8DmrhZmjLa\"> It is a very broad concept which can be applied across many different domains, for example:</span></span></p><p><span class=\"by_Kkruzub8DmrhZmjLa\">In </span><strong><span class=\"by_Kkruzub8DmrhZmjLa\">graph theory</span></strong><span class=\"by_Kkruzub8DmrhZmjLa\">, modularity can be measured in terms of a graph's structure. For instance, there is a </span><a href=\"https://en.wikipedia.org/wiki/Modularity_(networks)\"><span class=\"by_Kkruzub8DmrhZmjLa\">common mathematical definition</span></a><span class=\"by_Kkruzub8DmrhZmjLa\"> for graphs, which is based on the idea of modules as partitions of the nodes of a graph, with more edges within modules than would be expected by chance.</span></p><p><span class=\"by_Kkruzub8DmrhZmjLa\">In </span><strong><span class=\"by_Kkruzub8DmrhZmjLa\">evolutionary biology</span></strong><span class=\"by_Kkruzub8DmrhZmjLa\">, modularity has been observed at many different levels, from protein structure to organ systems. There is currently no universally accepted explanation for why modularity seems to have been produced by evolution, and less so by genetic algorithms, although there are widely-held theories (most notably the idea of modularly varying goals, as proposed by </span><a href=\"https://www.pnas.org/doi/10.1073/pnas.0503610102\"><span class=\"by_Kkruzub8DmrhZmjLa\">Kashtan &amp; Alon</span></a><span class=\"by_Kkruzub8DmrhZmjLa\">).</span></p><p><span class=\"by_Kkruzub8DmrhZmjLa\">Modularity may be highly relevant for alignment research, because the more modular a system is, the greater the chances that we will be able to understand the cognition it is performing in terms of high-level abstractions.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "**Modularity** is the extent to which a system can be divided into clusters. It is a very broad concept which can be applied across many different domains, for example:\n\nIn **graph theory**, modularity can be measured in terms of a graph's structure. For instance, there is a [common mathematical definition](https://en.wikipedia.org/wiki/Modularity_(networks)) for graphs, which is based on the idea of modules as partitions of the nodes of a graph, with more edges within modules than would be expected by chance.\n\nIn **evolutionary biology**, modularity has been observed at many different levels, from protein structure to organ systems. There is currently no universally accepted explanation for why modularity seems to have been produced by evolution, and less so by genetic algorithms, although there are widely-held theories (most notably the idea of modularly varying goals, as proposed by [Kashtan & Alon](https://www.pnas.org/doi/10.1073/pnas.0503610102)).\n\nModularity may be highly relevant for alignment research, because the more modular a system is, the greater the chances that we will be able to understand the cognition it is performing in terms of high-level abstractions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yvn2ZYwS9Qq4TnT9B",
    "name": "Biosecurity",
    "core": false,
    "slug": "biosecurity",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aozxtmy7FEkkvfWzE",
    "name": "Grabby Aliens",
    "core": false,
    "slug": "grabby-aliens",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Grabby Aliens </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is a theory that explains why humans appeared relatively early in the history of the universe (13.8 </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">billion</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> years after the Big Bang, while the average star will last over five </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">trillion</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> years). It was developed by Robin Hanson, Daniel Martin, Calvin McCarter, and Jonathan Paulson.</span></p><p><strong id=\"External_Links_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong></p><ul><li><a href=\"https://grabbyaliens.com/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Website</span></a></li><li><a href=\"https://arxiv.org/abs/2102.01522\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Original Paper</span></a></li><li><a href=\"https://youtu.be/l3whaviTqqg\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Video by Rational Animations (part 1)</span></a></li><li><a href=\"https://youtu.be/LceY7nhi6j4\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Video by Rational Animations (part 2)</span></a></li></ul><p><strong id=\"Related_Pages_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong></p><ul><li><a href=\"https://www.lesswrong.com/tag/great-filter\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Great Filter</span></a></li><li><a href=\"https://www.lesswrong.com/tag/anthropics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Anthropics</span></a></li></ul>",
      "sections": [
        {
          "title": "External Links:",
          "anchor": "External_Links_",
          "level": 1
        },
        {
          "title": "Related Pages:",
          "anchor": "Related_Pages_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 12,
    "description": {
      "markdown": "**Grabby Aliens** is a theory that explains why humans appeared relatively early in the history of the universe (13.8 *billion* years after the Big Bang, while the average star will last over five *trillion* years). It was developed by Robin Hanson, Daniel Martin, Calvin McCarter, and Jonathan Paulson.\n\n**External Links:**\n\n*   [Website](https://grabbyaliens.com/)\n*   [Original Paper](https://arxiv.org/abs/2102.01522)\n*   [Video by Rational Animations (part 1)](https://youtu.be/l3whaviTqqg)\n*   [Video by Rational Animations (part 2)](https://youtu.be/LceY7nhi6j4)\n\n**Related Pages:**\n\n*   [Great Filter](https://www.lesswrong.com/tag/great-filter)\n*   [Anthropics](https://www.lesswrong.com/tag/anthropics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F6kBsuRJzi4AKJ7tL",
    "name": "Nuclear War",
    "core": false,
    "slug": "nuclear-war",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fWEFt4e9asdpSseqf",
    "name": "Civilizational Collapse",
    "core": false,
    "slug": "civilizational-collapse",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_6Nitmdekyr6gWxzKy\">Civilizational Collapse</span></strong><span class=\"by_6Nitmdekyr6gWxzKy\"> is the process by which a large complex social organization breaks down and becomes incapable of performing its usual functions. It has occurred many times in history in specific regions, and some hypothesize that if it happened now it would dramatically affect almost all humans due to the interconnected nature of the global economy. Possible causes include severe natural or bioengineered pandemics, nuclear war and the following nuclear winter, or fragility and cascading systems failure.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Civilizational Collapse** is the process by which a large complex social organization breaks down and becomes incapable of performing its usual functions. It has occurred many times in history in specific regions, and some hypothesize that if it happened now it would dramatically affect almost all humans due to the interconnected nature of the global economy. Possible causes include severe natural or bioengineered pandemics, nuclear war and the following nuclear winter, or fragility and cascading systems failure."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cv2dqCjh5cpkowzSp",
    "name": "Ukraine/Russia Conflict (2022)",
    "core": false,
    "slug": "ukraine-russia-conflict-2022",
    "tableOfContents": {
      "html": "<p><span class=\"by_Xn6ACr6Cua8upALWQ\">On the 24th of February 2022, Russia began a military invasion into Ukraine.&nbsp;</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">This is relevant to LessWrong both because it seems to be generally an important topic for understanding the world, and because of its potential implications for great-power conflict, nuclear war, and existential risk generally.&nbsp;</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">See Also:</span></p><ul><li><a href=\"https://www.lesswrong.com/tag/war\"><span class=\"by_Xn6ACr6Cua8upALWQ\">War</span></a></li><li><a href=\"https://www.lesswrong.com/tag/nuclear-war\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Nuclear War</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Existential Risk</span></a></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 56,
    "description": {
      "markdown": "On the 24th of February 2022, Russia began a military invasion into Ukraine. \n\nThis is relevant to LessWrong both because it seems to be generally an important topic for understanding the world, and because of its potential implications for great-power conflict, nuclear war, and existential risk generally. \n\nSee Also:\n\n*   [War](https://www.lesswrong.com/tag/war)\n*   [Nuclear War](https://www.lesswrong.com/tag/nuclear-war)\n*   [Existential Risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6voWoNt3q3jpEcPzk",
    "name": "Transformers",
    "core": false,
    "slug": "transformers",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "m2DsR4r4HRSaLSPW3",
    "name": "Security Mindset",
    "core": false,
    "slug": "security-mindset",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Security Mindset is a predisposition for thinking about the world in a security-oriented way. A large part of this way of thinking involves always being on the lookout for exploits.&nbsp;</span></p><blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Uncle Milton Industries has been selling ant farms to children since 1956. Some years ago, I remember opening one up with a friend. There were no actual ants included in the box. Instead, there was a card that you filled in with your address, and the company would mail you some ants. My friend expressed surprise that you could get ants sent to you in the mail.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">I replied: “What’s really interesting is that these people will send a tube of live ants to anyone you tell them to.”</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Security requires a particular mindset. Security professionals — at least the good ones — see the world differently. They can’t walk into a store without noticing how they might shoplift. They can’t use a computer without wondering about the security vulnerabilities. They can’t vote without trying to figure out how to vote twice. They just can’t help it.</span></p><p><a href=\"http://www.smartwater.com/products/securitySolutions.html\"><u><span class=\"by_Q7NW4XaWQmfPfdcFj\">SmartWater</span></u></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a liquid with a unique identifier linked to a particular owner. “The idea is for me to paint this stuff on my valuables as proof of ownership,” I </span><a href=\"http://www.schneier.com/blog/archives/2005/02/smart_water.html\"><u><span class=\"by_Q7NW4XaWQmfPfdcFj\">wrote</span></u></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> when I first learned about the idea. “I think a better idea would be for me to paint it on </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">your</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> valuables, and then call the police.”</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Really, we can’t help it.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">-- Bruce Schneier, </span><a href=\"https://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">The security Mindset</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, Schneier on Security</span></p></blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">[I'm unsure of the origin of the term, but Schneier is at least an outspoken advocate. --Abram]</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In 2017, Eliezer Yudkowsky wrote a pair of posts on the security mindset:</span></p><ul><li><a href=\"https://www.lesswrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Security Mindset and Ordinary Paranoia</span></a></li><li><a href=\"https://www.lesswrong.com/posts/cpdsMuAHSWhWnKdog/security-mindset-and-the-logistic-success-curve\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Security Mindset and the Logistic Success Curve</span></a></li></ul><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Amongst other things, these posts forwarded the idea that true security mindset </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">is not just</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> the tendency to spot lots and lots of security flaws. Spotting security flaws is not in itself enough to build secure systems, because you could be spotting flaws with your design forever, patching specific weak points, and moving on to find yet more flaws.&nbsp;</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Building secure systems requires </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">coming up with strong positive arguments for the security of a system</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">. These positive arguments have several important features:</span></p><ol><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">They have as few assumptions as possible, because each assumption is an additional chance to be wrong.&nbsp;</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Each assumption is individually very certain.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">The conclusion of the argument is a meaningful security guarantee.&nbsp;</span></li></ol><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The mindset required to build tight security arguments like this is different from the mindset required to find security holes.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "Security Mindset is a predisposition for thinking about the world in a security-oriented way. A large part of this way of thinking involves always being on the lookout for exploits. \n\n> Uncle Milton Industries has been selling ant farms to children since 1956. Some years ago, I remember opening one up with a friend. There were no actual ants included in the box. Instead, there was a card that you filled in with your address, and the company would mail you some ants. My friend expressed surprise that you could get ants sent to you in the mail.\n> \n> I replied: “What’s really interesting is that these people will send a tube of live ants to anyone you tell them to.”\n> \n> Security requires a particular mindset. Security professionals — at least the good ones — see the world differently. They can’t walk into a store without noticing how they might shoplift. They can’t use a computer without wondering about the security vulnerabilities. They can’t vote without trying to figure out how to vote twice. They just can’t help it.\n> \n> [SmartWater](http://www.smartwater.com/products/securitySolutions.html) is a liquid with a unique identifier linked to a particular owner. “The idea is for me to paint this stuff on my valuables as proof of ownership,” I [wrote](http://www.schneier.com/blog/archives/2005/02/smart_water.html) when I first learned about the idea. “I think a better idea would be for me to paint it on *your* valuables, and then call the police.”\n> \n> Really, we can’t help it.\n> \n> \\-\\- Bruce Schneier, [The security Mindset](https://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html), Schneier on Security\n\n\\[I'm unsure of the origin of the term, but Schneier is at least an outspoken advocate. --Abram\\]\n\nIn 2017, Eliezer Yudkowsky wrote a pair of posts on the security mindset:\n\n*   [Security Mindset and Ordinary Paranoia](https://www.lesswrong.com/posts/8gqrbnW758qjHFTrH/security-mindset-and-ordinary-paranoia)\n*   [Security Mindset and the Logistic Success Curve](https://www.lesswrong.com/posts/cpdsMuAHSWhWnKdog/security-mindset-and-the-logistic-success-curve)\n\nAmongst other things, these posts forwarded the idea that true security mindset *is not just* the tendency to spot lots and lots of security flaws. Spotting security flaws is not in itself enough to build secure systems, because you could be spotting flaws with your design forever, patching specific weak points, and moving on to find yet more flaws. \n\nBuilding secure systems requires *coming up with strong positive arguments for the security of a system*. These positive arguments have several important features:\n\n1.  They have as few assumptions as possible, because each assumption is an additional chance to be wrong. \n2.  Each assumption is individually very certain.\n3.  The conclusion of the argument is a meaningful security guarantee. \n\nThe mindset required to build tight security arguments like this is different from the mindset required to find security holes."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cFDQeyr8SpWuKpweq",
    "name": "Naturalism",
    "core": false,
    "slug": "naturalism",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e7LAgAKaovFX2GQhg",
    "name": "Unbounded Utilities",
    "core": false,
    "slug": "unbounded-utilities",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9mcR3NTzqwd3ugeRE",
    "name": "Astronomy",
    "core": false,
    "slug": "astronomy",
    "tableOfContents": {
      "html": "<p><span class=\"by_qmJFRN7jitjPsuF3f\">See also </span><a href=\"https://www.lesswrong.com/tag/physics\"><span class=\"by_qmJFRN7jitjPsuF3f\">Physics</span></a><span class=\"by_qmJFRN7jitjPsuF3f\">, </span><a href=\"https://www.lesswrong.com/tag/astrobiology\"><span class=\"by_qmJFRN7jitjPsuF3f\">Astrobiology</span></a><span class=\"by_qmJFRN7jitjPsuF3f\">, </span><a href=\"https://www.lesswrong.com/tag/great-filter\"><span class=\"by_qmJFRN7jitjPsuF3f\">Great Filter</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "See also [Physics](https://www.lesswrong.com/tag/physics), [Astrobiology](https://www.lesswrong.com/tag/astrobiology), [Great Filter](https://www.lesswrong.com/tag/great-filter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p2aBoEmJhjZYpkaPZ",
    "name": "Guild of the Rose",
    "core": false,
    "slug": "guild-of-the-rose",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RyNWXFjKNcafRKvPh",
    "name": "Agent Foundations",
    "core": false,
    "slug": "agent-foundations",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 14,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mSTmKrSkFBswHaS3T",
    "name": "Eliciting Latent Knowledge (ELK)",
    "core": false,
    "slug": "eliciting-latent-knowledge-elk",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Eliciting Latent Knowledge</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is an open problem in </span><a href=\"ai\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> safety.</span></p><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">In these cases, the prediction model \"knows\" facts (like \"the camera was tampered with\") that are not visible on camera but would change our evaluation of the predicted future if we learned them.&nbsp;</span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">How can we train this model to report its latent knowledge of off-screen events?</span></strong></p></blockquote><p><a href=\"https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit\"><span class=\"by_HoGziwmhpMGqGeWZy\">--ARC report</span></a></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai\"><span class=\"by_HoGziwmhpMGqGeWZy\">Transparency/Interpretability</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 45,
    "description": {
      "markdown": "**Eliciting Latent Knowledge** is an open problem in [AI](ai) safety.\n\n> Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us.\n> \n> But some action sequences could tamper with the cameras so they show happy humans regardless of what’s really happening. More generally, some futures look great on camera but are actually catastrophically bad.\n> \n> In these cases, the prediction model \"knows\" facts (like \"the camera was tampered with\") that are not visible on camera but would change our evaluation of the predicted future if we learned them. **How can we train this model to report its latent knowledge of off-screen events?**\n\n[--ARC report](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit)\n\nSee also: [Transparency/Interpretability](https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aHay2tebonHAYKtac",
    "name": "Anthropic",
    "core": false,
    "slug": "anthropic",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Anthropic </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is an AI organization.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Not to be confused with </span><a href=\"anthropics\"><span class=\"by_HoGziwmhpMGqGeWZy\">anthropics</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Anthropic** is an AI organization.\n\nNot to be confused with [anthropics](anthropics)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xi9JLxEtEoPCgRXRj",
    "name": "Transformer Circuits",
    "core": false,
    "slug": "transformer-circuits",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8Ec9rD286qNstoiGH",
    "name": "AXRP",
    "core": false,
    "slug": "axrp",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">AI X-</span><span class=\"by_DgsGzjyBXN8XSK22q\">risk Research</span><span class=\"by_HoGziwmhpMGqGeWZy\"> Podcast</span></span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a podcast hosted by Daniel Filan.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Website: </span><a href=\"https://axrp.net/\"><span class=\"by_HoGziwmhpMGqGeWZy\">axrp.net</span></a></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"audio\"><span class=\"by_HoGziwmhpMGqGeWZy\">Audio</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"interviews\"><span class=\"by_HoGziwmhpMGqGeWZy\">Interviews</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "The **AI X-risk Research Podcast** is a podcast hosted by Daniel Filan.\n\nWebsite: [axrp.net](https://axrp.net/)\n\nSee also: [Audio](audio), [Interviews](interviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DGBnywFjJtbAn4q7A",
    "name": "LessWrong Books",
    "core": false,
    "slug": "lesswrong-books",
    "tableOfContents": {
      "html": "<p><strong id=\"LessWrong_Books\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LessWrong Books</span></strong></p>",
      "sections": [
        {
          "title": "LessWrong Books",
          "anchor": "LessWrong_Books",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 4,
    "description": {
      "markdown": "**LessWrong Books**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AJDHQ4mFnsNbBzPhT",
    "name": "Gödelian Logic",
    "core": false,
    "slug": "goedelian-logic",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Gödelian Logic</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> refers to logic, math, and arguments in the style of </span><a href=\"https://en.wikipedia.org/wiki/Kurt_G%C3%B6del\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Kurt Gödel</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">. Specifically - his two incompleteness theorems, and one completeness theorem. Due to their tricky and subtle nature, his incompleteness theorems are possibly the most misunderstood theorems of all time.</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">“All the limitative theorems of metamathematics and the theory of computation suggest that once the ability to represent your own structure has reached a certain critical point, that is the kiss of death: it guarantees that you can never represent yourself totally. </span><a href=\"https://en.wikipedia.org/wiki/Gödel's_incompleteness_theorems\"><u><span class=\"by_sKAL2jzfkYkDbQmx9\">Gödel’s Incompleteness Theorem</span></u></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><u><span class=\"by_sKAL2jzfkYkDbQmx9\">Church’s Undecidability Theorem</span></u><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><a href=\"http://en.wikipedia.org/wiki/Halting_problem\"><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><u><span class=\"by_sKAL2jzfkYkDbQmx9\">Turing’s Halting Theorem</span></u></a><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><a href=\"http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem\"><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><u><span class=\"by_sKAL2jzfkYkDbQmx9\">Tarski’s Truth Theorem</span></u></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> — all have the flavour of some ancient fairy tale which warns you that “To seek self-knowledge is to embark on a journey which … will always be incomplete, cannot be charted on any map, will never halt, cannot be described.” </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">- Douglas Hofstadter, Gödel, Escher, Bach</span></i></p></blockquote><h3 id=\"G_del_s_Completeness_Theorem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gödel's Completeness Theorem</span></h3><p><span class=\"by_sKAL2jzfkYkDbQmx9\">This theorem is less well known than the other two, which came after it, but also less misunderstood.&nbsp;</span></p><h3 id=\"G_del_s_First_Incompleteness_Theorem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gödel's First Incompleteness Theorem</span></h3><h3 id=\"G_del_s_Second_Incompleteness_Theorem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gödel's Second Incompleteness Theorem</span></h3><p><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></p><h3 id=\"Probabilistic_Solutions\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Probabilistic Solutions</span></h3><p><span class=\"by_sKAL2jzfkYkDbQmx9\">One way you might think to get around Gödel's Incompleteness, is to leave behind logical certainty, and instead </span><a href=\"https://www.lesswrong.com/posts/duAkuSqJhGDcfMaTA/reflection-in-probabilistic-logic\"><span class=\"by_sKAL2jzfkYkDbQmx9\">assign probabilities to logical statements</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.&nbsp;</span></p><h3 id=\"External_Resources_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Resources:</span></h3><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Quanta Magazine: </span><a href=\"https://www.quantamagazine.org/how-godels-incompleteness-theorems-work-20200714\"><span class=\"by_sKAL2jzfkYkDbQmx9\">How Gödel's Proof Works</span></a></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Stanford Encyclopedia of Philosophy:</span><ul><li><a href=\"https://plato.stanford.edu/entries/goedel-incompleteness\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gödel’s Incompleteness Theorems</span></a></li><li><a href=\"https://plato.stanford.edu/entries/goedel\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Kurt Gödel</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (including his </span><a href=\"https://plato.stanford.edu/entries/goedel/#ComThe\"><span class=\"by_sKAL2jzfkYkDbQmx9\">completeness theorem</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></li></ul></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia:</span><ul><li><a href=\"https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Incompleteness Theorems</span></a></li><li><a href=\"https://en.wikipedia.org/wiki/G%C3%B6del%27s_completeness_theorem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Completeness Theorem</span></a></li><li><a href=\"https://en.wikipedia.org/wiki/First-order_logic\"><span class=\"by_sKAL2jzfkYkDbQmx9\">First-order logic</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> and </span><a href=\"https://en.wikipedia.org/wiki/Second-order_logic\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Second-order logic</span></a></li><li><a href=\"https://en.wikipedia.org/wiki/Kurt_G%C3%B6del\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Kurt Gödel</span></a></li></ul></li></ul>",
      "sections": [
        {
          "title": "Gödel's Completeness Theorem",
          "anchor": "G_del_s_Completeness_Theorem",
          "level": 1
        },
        {
          "title": "Gödel's First Incompleteness Theorem",
          "anchor": "G_del_s_First_Incompleteness_Theorem",
          "level": 1
        },
        {
          "title": "Gödel's Second Incompleteness Theorem",
          "anchor": "G_del_s_Second_Incompleteness_Theorem",
          "level": 1
        },
        {
          "title": "Probabilistic Solutions",
          "anchor": "Probabilistic_Solutions",
          "level": 1
        },
        {
          "title": "External Resources:",
          "anchor": "External_Resources_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 20,
    "description": {
      "markdown": "**Gödelian Logic** refers to logic, math, and arguments in the style of [Kurt Gödel](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del). Specifically - his two incompleteness theorems, and one completeness theorem. Due to their tricky and subtle nature, his incompleteness theorems are possibly the most misunderstood theorems of all time.\n\n> “All the limitative theorems of metamathematics and the theory of computation suggest that once the ability to represent your own structure has reached a certain critical point, that is the kiss of death: it guarantees that you can never represent yourself totally. [Gödel’s Incompleteness Theorem](https://en.wikipedia.org/wiki/Gödel's_incompleteness_theorems), Church’s Undecidability Theorem, [Turing’s Halting Theorem](http://en.wikipedia.org/wiki/Halting_problem), [Tarski’s Truth Theorem](http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem) — all have the flavour of some ancient fairy tale which warns you that “To seek self-knowledge is to embark on a journey which … will always be incomplete, cannot be charted on any map, will never halt, cannot be described.” *\\- Douglas Hofstadter, Gödel, Escher, Bach*\n\n### Gödel's Completeness Theorem\n\nThis theorem is less well known than the other two, which came after it, but also less misunderstood. \n\n### Gödel's First Incompleteness Theorem\n\n### Gödel's Second Incompleteness Theorem\n\n### Probabilistic Solutions\n\nOne way you might think to get around Gödel's Incompleteness, is to leave behind logical certainty, and instead [assign probabilities to logical statements](https://www.lesswrong.com/posts/duAkuSqJhGDcfMaTA/reflection-in-probabilistic-logic). \n\n### External Resources:\n\n*   Quanta Magazine: [How Gödel's Proof Works](https://www.quantamagazine.org/how-godels-incompleteness-theorems-work-20200714)\n*   Stanford Encyclopedia of Philosophy:\n    *   [Gödel’s Incompleteness Theorems](https://plato.stanford.edu/entries/goedel-incompleteness)\n    *   [Kurt Gödel](https://plato.stanford.edu/entries/goedel) (including his [completeness theorem](https://plato.stanford.edu/entries/goedel/#ComThe))\n*   Wikipedia:\n    *   [Incompleteness Theorems](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems)\n    *   [Completeness Theorem](https://en.wikipedia.org/wiki/G%C3%B6del%27s_completeness_theorem)\n    *   [First-order logic](https://en.wikipedia.org/wiki/First-order_logic) and [Second-order logic](https://en.wikipedia.org/wiki/Second-order_logic)\n    *   [Kurt Gödel](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hyxRtKmvhtruSAp2C",
    "name": "Copenhagen Interpretation of Ethics",
    "core": false,
    "slug": "copenhagen-interpretation-of-ethics",
    "tableOfContents": {
      "html": "<p><span class=\"by_nLbwLhBaQeG6tCNDN\">The </span><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Copenhagen Interpretation of Ethics</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> says that when you interact with a problem in any way, you can be blamed for it.</span></p><h2 id=\"External_Links\"><span class=\"by_nLbwLhBaQeG6tCNDN\">External Links</span></h2><ul><li><a href=\"https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/\"><span class=\"by_nLbwLhBaQeG6tCNDN\">The Copenhagen Interpretation of Ethics</span></a></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": null,
    "description": {
      "markdown": "The **Copenhagen Interpretation of Ethics** says that when you interact with a problem in any way, you can be blamed for it.\n\nExternal Links\n--------------\n\n*   [The Copenhagen Interpretation of Ethics](https://blog.jaibot.com/the-copenhagen-interpretation-of-ethics/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ximq2Bqv2B3TZ27eD",
    "name": "Missing Moods",
    "core": false,
    "slug": "missing-moods",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wRdzfoH39vxqtNf37",
    "name": "The Signaling Trilemma",
    "core": false,
    "slug": "the-signaling-trilemma",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">belief signaling trilemma</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> (or </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">signaling trilemma</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> for simplicity) points out that (a) people assign reputation based on claims; (b) people want to maintain their reputation; therefore, (c) people warp their claims. This presents a trilemma:</span></p><ol><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">We could agree to stop assigning reputation based on beliefs, but this would deprive us of an extremely valuable tool for evaluating others, besides being impossible to enforce.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">We could agree to always report honest beliefs, but this could be very costly for cooperators and again impossible to enforce.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">We could embrace dishonest reporting of beliefs, but this can severely warp the discourse.</span></li></ol><p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Related tags/pages:</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> </span><a href=\"https://www.lesswrong.com/tag/deception\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Deception</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><a href=\"https://www.lesswrong.com/tag/honesty\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Honesty</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><a href=\"https://www.lesswrong.com/tag/meta-honesty\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Meta-Honesty</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><a href=\"https://www.lesswrong.com/tag/signaling\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Signaling</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "The **belief signaling trilemma** (or *signaling trilemma* for simplicity) points out that (a) people assign reputation based on claims; (b) people want to maintain their reputation; therefore, (c) people warp their claims. This presents a trilemma:\n\n1.  We could agree to stop assigning reputation based on beliefs, but this would deprive us of an extremely valuable tool for evaluating others, besides being impossible to enforce.\n2.  We could agree to always report honest beliefs, but this could be very costly for cooperators and again impossible to enforce.\n3.  We could embrace dishonest reporting of beliefs, but this can severely warp the discourse.\n\n**Related tags/pages:** [Deception](https://www.lesswrong.com/tag/deception), [Honesty](https://www.lesswrong.com/tag/honesty), [Meta-Honesty](https://www.lesswrong.com/tag/meta-honesty), [Signaling](https://www.lesswrong.com/tag/signaling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "j2coh2kwvyBZc3GAe",
    "name": "COVID-19-Booster",
    "core": false,
    "slug": "covid-19-booster",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LkShkiJwNL2kBbWbM",
    "name": "EfficientZero",
    "core": false,
    "slug": "efficientzero",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CbucWMrhobAmtzpMF",
    "name": "The Problem of the Criterion",
    "core": false,
    "slug": "the-problem-of-the-criterion",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_gjoi5eBQob27Lww62\">The problem of the criterion </span></strong><span><span class=\"by_gjoi5eBQob27Lww62\">is </span><span class=\"by_sKAL2jzfkYkDbQmx9\">an </span></span><a href=\"https://www.lesswrong.com/tag/open-problems\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">open</span><span class=\"by_gjoi5eBQob27Lww62\"> problem</span></span></a><span class=\"by_gjoi5eBQob27Lww62\"> in </span><a href=\"https://www.lesswrong.com/tag/epistemology\"><span class=\"by_gjoi5eBQob27Lww62\">epistemology</span></a><span class=\"by_gjoi5eBQob27Lww62\"> that creates </span><a href=\"https://iep.utm.edu/ep-circ/\"><span class=\"by_gjoi5eBQob27Lww62\">epistemic circularity</span></a><span class=\"by_gjoi5eBQob27Lww62\"> via the following loop:</span></p><ul><li><span class=\"by_gjoi5eBQob27Lww62\">If you know something to be true, then it must satisfy some criterion of truth that you know</span></li><li><span class=\"by_gjoi5eBQob27Lww62\">If you know the criterion of truth to be true, then it must satisfy the criterion of truth</span></li></ul><p><span class=\"by_gjoi5eBQob27Lww62\">The implication of the problem of the criterion is that knowledge cannot be well grounded with more knowledge or truth, and instead must be grounded in something else. Common approaches are to ground knowledge in self-consistency, in ungrounded assumptions, or in purpose.</span></p><p><strong id=\"External_Resources_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Resources:</span></strong></p><ul><li><a href=\"https://iep.utm.edu/criterio/\"><span class=\"by_gjoi5eBQob27Lww62\">The Problem of the Criterion (IEP)</span></a></li><li><a href=\"https://en.wikipedia.org/wiki/Problem_of_the_criterion\"><span class=\"by_gjoi5eBQob27Lww62\">Problem of the criterion (Wikipedia)</span></a></li></ul><p><strong id=\"Related_Pages_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong></p><ul><li><a href=\"https://www.lesswrong.com/tag/epistemology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">epistemology</span></a></li><li><a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Truth, Semantics, &amp; Meaning</span></a></li></ul>",
      "sections": [
        {
          "title": "External Resources:",
          "anchor": "External_Resources_",
          "level": 1
        },
        {
          "title": "Related Pages:",
          "anchor": "Related_Pages_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 7,
    "description": {
      "markdown": "**The problem of the criterion** is an [open problem](https://www.lesswrong.com/tag/open-problems) in [epistemology](https://www.lesswrong.com/tag/epistemology) that creates [epistemic circularity](https://iep.utm.edu/ep-circ/) via the following loop:\n\n*   If you know something to be true, then it must satisfy some criterion of truth that you know\n*   If you know the criterion of truth to be true, then it must satisfy the criterion of truth\n\nThe implication of the problem of the criterion is that knowledge cannot be well grounded with more knowledge or truth, and instead must be grounded in something else. Common approaches are to ground knowledge in self-consistency, in ungrounded assumptions, or in purpose.\n\n**External Resources:**\n\n*   [The Problem of the Criterion (IEP)](https://iep.utm.edu/criterio/)\n*   [Problem of the criterion (Wikipedia)](https://en.wikipedia.org/wiki/Problem_of_the_criterion)\n\n**Related Pages:**\n\n*   [epistemology](https://www.lesswrong.com/tag/epistemology)\n*   [Truth, Semantics, & Meaning](https://www.lesswrong.com/tag/truth-semantics-and-meaning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jxMoPnsWXBnDzXBwE",
    "name": "Negotiation",
    "core": false,
    "slug": "negotiation",
    "tableOfContents": {
      "html": "<p><strong id=\"Negotiation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Negotiation</span></strong></p>",
      "sections": [
        {
          "title": "Negotiation",
          "anchor": "Negotiation",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 11,
    "description": {
      "markdown": "**Negotiation**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PgXFLqriw5Far9v7x",
    "name": "AI Success Models",
    "core": false,
    "slug": "ai-success-models",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">AI Success Models</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> are proposed paths to an existential win via aligned AI. They are (so far) high level overviews and won't contain all the details, but present at least a sketch of what a full solution might look like. They can be contrasted with </span><a href=\"https://www.lesswrong.com/tag/threat-models\"><span class=\"by_Sp5wM4aRAhNERd4oY\">threat models</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, which are stories about how AI might lead to major problems.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "**AI Success Models** are proposed paths to an existential win via aligned AI. They are (so far) high level overviews and won't contain all the details, but present at least a sketch of what a full solution might look like. They can be contrasted with [threat models](https://www.lesswrong.com/tag/threat-models), which are stories about how AI might lead to major problems."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QT87jxkk6DXuS8hGA",
    "name": "Explicit Reasoning",
    "core": false,
    "slug": "explicit-reasoning",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In the simple case, </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">explicit reasoning </span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">is reasoning which:</span></p><ul><li><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Can be </span><span class=\"by_sKAL2jzfkYkDbQmx9\">explained</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> in language.</span></span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Uses well-defined terms.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Relies on well-understood reasoning steps.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Has clear assumptions and conclusions.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">You are aware of doing when you do it.</span></li></ul><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">\"What do I mean by </span><i><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">explicit reason</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">?</span></i><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> I don’t refer merely</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> to </span><span class=\"by_sKAL2jzfkYkDbQmx9\">“</span></span><a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\"><span class=\"by_sKAL2jzfkYkDbQmx9\">System 2</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">”,</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">brain’s slow, sequential, analytical, fully conscious, and effortful mode of cognition. I refer to the </span></span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">informed</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> application of this type of thinking. Gathering data with real effort to find out, crunching the numbers with a grasp of the math, modeling the world with testable predictions, reflection on your thinking with an awareness of biases. Reason requires good inputs and a lot of effort.\" - </span><a href=\"https://www.lesswrong.com/users/jacob-falkovich\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Jacob Falkovich</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/posts/YcdArE79SDxwWAuyF/the-treacherous-path-to-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Treacherous Path to Rationality</span></a></p></blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">However, the exact definition may vary based on context. For example, explicit reasoning might be operationalized as imaginary verbal reasoning taking place in a person's inner monologue (ie, in auditory working memory). In other cases, we might have a much higher standard, eg actual symbolic logic written on an external medium such as paper. So, reasoning can be more and less explicit, along several dimensions.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Explicit reasoning is one of many modes of reasoning by which humans may reach conclusions. While it is not always the best mode of reasoning, it has the advantage of being </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">scrutable</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">, ie, open to inspection. This makes it easier to correct, in particular through imaginary verbal reasoning modelled after dialogue (ie, mentally responding to yourself as if you were another person, with critiques and corrections). Since it can easily be recorded, it can also be subject to feedback from many other people, which can further improve the quality of this type of reasoning. Also, explicit reasoning can easily be chained together to reach less obvious conclusions.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Other types of reasoning include </span><a href=\"https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">inner sim</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, gestalt pattern recognition, mental imagery, and </span><a href=\"https://www.lesswrong.com/tag/focusing\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Gendlin's Focusing</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">. Important questions include when to trust different modes of reasoning, how to combine the results of different reasoning modes, and how to best facilitate communication between different modes of reasoning.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Related to: </span><a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Dual Process Theory</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "In the simple case, **explicit reasoning** is reasoning which:\n\n*   Can be explained in language.\n*   Uses well-defined terms.\n*   Relies on well-understood reasoning steps.\n*   Has clear assumptions and conclusions.\n*   You are aware of doing when you do it.\n\n> \"What do I mean by ***explicit reason**?* I don’t refer merely to “[System 2](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2)”, the brain’s slow, sequential, analytical, fully conscious, and effortful mode of cognition. I refer to the *informed* application of this type of thinking. Gathering data with real effort to find out, crunching the numbers with a grasp of the math, modeling the world with testable predictions, reflection on your thinking with an awareness of biases. Reason requires good inputs and a lot of effort.\" - [Jacob Falkovich](https://www.lesswrong.com/users/jacob-falkovich), [The Treacherous Path to Rationality](https://www.lesswrong.com/posts/YcdArE79SDxwWAuyF/the-treacherous-path-to-rationality)\n\nHowever, the exact definition may vary based on context. For example, explicit reasoning might be operationalized as imaginary verbal reasoning taking place in a person's inner monologue (ie, in auditory working memory). In other cases, we might have a much higher standard, eg actual symbolic logic written on an external medium such as paper. So, reasoning can be more and less explicit, along several dimensions.\n\nExplicit reasoning is one of many modes of reasoning by which humans may reach conclusions. While it is not always the best mode of reasoning, it has the advantage of being *scrutable*, ie, open to inspection. This makes it easier to correct, in particular through imaginary verbal reasoning modelled after dialogue (ie, mentally responding to yourself as if you were another person, with critiques and corrections). Since it can easily be recorded, it can also be subject to feedback from many other people, which can further improve the quality of this type of reasoning. Also, explicit reasoning can easily be chained together to reach less obvious conclusions.\n\nOther types of reasoning include [inner sim](https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter), gestalt pattern recognition, mental imagery, and [Gendlin's Focusing](https://www.lesswrong.com/tag/focusing). Important questions include when to trust different modes of reasoning, how to combine the results of different reasoning modes, and how to best facilitate communication between different modes of reasoning.\n\nRelated to: [Dual Process Theory](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sJYvMHBHnR6DbJQcN",
    "name": "Journaling",
    "core": false,
    "slug": "journaling",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Journaling </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is the practice of recording events in your life, usually by writing about it daily.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Journaling can help to remember things in your past - such as special events, belief updates, mood and emotional states - and track how they change across time. It can also help you notice and reflect on things, either naturally by thinking back on things that happened, or more directly helping you focus on things through prompting, like </span><a href=\"https://www.lesswrong.com/tag/gratitude\"><span class=\"by_sKAL2jzfkYkDbQmx9\">gratitude</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> journaling which can help you focus on things you're grateful for.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">There are various forms of jounraling:</span></p><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Daily / Non-daily / Irregularly: Most people jounral daily, but journaling can also be done on a different cycle, or irregularly.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Prompts / Free-form / Mixed: Prompts can help you focus on specific things you want to journal about, but may also feel cumbersome for some people or at some times.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Digital / Handwritten / Audio: Classically, of course, journaling was solely handwritten. Today it's also possible to write in a digital format. And it's also possible to keep an audio journal by recording yourself instead of writing.</span></li></ul><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/gratitude\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gratitude</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Journaling** is the practice of recording events in your life, usually by writing about it daily.\n\nJournaling can help to remember things in your past - such as special events, belief updates, mood and emotional states - and track how they change across time. It can also help you notice and reflect on things, either naturally by thinking back on things that happened, or more directly helping you focus on things through prompting, like [gratitude](https://www.lesswrong.com/tag/gratitude) journaling which can help you focus on things you're grateful for.\n\nThere are various forms of jounraling:\n\n*   Daily / Non-daily / Irregularly: Most people jounral daily, but journaling can also be done on a different cycle, or irregularly.\n*   Prompts / Free-form / Mixed: Prompts can help you focus on specific things you want to journal about, but may also feel cumbersome for some people or at some times.\n*   Digital / Handwritten / Audio: Classically, of course, journaling was solely handwritten. Today it's also possible to write in a digital format. And it's also possible to keep an audio journal by recording yourself instead of writing.\n\n**Related Pages:** [Gratitude](https://www.lesswrong.com/tag/gratitude)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tJv2Zbtx37mBGBJk6",
    "name": "Symbol Grounding",
    "core": false,
    "slug": "symbol-grounding",
    "tableOfContents": {
      "html": "<p><strong id=\"Symbol_Grounding\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Symbol Grounding</span></strong></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Truth, Semantics, &amp; Meaning</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/philosophy-of-language\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Philosophy of Language</span></a></p>",
      "sections": [
        {
          "title": "Symbol Grounding",
          "anchor": "Symbol_Grounding",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 11,
    "description": {
      "markdown": "**Symbol Grounding**\n\n**Related Pages:** [Truth, Semantics, & Meaning](https://www.lesswrong.com/tag/truth-semantics-and-meaning), [Philosophy of Language](https://www.lesswrong.com/tag/philosophy-of-language)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dHfxtPwAmrij4KEce",
    "name": "Redwood Research",
    "core": false,
    "slug": "redwood-research",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BcnLB8PkrkqPhZ6XY",
    "name": "Bureaucracy",
    "core": false,
    "slug": "bureaucracy-1",
    "tableOfContents": {
      "html": "<p><strong id=\"Bureaucracy\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bureaucracy</span></strong></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/moral-mazes\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Moral Mazes</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/politics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Politics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/mechanism-design\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mechanism Design</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/stagnation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Stagnation</span></a></p>",
      "sections": [
        {
          "title": "Bureaucracy",
          "anchor": "Bureaucracy",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 12,
    "description": {
      "markdown": "**Bureaucracy**\n\n**Related Pages:** [Moral Mazes](https://www.lesswrong.com/tag/moral-mazes), [Politics](https://www.lesswrong.com/tag/politics), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), [Stagnation](https://www.lesswrong.com/tag/stagnation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bkgy24HML6nLSLqcj",
    "name": "Stagnation",
    "core": false,
    "slug": "stagnation",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Quoting Jason Crawford's </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">Technological Stagnation: Why I came around:</span></i></p><blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Thiel, along with economists such as Tyler Cowen (</span><a href=\"https://www.amazon.com/dp/B004H0M8QS?tag=jasocraw-20\"><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">The Great Stagnation</span></i></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">) and Robert Gordon (</span><a href=\"https://rootsofprogress.org/summary-the-rise-and-fall-of-american-growth\"><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">The Rise and Fall of American Growth</span></i></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">), promotes a “stagnation hypothesis”: that there has been a significant slowdown in scientific, technological, and economic progress in recent decades—say, for a round number, since about 1970, or the last ~50 years.</span></p></blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Eliezer has posted </span><a href=\"https://www.reddit.com/r/WritingPrompts/comments/3xgqj6/wp_write_a_dystopian_vision_of_the_future_from/cy4zyfd/\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">a short work of fiction on </span><span class=\"by_mBbethzTYSABWSDjX\">Reddit</span></span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> which explores related ideas.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/progress-studies\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Progress Studies</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/bureaucracy-1\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bureaucracy</span></a><span class=\"by_mBbethzTYSABWSDjX\">, </span><a href=\"https://www.lesswrong.com/tag/world-modeling\"><span class=\"by_mBbethzTYSABWSDjX\">World Modeling</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "Quoting Jason Crawford's *Technological Stagnation: Why I came around:*\n\n> Thiel, along with economists such as Tyler Cowen ([*The Great Stagnation*](https://www.amazon.com/dp/B004H0M8QS?tag=jasocraw-20)) and Robert Gordon ([*The Rise and Fall of American Growth*](https://rootsofprogress.org/summary-the-rise-and-fall-of-american-growth)), promotes a “stagnation hypothesis”: that there has been a significant slowdown in scientific, technological, and economic progress in recent decades—say, for a round number, since about 1970, or the last ~50 years.\n\nEliezer has posted [a short work of fiction on Reddit](https://www.reddit.com/r/WritingPrompts/comments/3xgqj6/wp_write_a_dystopian_vision_of_the_future_from/cy4zyfd/) which explores related ideas.\n\n**Related Pages:** [Progress Studies](https://www.lesswrong.com/tag/progress-studies), [Bureaucracy](https://www.lesswrong.com/tag/bureaucracy-1), [World Modeling](https://www.lesswrong.com/tag/world-modeling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YgizoZqa7LEb3LEJn",
    "name": "AMA",
    "core": false,
    "slug": "ama",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">An </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Ask Me Anything (AMA)</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> post is an invitation to ask the author questions.&nbsp;</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"https://www.lesswrong.com/tag/q-and-a-format\"><span class=\"by_HoGziwmhpMGqGeWZy\">Q&amp;A (format)</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/interviews\"><span class=\"by_HoGziwmhpMGqGeWZy\">Interviews</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 22,
    "description": {
      "markdown": "An **Ask Me Anything (AMA)** post is an invitation to ask the author questions. \n\nSee also: [Q&A (format)](https://www.lesswrong.com/tag/q-and-a-format), [Interviews](https://www.lesswrong.com/tag/interviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mxSBcaTrakvCkgLzL",
    "name": "Mind Crime",
    "core": false,
    "slug": "mind-crime",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Mind Crime</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> occurs when a computational process which has moral value is mistreated. For example, an advanced AI trying to predict human behavior might create simulations of humans so detailed as to be conscious observers, which would then suffer through whatever hypothetical scenarios the AI wanted to test and then be discarded.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Mind crime on a large scale constitutes a </span><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_HoGziwmhpMGqGeWZy\">risk of astronomical suffering</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Mind crime is different from other AI risks in that the AI need not even affect anything outside its box for the catastrophe to occur.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The term was coined by Nick Bostrom in </span><i><span class=\"by_HoGziwmhpMGqGeWZy\">Superintelligence: Paths, Dangers, Strategies.</span></i></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Not the same as </span><a href=\"https://en.wikipedia.org/wiki/Thoughtcrime\"><span class=\"by_HoGziwmhpMGqGeWZy\">thoughtcrime</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, a term for having beliefs considered unacceptable by society.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "**Mind Crime** occurs when a computational process which has moral value is mistreated. For example, an advanced AI trying to predict human behavior might create simulations of humans so detailed as to be conscious observers, which would then suffer through whatever hypothetical scenarios the AI wanted to test and then be discarded.\n\nMind crime on a large scale constitutes a [risk of astronomical suffering](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks).\n\nMind crime is different from other AI risks in that the AI need not even affect anything outside its box for the catastrophe to occur.\n\nThe term was coined by Nick Bostrom in *Superintelligence: Paths, Dangers, Strategies.*\n\nNot the same as [thoughtcrime](https://en.wikipedia.org/wiki/Thoughtcrime), a term for having beliefs considered unacceptable by society."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EmaCLRKb4baBFq4ra",
    "name": "dath ilan",
    "core": false,
    "slug": "dath-ilan",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_nLbwLhBaQeG6tCNDN\">Dath</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> ilan</span></span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a fictional world and civilization invented by Eliezer Yudkowsky. It is a parallel reality of earth where society is much better at coordination and various good policies have been implemented, though technology is only as advanced as earth's, if not slightly less. It can be thought as a more practical and realistic form of </span><a href=\"https://slatestarcodex.com/2014/08/24/the-invisible-nation-reconciling-utilitarianism-and-contractualism/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Economist's Paradise</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">. Eliezer first introduced it in his </span><a href=\"https://www.lesswrong.com/tag/april-fool-s\"><span class=\"by_sKAL2jzfkYkDbQmx9\">April Fool's</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> day post '</span><a href=\"https://yudkowsky.tumblr.com/post/81447230971/my-april-fools-day-confession\"><span class=\"by_sKAL2jzfkYkDbQmx9\">My April Fools Day Confession</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">', where he claimed that he </span><span class=\"by_nmk3nLpQE89dMRzzN\">was an average person</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> from that world and none of his ideas were original.</span></span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">This world was further fleshed out (and some its backstory changed) in a later April Fool's post:</span></p><ul><li><a href=\"https://www.lesswrong.com/posts/gvA4j8pGYG4xtaTkw/i-m-from-a-parallel-earth-with-much-higher-coordination-ama\"><span class=\"by_nmk3nLpQE89dMRzzN\">I'm from a parallel Earth with much higher coordination: AMA</span></a></li></ul><p><span class=\"by_nmk3nLpQE89dMRzzN\">And in a series of glowfics featuring dath ilani characters:</span></p><ul><li><a href=\"https://www.glowfic.com/posts/4503\"><span class=\"by_nmk3nLpQE89dMRzzN\">a dath ilani matchmaker in King Randale's court</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> (incomplete)</span></li><li><a href=\"https://www.glowfic.com/posts/4508\"><span class=\"by_nmk3nLpQE89dMRzzN\">but hurting people is wrong</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> (complete)</span></li><li><a href=\"https://www.glowfic.com/posts/4582\"><span class=\"by_nmk3nLpQE89dMRzzN\">mad investor chaos and the woman of asmodeus</span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\"> (ongoing as of </span><span class=\"by_drtzESkjzzrctW2Hr\">Apr 2022)</span></span></li><li><a href=\"https://www.glowfic.com/posts/5633\"><span class=\"by_drtzESkjzzrctW2Hr\">a dath ilani EMT in queen abrogail's court</span></a><span class=\"by_drtzESkjzzrctW2Hr\"> (ongoing as of Apr 2022)</span></li></ul><p><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Some </span><span class=\"by_nmk3nLpQE89dMRzzN\">known features</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> of dath </span><span class=\"by_nmk3nLpQE89dMRzzN\">ilan:</span></span></p><ul><li><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Standard education includes rationality training</span><span class=\"by_nmk3nLpQE89dMRzzN\"> (from which, allegedly, all of Eliezer's own ideas are plagiarized).</span></span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">What this Earth knows as \"</span><a href=\"https://arbital.com/p/logical_dt/\"><span class=\"by_nmk3nLpQE89dMRzzN\">logical decision theory</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">\" / \"Functional Decision Theory\" is just standard decision theory as formulated in dath ilan.</span></li><li><a href=\"https://en.wikipedia.org/wiki/Land_value_tax\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Land Value Tax</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://en.wikipedia.org/wiki/Positional_good\"><span class=\"by_sKAL2jzfkYkDbQmx9\">positional-goods</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> tax, status-goods tax, marketing-</span><span class=\"by_nmk3nLpQE89dMRzzN\">tax,</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> no income tax.</span></span></li><li><a href=\"https://www.lesswrong.com/posts/cmiRk9XtT9Psnd3Yr/movable-housing-for-scalable-cities\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Movable Homes</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Autonomous electric cars in tunnels instead of </span><a href=\"https://en.wikipedia.org/wiki/Internal_combustion_engine\"><span class=\"by_sKAL2jzfkYkDbQmx9\">ICE</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> cars on roads.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">No streetlights at night, except for red lights along walkways, which for 45 minutes each night also turn off to see the sky. And on winter solstice (night of stars) the lights stay off the whole night, to give a perfect view of the night sky.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">The average IQ in Earth-equivalent terms is 143, after an unknown number of generations of \"heritage optimization\" pursued via positive government subsidies.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">Dath ilan has a lower population than Earth (1 billion people), since their higher intelligence and higher coordination enabled them to make tech progress without having first attained a higher population than that.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">There exists a single global civilization, calling itself \"Civilization\", with one world government, called \"Governance\".</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">All humans (and other hominids, like chimpanzees) are cryopreserved upon death.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">If you commit a murder but don't want to consign your victim to true death, you can call in a government service, the </span><a href=\"https://www.facebook.com/yudkowsky/posts/10159752926899228\"><span class=\"by_nmk3nLpQE89dMRzzN\">Surreptitious Head Removers</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">, who will remove your victim's head for cryonic preservation, while being sworn not to report on this.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">Dath ilani holidays are known to include the annual Alien Invasion Rehearsal Festival and the annual Oops It's Time To Overthrow the Government Festival.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">There exists a group called Keepers who undergo much more rigorous rationality training and act as a counterbalance to Governance; the dath ilani equivalent of Leo Szilard eventually told Governance about her idea for nuclear weapons, but she talked to the Keepers first.</span></li><li><span class=\"by_nmk3nLpQE89dMRzzN\">In the fictional setting (though this concept didn't appear in the original April Fools post) dath ilan has found it necessary to try to causally screen off its entire history from its present - all old books hidden away, all old cities mothballed. &nbsp;As the Doylistic result, dath ilani characters ending up in settings with much lower social technology levels, such as Valdemar, Golarion, or Earth, start with little prior concept of how a less-coordinated society looks, and must learn from experience.</span></li></ul><p><strong id=\"Not_tagged_due_to_spoilers_\"><span class=\"by_HoGziwmhpMGqGeWZy\">Not tagged due to spoilers:</span></strong></p><div class=\"spoilers\"><p><a href=\"https://www.lesswrong.com/posts/uyBeAN5jPEATMqKkX/lies-told-to-children-1\"><span class=\"by_HoGziwmhpMGqGeWZy\">Lies Told To Children</span></a></p></div><p><strong id=\"External_Posts_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Posts:</span></strong></p><ul><li><a href=\"https://hivewired.wordpress.com/2017/06/04/until-we-build-dath-ilan/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Until we Build dath ilan</span></a></li></ul>",
      "sections": [
        {
          "title": "Not tagged due to spoilers:",
          "anchor": "Not_tagged_due_to_spoilers_",
          "level": 1
        },
        {
          "title": "External Posts:",
          "anchor": "External_Posts_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 23,
    "description": {
      "markdown": "**Dath ilan** is a fictional world and civilization invented by Eliezer Yudkowsky. It is a parallel reality of earth where society is much better at coordination and various good policies have been implemented, though technology is only as advanced as earth's, if not slightly less. It can be thought as a more practical and realistic form of [Economist's Paradise](https://slatestarcodex.com/2014/08/24/the-invisible-nation-reconciling-utilitarianism-and-contractualism/). Eliezer first introduced it in his [April Fool's](https://www.lesswrong.com/tag/april-fool-s) day post '[My April Fools Day Confession](https://yudkowsky.tumblr.com/post/81447230971/my-april-fools-day-confession)', where he claimed that he was an average person from that world and none of his ideas were original.\n\nThis world was further fleshed out (and some its backstory changed) in a later April Fool's post:\n\n*   [I'm from a parallel Earth with much higher coordination: AMA](https://www.lesswrong.com/posts/gvA4j8pGYG4xtaTkw/i-m-from-a-parallel-earth-with-much-higher-coordination-ama)\n\nAnd in a series of glowfics featuring dath ilani characters:\n\n*   [a dath ilani matchmaker in King Randale's court](https://www.glowfic.com/posts/4503) (incomplete)\n*   [but hurting people is wrong](https://www.glowfic.com/posts/4508) (complete)\n*   [mad investor chaos and the woman of asmodeus](https://www.glowfic.com/posts/4582) (ongoing as of Apr 2022)\n*   [a dath ilani EMT in queen abrogail's court](https://www.glowfic.com/posts/5633) (ongoing as of Apr 2022)\n\nSome known features of dath ilan:\n\n*   Standard education includes rationality training (from which, allegedly, all of Eliezer's own ideas are plagiarized).\n*   What this Earth knows as \"[logical decision theory](https://arbital.com/p/logical_dt/)\" / \"Functional Decision Theory\" is just standard decision theory as formulated in dath ilan.\n*   [Land Value Tax](https://en.wikipedia.org/wiki/Land_value_tax), [positional-goods](https://en.wikipedia.org/wiki/Positional_good) tax, status-goods tax, marketing-tax, no income tax.\n*   [Movable Homes](https://www.lesswrong.com/posts/cmiRk9XtT9Psnd3Yr/movable-housing-for-scalable-cities).\n*   Autonomous electric cars in tunnels instead of [ICE](https://en.wikipedia.org/wiki/Internal_combustion_engine) cars on roads.\n*   No streetlights at night, except for red lights along walkways, which for 45 minutes each night also turn off to see the sky. And on winter solstice (night of stars) the lights stay off the whole night, to give a perfect view of the night sky.\n*   The average IQ in Earth-equivalent terms is 143, after an unknown number of generations of \"heritage optimization\" pursued via positive government subsidies.\n*   Dath ilan has a lower population than Earth (1 billion people), since their higher intelligence and higher coordination enabled them to make tech progress without having first attained a higher population than that.\n*   There exists a single global civilization, calling itself \"Civilization\", with one world government, called \"Governance\".\n*   All humans (and other hominids, like chimpanzees) are cryopreserved upon death.\n*   If you commit a murder but don't want to consign your victim to true death, you can call in a government service, the [Surreptitious Head Removers](https://www.facebook.com/yudkowsky/posts/10159752926899228), who will remove your victim's head for cryonic preservation, while being sworn not to report on this.\n*   Dath ilani holidays are known to include the annual Alien Invasion Rehearsal Festival and the annual Oops It's Time To Overthrow the Government Festival.\n*   There exists a group called Keepers who undergo much more rigorous rationality training and act as a counterbalance to Governance; the dath ilani equivalent of Leo Szilard eventually told Governance about her idea for nuclear weapons, but she talked to the Keepers first.\n*   In the fictional setting (though this concept didn't appear in the original April Fools post) dath ilan has found it necessary to try to causally screen off its entire history from its present - all old books hidden away, all old cities mothballed.  As the Doylistic result, dath ilani characters ending up in settings with much lower social technology levels, such as Valdemar, Golarion, or Earth, start with little prior concept of how a less-coordinated society looks, and must learn from experience.\n\n**Not tagged due to spoilers:**\n\n[Lies Told To Children](https://www.lesswrong.com/posts/uyBeAN5jPEATMqKkX/lies-told-to-children-1)\n\n**External Posts:**\n\n*   [Until we Build dath ilan](https://hivewired.wordpress.com/2017/06/04/until-we-build-dath-ilan/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TDcpBdA8h7xsoWZWe",
    "name": "Lightcone Infrastructure",
    "core": false,
    "slug": "lightcone-infrastructure",
    "tableOfContents": {
      "html": "<p><a href=\"https://www.lightconeinfrastructure.com/\"><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Lightcone Infrastructure</span></strong></a><strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">is the parent organization of LessWrong. Its </span><span class=\"by_W7ETRtvRMqYetyQE9\">mission</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> is to build infrastructure for people who are helping the </span></span><a href=\"https://www.lesswrong.com/tag/longtermism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">long term</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> future of humanity. It was </span><a href=\"https://www.lesswrong.com/posts/eR7Su77N2nK3e5YRZ/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">announced</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> on </span><span class=\"by_W7ETRtvRMqYetyQE9\">October 1st,</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> 2021.</span></span></p><figure class=\"image image_resized\" style=\"width:59.56%\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_200 200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_400 400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_600 600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_800 800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_1000 1000w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_1200 1200w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_1400 1400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_1600 1600w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_1800 1800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png/w_2000 2000w\"></figure><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Website:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lightconeinfrastructure.com/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">lightconeinfrastructure.com</span></a></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"lw-moderation\"><span class=\"by_HoGziwmhpMGqGeWZy\">LW Moderation</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"site-meta\"><span class=\"by_HoGziwmhpMGqGeWZy\">Site Meta</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": "[**Lightcone Infrastructure**](https://www.lightconeinfrastructure.com/)  is the parent organization of LessWrong. Its mission is to build infrastructure for people who are helping the [long term](https://www.lesswrong.com/tag/longtermism) future of humanity. It was [announced](https://www.lesswrong.com/posts/eR7Su77N2nK3e5YRZ/) on October 1st, 2021.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/19f81bb3a671af655ea5d7ac08f1916c211ec158b55f78c7.png)\n\n**Website:** [lightconeinfrastructure.com](https://www.lightconeinfrastructure.com/)\n\nSee also: [LW Moderation](lw-moderation), [Site Meta](site-meta)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sYbszETv5rKst6gxD",
    "name": "Leverage Research",
    "core": false,
    "slug": "leverage-research",
    "tableOfContents": {
      "html": "<ul><li><span class=\"by_xmtrQfQhhb3peY3ER\">Official Website: </span><a href=\"http://leverageresearch.org/\"><u><span class=\"by_xmtrQfQhhb3peY3ER\">http://leverageresearch.org/</span></u></a></li><li><span class=\"by_xmtrQfQhhb3peY3ER\">Leverage Research quarterly newsletter: </span><a href=\"https://www.leverageresearch.org/updates\"><u><span class=\"by_xmtrQfQhhb3peY3ER\">https://www.leverageresearch.org/updates</span></u></a></li><li><span class=\"by_xmtrQfQhhb3peY3ER\">Leverage Research 2019-2020 Annual Report: </span><a href=\"https://www.leverageresearch.org/annual-report-2019-2020\"><u><span class=\"by_xmtrQfQhhb3peY3ER\">https://www.leverageresearch.org/annual-report-2019-2020</span></u></a></li><li><span class=\"by_xmtrQfQhhb3peY3ER\">Leverage Research Twitter: </span><a href=\"https://twitter.com/leverageres\"><u><span class=\"by_xmtrQfQhhb3peY3ER\">https://twitter.com/leverageres</span></u></a></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "*   Official Website: [http://leverageresearch.org/](http://leverageresearch.org/)\n*   Leverage Research quarterly newsletter: [https://www.leverageresearch.org/updates](https://www.leverageresearch.org/updates)\n*   Leverage Research 2019-2020 Annual Report: [https://www.leverageresearch.org/annual-report-2019-2020](https://www.leverageresearch.org/annual-report-2019-2020)\n*   Leverage Research Twitter: [https://twitter.com/leverageres](https://twitter.com/leverageres)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bbdbpGWWPMfKBzk7z",
    "name": "Gradient Hacking",
    "core": false,
    "slug": "gradient-hacking",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FJ3MGb684F88BoN2o",
    "name": "Formal Proof",
    "core": false,
    "slug": "formal-proof",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">A </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Formal Proof</span></strong><span><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a </span><span class=\"by_W7ETRtvRMqYetyQE9\">finite sequence</span><span class=\"by_Sp5wM4aRAhNERd4oY\"> of steps from </span></span><a href=\"https://en.wikipedia.org/wiki/Axiom\"><span class=\"by_Sp5wM4aRAhNERd4oY\">axiom(s)</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> or previous derived proof(s) which strictly follow the allowed rules of </span><a href=\"https://en.wikipedia.org/wiki/Inference\"><span class=\"by_Sp5wM4aRAhNERd4oY\">inference</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> of the </span><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics\"><span class=\"by_Sp5wM4aRAhNERd4oY\">mathematical system</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> in which it exists. They are used to establish statements as true within a mathematical framework in a way which can be independently verified with extremely high certainty, with the most reliable flavor of proof being machine-checked proofs generated by </span><a href=\"https://en.wikipedia.org/wiki/Proof_assistant\"><span class=\"by_Sp5wM4aRAhNERd4oY\">proof assistants</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> since they have even less room for human error.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 31,
    "description": {
      "markdown": "A **Formal Proof** is a finite sequence of steps from [axiom(s)](https://en.wikipedia.org/wiki/Axiom) or previous derived proof(s) which strictly follow the allowed rules of [inference](https://en.wikipedia.org/wiki/Inference) of the [mathematical system](https://www.lesswrong.com/tag/logic-and-mathematics) in which it exists. They are used to establish statements as true within a mathematical framework in a way which can be independently verified with extremely high certainty, with the most reliable flavor of proof being machine-checked proofs generated by [proof assistants](https://en.wikipedia.org/wiki/Proof_assistant) since they have even less room for human error."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KmgkrftQuX7jmjjp5",
    "name": "Language Models",
    "core": false,
    "slug": "language-models",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Language Models</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> are a class of </span><a href=\"https://www.lesswrong.com/tag/ai\"><span class=\"by_Sp5wM4aRAhNERd4oY\">AI</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> trained on text, usually to predict the next word or a word which has been obscured. They have the ability to generate novel prose or code based on an initial prompt, which gives rise to a kind of natural language programming called prompt engineering. The most popular architecture for very large language models is called a </span><a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\"><span class=\"by_Sp5wM4aRAhNERd4oY\">transformer</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, which follows consistent </span><a href=\"https://www.lesswrong.com/tag/scaling-laws\"><span class=\"by_Sp5wM4aRAhNERd4oY\">scaling laws</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> with respect to the size of the model being trained, meaning that a larger model trained with the same amount of compute will produce results which are better by a predictable amount (when measured by the 'perplexity', or how surprised the AI is by a test set of human-generated text).</span></p><h3 id=\"See_also\"><span class=\"by_Sp5wM4aRAhNERd4oY\">See also</span></h3><ul><li><a href=\"https://www.lesswrong.com/tag/gpt\"><span class=\"by_Sp5wM4aRAhNERd4oY\">GPT</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> - A family of large language models created by </span><a href=\"https://www.lesswrong.com/tag/openai\"><span class=\"by_Sp5wM4aRAhNERd4oY\">OpenAI</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 98,
    "description": {
      "markdown": "**Language Models** are a class of [AI](https://www.lesswrong.com/tag/ai) trained on text, usually to predict the next word or a word which has been obscured. They have the ability to generate novel prose or code based on an initial prompt, which gives rise to a kind of natural language programming called prompt engineering. The most popular architecture for very large language models is called a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)), which follows consistent [scaling laws](https://www.lesswrong.com/tag/scaling-laws) with respect to the size of the model being trained, meaning that a larger model trained with the same amount of compute will produce results which are better by a predictable amount (when measured by the 'perplexity', or how surprised the AI is by a test set of human-generated text).\n\n### See also\n\n*   [GPT](https://www.lesswrong.com/tag/gpt) \\- A family of large language models created by [OpenAI](https://www.lesswrong.com/tag/openai)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jygEZ8peqh6QRZYry",
    "name": "Autonomous Weapons",
    "core": false,
    "slug": "autonomous-weapons",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Autonomous Weapons</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> are military systems designed to operate without a human in the loop. Some have argued that they may present a destabilizing influence on geopolitics, or even a potential existential risk.</span></p><h3 id=\"See_also\"><span class=\"by_Sp5wM4aRAhNERd4oY\">See also</span></h3><ul><li><a href=\"https://en.wikipedia.org/wiki/Lethal_autonomous_weapon\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Wikipedia page on Lethal Autonomous Weapons</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 6,
    "description": {
      "markdown": "**Autonomous Weapons** are military systems designed to operate without a human in the loop. Some have argued that they may present a destabilizing influence on geopolitics, or even a potential existential risk.\n\n### See also\n\n*   [Wikipedia page on Lethal Autonomous Weapons](https://en.wikipedia.org/wiki/Lethal_autonomous_weapon)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NGtNzdS88JtEQdRP4",
    "name": "Extraterrestrial Life",
    "core": false,
    "slug": "extraterrestrial-life",
    "tableOfContents": {
      "html": "<p><strong id=\"Extraterrestrial_Life\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Extraterrestrial Life</span></strong></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span><a href=\"https://www.lesswrong.com/tag/great-filter\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Great Filter</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/space-exploration-and-colonization\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Space Exploration &amp; Colonization</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/biology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Biology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Evolution</span></a></p>",
      "sections": [
        {
          "title": "Extraterrestrial Life",
          "anchor": "Extraterrestrial_Life",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 21,
    "description": {
      "markdown": "**Extraterrestrial Life**\n\nRelated Pages: [Great Filter](https://www.lesswrong.com/tag/great-filter), [Space Exploration & Colonization](https://www.lesswrong.com/tag/space-exploration-and-colonization), [Biology](https://www.lesswrong.com/tag/biology), [Evolution](https://www.lesswrong.com/tag/evolution)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Myz8DA9AghgpNei9i",
    "name": "Gradient Descent",
    "core": false,
    "slug": "gradient-descent",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">stub</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "stub"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bHGixy9hHdmENhoe6",
    "name": "External Events",
    "core": false,
    "slug": "external-events",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">This is for events of interested to people not posted through the normal LessWrong event system, e.g., conferences.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "This is for events of interested to people not posted through the normal LessWrong event system, e.g., conferences."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wFDrB47FAhkLgp4bJ",
    "name": "Superrationality",
    "core": false,
    "slug": "superrationality",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Superrationality</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a concept invented by Douglas Hofstadter. He thought that agents should cooperate in Prisoner's Dilemma, but the primary notion of \"rationality\" which had been deeply developed by economists, decision-theorists, and game-theorists disagreed. Rather than fighting over the definition of rational, Douglas Hofstadter coined the term superrational for the kind of rationality he was interested in.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Douglas Hofstadter did not publish a full theory of superrationality, instead focusing on the prisoner's dilemma and other specific examples to communicate his intuition. However, some principles can be clearly inferred from his writing, such as reasoning as if you have control over the other player's policy (in symmetric games).</span></p><p><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Eliezer </span><span class=\"by_W7ETRtvRMqYetyQE9\">Yudkowsky</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> shared the same core intuition with Douglas Hofstadter, but took the path of trying to reclaim the word </span></span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">rational</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> for what he meant. As a result, LessWrong does not consistently use superrational/superrationality.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "**Superrationality** is a concept invented by Douglas Hofstadter. He thought that agents should cooperate in Prisoner's Dilemma, but the primary notion of \"rationality\" which had been deeply developed by economists, decision-theorists, and game-theorists disagreed. Rather than fighting over the definition of rational, Douglas Hofstadter coined the term superrational for the kind of rationality he was interested in.\n\nDouglas Hofstadter did not publish a full theory of superrationality, instead focusing on the prisoner's dilemma and other specific examples to communicate his intuition. However, some principles can be clearly inferred from his writing, such as reasoning as if you have control over the other player's policy (in symmetric games).\n\nEliezer Yudkowsky shared the same core intuition with Douglas Hofstadter, but took the path of trying to reclaim the word *rational* for what he meant. As a result, LessWrong does not consistently use superrational/superrationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hEKzJqK2NWcoZmcwz",
    "name": "Welcome Threads",
    "core": false,
    "slug": "welcome-threads",
    "tableOfContents": {
      "html": "<p><span class=\"by_CFPfSn52jRARrjACG\">If you are new to LessWrong, the current iteration of this, is the place to introduce yourself.</span></p><p><span class=\"by_CFPfSn52jRARrjACG\">Personal stories, anecdotes, or just general comments on how you found us and what you hope to get from the site and community are invited.</span></p><p><span class=\"by_CFPfSn52jRARrjACG\">This is also the place to discuss feature requests and other ideas you have for the site, if you don't want to write a full top-level post.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "If you are new to LessWrong, the current iteration of this, is the place to introduce yourself.\n\nPersonal stories, anecdotes, or just general comments on how you found us and what you hope to get from the site and community are invited.\n\nThis is also the place to discuss feature requests and other ideas you have for the site, if you don't want to write a full top-level post."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vH8zJLkhiqdJzK5ej",
    "name": "Bragging Threads",
    "core": false,
    "slug": "bragging-threads",
    "tableOfContents": {
      "html": "<p><span class=\"by_CFPfSn52jRARrjACG\">Your job, should you choose to accept it, is to comment on this thread explaining&nbsp;</span><strong><span class=\"by_CFPfSn52jRARrjACG\">the most awesome thing you've done this month</span></strong><span class=\"by_CFPfSn52jRARrjACG\">. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself&nbsp;</span><i><span class=\"by_CFPfSn52jRARrjACG\">the coolest freaking person ever</span></i><span class=\"by_CFPfSn52jRARrjACG\">&nbsp;because of that awesome thing you're dying to tell everyone about. This is the place to do just that.</span></p><p><span class=\"by_CFPfSn52jRARrjACG\">Remember, however, that this&nbsp;</span><strong><span class=\"by_CFPfSn52jRARrjACG\">isn't</span></strong><span class=\"by_CFPfSn52jRARrjACG\">&nbsp;any kind of progress thread. Nor is it any kind of proposal thread.&nbsp;</span><i><span class=\"by_CFPfSn52jRARrjACG\">This thread is solely for people to talk about the awesome things they have done. Not \"will do\". Not \"are working on\"</span></i><span class=\"by_CFPfSn52jRARrjACG\">.&nbsp;</span><strong><span class=\"by_CFPfSn52jRARrjACG\">Have already done.</span></strong><span class=\"by_CFPfSn52jRARrjACG\">&nbsp;This is to cultivate an environment of object level productivity rather than meta-productivity methods.</span></p><p><span class=\"by_CFPfSn52jRARrjACG\">So, what's the coolest thing you've done this month?</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "Your job, should you choose to accept it, is to comment on this thread explaining **the most awesome thing you've done this month**. You may be as blatantly proud of yourself as you feel. You may unabashedly consider yourself *the coolest freaking person ever* because of that awesome thing you're dying to tell everyone about. This is the place to do just that.\n\nRemember, however, that this **isn't** any kind of progress thread. Nor is it any kind of proposal thread. *This thread is solely for people to talk about the awesome things they have done. Not \"will do\". Not \"are working on\"*. **Have already done.** This is to cultivate an environment of object level productivity rather than meta-productivity methods.\n\nSo, what's the coolest thing you've done this month?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ocGoDbHKBv46AwXnT",
    "name": "Narrow AI",
    "core": false,
    "slug": "narrow-ai",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">A</span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> Narrow AI</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an </span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Artificial General Intelligence</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">. Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "A **Narrow AI** is capable of operating only in a relatively limited domain, such as chess or driving, rather than capable of learning a broad range of tasks like a human or an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence). Narrow vs General is not a perfectly binary classification, there are degrees of generality with, for example, large language models having a fairly large degree of generality (as the domain of text is large) without being as general as a human, and we may eventually build systems that are significantly more general than humans."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iKYWGuFx2qH2nYu6J",
    "name": "AI Capabilities",
    "core": false,
    "slug": "ai-capabilities",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">AI Capabilities</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> are the growing abilities of AIs to act effectively in increasingly complex environments. It is often compared to to AI Alignment, which refers to efforts to ensure that these effective actions taken by AIs are also intended by the creators and beneficial to humanity.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 41,
    "description": {
      "markdown": "**AI Capabilities** are the growing abilities of AIs to act effectively in increasingly complex environments. It is often compared to to AI Alignment, which refers to efforts to ensure that these effective actions taken by AIs are also intended by the creators and beneficial to humanity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XtnxF4TsAEyWBRJZq",
    "name": "Scoring Rules",
    "core": false,
    "slug": "scoring-rules",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Scoring Rules</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> are ways to score answers on a test, a prediction, or any other performance.</span></p><p><span class=\"by_5snwaf7CSi8LSs9Wp\">Of special interest are </span><a href=\"https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules\"><span class=\"by_5snwaf7CSi8LSs9Wp\">proper scoring rules </span></a><span><span class=\"by_5snwaf7CSi8LSs9Wp\">- rules such that</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> the </span><span class=\"by_5snwaf7CSi8LSs9Wp\">strategy for maximizing</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> the </span><span class=\"by_5snwaf7CSi8LSs9Wp\">expected score coincides with noting down your true beliefs about the question.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/calibration\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Calibration</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Forecasting &amp; Prediction</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/skill-expertise-assessment\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Skill / Expertise Assessment</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Prediction Markets</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Scoring Rules** are ways to score answers on a test, a prediction, or any other performance.\n\nOf special interest are [proper scoring rules](https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules) \\- rules such that the strategy for maximizing the expected score coincides with noting down your true beliefs about the question.\n\n**Related Pages:** [Calibration](https://www.lesswrong.com/tag/calibration), [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [Skill / Expertise Assessment](https://www.lesswrong.com/tag/skill-expertise-assessment), [Prediction Markets](https://www.lesswrong.com/tag/prediction-markets)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5A5ZGTQovxbay6fpr",
    "name": "Fairness",
    "core": false,
    "slug": "fairness",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Fairness</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a property of a system or process where no part is privileged, along some measure. A common problem is how to divide a cake between 3 people. Should every one get a third? what if one of them is hungrier? What if they don't agree about what is fair? This is an example of </span><a href=\"https://en.wikipedia.org/wiki/Fair_division\"><span class=\"by_sKAL2jzfkYkDbQmx9\">fair division</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.&nbsp;</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Talking about fairness is difficult, as it runs into many </span><a href=\"Philosophy of Language\"><span class=\"by_sKAL2jzfkYkDbQmx9\">problems with language </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">(as surely the first sentence in this page does). What does privilege mean? what does it mean for someone to be </span><a href=\"https://en.wikipedia.org/wiki/Entitlement_(fair_division)\"><span class=\"by_sKAL2jzfkYkDbQmx9\">entitled</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> to something?</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">It also runs it problems of value. How do we measure value? is value objective or subjective? How do we know how much value something is worth to someone? Should an arbiter decide or can people decide between themselves?</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Objective value is considered impossible, so objective fairness is considered impossible too. Which means an arbiter can't be used, and participants have to find a arrangement which is subjectively fair to them.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Which opens up problems of </span><a href=\"https://www.lesswrong.com/tag/honesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">truthfulness</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, strategy and verification.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Several posts on LessWrong deal with this topic, and it's also a field of research called Fair Division.</span></p><p><strong id=\"Related_pages_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages:</span></strong></p><ul><li><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Game Theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/voting-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Voting Theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/economics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Economics</span></a></li></ul><p><strong id=\"External_links_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External links:</span></strong></p><ul><li><a href=\"https://en.wikipedia.org/wiki/Fairness\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia: Fairness</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></li><li><a href=\"https://en.wikipedia.org/wiki/Fair_division\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia: Fair division</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></li><li><a href=\"https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_fair_division\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia: List of unsolved problems in fair division</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></li></ul>",
      "sections": [
        {
          "title": "Related pages:",
          "anchor": "Related_pages_",
          "level": 1
        },
        {
          "title": "External links:",
          "anchor": "External_links_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 21,
    "description": {
      "markdown": "**Fairness** is a property of a system or process where no part is privileged, along some measure. A common problem is how to divide a cake between 3 people. Should every one get a third? what if one of them is hungrier? What if they don't agree about what is fair? This is an example of [fair division](https://en.wikipedia.org/wiki/Fair_division). \n\nTalking about fairness is difficult, as it runs into many [problems with language](Philosophy of Language) (as surely the first sentence in this page does). What does privilege mean? what does it mean for someone to be [entitled](https://en.wikipedia.org/wiki/Entitlement_(fair_division)) to something?\n\nIt also runs it problems of value. How do we measure value? is value objective or subjective? How do we know how much value something is worth to someone? Should an arbiter decide or can people decide between themselves?\n\nObjective value is considered impossible, so objective fairness is considered impossible too. Which means an arbiter can't be used, and participants have to find a arrangement which is subjectively fair to them.\n\nWhich opens up problems of [truthfulness](https://www.lesswrong.com/tag/honesty), strategy and verification.\n\nSeveral posts on LessWrong deal with this topic, and it's also a field of research called Fair Division.\n\n**Related pages:**\n\n*   [Game Theory](https://www.lesswrong.com/tag/game-theory)\n*   [Voting Theory](https://www.lesswrong.com/tag/voting-theory)\n*   [Economics](https://www.lesswrong.com/tag/economics)\n\n**External links:**\n\n*   [Wikipedia: Fairness](https://en.wikipedia.org/wiki/Fairness) \n*   [Wikipedia: Fair division](https://en.wikipedia.org/wiki/Fair_division) \n*   [Wikipedia: List of unsolved problems in fair division](https://en.wikipedia.org/wiki/List_of_unsolved_problems_in_fair_division)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "npoWi8AYpegPcSYcw",
    "name": "D&D.Sci",
    "core": false,
    "slug": "d-and-d-sci",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Dungeons and Data Science</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">, or </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">D&amp;D.Sci,</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is</span><span class=\"by_FEYrj3QwEyf8z3oQf\"> a series of analytical exercises</span><span class=\"by_HoGziwmhpMGqGeWZy\"> played on Less Wrong,</span><span class=\"by_FEYrj3QwEyf8z3oQf\"> framed as problems in a D&amp;D style adventuring world.</span></span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Generally D&amp;D.Sci exercises will give the player a set of data but not the rules used to generate it, and the player must extrapolate patterns from the data in order to optimize the solution to some problem. After each exercise there is usually a follow-up post which explains the underlying rules of the scenario and gives a score to each player who participated at the time.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 46,
    "description": {
      "markdown": "**Dungeons and Data Science**, or **D&D.Sci,** is a series of analytical exercises played on Less Wrong, framed as problems in a D&D style adventuring world.\n\nGenerally D&D.Sci exercises will give the player a set of data but not the rules used to generate it, and the player must extrapolate patterns from the data in order to optimize the solution to some problem. After each exercise there is usually a follow-up post which explains the underlying rules of the scenario and gives a score to each player who participated at the time."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DHZhAxgE5edGxxh8d",
    "name": "Object level and Meta level",
    "core": false,
    "slug": "object-level-and-meta-level",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Object level</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> vs </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Meta level</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a distinction between levels of </span><a href=\"https://www.lesswrong.com/tag/abstraction\"><span class=\"by_Sp5wM4aRAhNERd4oY\">abstraction</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, with the object level being directly about the specifics of the issue at hand, while meta level is concerned with the general principles which apply to a wider range of examples. It is often useful to move up and down the ladder of abstraction to get points across clearly, with the object level providing firm grounding and concrete examples for a person to grasp, while the meta layer allows them to generalize a new concept to a broad domain.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Object level** vs **Meta level** is a distinction between levels of [abstraction](https://www.lesswrong.com/tag/abstraction), with the object level being directly about the specifics of the issue at hand, while meta level is concerned with the general principles which apply to a wider range of examples. It is often useful to move up and down the ladder of abstraction to get points across clearly, with the object level providing firm grounding and concrete examples for a person to grasp, while the meta layer allows them to generalize a new concept to a broad domain."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qNF7Ti87CLfHhbttj",
    "name": "Treacherous Turn",
    "core": false,
    "slug": "treacherous-turn",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">A </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Treacherous Turn</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a hypothetical event where an advanced </span><a href=\"ai\"><span class=\"by_Sp5wM4aRAhNERd4oY\">AI</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> system which has been pretending to be aligned due to its relative weakness turns on humanity once it achieves sufficient power that it can pursue its true objective without risk.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "A **Treacherous Turn** is a hypothetical event where an advanced [AI](ai) system which has been pretending to be aligned due to its relative weakness turns on humanity once it achieves sufficient power that it can pursue its true objective without risk."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bXSkC7FFgApRnSajw",
    "name": "General Semantics",
    "core": false,
    "slug": "general-semantics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DtsGBCwAbrckp6BRv",
    "name": "AF Non Member Popup First",
    "core": false,
    "slug": "af-non-member-popup-first",
    "tableOfContents": {
      "html": "<p><strong id=\"Note__This_is_how_submissions_work_for_non_members_of_the_AI_Alignment_Forum\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Note! This is how submissions work for non-members of the AI Alignment Forum</span></strong></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">If you are seeing this, you are not a full-member.</span></i></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">If you post or comment, your submission will enter a review queue and a decision to accept or reject it from the Alignment Forum will be made within three days. If it is rejected, you will receive a minimum one-sentence explanation.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">In the meantime (and regardless of outcome), your post or comment will be published to </span><a href=\"https://www.lesswrong.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. There it can be immediately viewed and discussed by everyone, and edited by you.</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">For more detail, see our </span><a href=\"https://alignmentforum.org/faq\"><span class=\"by_qgdGA4ZEyW7zNdK84\">FAQ</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> where we answer questions like </span><a href=\"https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq#How_can_non_members_participate_in_the_Forum_\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">How can non-members participate in the Forum?</span></i></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq#How_do_I_join_the_Alignment_Forum_\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">How do I join the Alignment Forum?</span></i></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> Also feel free to shoot us a message via Intercom (bottom right of the page), or email us at team@lesswrong.com</span></p>",
      "sections": [
        {
          "title": "Note! This is how submissions work for non-members of the AI Alignment Forum",
          "anchor": "Note__This_is_how_submissions_work_for_non_members_of_the_AI_Alignment_Forum",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": null,
    "description": {
      "markdown": "**Note! This is how submissions work for non-members of the AI Alignment Forum**\n\n*If you are seeing this, you are not a full-member.*\n\n*   If you post or comment, your submission will enter a review queue and a decision to accept or reject it from the Alignment Forum will be made within three days. If it is rejected, you will receive a minimum one-sentence explanation.\n*   In the meantime (and regardless of outcome), your post or comment will be published to [LessWrong](https://www.lesswrong.com/). There it can be immediately viewed and discussed by everyone, and edited by you.\n\nFor more detail, see our [FAQ](https://alignmentforum.org/faq) where we answer questions like [*How can non-members participate in the Forum?*](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq#How_can_non_members_participate_in_the_Forum_) and [*How do I join the Alignment Forum?*](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq#How_do_I_join_the_Alignment_Forum_) Also feel free to shoot us a message via Intercom (bottom right of the page), or email us at team@lesswrong.com"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Eha62RrqBtEbpcEza",
    "name": "Repository",
    "core": false,
    "slug": "repository-1",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Repositories</span></strong><span class=\"by_9aki6x4AnrRLYJ8iX\"> are pages that are meant to collect information and advice of a specific type or area from the LW community.&nbsp;</span></p><p><span class=\"by_9aki6x4AnrRLYJ8iX\">They are similar to but distinct from </span><a href=\"https://www.lesswrong.com/tag/list-of-links\"><span class=\"by_9aki6x4AnrRLYJ8iX\">List of Links</span></a><span class=\"by_9aki6x4AnrRLYJ8iX\"> pages, which focus on collecting existing resources (either LW articles or external) rather than collecting advice &amp; data directly from participants.</span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": {
      "markdown": "**Repositories** are pages that are meant to collect information and advice of a specific type or area from the LW community. \n\nThey are similar to but distinct from [List of Links](https://www.lesswrong.com/tag/list-of-links) pages, which focus on collecting existing resources (either LW articles or external) rather than collecting advice & data directly from participants.\n\n*See also:*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KqfqD7YSMeFTLJCcs",
    "name": "Tripwire",
    "core": false,
    "slug": "tripwire",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">In AI safety, a </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">tripwire</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a mechanism designed to detect signs of misalignment in an advanced artificial intelligence and shut it down automatically.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "In AI safety, a **tripwire** is a mechanism designed to detect signs of misalignment in an advanced artificial intelligence and shut it down automatically."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7YH2RGRdysFwaAPav",
    "name": "Cancer",
    "core": false,
    "slug": "cancer-1",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FnnEaFGmG3FScvzj7",
    "name": "Transposons",
    "core": false,
    "slug": "transposons",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zFrdtg6wCuzWu6mhy",
    "name": "Finite Factored Sets",
    "core": false,
    "slug": "finite-factored-sets",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P2jmr8zbGbGnoMG8a",
    "name": "Dementia",
    "core": false,
    "slug": "dementia",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WiMZ8obYsfJPog3Ph",
    "name": "Apprenticeship",
    "core": false,
    "slug": "apprenticeship",
    "tableOfContents": {
      "html": "<p><strong id=\"Apprenticeship\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Apprenticeship</span></strong></p>",
      "sections": [
        {
          "title": "Apprenticeship",
          "anchor": "Apprenticeship",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 12,
    "description": {
      "markdown": "**Apprenticeship**"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6DDtyKtotNehTjmRn",
    "name": "Coherence Arguments",
    "core": false,
    "slug": "coherence-arguments",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X2rXNCtSnTtTLBYpq",
    "name": "Ivermectin (drug)",
    "core": false,
    "slug": "ivermectin-drug",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Ivermectin </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is an anti-parasitic drug that's been investigated as a treatment for Covid-19.</span></p><p><strong id=\"External_Articles_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Articles:</span></strong></p><ul><li><a href=\"https://astralcodexten.substack.com/p/ivermectin-much-more-than-you-wanted\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Ivermectin: Much More Than You Wanted To Know</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> by </span><span class=\"by_qf77EiaoMw7tH3GSr\">Scott</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> Alexander</span></span></li></ul>",
      "sections": [
        {
          "title": "External Articles:",
          "anchor": "External_Articles_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 7,
    "description": {
      "markdown": "**Ivermectin** is an anti-parasitic drug that's been investigated as a treatment for Covid-19.\n\n**External Articles:**\n\n*   [Ivermectin: Much More Than You Wanted To Know](https://astralcodexten.substack.com/p/ivermectin-much-more-than-you-wanted) by Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qDvaStt6c3KuqLr2P",
    "name": "AI Safety Camp",
    "core": false,
    "slug": "ai-safety-camp",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_BpBzKEueak7J8vHNi\">AI Safety Camp</span></strong><span class=\"by_BpBzKEueak7J8vHNi\"> (</span><strong><span class=\"by_BpBzKEueak7J8vHNi\">AISC</span></strong><span class=\"by_BpBzKEueak7J8vHNi\">) is a non-profit initiative to run programs for diversely skilled researchers who want to try collaborate on an open problem for reducing AI </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_BpBzKEueak7J8vHNi\">existential risk</span></a><span class=\"by_BpBzKEueak7J8vHNi\">.</span></p><p><a href=\"https://aisafety.camp/\"><span class=\"by_XevT6kRW9vHrqTubQ\">Official Website</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": {
      "markdown": "**AI Safety Camp** (**AISC**) is a non-profit initiative to run programs for diversely skilled researchers who want to try collaborate on an open problem for reducing AI [existential risk](https://www.lesswrong.com/tag/existential-risk).\n\n[Official Website](https://aisafety.camp/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y499WMAzNbrBM7Ytt",
    "name": "Veganism",
    "core": false,
    "slug": "veganism",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AKvmQFvDjxJNetTuk",
    "name": "Scaling Laws",
    "core": false,
    "slug": "scaling-laws",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Scaling Laws</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> refer to the observed trend of some machine learning architectures (notably </span><a href=\"https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\"><span class=\"by_Sp5wM4aRAhNERd4oY\">transformers</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">) to scale their performance on predictable power law when given more compute, data, or parameters (model size), assuming they are not bottlenecked on one of the other resources. This has been observed as highly consistent over more than six orders of magnitude.</span></p><figure class=\"image\"><img src=\"https://i.imgur.com/7lhHT8n.png\"><figcaption><span class=\"by_Sp5wM4aRAhNERd4oY\">Scaling laws graph from </span><a href=\"https://arxiv.org/pdf/2001.08361.pdf\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Scaling Laws for Neural Language Models</span></a></figcaption></figure>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 29,
    "description": {
      "markdown": "**Scaling Laws** refer to the observed trend of some machine learning architectures (notably [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))) to scale their performance on predictable power law when given more compute, data, or parameters (model size), assuming they are not bottlenecked on one of the other resources. This has been observed as highly consistent over more than six orders of magnitude.\n\n![](https://i.imgur.com/7lhHT8n.png)\n\nScaling laws graph from [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Qyeqh8wycbSapBNsp",
    "name": "Pomodoro Technique",
    "core": false,
    "slug": "pomodoro-technique",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Pomodoro Technique</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a </span><a href=\"lesswrong.com/tag/productivity\"><span class=\"by_HoGziwmhpMGqGeWZy\">productivity</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> technique where you alternate between 25 minutes of work and 5 minutes of break time.</span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">It gets its name from a kitchen timer shaped like a tomato </span><span class=\"by_W7ETRtvRMqYetyQE9\">(</span></span><i><span class=\"by_W7ETRtvRMqYetyQE9\">pomodoro</span></i><span><span class=\"by_HoGziwmhpMGqGeWZy\"> in </span><span class=\"by_W7ETRtvRMqYetyQE9\">Italian)</span><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">&nbsp;</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The basic intuition for the pomodoro technique is that:</span></p><ol><li><span class=\"by_Xn6ACr6Cua8upALWQ\">People concentrate most effectively shortly after a break</span></li><li><span class=\"by_Xn6ACr6Cua8upALWQ\">Most people are not innately good at noticing when they are not concentrating effectively</span></li></ol><p><span class=\"by_Xn6ACr6Cua8upALWQ\">By setting an actual, physical timer (for &nbsp;work time as well as for &nbsp;breaks), people are more likely to work effectively. Traditionally, this is set for 25 minutes of work and 5 minutes of time, though for some tasks other lengths may be appropriate. </span><a href=\"https://www.lesswrong.com/tag/self-experimentation\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Experimentation</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> is encouraged.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The 'full' pomodoro technique consists of 6 steps (from </span><a href=\"https://en.wikipedia.org/wiki/Pomodoro_Technique\"><span class=\"by_Xn6ACr6Cua8upALWQ\">wikipedia</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">):</span></p><blockquote><ol><li><span class=\"by_Xn6ACr6Cua8upALWQ\">Decide on the task to be done.</span></li><li><span class=\"by_Xn6ACr6Cua8upALWQ\">Set the pomodoro timer (traditionally to 25 minutes).</span><a href=\"https://en.wikipedia.org/wiki/Pomodoro_Technique#cite_note-Cirillo-1\"><sup><span class=\"by_Xn6ACr6Cua8upALWQ\">[1]</span></sup></a></li><li><span class=\"by_Xn6ACr6Cua8upALWQ\">Work on the task.</span></li><li><span class=\"by_Xn6ACr6Cua8upALWQ\">End work when the timer rings and put a checkmark on a piece of paper.</span><a href=\"https://en.wikipedia.org/wiki/Pomodoro_Technique#cite_note-CirilloHow-5\"><sup><span class=\"by_Xn6ACr6Cua8upALWQ\">[5]</span></sup></a></li><li><i><span class=\"by_Xn6ACr6Cua8upALWQ\">If you have fewer than four checkmarks,</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\"> take a short break (3–5 minutes) and then return to step 2; </span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">otherwise</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\"> continue to step 6.</span></li><li><i><span class=\"by_Xn6ACr6Cua8upALWQ\">After four pomodoros,</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\"> take a longer break (15–30 minutes), reset your checkmark count to zero, then go to step 1.</span></li></ol></blockquote><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Some who use the technique also encourage focussing on a single task for each pomodoro, or reviewing the work of the previous pomodoro for the first few minutes of each (a type of </span><a href=\"https://www.lesswrong.com/tag/spaced-repetition\"><span class=\"by_Xn6ACr6Cua8upALWQ\">spaced repetition</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">).</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Several tools are available to manage pomodoro timers. </span><a href=\"https://pomofocus.io/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Pomofocus</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> is a good, simple, online option. </span><a href=\"https://flowapp.info/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Flow</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> (Mac/iPhone/iPad) and </span><a href=\"https://www.microsoft.com/en-gb/p/pomodoro-flow/9p4btjxsv5nl?activetab=pivot:overviewtab\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Pomodoro Flow</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> (windows) are some examples of downloadable ones.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "The **Pomodoro Technique** is a [productivity](lesswrong.com/tag/productivity) technique where you alternate between 25 minutes of work and 5 minutes of break time.\n\nIt gets its name from a kitchen timer shaped like a tomato (*pomodoro* in Italian).\n\nThe basic intuition for the pomodoro technique is that:\n\n1.  People concentrate most effectively shortly after a break\n2.  Most people are not innately good at noticing when they are not concentrating effectively\n\nBy setting an actual, physical timer (for  work time as well as for  breaks), people are more likely to work effectively. Traditionally, this is set for 25 minutes of work and 5 minutes of time, though for some tasks other lengths may be appropriate. [Experimentation](https://www.lesswrong.com/tag/self-experimentation) is encouraged.\n\nThe 'full' pomodoro technique consists of 6 steps (from [wikipedia](https://en.wikipedia.org/wiki/Pomodoro_Technique)):\n\n> 1.  Decide on the task to be done.\n> 2.  Set the pomodoro timer (traditionally to 25 minutes).[^\\[1\\]^](https://en.wikipedia.org/wiki/Pomodoro_Technique#cite_note-Cirillo-1)\n> 3.  Work on the task.\n> 4.  End work when the timer rings and put a checkmark on a piece of paper.[^\\[5\\]^](https://en.wikipedia.org/wiki/Pomodoro_Technique#cite_note-CirilloHow-5)\n> 5.  *If you have fewer than four checkmarks,* take a short break (3–5 minutes) and then return to step 2; *otherwise* continue to step 6.\n> 6.  *After four pomodoros,* take a longer break (15–30 minutes), reset your checkmark count to zero, then go to step 1.\n\nSome who use the technique also encourage focussing on a single task for each pomodoro, or reviewing the work of the previous pomodoro for the first few minutes of each (a type of [spaced repetition](https://www.lesswrong.com/tag/spaced-repetition)).\n\nSeveral tools are available to manage pomodoro timers. [Pomofocus](https://pomofocus.io/) is a good, simple, online option. [Flow](https://flowapp.info/) (Mac/iPhone/iPad) and [Pomodoro Flow](https://www.microsoft.com/en-gb/p/pomodoro-flow/9p4btjxsv5nl?activetab=pivot:overviewtab) (windows) are some examples of downloadable ones."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LJxkaxERSGYKBBJp2",
    "name": "Landmark Forum",
    "core": false,
    "slug": "landmark-forum",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NzpKMYopbkgEPN486",
    "name": "Homunculus Fallacy",
    "core": false,
    "slug": "homunculus-fallacy",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">homunculus fallacy</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a mistake in reasoning in which one attempts to explain agency, consciousness, or related phenomena </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">by appealing to a module which solves that very problem</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The classic example is an \"explanation\" of visual processing in which an image is recorded by the retina, conveyed by the optic nerve, and then transmitted by the visual cortex to the rest of the brain, which \"sees\" everything. While it is true that images are recorded by the retina and conveyed to the rest of the brain by the optic nerve, this does not explain how the brain processes visual data. Instead, one imagines something like a little person or a floating consciousness (the \"homunculus\") who does the real work.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">This position is typically refuted by arguing that this requires an infinite recursion: if we then ask how the homunculus \"sees\", we would start with how its \"eyes\" take in data, and feed them into its own little visual cortex, where we are faced with the same problem again.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">This is a </span><a href=\"https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/6i3zToomS86oj9bS6\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">mysterious answer</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, but not a </span><a href=\"https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/FWMfQKG3RpZx6irjm\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">semantic stopsign</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">. If someone answers questions of the form \"why do humans do X\" with \"free will\", and answers \"why does free will do X\" with \"free will is free; you don't get to ask that!\", this is a semantic stopsign type mysterious-answer. Like vitalism, this person is </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">explicitly</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> trying to make an explanation mysterious. The homunculus fallacy is more like phlogiston: an attempt at a reductionistic answer which fails. From </span><a href=\"https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/6i3zToomS86oj9bS6\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Mysterious Answers to Mysterious Questions</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">:</span></p><blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Vitalism shared with phlogiston the error of </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">encapsulating the mystery as a substance.</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> Fire was mysterious, and the phlogiston theory encapsulated the mystery in a mysterious substance called “phlogiston.” Life was a sacred mystery, and vitalism encapsulated the sacred mystery in a mysterious substance called “Élan vital.” Neither answer helped concentrate the model’s probability density—helped make some outcomes easier to explain than others. The “explanation” just wrapped up the question as a small, hard, opaque black ball.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In a comedy written by Molière, a physician explains the power of a soporific by saying that it contains a “dormitive potency.” Same principle. It is a failure of human psychology that, faced with a mysterious phenomenon, we more readily postulate mysterious inherent substances than complex underlying processes.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">But the deeper failure is supposing that an </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">answer</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> can be mysterious. If a phenomenon feels mysterious, that is a fact about our state of knowledge, not a fact about the phenomenon itself. The vitalists saw a mysterious gap in their knowledge, and postulated a mysterious stuff that plugged the gap. In doing so, they mixed up the map with the territory. All confusion and bewilderment exist in the mind, not in encapsulated substances.</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "The **homunculus fallacy** is a mistake in reasoning in which one attempts to explain agency, consciousness, or related phenomena *by appealing to a module which solves that very problem*.\n\nThe classic example is an \"explanation\" of visual processing in which an image is recorded by the retina, conveyed by the optic nerve, and then transmitted by the visual cortex to the rest of the brain, which \"sees\" everything. While it is true that images are recorded by the retina and conveyed to the rest of the brain by the optic nerve, this does not explain how the brain processes visual data. Instead, one imagines something like a little person or a floating consciousness (the \"homunculus\") who does the real work.\n\nThis position is typically refuted by arguing that this requires an infinite recursion: if we then ask how the homunculus \"sees\", we would start with how its \"eyes\" take in data, and feed them into its own little visual cortex, where we are faced with the same problem again.\n\nThis is a [mysterious answer](https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/6i3zToomS86oj9bS6), but not a [semantic stopsign](https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/FWMfQKG3RpZx6irjm). If someone answers questions of the form \"why do humans do X\" with \"free will\", and answers \"why does free will do X\" with \"free will is free; you don't get to ask that!\", this is a semantic stopsign type mysterious-answer. Like vitalism, this person is *explicitly* trying to make an explanation mysterious. The homunculus fallacy is more like phlogiston: an attempt at a reductionistic answer which fails. From [Mysterious Answers to Mysterious Questions](https://www.lesswrong.com/s/5uZQHpecjn7955faL/p/6i3zToomS86oj9bS6):\n\n> Vitalism shared with phlogiston the error of *encapsulating the mystery as a substance.* Fire was mysterious, and the phlogiston theory encapsulated the mystery in a mysterious substance called “phlogiston.” Life was a sacred mystery, and vitalism encapsulated the sacred mystery in a mysterious substance called “Élan vital.” Neither answer helped concentrate the model’s probability density—helped make some outcomes easier to explain than others. The “explanation” just wrapped up the question as a small, hard, opaque black ball.\n> \n> In a comedy written by Molière, a physician explains the power of a soporific by saying that it contains a “dormitive potency.” Same principle. It is a failure of human psychology that, faced with a mysterious phenomenon, we more readily postulate mysterious inherent substances than complex underlying processes.\n> \n> But the deeper failure is supposing that an *answer* can be mysterious. If a phenomenon feels mysterious, that is a fact about our state of knowledge, not a fact about the phenomenon itself. The vitalists saw a mysterious gap in their knowledge, and postulated a mysterious stuff that plugged the gap. In doing so, they mixed up the map with the territory. All confusion and bewilderment exist in the mind, not in encapsulated substances."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KRcsSxBTLGRrSbHsW",
    "name": "Transformative AI",
    "core": false,
    "slug": "transformative-ai",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_XtphY3uYHwruKqDyG\">Transformative</span><span class=\"by_gjoi5eBQob27Lww62\"> AI</span></span></strong><span><span class=\"by_gjoi5eBQob27Lww62\"> </span><span class=\"by_XtphY3uYHwruKqDyG\">is</span><span class=\"by_gjoi5eBQob27Lww62\"> </span><span class=\"by_Sp5wM4aRAhNERd4oY\">\"[...] AI</span><span class=\"by_gjoi5eBQob27Lww62\"> that precipitates a transition comparable to (or more significant than) the agricultural or industrial </span><span class=\"by_Sp5wM4aRAhNERd4oY\">revolution.\"</span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefhy8b4kflu8\"><sup><a href=\"#fnhy8b4kflu8\"><span class=\"by_Sp5wM4aRAhNERd4oY\">[1]</span></a></sup></span><span><span class=\"by_Sp5wM4aRAhNERd4oY\">&nbsp;The concept refers</span><span class=\"by_gjoi5eBQob27Lww62\"> to</span><span class=\"by_Sp5wM4aRAhNERd4oY\"> the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms</span><span class=\"by_gjoi5eBQob27Lww62\"> </span></span><a href=\"https://www.lesswrong.com/tag/superintelligence\"><span class=\"by_gjoi5eBQob27Lww62\">Superintelligent AI</span></a><span><span class=\"by_gjoi5eBQob27Lww62\"> </span><span class=\"by_Sp5wM4aRAhNERd4oY\">and</span><span class=\"by_gjoi5eBQob27Lww62\"> </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_gjoi5eBQob27Lww62\">Artificial General Intelligence</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">).</span></p><p><span class=\"by_Sp5wM4aRAhNERd4oY\">Holden Karnofsky gives a more detailed definition in </span><a href=\"https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/\"><span class=\"by_Sp5wM4aRAhNERd4oY\">another OpenPhil 2016 post</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">:</span></p><blockquote><p><span><span class=\"by_Sp5wM4aRAhNERd4oY\">[...] Transformative AI is anything that fits one or more</span><span class=\"by_gjoi5eBQob27Lww62\"> of the </span><span class=\"by_Sp5wM4aRAhNERd4oY\">following descriptions (emphasis original):</span></span></p><ul><li><span><span class=\"by_gjoi5eBQob27Lww62\">AI</span><span class=\"by_Sp5wM4aRAhNERd4oY\"> systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems </span></span><i><span class=\"by_Sp5wM4aRAhNERd4oY\">could</span></i><span class=\"by_Sp5wM4aRAhNERd4oY\"> accomplish such a thing unaided by humans doesn’t mean they </span><i><span class=\"by_Sp5wM4aRAhNERd4oY\">would</span></i><span><span class=\"by_Sp5wM4aRAhNERd4oY\">; it’s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction</span><span class=\"by_gjoi5eBQob27Lww62\"> from the </span><span class=\"by_Sp5wM4aRAhNERd4oY\">types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.</span></span></li><li><span class=\"by_Sp5wM4aRAhNERd4oY\">AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.</span></li><li><span class=\"by_Sp5wM4aRAhNERd4oY\">Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)</span></li></ul></blockquote><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnhy8b4kflu8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefhy8b4kflu8\"><span class=\"by_Sp5wM4aRAhNERd4oY\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span><span class=\"by_Sp5wM4aRAhNERd4oY\">As defined</span><span class=\"by_gjoi5eBQob27Lww62\"> by </span></span><a href=\"https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Open Philanthropy's Holden Karnofsky in 2016</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, and reused by </span><a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf\"><span><span class=\"by_gjoi5eBQob27Lww62\">the </span><span class=\"by_Sp5wM4aRAhNERd4oY\">Center for the Governance of AI in 2018</span></span></a></p></div></li></ol>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "**Transformative AI** is \"\\[...\\] AI that precipitates a transition comparable to (or more significant than) the agricultural or industrial revolution.\"^[\\[1\\]](#fnhy8b4kflu8)^ The concept refers to the large effects of AI systems on our well-being, the global economy, state power, international security, etc. and not to specific capabilities that AI might have (unlike the related terms [Superintelligent AI](https://www.lesswrong.com/tag/superintelligence) and [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)).\n\nHolden Karnofsky gives a more detailed definition in [another OpenPhil 2016 post](https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/):\n\n> \\[...\\] Transformative AI is anything that fits one or more of the following descriptions (emphasis original):\n> \n> *   AI systems capable of fulfilling all the necessary functions of human scientists, unaided by humans, in developing another technology (or set of technologies) that ultimately becomes widely credited with being the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. Note that just because AI systems *could* accomplish such a thing unaided by humans doesn’t mean they *would*; it’s possible that human scientists would provide an important complement to such systems, and could make even faster progress working in tandem than such systems could achieve unaided. I emphasize the hypothetical possibility of AI systems conducting substantial unaided research to draw a clear distinction from the types of AI systems that exist today. I believe that AI systems capable of such broad contributions to the relevant research would likely dramatically accelerate it.\n> *   AI systems capable of performing tasks that currently (in 2016) account for the majority of full-time jobs worldwide, and/or over 50% of total world wages, unaided and for costs in the same range as what it would cost to employ humans. Aside from the fact that this would likely be sufficient for a major economic transformation relative to today, I also think that an AI with such broad abilities would likely be able to far surpass human abilities in a subset of domains, making it likely to meet one or more of the other criteria laid out here.\n> *   Surveillance, autonomous weapons, or other AI-centric technology that becomes sufficiently advanced to be the most significant driver of a transition comparable to (or more significant than) the agricultural or industrial revolution. (This contrasts with the first point because it refers to transformative technology that is itself AI-centric, whereas the first point refers to AI used to speed research on some other transformative technology.)\n\n1.  ^**[^](#fnrefhy8b4kflu8)**^\n    \n    As defined by [Open Philanthropy's Holden Karnofsky in 2016](https://www.openphilanthropy.org/research/potential-risks-from-advanced-artificial-intelligence-the-philanthropic-opportunity/), and reused by [the Center for the Governance of AI in 2018](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "syeEaqfH9buYEbanF",
    "name": "Derisking",
    "core": false,
    "slug": "derisking",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EFoa78oNx2jskHfYt",
    "name": "Lottery Ticket Hypothesis",
    "core": false,
    "slug": "lottery-ticket-hypothesis",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Lottery Ticket Hypothesis</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> claims that neural networks used in </span><a href=\"machine-learning\"><span class=\"by_HoGziwmhpMGqGeWZy\">machine learning</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> get most of their performance from sub-networks that are already present at initialization that approximate the final policy (\"winning tickets\"). The training process would, under this model, work by increasing weight on the lottery ticket sub-network and reducing weight on the rest of the network.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The hypothesis was proposed in </span><a href=\"https://arxiv.org/pdf/1803.03635.pdf\"><span class=\"by_HoGziwmhpMGqGeWZy\">a paper</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by Jonathan Frankle and Micheal Carbin of MIT CSAIL.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "The **Lottery Ticket Hypothesis** claims that neural networks used in [machine learning](machine-learning) get most of their performance from sub-networks that are already present at initialization that approximate the final policy (\"winning tickets\"). The training process would, under this model, work by increasing weight on the lottery ticket sub-network and reducing weight on the rest of the network.\n\nThe hypothesis was proposed in [a paper](https://arxiv.org/pdf/1803.03635.pdf) by Jonathan Frankle and Micheal Carbin of MIT CSAIL."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JX69nZB8tfxnx5nGH",
    "name": "Threat Models",
    "core": false,
    "slug": "threat-models",
    "tableOfContents": {
      "html": "<p><span class=\"by_qqwfzAYaLsfmkwbsK\">A threat model is a story of how a particular risk (e.g. AI) plays out.</span></p><span class=\"by_qqwfzAYaLsfmkwbsK\">\n</span><p><span class=\"by_qqwfzAYaLsfmkwbsK\">In the AI case, </span><a href=\"https://ssconlinemeetup.substack.com/p/video-from-rohins-talk\"><span class=\"by_qqwfzAYaLsfmkwbsK\">according to Rohin Shah</span></a><span class=\"by_qqwfzAYaLsfmkwbsK\">, a threat model is ideally:</span></p><span class=\"by_qqwfzAYaLsfmkwbsK\">\n</span><blockquote><span class=\"by_qqwfzAYaLsfmkwbsK\">\n</span><p><span class=\"by_qqwfzAYaLsfmkwbsK\">Combination of a development model that says how we get AGI and a risk model that says how AGI leads to existential catastrophe.</span></p><span class=\"by_qqwfzAYaLsfmkwbsK\">\n</span></blockquote><span class=\"by_qqwfzAYaLsfmkwbsK\">\n</span>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 22,
    "description": {
      "markdown": "A threat model is a story of how a particular risk (e.g. AI) plays out.\n\nIn the AI case, [according to Rohin Shah](https://ssconlinemeetup.substack.com/p/video-from-rohins-talk), a threat model is ideally: \n> Combination of a development model that says how we get AGI and a risk model that says how AGI leads to existential catastrophe.\n"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "urfLuDdvfqGeTfccP",
    "name": "Selection vs Control",
    "core": false,
    "slug": "selection-vs-control",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">\"Selection vs Control\" is an attempt to further clarify the notion of \"optimization process\" which has become common on LessWrong, by splitting it into several analogous-but-distinct concepts.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "\"Selection vs Control\" is an attempt to further clarify the notion of \"optimization process\" which has become common on LessWrong, by splitting it into several analogous-but-distinct concepts."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uxmGtpeE3KoE7pzSL",
    "name": "Chemistry",
    "core": false,
    "slug": "chemistry",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qf3kDBak4BQDDw3f2",
    "name": "Modest Epistemology",
    "core": false,
    "slug": "modest-epistemology",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Modest Epistemology</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is the claim that average opinions are more accurate that individual opinions, and individuals should take advantage of this by moving toward average opinions, even in cases where they have strong arguments for their own views and against more typical views. (Another name for this concept is \"the wisdom of crowds\" -- that name is much more popular outside of LessWrong.) In terms of </span><a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">inside view vs outside view</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, we can describe modest epistemology as the belief that inside views are quite fallible and outside views much more robust; therefore, we should weigh outside-view considerations much more heavily.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In LessWrong parlance, \"modesty\" and \"humility\" should not be confused. While Eliezer lists \"humility\" as a virtue, he provides many arguments </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">against</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> modesty (most extensively, in the book </span><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\"><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">Inadequate Equilibria</span></i></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">; but also in many earlier sources.) </span><a href=\"https://www.lesswrong.com/tag/humility-1\"><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Humility</span></strong></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is the general idea that you should expect to be fallible. Modest Epistemology is specifically the view that, due to your own fallibility, you should </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">rely heavily on </span></i><a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">outside-view</span></i></a><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> Modest epistemology says that you should trust average opinions more than your own opinion, even when you have strong arguments for your own views and against more typical views.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Historically, Robin Hanson has argued in favor of epistemic modesty and outside-view, while Eliezer has argued against epistemic modesty and for a strong inside views. For example, this disagreement played a role in </span><a href=\"https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">The Foom Debate</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">. Eliezer and Hanson both agree that </span><a href=\"https://www.lesswrong.com/tag/aumann-s-agreement-theorem\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Aumann's Agreement Theorem</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> implies that rational agents should converge to agreement; however, they have very different opinions about whether/how this breaks down in the absence of perfect rationality. Eliezer sees little reason to move one's opinion toward that of an irrational person's. Hanson thinks irrational agents still benefit from moving their opinions toward each other. One of Hanson's arguments involves </span><a href=\"https://www.lesswrong.com/tag/hansonian-pre-rationality\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">pre-priors</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></p><p><strong id=\"External_Posts_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Posts:</span></strong></p><p><a href=\"http://www.overcomingbias.com/2008/09/immodest-caplan.html\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Immodest Caplan</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Robin Hanson</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Inadequate Equilibria</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">modesty</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/humility-1\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Humility</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Inside/Outside View</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/egalitarianism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Egalitarianism</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/modesty-argument\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Modesty argument</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/disagreement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Disagreement</span></a></p>",
      "sections": [
        {
          "title": "External Posts:",
          "anchor": "External_Posts_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 15,
    "description": {
      "markdown": "**Modest Epistemology** is the claim that average opinions are more accurate that individual opinions, and individuals should take advantage of this by moving toward average opinions, even in cases where they have strong arguments for their own views and against more typical views. (Another name for this concept is \"the wisdom of crowds\" -- that name is much more popular outside of LessWrong.) In terms of [inside view vs outside view](https://www.lesswrong.com/tag/inside-outside-view), we can describe modest epistemology as the belief that inside views are quite fallible and outside views much more robust; therefore, we should weigh outside-view considerations much more heavily.\n\nIn LessWrong parlance, \"modesty\" and \"humility\" should not be confused. While Eliezer lists \"humility\" as a virtue, he provides many arguments *against* modesty (most extensively, in the book [*Inadequate Equilibria*](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d); but also in many earlier sources.) [**Humility**](https://www.lesswrong.com/tag/humility-1) is the general idea that you should expect to be fallible. Modest Epistemology is specifically the view that, due to your own fallibility, you should *rely heavily on* [*outside-view*](https://www.lesswrong.com/tag/inside-outside-view)*.* Modest epistemology says that you should trust average opinions more than your own opinion, even when you have strong arguments for your own views and against more typical views.\n\nHistorically, Robin Hanson has argued in favor of epistemic modesty and outside-view, while Eliezer has argued against epistemic modesty and for a strong inside views. For example, this disagreement played a role in [The Foom Debate](https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate). Eliezer and Hanson both agree that [Aumann's Agreement Theorem](https://www.lesswrong.com/tag/aumann-s-agreement-theorem) implies that rational agents should converge to agreement; however, they have very different opinions about whether/how this breaks down in the absence of perfect rationality. Eliezer sees little reason to move one's opinion toward that of an irrational person's. Hanson thinks irrational agents still benefit from moving their opinions toward each other. One of Hanson's arguments involves [pre-priors](https://www.lesswrong.com/tag/hansonian-pre-rationality).\n\n**External Posts:**\n\n[Immodest Caplan](http://www.overcomingbias.com/2008/09/immodest-caplan.html) by Robin Hanson\n\n**Related Sequences:** [Inadequate Equilibria](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d)\n\n**Related Pages:** [modesty](https://www.lesswrong.com/tag/modesty), [Humility](https://www.lesswrong.com/tag/humility-1), [Inside/Outside View](https://www.lesswrong.com/tag/inside-outside-view), [Egalitarianism](https://www.lesswrong.com/tag/egalitarianism), [Modesty argument](https://www.lesswrong.com/tag/modesty-argument), [Disagreement](https://www.lesswrong.com/tag/disagreement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zYPQsxiMcTiY4yE3S",
    "name": "Values handshakes",
    "core": false,
    "slug": "values-handshakes",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Values handshakes </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">are a proposed form of trade between superintelligences. From </span><a href=\"https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Hour I First Believed</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander:</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Suppose that humans make an AI which wants to convert the universe into paperclips. And suppose that aliens in the Andromeda Galaxy make an AI which wants to convert the universe into thumbtacks.</span></p></blockquote><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">When they meet in the middle, they might be tempted to fight for the fate of the galaxy. But this has many disadvantages. First, there’s the usual risk of losing and being wiped out completely. Second, there’s the usual deadweight loss of war, devoting resources to military buildup instead of paperclip production or whatever. Third, there’s the risk of a Pyrrhic victory that leaves you weakened and easy prey for some third party. Fourth, nobody knows what kind of scorched-earth strategy a losing superintelligence might be able to use to thwart its conqueror, but it could potentially be really bad – eg initiating vacuum collapse and destroying the universe. Also, since both parties would have superintelligent prediction abilities, they might both know who would win the war and how before actually fighting. This would make the fighting redundant and kind of stupid.</span></p></blockquote><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Although they would have the usual peace treaty options, like giving half the universe to each of them, superintelligences that trusted each other would have an additional, more attractive option. They could merge into a superintelligence that shared the values of both parent intelligences in proportion to their strength (or chance of military victory, or whatever). So if there’s a 60% chance our AI would win, and a 40% chance their AI would win, and both AIs know and agree on these odds, they might both rewrite their own programming with that of a previously-agreed-upon child superintelligence trying to convert the universe to paperclips and thumbtacks in a 60-40 mix.</span></p></blockquote><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">This has a lot of advantages over the half-the-universe-each treaty proposal. For one thing, if some resources were better for making paperclips, and others for making thumbtacks, both AIs could use all their resources maximally efficiently without having to trade. And if they were ever threatened by a third party, they would be able to present a completely unified front.</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "**Values handshakes** are a proposed form of trade between superintelligences. From [The Hour I First Believed](https://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) by Scott Alexander:\n\n> Suppose that humans make an AI which wants to convert the universe into paperclips. And suppose that aliens in the Andromeda Galaxy make an AI which wants to convert the universe into thumbtacks.\n\n> When they meet in the middle, they might be tempted to fight for the fate of the galaxy. But this has many disadvantages. First, there’s the usual risk of losing and being wiped out completely. Second, there’s the usual deadweight loss of war, devoting resources to military buildup instead of paperclip production or whatever. Third, there’s the risk of a Pyrrhic victory that leaves you weakened and easy prey for some third party. Fourth, nobody knows what kind of scorched-earth strategy a losing superintelligence might be able to use to thwart its conqueror, but it could potentially be really bad – eg initiating vacuum collapse and destroying the universe. Also, since both parties would have superintelligent prediction abilities, they might both know who would win the war and how before actually fighting. This would make the fighting redundant and kind of stupid.\n\n> Although they would have the usual peace treaty options, like giving half the universe to each of them, superintelligences that trusted each other would have an additional, more attractive option. They could merge into a superintelligence that shared the values of both parent intelligences in proportion to their strength (or chance of military victory, or whatever). So if there’s a 60% chance our AI would win, and a 40% chance their AI would win, and both AIs know and agree on these odds, they might both rewrite their own programming with that of a previously-agreed-upon child superintelligence trying to convert the universe to paperclips and thumbtacks in a 60-40 mix.\n\n> This has a lot of advantages over the half-the-universe-each treaty proposal. For one thing, if some resources were better for making paperclips, and others for making thumbtacks, both AIs could use all their resources maximally efficiently without having to trade. And if they were ever threatened by a third party, they would be able to present a completely unified front."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9DmA84e4ZvYoYu6q8",
    "name": "Motivational Intro Posts",
    "core": false,
    "slug": "motivational-intro-posts",
    "tableOfContents": {
      "html": "<p><span class=\"by_nLbwLhBaQeG6tCNDN\">Posts which (we theorize) are good to show to new users, to get them excited about rationality. Posts listed here should be high-quality classics, should be accessible without having previously read the Sequences or anything else on LessWrong, and should somehow convince a certain sort of reader that rationality is important, and they want to read more about it. A good motivational intro post might argue the value of rationality directly, or it might point out a reasoning flaw which people recognize strongly in themselves, or it might introduce a rationality concept which is particularly sticky.</span></p><p><span class=\"by_nLbwLhBaQeG6tCNDN\">This tag will be treated as a special case in recommendations, and is configured such that it isn't displayed on posts and doesn't appear in search results when adding tags from a post page (but you can still apply it from here). If you aren't sure whether a post meets the criteria, use the discussion page, or just add it (someone else can downvote the tag's relevance later).</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "Posts which (we theorize) are good to show to new users, to get them excited about rationality. Posts listed here should be high-quality classics, should be accessible without having previously read the Sequences or anything else on LessWrong, and should somehow convince a certain sort of reader that rationality is important, and they want to read more about it. A good motivational intro post might argue the value of rationality directly, or it might point out a reasoning flaw which people recognize strongly in themselves, or it might introduce a rationality concept which is particularly sticky.\n\nThis tag will be treated as a special case in recommendations, and is configured such that it isn't displayed on posts and doesn't appear in search results when adding tags from a post page (but you can still apply it from here). If you aren't sure whether a post meets the criteria, use the discussion page, or just add it (someone else can downvote the tag's relevance later)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "73btkq64uWfoWGfpF",
    "name": "Internal Family Systems",
    "core": false,
    "slug": "internal-family-systems",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_gjoi5eBQob27Lww62\">Internal Family </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Systems</span></span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_gjoi5eBQob27Lww62\"> or IFS, is a </span><span class=\"by_qxJ28GN72aiJu96iF\">family therapy-</span><span class=\"by_gjoi5eBQob27Lww62\">descended model of the psyche and psychotherapy method based on the idea that the mind is split between </span></span><i><span class=\"by_gjoi5eBQob27Lww62\">parts</span></i><span class=\"by_gjoi5eBQob27Lww62\"> called </span><i><span class=\"by_gjoi5eBQob27Lww62\">exiles</span></i><span class=\"by_gjoi5eBQob27Lww62\">, </span><i><span class=\"by_gjoi5eBQob27Lww62\">managers</span></i><span class=\"by_gjoi5eBQob27Lww62\">, and </span><i><span class=\"by_gjoi5eBQob27Lww62\">firefighters</span></i><span class=\"by_gjoi5eBQob27Lww62\">. Therapy consists of methods for allowing these parts to \"talk\" to each other so, for example, exiles can be reintegrated.</span></p><p><a href=\"https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model\"><span class=\"by_qxJ28GN72aiJu96iF\">Building up to an Internal Family Systems model</span></a><span><span class=\"by_gjoi5eBQob27Lww62\"> </span><span class=\"by_qxJ28GN72aiJu96iF\">offers</span><span class=\"by_gjoi5eBQob27Lww62\"> a more thorough introduction to the IFS model.</span></span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">For a related technique developed by CFAR, see </span><a href=\"https://www.lesswrong.com/tag/internal-double-crux\"><span class=\"by_qxJ28GN72aiJu96iF\">Internal Double Crux</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. Rather than thinking of the mind as an entity with one set of goals and beliefs, IFS includes many independently acting components, each of which might have varying goals and beliefs; see </span><a href=\"https://www.lesswrong.com/tag/subagents\"><span class=\"by_qxJ28GN72aiJu96iF\">subagents</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> for the more general form of this idea.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": {
      "markdown": "**Internal Family Systems**, or IFS, is a family therapy-descended model of the psyche and psychotherapy method based on the idea that the mind is split between *parts* called *exiles*, *managers*, and *firefighters*. Therapy consists of methods for allowing these parts to \"talk\" to each other so, for example, exiles can be reintegrated.\n\n[Building up to an Internal Family Systems model](https://www.lesswrong.com/posts/5gfqG3Xcopscta3st/building-up-to-an-internal-family-systems-model) offers a more thorough introduction to the IFS model.\n\nFor a related technique developed by CFAR, see [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux). Rather than thinking of the mind as an entity with one set of goals and beliefs, IFS includes many independently acting components, each of which might have varying goals and beliefs; see [subagents](https://www.lesswrong.com/tag/subagents) for the more general form of this idea."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XEo9oS2AQvS4spLco",
    "name": "Domain Theory",
    "core": false,
    "slug": "domain-theory",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vjKs7Pvz3MbgMc75C",
    "name": "Audio",
    "core": false,
    "slug": "audio",
    "tableOfContents": {
      "html": "<p><span class=\"by_BpBzKEueak7J8vHNi\">Posts that you can also listen to. &nbsp;Authors: please link your audio narration or talk at the top of your post.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Sequences:</span></strong><br><span class=\"by_sKAL2jzfkYkDbQmx9\">Ruby's </span><a href=\"https://www.lesswrong.com/s/k2fboiMkdfbCdgFzx\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wedding Ceremony</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/list-of-podcasts\"><span class=\"by_sKAL2jzfkYkDbQmx9\">List of Podcasts</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 42,
    "description": {
      "markdown": "Posts that you can also listen to.  Authors: please link your audio narration or talk at the top of your post.\n\n**Sequences:**  \nRuby's [Wedding Ceremony](https://www.lesswrong.com/s/k2fboiMkdfbCdgFzx)\n\n**Related Pages:** [List of Podcasts](https://www.lesswrong.com/tag/list-of-podcasts)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "adbooNSipZtMrXbzP",
    "name": "Data Science",
    "core": false,
    "slug": "data-science",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Aep2BhcHpPXvYCSJc",
    "name": "Ethical Offsets",
    "core": false,
    "slug": "ethical-offsets",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jKAkRrhnHedfowNYy",
    "name": "Islam",
    "core": false,
    "slug": "islam",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uG75MELqjCEfciaRp",
    "name": "Population Ethics",
    "core": false,
    "slug": "population-ethics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Population Ethics</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> addresses the question: how should utilitarians deal with </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">people coming in and out of existence?</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">&nbsp;</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">For example, a classical problem in population ethics: is it overall better for a happy person to exist, if they bring down the </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">average</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> happiness but increase the </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">total</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">? Those who would answer \"yes\" are termed </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">total (hedonic) utilitarians</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">; those who would answer \"no\" are termed </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">average (hedonic) utilitarians.</span></i></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Note that </span><a href=\"https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">utility is not comparable</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, so we cannot simply speak of \"increasing average utility\" vs \"increasing total utility\". Hence, </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">average utilitarianism</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> vs </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">total utilitarianism</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is not a well-defined distinction in the abstract; we have to specify e.g. </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">hedonic</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> utilitarianism before this distinction becomes well-defined (because average vs total happiness is a meaningful distinction).</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">However, although preference-utilitarians cannot necessarily make a meaningful distinction between averaging and totaling, they do not escape population-ethics dilemmas. It's still a fair question: when is it overall preferable to bring someone into existence (or for someone to pass out of existence)? What can/should ethics say about this?</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "**Population Ethics** addresses the question: how should utilitarians deal with *people coming in and out of existence?* \n\nFor example, a classical problem in population ethics: is it overall better for a happy person to exist, if they bring down the *average* happiness but increase the *total*? Those who would answer \"yes\" are termed *total (hedonic) utilitarians*; those who would answer \"no\" are termed *average (hedonic) utilitarians.*\n\nNote that [utility is not comparable](https://www.lesswrong.com/posts/cYsGrWEzjb324Zpjx/comparing-utilities), so we cannot simply speak of \"increasing average utility\" vs \"increasing total utility\". Hence, *average utilitarianism* vs *total utilitarianism* is not a well-defined distinction in the abstract; we have to specify e.g. *hedonic* utilitarianism before this distinction becomes well-defined (because average vs total happiness is a meaningful distinction).\n\nHowever, although preference-utilitarians cannot necessarily make a meaningful distinction between averaging and totaling, they do not escape population-ethics dilemmas. It's still a fair question: when is it overall preferable to bring someone into existence (or for someone to pass out of existence)? What can/should ethics say about this?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9vuoKSLDTE8kbKWEA",
    "name": "Deconfusion",
    "core": false,
    "slug": "deconfusion",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Narrowly, </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">deconfusion</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a specific branch of AI alignment research, discussed in </span><a href=\"https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">MIRI's 2018 research update</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">. More broadly, the term applies to any domain. Quoting from the research update:</span></p><blockquote><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">By deconfusion, I mean something like “making it so that you can think about a given topic without continuously accidentally spouting nonsense.”</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": {
      "markdown": "Narrowly, **deconfusion** is a specific branch of AI alignment research, discussed in [MIRI's 2018 research update](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/). More broadly, the term applies to any domain. Quoting from the research update:\n\n> By deconfusion, I mean something like “making it so that you can think about a given topic without continuously accidentally spouting nonsense.”"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vHzreEfefos3Kqt2p",
    "name": "Cognitive Reduction",
    "core": false,
    "slug": "cognitive-reduction",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">A </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">cognitive reduction</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a form of reductive analysis where, rather than reducing something to physical phenomena, we reduce something to the cognitive machinery which give rise to the idea.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">For example, Bayesian probability (ie subjective probability, or credence) can be seen as a cognitive reduction of randomness: rather than seeking physical causes of randomness in the world, we seek the impression of randomness in the mind. We then assert that randomness exists in the map, not the territory.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In other cases, we may still think the phenomenon exists in the territory, but nonetheless seek a cognitive reduction. For example, while we may think \"apples\" are a real thing that exists, we might be confused about borderline cases (such as a hypothetical fruit which has 90% apple genes and 10% pear genes). A cognitive reduction of \"apple\" helps us understand what it even means to assert one thing or another about borderline cases, while not </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">necessarily</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> giving up the claim that apples are real things which exist.</span></p><h2 id=\"See_also\"><span class=\"by_j8TwwtYJusmkqvGfh\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\"><span class=\"by_j8TwwtYJusmkqvGfh\">How An Algorithm Feels</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 12,
    "description": {
      "markdown": "A **cognitive reduction** is a form of reductive analysis where, rather than reducing something to physical phenomena, we reduce something to the cognitive machinery which give rise to the idea.\n\nFor example, Bayesian probability (ie subjective probability, or credence) can be seen as a cognitive reduction of randomness: rather than seeking physical causes of randomness in the world, we seek the impression of randomness in the mind. We then assert that randomness exists in the map, not the territory.\n\nIn other cases, we may still think the phenomenon exists in the territory, but nonetheless seek a cognitive reduction. For example, while we may think \"apples\" are a real thing that exists, we might be confused about borderline cases (such as a hypothetical fruit which has 90% apple genes and 10% pear genes). A cognitive reduction of \"apple\" helps us understand what it even means to assert one thing or another about borderline cases, while not *necessarily* giving up the claim that apples are real things which exist.\n\nSee also\n--------\n\n*   [How An Algorithm Feels](https://www.lesswrong.com/tag/how-an-algorithm-feels)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RMtdp6eGNjTZcmwJ6",
    "name": "Dissolving the Question",
    "core": false,
    "slug": "dissolving-the-question",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Dissolving the question</span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the act of </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">making a question no longer necessary: satisfying all associated curiosity, resolving all related confusions, but without answering the question.</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> The classic example is the question \"If a tree falls in a forest and no one hears it, does it make a sound?\". The </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">apparent paradox of the question is, in this case, resolved by pointing out the ambiguity of the term \"sound\". The </span><span class=\"by_sKAL2jzfkYkDbQmx9\">question can be dissolved by </span></span><a href=\"https://www.lesswrong.com/tag/distinctions\"><span class=\"by_sKAL2jzfkYkDbQmx9\">distinguishing</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> between \"Sound\" as referring to auditory experience and \"Sound\" as referring to vibrations in the air.</span><span class=\"by_Q7NW4XaWQmfPfdcFj\">&nbsp;</span></span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">“Many philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct: If you give them a question, they try to answer it.” - Eliezer Yudkowsky, </span><a href=\"https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Dissolving the Question</span></a></p></blockquote><p><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Sometimes </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">a question </span></span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">does</span></i><span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> have a strong</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> answer </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">as stated, but also needs </span><span class=\"by_sKAL2jzfkYkDbQmx9\">to </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">be dissolved.</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> This is</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> (</span></span><a href=\"https://www.lesswrong.com/posts/NEeW7eSXThPz7o4Ne/thou-art-physics\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">arguably</span></a><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">)</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> the case with </span></span><a href=\"https://www.lesswrong.com/tag/free-will\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Free Will</span></a><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">,</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> for </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">example:</span></span></p><ul><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">If we do have free will, there's still an additional question of why so many philosophers would conclude otherwise.</span></li><li><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">If</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> we don't have free </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">will, there's still the question of why so many philosophers think we do.</span></span></li></ul><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">This is (probably) not just a case of \"the other side is being silly\": there does indeed seem to be something weird about the question which deserves scrutiny.</span></p><p><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">In other words, answering the question</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> doesn't </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">fully address the confusion that the question represents!</span></span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">A </span><a href=\"https://www.lesswrong.com/tag/failure-mode\"><span class=\"by_sKAL2jzfkYkDbQmx9\">failure mode</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> in this step is giving justifications instead of explaining the process itself. Arguing that the reason we have an illusion of free will was that it was </span><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">evolutionarily adaptive</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> falls into this failure mode, as it doesn't explain the cognitive algorithm which produces the feeling of free will, and so doesn't dissolve the question. This can also be thought of as answering the \"Why\" instead of the \"How\", or as failing to provide a </span><a href=\"https://www.lesswrong.com/tag/gears-level\"><span class=\"by_sKAL2jzfkYkDbQmx9\">gears level model</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Dissolving a question usually (always?) involves providing a </span><a href=\"https://www.lesswrong.com/tag/cognitive-reduction\"><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">cognitive reduction</span></strong></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> of the question.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">See also: </span><a href=\"https://www.lesswrong.com/tag/deconfusion\"><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">deconfusion</span></strong></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><a href=\"https://www.lesswrong.com/tag/cognitive-reduction\"><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">cognitive reduction</span></strong></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Dissolving the question** is the act of making a question no longer necessary: satisfying all associated curiosity, resolving all related confusions, but without answering the question. The classic example is the question \"If a tree falls in a forest and no one hears it, does it make a sound?\". The apparent paradox of the question is, in this case, resolved by pointing out the ambiguity of the term \"sound\". The question can be dissolved by [distinguishing](https://www.lesswrong.com/tag/distinctions) between \"Sound\" as referring to auditory experience and \"Sound\" as referring to vibrations in the air. \n\n> “Many philosophers—particularly amateur philosophers, and ancient philosophers—share a dangerous instinct: If you give them a question, they try to answer it.” - Eliezer Yudkowsky, [Dissolving the Question](https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question)\n\nSometimes a question *does* have a strong answer as stated, but also needs to be dissolved. This is ([arguably](https://www.lesswrong.com/posts/NEeW7eSXThPz7o4Ne/thou-art-physics)) the case with [Free Will](https://www.lesswrong.com/tag/free-will), for example:\n\n*   If we do have free will, there's still an additional question of why so many philosophers would conclude otherwise.\n*   If we don't have free will, there's still the question of why so many philosophers think we do.\n\nThis is (probably) not just a case of \"the other side is being silly\": there does indeed seem to be something weird about the question which deserves scrutiny.\n\nIn other words, answering the question doesn't fully address the confusion that the question represents!\n\nA [failure mode](https://www.lesswrong.com/tag/failure-mode) in this step is giving justifications instead of explaining the process itself. Arguing that the reason we have an illusion of free will was that it was [evolutionarily adaptive](https://www.lesswrong.com/tag/evolutionary-psychology) falls into this failure mode, as it doesn't explain the cognitive algorithm which produces the feeling of free will, and so doesn't dissolve the question. This can also be thought of as answering the \"Why\" instead of the \"How\", or as failing to provide a [gears level model](https://www.lesswrong.com/tag/gears-level).\n\nDissolving a question usually (always?) involves providing a [**cognitive reduction**](https://www.lesswrong.com/tag/cognitive-reduction) of the question.\n\nSee also: [**deconfusion**](https://www.lesswrong.com/tag/deconfusion), [**cognitive reduction**](https://www.lesswrong.com/tag/cognitive-reduction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QZn3ujmgnvStbuLEc",
    "name": "Lifelogging",
    "core": false,
    "slug": "lifelogging",
    "tableOfContents": {
      "html": "<p><span class=\"by_Tw9etd8rMnHLeSQ9q\">Lifelogging is the practice of intentionally recording one's life, whether for </span><a href=\"https://www.lesswrong.com/posts/k8mwvvvpjMGcZLAKH/the-case-for-lifelogging-as-life-extension\"><span class=\"by_Tw9etd8rMnHLeSQ9q\">life extension purposes</span></a><span class=\"by_Tw9etd8rMnHLeSQ9q\"> or </span><a href=\"https://matiroy.com/writings/Should-I-record-my-life.html\"><span class=\"by_Tw9etd8rMnHLeSQ9q\">any other purposes</span></a></p><span class=\"by_Tw9etd8rMnHLeSQ9q\">\n</span>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "Lifelogging is the practice of intentionally recording one's life, whether for [life extension purposes](https://www.lesswrong.com/posts/k8mwvvvpjMGcZLAKH/the-case-for-lifelogging-as-life-extension) or [any other purposes](https://matiroy.com/writings/Should-I-record-my-life.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LY3xMQMKSXiYr7rgw",
    "name": "Quantilization",
    "core": false,
    "slug": "quantilization",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">A </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Quantilizer</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a proposed AI design which aims to reduce the harms from </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Goodhart's law</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> and specification gaming by selecting reasonably effective actions from a distribution of human-like actions, rather than maximizing over actions. It it more of a theoretical tool for exploring ways around these problems than a practical buildable design.</span></p><h3 id=\"See_also\"><span class=\"by_Sp5wM4aRAhNERd4oY\">See also</span></h3><ul><li><a href=\"https://www.youtube.com/watch?v=gdKMG6kTl6Y\"><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Rob Miles's Quantilizers: AI That Doesn't Try Too Hard</span></strong></a></li><li><a href=\"https://arbital.com/p/soft_optimizer?l=2r8#Quantilizing\"><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Arbital page on Quantilizers</span></strong></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 6,
    "description": {
      "markdown": "A **Quantilizer** is a proposed AI design which aims to reduce the harms from [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law) and specification gaming by selecting reasonably effective actions from a distribution of human-like actions, rather than maximizing over actions. It it more of a theoretical tool for exploring ways around these problems than a practical buildable design.\n\n### See also\n\n*   [**Rob Miles's Quantilizers: AI That Doesn't Try Too Hard**](https://www.youtube.com/watch?v=gdKMG6kTl6Y)\n*   [**Arbital page on Quantilizers**](https://arbital.com/p/soft_optimizer?l=2r8#Quantilizing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3JnSXmK9nYxzd5Riy",
    "name": "Asymmetric Weapons",
    "core": false,
    "slug": "asymmetric-weapons",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Asymmetric Weapons</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> are weapons that are inherently more powerful to one side of a conflict than another - Unlike a symmetric weapon, which can used as effectively by both sides. The term was originally introduced by Scott Alexander in his essay </span><a href=\"https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Guided By The Beauty Of Our Weapons</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, where he argued that truth, facts, and logic are asymmetric weapons for \"the good guys\":</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Logical debate has one advantage over narrative, rhetoric, and violence: it’s an </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">asymmetric weapon</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">. That is, it’s a weapon which is stronger in the hands of the good guys than in the hands of the bad guys. In ideal conditions (which may or may not ever happen in real life) – the kind of conditions where everyone is charitable and intelligent and wise – the good guys will be able to present stronger evidence, cite more experts, and invoke more compelling moral principles. The whole point of logic is that, when done right, it can only prove things that are true.</span></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">In the same essay he gave rhetoric and violence as examples of symmetric weapons.</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Violence is a </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">symmetric weapon</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">; the bad guys’ punches hit just as hard as the good guys’ do. [...] the same is true of rhetoric. Martin Luther King was able to make persuasive emotional appeals for good things. But Hitler was able to make persuasive emotional appeals for bad things. I’ve </span><a href=\"https://slatestarscratchpad.tumblr.com/post/103708539246/nostalgebraist-at-various-points-bostrom-like\"><span class=\"by_sKAL2jzfkYkDbQmx9\">previously argued</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> that Mohammed counts as the most successful persuader of all time. These three people pushed three very different ideologies, and rhetoric worked for them all.&nbsp;</span></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Scott uses this terminology mainly to separate strategies that work especially well for \"the good guys\" and strategies that work equally well for both sides. Davis Kingsley </span><a href=\"https://www.lesswrong.com/posts/Nd5KiuN8pPBrMT82Z/asymmetric-weapons-aren-t-always-on-your-side-1\"><span class=\"by_sKAL2jzfkYkDbQmx9\">warns</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> that strategies can also be asymmetric in favor of \"the bad guys\".</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">\"Unless you use asymmetric weapons, the best you can hope for is to win by coincidence.\" - Scott Alexander</span></p></blockquote><h2 id=\"External_Links\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links</span></h2><ul><li><a href=\"https://slatestarcodex.com/2019/06/06/asymmetric-weapons-gone-bad/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Asymmetric Weapons Gone Bad</span></a></li></ul><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/memetics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Memetics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/public-discourse\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Public Discourse</span></a></p>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 5,
    "description": {
      "markdown": "**Asymmetric Weapons** are weapons that are inherently more powerful to one side of a conflict than another - Unlike a symmetric weapon, which can used as effectively by both sides. The term was originally introduced by Scott Alexander in his essay [Guided By The Beauty Of Our Weapons](https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons), where he argued that truth, facts, and logic are asymmetric weapons for \"the good guys\":\n\n> Logical debate has one advantage over narrative, rhetoric, and violence: it’s an *asymmetric weapon*. That is, it’s a weapon which is stronger in the hands of the good guys than in the hands of the bad guys. In ideal conditions (which may or may not ever happen in real life) – the kind of conditions where everyone is charitable and intelligent and wise – the good guys will be able to present stronger evidence, cite more experts, and invoke more compelling moral principles. The whole point of logic is that, when done right, it can only prove things that are true.\n\nIn the same essay he gave rhetoric and violence as examples of symmetric weapons.\n\n> Violence is a *symmetric weapon*; the bad guys’ punches hit just as hard as the good guys’ do. \\[...\\] the same is true of rhetoric. Martin Luther King was able to make persuasive emotional appeals for good things. But Hitler was able to make persuasive emotional appeals for bad things. I’ve [previously argued](https://slatestarscratchpad.tumblr.com/post/103708539246/nostalgebraist-at-various-points-bostrom-like) that Mohammed counts as the most successful persuader of all time. These three people pushed three very different ideologies, and rhetoric worked for them all. \n\nScott uses this terminology mainly to separate strategies that work especially well for \"the good guys\" and strategies that work equally well for both sides. Davis Kingsley [warns](https://www.lesswrong.com/posts/Nd5KiuN8pPBrMT82Z/asymmetric-weapons-aren-t-always-on-your-side-1) that strategies can also be asymmetric in favor of \"the bad guys\".\n\n> \"Unless you use asymmetric weapons, the best you can hope for is to win by coincidence.\" - Scott Alexander\n\nExternal Links\n--------------\n\n*   [Asymmetric Weapons Gone Bad](https://slatestarcodex.com/2019/06/06/asymmetric-weapons-gone-bad/)\n\n**Related Pages:** [Memetics](https://www.lesswrong.com/tag/memetics), [Public Discourse](https://www.lesswrong.com/tag/public-discourse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "42JnNJ8fqfdCSqdmM",
    "name": "Marriage",
    "core": false,
    "slug": "marriage",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RGPpwYoCHrPNB86TW",
    "name": "Futarchy",
    "core": false,
    "slug": "futarchy",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Futarchy</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a proposed government system in which decisions are made based on betting markets. It was </span><a href=\"http://mason.gmu.edu/~rhanson/futarchy.html\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">originally proposed by Robin Hanson</span></a><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">, who gave the motto \"Vote </span><span class=\"by_W7ETRtvRMqYetyQE9\">on </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Values, But Bet</span><span class=\"by_W7ETRtvRMqYetyQE9\"> on</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> Beliefs\".</span></span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">A futarchic government holds an election to determine </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">what metrics to optimize; </span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">for example, a ballot might allow citizens to vote on the following options:</span></p><ul><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Gross Domestic Product (GDP) over the next 4 years, as estimated by an impartial council of economists.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">GDP over the next 8 years.</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">GDP over the next 30 years (etc).</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Happiness, as measured by a large survey. (Over the next 4 years, 8 years, etc.)</span></li><li><span class=\"by_Q7NW4XaWQmfPfdcFj\">Family values, as measured by (such-and-such procedure)</span></li></ul><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">And so on.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">A futarchy then sets up betting markets for the effectiveness of various policies. The policies with the best estimated effectiveness get implemented. The consequences are then observed, so bets about the implemented policies can pay out.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Prediction Markets</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/government\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Government</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/law-and-legal-systems\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Law and Legal systems</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/voting-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Voting Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/politics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Politics</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": {
      "markdown": "**Futarchy** is a proposed government system in which decisions are made based on betting markets. It was [originally proposed by Robin Hanson](http://mason.gmu.edu/~rhanson/futarchy.html), who gave the motto \"Vote on Values, But Bet on Beliefs\".\n\nA futarchic government holds an election to determine *what metrics to optimize;* for example, a ballot might allow citizens to vote on the following options:\n\n*   Gross Domestic Product (GDP) over the next 4 years, as estimated by an impartial council of economists.\n*   GDP over the next 8 years.\n*   GDP over the next 30 years (etc).\n*   Happiness, as measured by a large survey. (Over the next 4 years, 8 years, etc.)\n*   Family values, as measured by (such-and-such procedure)\n\nAnd so on.\n\nA futarchy then sets up betting markets for the effectiveness of various policies. The policies with the best estimated effectiveness get implemented. The consequences are then observed, so bets about the implemented policies can pay out.\n\n**Related Pages:** [Prediction Markets](https://www.lesswrong.com/tag/prediction-markets), [Government](https://www.lesswrong.com/tag/government), [Law and Legal systems](https://www.lesswrong.com/tag/law-and-legal-systems), [Voting Theory](https://www.lesswrong.com/tag/voting-theory), [Politics](https://www.lesswrong.com/tag/politics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Yd7JES6Wmbc2PBgid",
    "name": "Appeal to Consequence",
    "core": false,
    "slug": "appeal-to-consequence",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">An </span><i><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">appeal to consequences</span></strong></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is an argument against saying or believing something, which addresses external consequences of saying/believing, rather than the truth of the matter. This creates a conflict between epistemic rationality and instrumental rationality (or at least, raises the possibility of such a conflict). The argument asks you to compromise the soundness of your map, for the sake of the territory. If you give in, you may be accepting a falsehood. If you refuse, you may be shooting yourself in the foot.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "An ***appeal to consequences*** is an argument against saying or believing something, which addresses external consequences of saying/believing, rather than the truth of the matter. This creates a conflict between epistemic rationality and instrumental rationality (or at least, raises the possibility of such a conflict). The argument asks you to compromise the soundness of your map, for the sake of the territory. If you give in, you may be accepting a falsehood. If you refuse, you may be shooting yourself in the foot."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b7ZSAGimsbzrLR5CR",
    "name": "Family planning",
    "core": false,
    "slug": "family-planning",
    "tableOfContents": {
      "html": "<p><span class=\"by_j8TwwtYJusmkqvGfh\">Posts about deciding whether to have children, how many children to have, when to have children, etc. Also called parenthood decision-making.</span></p><p><span class=\"by_j8TwwtYJusmkqvGfh\">For how to raise children once one has decided to have them, see </span><a href=\"https://www.lesswrong.com/tag/parenting\"><span class=\"by_j8TwwtYJusmkqvGfh\">parenting</span></a><span class=\"by_j8TwwtYJusmkqvGfh\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong><br><a href=\"https://en.wikipedia.org/wiki/Antinatalism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Antinatalism</span></a><br><a href=\"https://en.wikipedia.org/wiki/Family_planning\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Family Planning</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "Posts about deciding whether to have children, how many children to have, when to have children, etc. Also called parenthood decision-making.\n\nFor how to raise children once one has decided to have them, see [parenting](https://www.lesswrong.com/tag/parenting).\n\n**External Links:**  \n[Antinatalism](https://en.wikipedia.org/wiki/Antinatalism)  \n[Family Planning](https://en.wikipedia.org/wiki/Family_planning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7ngr5dCMapYsuhst7",
    "name": "Charter Schools",
    "core": false,
    "slug": "charter-schools",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wGaBT3HSWWFqPfXB9",
    "name": "DIY",
    "core": false,
    "slug": "diy",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">DIY</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> means </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Do It Yourself</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">.&nbsp;</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/more-dakka\"><span class=\"by_sKAL2jzfkYkDbQmx9\">More Dakka</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "**DIY** means **Do It Yourself**. \n\n**Related Pages:** [More Dakka](https://www.lesswrong.com/tag/more-dakka)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fM6pmeSEncbzxoGpr",
    "name": "Functional Decision Theory",
    "core": false,
    "slug": "functional-decision-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Functional Decision Theory</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">decision theory</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><span class=\"by_2aoRX3ookcCozcb3m\">described</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Eliezer Yudkowsky and Nate Soares which says that agents should treat one’s decision as the output of a ﬁxed mathematical function that answers the question, “Which output of this very function would yield the best outcome?”. It is a replacement of </span></span><a href=\"https://www.lesswrong.com/tag/timeless-decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Timeless Decision Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, and it outperforms other decision theories such as </span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Causal Decision Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (CDT) and </span><a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Evidential Decision Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (EDT). For example, it does better than CDT on </span><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Newcomb's Problem</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, better than EDT on the </span><a href=\"https://www.lesswrong.com/tag/smoking-lesion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">smoking lesion problem</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, and better than both in </span><a href=\"https://www.lesswrong.com/tag/parfits-hitchhiker\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Parﬁt’s hitchhiker problem</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><p><span class=\"by_ibW3cznxxg96857o3\">In Newcomb's Problem, an FDT agent reasons that Omega must have used some kind of model of her decision procedure in order to make an accurate prediction of her behavior. Omega's model and the agent are therefore both calculating the same function (the agent's decision procedure): they are </span><i><span class=\"by_ibW3cznxxg96857o3\">subjunctively dependent </span></i><span class=\"by_ibW3cznxxg96857o3\">on that function. Given perfect prediction by Omega, there are therefore only two outcomes in Newcomb's Problem: either the agent one-boxes and Omega predicted it (because its model also one-boxed), or the agent two-boxes and Omega predicted </span><i><span class=\"by_ibW3cznxxg96857o3\">that</span></i><span class=\"by_ibW3cznxxg96857o3\">. Because one-boxing then results in a million and two-boxing only in a thousand dollars, the FDT agent one-boxes.</span></p><p><strong id=\"External_links_\"><span><span class=\"by_2aoRX3ookcCozcb3m\">External</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> links:</span></span></strong></p><ul><li><a href=\"https://intelligence.org/2017/10/22/fdt\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Functional decision theory: A new theory of instrumental rationality</span></a></li><li><a href=\"https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Cheating Death in Damascus</span></a></li><li><a href=\"https://intelligence.org/2017/04/07/decisions-are-for-making-bad-outcomes-inconsistent/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Decisions are for making bad outcomes inconsistent</span></a></li></ul><p><strong id=\"See_Also_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">See Also:</span></strong></p><ul><li><a href=\"https://www.lesswrong.com/tag/timeless-decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Timeless Decision Theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Updateless Decision Theory</span></a></li></ul>",
      "sections": [
        {
          "title": "External links:",
          "anchor": "External_links_",
          "level": 1
        },
        {
          "title": "See Also:",
          "anchor": "See_Also_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 19,
    "description": {
      "markdown": "**Functional Decision Theory** is a [decision theory](https://www.lesswrong.com/tag/decision-theory) described by Eliezer Yudkowsky and Nate Soares which says that agents should treat one’s decision as the output of a ﬁxed mathematical function that answers the question, “Which output of this very function would yield the best outcome?”. It is a replacement of [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory), and it outperforms other decision theories such as [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) (CDT) and [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) (EDT). For example, it does better than CDT on [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem), better than EDT on the [smoking lesion problem](https://www.lesswrong.com/tag/smoking-lesion), and better than both in [Parﬁt’s hitchhiker problem](https://www.lesswrong.com/tag/parfits-hitchhiker).\n\nIn Newcomb's Problem, an FDT agent reasons that Omega must have used some kind of model of her decision procedure in order to make an accurate prediction of her behavior. Omega's model and the agent are therefore both calculating the same function (the agent's decision procedure): they are *subjunctively dependent* on that function. Given perfect prediction by Omega, there are therefore only two outcomes in Newcomb's Problem: either the agent one-boxes and Omega predicted it (because its model also one-boxed), or the agent two-boxes and Omega predicted *that*. Because one-boxing then results in a million and two-boxing only in a thousand dollars, the FDT agent one-boxes.\n\n**External links:**\n\n*   [Functional decision theory: A new theory of instrumental rationality](https://intelligence.org/2017/10/22/fdt)\n*   [Cheating Death in Damascus](https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/)\n*   [Decisions are for making bad outcomes inconsistent](https://intelligence.org/2017/04/07/decisions-are-for-making-bad-outcomes-inconsistent/)\n\n**See Also:**\n\n*   [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory)\n*   [Updateless Decision Theory](https://www.lesswrong.com/tag/updateless-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vmdE9sHtHqPhNDWys",
    "name": "Stag Hunt",
    "core": false,
    "slug": "stag-hunt",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Stag Hunt</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> is a game-theoretic model of coordination in which each player chooses to either hunt Rabbit (small guaranteed return) or Stag (larger return if everyone else also chooses stag, no return otherwise).</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "**Stag Hunt** is a game-theoretic model of coordination in which each player chooses to either hunt Rabbit (small guaranteed return) or Stag (larger return if everyone else also chooses stag, no return otherwise)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EGqrmaw8Tpr3oQ2eX",
    "name": "Product Reviews",
    "core": false,
    "slug": "product-reviews",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EvPPocx6FHcoDfygQ",
    "name": "Consensus",
    "core": false,
    "slug": "consensus",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">A </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Consensus </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is a general or full agreement between members of a group. A consensus can be useful in deciding what's true (e.g, a scientific consensus), or as a criteria in decision making. A </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">False Consensus</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> can happen when someone thinks a position is in consensus when it isn't. one can also claim a consensus falsely to advance their position and make it difficult for others to oppose it. a </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">False Controversy</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> can happen when one mistakes something to not be in consensus when in fact it is. Claiming false controversies is a common way of creating uncertainty and doubt.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">There are many things that are considered a consensus on LessWrong, even though they're are not considered a consensus in the scientific community, such as: </span><a href=\"https://www.lesswrong.com/tag/one-boxing\"><span class=\"by_sKAL2jzfkYkDbQmx9\">One-Boxing</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, cooperating on the </span><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Prisoner's Dilemma</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, &nbsp;</span><a href=\"https://www.lesswrong.com/tag/bayesianism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bayesianism</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> over </span><a href=\"https://en.wikipedia.org/wiki/Frequentist_probability\"><span class=\"by_sKAL2jzfkYkDbQmx9\">frequentist</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> probability (and more to be added)</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Notable things that </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">aren't</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> in consensus on LessWrong include </span><a href=\"https://www.lesswrong.com/tag/blackmail-extortion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Blackmail / Extortion</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/is-rationality-any-good\"><span class=\"by_sKAL2jzfkYkDbQmx9\">the benefits of rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/ai-timelines\"><span class=\"by_sKAL2jzfkYkDbQmx9\">AI Timelines</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> and </span><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span class=\"by_sKAL2jzfkYkDbQmx9\">AI Takeoff</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, as well as </span><a href=\"Friendly Artificial Intelligence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">AI alignment strategies</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">,&nbsp;</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/common-knowledge\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Common Knowledge</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/disagreement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Disagreement</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Modesty</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/modesty-argument\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Modesty argument</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/aumann-agreement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Aumann agreement</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/government\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Government</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (in the context of democracies), </span><a href=\"https://www.lesswrong.com/tag/contrarianism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Contrarianism</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://en.wikipedia.org/wiki/Consensus_(disambiguation)\"><span class=\"by_sKAL2jzfkYkDbQmx9\">consensus on wikipedia</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "A **Consensus** is a general or full agreement between members of a group. A consensus can be useful in deciding what's true (e.g, a scientific consensus), or as a criteria in decision making. A **False Consensus** can happen when someone thinks a position is in consensus when it isn't. one can also claim a consensus falsely to advance their position and make it difficult for others to oppose it. a **False Controversy** can happen when one mistakes something to not be in consensus when in fact it is. Claiming false controversies is a common way of creating uncertainty and doubt.\n\nThere are many things that are considered a consensus on LessWrong, even though they're are not considered a consensus in the scientific community, such as: [One-Boxing](https://www.lesswrong.com/tag/one-boxing), cooperating on the [Prisoner's Dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma),  [Bayesianism](https://www.lesswrong.com/tag/bayesianism) over [frequentist](https://en.wikipedia.org/wiki/Frequentist_probability) probability (and more to be added)\n\nNotable things that *aren't* in consensus on LessWrong include [Blackmail / Extortion](https://www.lesswrong.com/tag/blackmail-extortion), [the benefits of rationality](https://www.lesswrong.com/tag/is-rationality-any-good), [AI Timelines](https://www.lesswrong.com/tag/ai-timelines) and [AI Takeoff](https://www.lesswrong.com/tag/ai-takeoff), as well as [AI alignment strategies](Friendly Artificial Intelligence), \n\n**Related Pages:** [Common Knowledge](https://www.lesswrong.com/tag/common-knowledge), [Disagreement](https://www.lesswrong.com/tag/disagreement), [Modesty](https://www.lesswrong.com/tag/modesty), [Modesty argument](https://www.lesswrong.com/tag/modesty-argument), [Aumann agreement](https://www.lesswrong.com/tag/aumann-agreement), [Government](https://www.lesswrong.com/tag/government) (in the context of democracies), [Contrarianism](https://www.lesswrong.com/tag/contrarianism)\n\n**See also:** [consensus on wikipedia](https://en.wikipedia.org/wiki/Consensus_(disambiguation))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Hc9fuB89BRPkeH6c5",
    "name": "Qualia Research Institute",
    "core": false,
    "slug": "qualia-research-institute",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vMeJdnoedH7iL8LiR",
    "name": "Time (value of)",
    "core": false,
    "slug": "time-value-of",
    "tableOfContents": {
      "html": "<p><span class=\"by_PdzQ73mN7S4SvRMhu\">What is the value of an hour of your time? How can you spend money to free up time? When is that the right call? Etc.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "What is the value of an hour of your time? How can you spend money to free up time? When is that the right call? Etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e6j9tnTfaabjCgK6d",
    "name": "Literature Reviews",
    "core": false,
    "slug": "literature-reviews",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">A </span><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">literature review</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\">, a </span><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">literature survey</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\">, a </span><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">state of the art</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\"> is an extended write-up collecting, and often comparing and commenting on a set of publications addressing a specific topic/question.</span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">Their main purpose is not to present original research, nor serve as teaching material, but to provide a gateway to the latest and/or most valuable research on a particular topic. This can serve to jumpstart a bibliography, as a first chapter of a dissertation, or to guide newcomers to a field where textbooks aren't available, or too broad or too dated.</span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">Note the three terms are not synonymous but gather well under the same tag.</span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">See also: </span><a href=\"https://www.lesswrong.com/tag/book-reviews\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Book Reviews</span></a><span class=\"by_QBvPFLFyZyuHcBwFm\">, </span><a href=\"https://www.lesswrong.com/tag/epistemic-review\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Epistemic Review</span></a><span class=\"by_QBvPFLFyZyuHcBwFm\">. </span><a href=\"https://www.lesswrong.com/tag/summaries\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Summaries</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "A **literature review**, a **literature survey**, a **state of the art** is an extended write-up collecting, and often comparing and commenting on a set of publications addressing a specific topic/question.\n\nTheir main purpose is not to present original research, nor serve as teaching material, but to provide a gateway to the latest and/or most valuable research on a particular topic. This can serve to jumpstart a bibliography, as a first chapter of a dissertation, or to guide newcomers to a field where textbooks aren't available, or too broad or too dated.\n\nNote the three terms are not synonymous but gather well under the same tag.\n\nSee also: [Book Reviews](https://www.lesswrong.com/tag/book-reviews), [Epistemic Review](https://www.lesswrong.com/tag/epistemic-review). [Summaries](https://www.lesswrong.com/tag/summaries)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EPTcHwSJxRHXAipT2",
    "name": "Assurance contracts",
    "core": false,
    "slug": "assurance-contracts",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">An </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Assurance Contract</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a contract of the form \"I commit to X if Y other people do the same\". for example, \"I commit to come to a protest if 100K other people make the same commitment\". if less than 100K sign this contract, it has no effect. if 100K or more sign it, it goes into effect and everyone who signed it is expected to come to the protest.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Assurance Contracts, at least in theory, are useful coordination tools for problems where collective action is needed and individual actions is for some reason risky or not worth it for the individual if they end up doing it alone or as part of a too small group.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">There's has been much discussion on LessWrong about creating software tools that let people coordinate with assurance contracts. there were already a few similar attempts outside of the community, but it is generally agreed on LW that these tools aren't good enough yet.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/commitment-mechanisms\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Commitment Mechanisms</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/pre-commitment\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Pre-Commitment</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/coordination-cooperation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Coordination / Cooperation</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "An **Assurance Contract** is a contract of the form \"I commit to X if Y other people do the same\". for example, \"I commit to come to a protest if 100K other people make the same commitment\". if less than 100K sign this contract, it has no effect. if 100K or more sign it, it goes into effect and everyone who signed it is expected to come to the protest.\n\nAssurance Contracts, at least in theory, are useful coordination tools for problems where collective action is needed and individual actions is for some reason risky or not worth it for the individual if they end up doing it alone or as part of a too small group.\n\nThere's has been much discussion on LessWrong about creating software tools that let people coordinate with assurance contracts. there were already a few similar attempts outside of the community, but it is generally agreed on LW that these tools aren't good enough yet.\n\n**Related pages:** [Commitment Mechanisms](https://www.lesswrong.com/tag/commitment-mechanisms), [Pre-Commitment](https://www.lesswrong.com/tag/pre-commitment), [Coordination / Cooperation](https://www.lesswrong.com/tag/coordination-cooperation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YyGDbZhGtws5hEPda",
    "name": "Self Fulfilling/Refuting Prophecies",
    "core": false,
    "slug": "self-fulfilling-refuting-prophecies",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">A </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Self Fulfilling Prophecy</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a prophecy that, when made, affects the environment such that it becomes more likely. similarly, a </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Self Refuting Prophecy</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;is a prophecy that when made makes itself less likely. This is also relevant for beliefs that can affect reality directly without being voiced, for example, the belief \"I'm confident\" can increase a person confidence, thus making it true, while the opposite belief can reduce a person's confidence, thus also making it true.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/social-reality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Social Reality</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "A **Self Fulfilling Prophecy** is a prophecy that, when made, affects the environment such that it becomes more likely. similarly, a **Self Refuting Prophecy** is a prophecy that when made makes itself less likely. This is also relevant for beliefs that can affect reality directly without being voiced, for example, the belief \"I'm confident\" can increase a person confidence, thus making it true, while the opposite belief can reduce a person's confidence, thus also making it true.\n\n**Related pages:** [Social Reality](https://www.lesswrong.com/tag/social-reality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jcegcg2mqPGWBiXHc",
    "name": "Conceptual Media",
    "core": false,
    "slug": "conceptual-media",
    "tableOfContents": {
      "html": "<p><span class=\"by_xNAA4cNuEM7N4Zz3M\">A conceptual medium is a an expressive medium that primarily serves to express abstract thoughts and ideas. It is analogous to the concept of an artistic medium (e.g., painting, poetry, violin, etc), but pertains to the expression of conceptual thought rather than artistic expression.</span></p><p><span class=\"by_xNAA4cNuEM7N4Zz3M\">Examples of such media would be spoken argument, written prose text, programming languages, mathematical symbolic notation, geometrical diagrams, &nbsp;explanatory comics, conceptual network diagrams (a.k.a., mindmaps), interactive simulations, data visualizations, markup languages, and semantic web technologies.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": {
      "markdown": "A conceptual medium is a an expressive medium that primarily serves to express abstract thoughts and ideas. It is analogous to the concept of an artistic medium (e.g., painting, poetry, violin, etc), but pertains to the expression of conceptual thought rather than artistic expression.\n\nExamples of such media would be spoken argument, written prose text, programming languages, mathematical symbolic notation, geometrical diagrams,  explanatory comics, conceptual network diagrams (a.k.a., mindmaps), interactive simulations, data visualizations, markup languages, and semantic web technologies."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nYta24PsKTvg57KZy",
    "name": "Satisficer",
    "core": false,
    "slug": "satisficer",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">A </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Satisficer</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> aims to reach a set level of </span><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_Sp5wM4aRAhNERd4oY\">utility</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> rather than maximizing utility. It is a proposed optimization process to the open </span><a href=\"https://arbital.greaterwrong.com/p/otherizer?l=2r9\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Other-izer problem</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">.</span></p><p><a href=\"https://www.youtube.com/watch?v=Ao4jwLwT36M&amp;list=UULB7AzTwc6VFZrBsO2ucBMg&amp;index=5\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Video explaining Satisficers vs Maximizers</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> (and why satisficers are still dangerous)</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "A **Satisficer** aims to reach a set level of [utility](https://www.lesswrong.com/tag/utility-functions) rather than maximizing utility. It is a proposed optimization process to the open [Other-izer problem](https://arbital.greaterwrong.com/p/otherizer?l=2r9).\n\n[Video explaining Satisficers vs Maximizers](https://www.youtube.com/watch?v=Ao4jwLwT36M&list=UULB7AzTwc6VFZrBsO2ucBMg&index=5) (and why satisficers are still dangerous)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2dz4FEXCdbk2BwDpc",
    "name": "LessWrong Review",
    "core": false,
    "slug": "lesswrong-review",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">LessWrong Review</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a yearly event where posts from a previous year are nominated, reviewed, and voted on.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 40,
    "description": {
      "markdown": "The **LessWrong Review** is a yearly event where posts from a previous year are nominated, reviewed, and voted on."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nGLvnaZCH5mx8h8Mh",
    "name": "Selectorate Theory",
    "core": false,
    "slug": "selectorate-theory",
    "tableOfContents": {
      "html": "<p><a href=\"https://en.wikipedia.org/wiki/Selectorate_theory\"><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Selectorate Theory</span></strong></a><strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is a game theoretic theory of politics and power that aims to explain political behavior and structure as a consequence of a leader's motivation to gain power and keep it as long as he can.</span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">The theory posits that no leader can rule alone, and thus always has to satisfy some amount of key people. in autocracies that number is small, and in democracies that number is large.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">The theory separates the rest of the population (apart from the leader) into three groups.&nbsp;</span></p><ul><li><i><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">interchangeables </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">- </span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">are those who can influence the selection of the president (e.g, anyone who has voting rights)</span></li><li><i><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">influentials </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">- </span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">are those who actually influence (e.g, those who actually end up voting)</span></li><li><i><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">essentials </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">- </span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">those who's support is essential for the selection of the leader (e.g, the minimum amount of voters needed to be elected and the electoral college)</span></li></ul><figure class=\"image image_resized\" style=\"width:34.54%;\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_90 90w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_180 180w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_270 270w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_360 360w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_450 450w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_540 540w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_630 630w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_720 720w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_810 810w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png/w_900 900w\"><figcaption><span class=\"by_sKAL2jzfkYkDbQmx9\">Euler diagram of the three groups</span></figcaption></figure><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Leaders satisfy the essentials by giving them rewards. the more essentials there are the more expensive it is to reward them privately, and the more it's worth to create public goods from which both they and everyone else benefit. which is how the theory explains the difference between the amount of public goods in democracies versus autocracies.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">The less essentials there are the easier it is to satisfy them and stay in control. thus a leader's incentive is to keep the amount of essentials as low as possible.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">the incentive of the influentials and interchangeables is to increase the amount of essentials there are, such that the leader would have to use public goods to satisfy them.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">The essentials incentive, when there are very few essentials, is to have even fewer, and thus align with the leader's. but past a certain point the essentials start to prefer having more essentials and more public goods rather than to have a small amount of essentials and gain private rewards - which then becomes contrary to the leaders goal, which stayed the same.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">In the dictator's handbook the authors describe 5 rules which every leader must follow in order to gain power and keep it:</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">(1) The smaller the winning coalition the fewer people to satisfy to remain in control. (2) Having a large nominal selectorate gives a pool of potential people to replace dissenters in coalition. (3) Maintain control of revenue flows to redistribute to your friends. (4) But only pay friends enough that they will not consider overthrowing you and at the same time little enough so that they depend on you. (5) Don't take your friends' money and redistribute it to the masses. </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">(Wikipedia)</span></i></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">The theory was developed by </span><a href=\"https://en.wikipedia.org/wiki/Bruce_Bueno_de_Mesquita\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bruce Bueno de Mesquita</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, Alastair Smith, Randolph M. Siverson, </span><a href=\"https://en.wikipedia.org/wiki/James_D._Morrow\"><span class=\"by_sKAL2jzfkYkDbQmx9\">James D. Morrow</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, and introduced in </span><a href=\"https://en.wikipedia.org/wiki/The_Logic_of_Political_Survival\"><i><span class=\"by_sKAL2jzfkYkDbQmx9\">The Logic of Political Survival</span></i></a><i><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">and </span><a href=\"https://en.wikipedia.org/wiki/The_Dictator%27s_Handbook\"><i><span class=\"by_sKAL2jzfkYkDbQmx9\">The Dictator's Handbook</span></i></a><i><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "[**Selectorate Theory**](https://en.wikipedia.org/wiki/Selectorate_theory)  is a game theoretic theory of politics and power that aims to explain political behavior and structure as a consequence of a leader's motivation to gain power and keep it as long as he can.  \n  \nThe theory posits that no leader can rule alone, and thus always has to satisfy some amount of key people. in autocracies that number is small, and in democracies that number is large.\n\nThe theory separates the rest of the population (apart from the leader) into three groups. \n\n*   ***interchangeables** \\-* are those who can influence the selection of the president (e.g, anyone who has voting rights)\n*   ***influentials** \\-* are those who actually influence (e.g, those who actually end up voting)\n*   ***essentials** \\-* those who's support is essential for the selection of the leader (e.g, the minimum amount of voters needed to be elected and the electoral college)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1f0cd4742b265d95eb72e1df90d7cf836a3327dd8cb8fd26.png)\n\nEuler diagram of the three groups\n\nLeaders satisfy the essentials by giving them rewards. the more essentials there are the more expensive it is to reward them privately, and the more it's worth to create public goods from which both they and everyone else benefit. which is how the theory explains the difference between the amount of public goods in democracies versus autocracies.\n\nThe less essentials there are the easier it is to satisfy them and stay in control. thus a leader's incentive is to keep the amount of essentials as low as possible.\n\nthe incentive of the influentials and interchangeables is to increase the amount of essentials there are, such that the leader would have to use public goods to satisfy them.\n\nThe essentials incentive, when there are very few essentials, is to have even fewer, and thus align with the leader's. but past a certain point the essentials start to prefer having more essentials and more public goods rather than to have a small amount of essentials and gain private rewards - which then becomes contrary to the leaders goal, which stayed the same.\n\nIn the dictator's handbook the authors describe 5 rules which every leader must follow in order to gain power and keep it:\n\n> (1) The smaller the winning coalition the fewer people to satisfy to remain in control. (2) Having a large nominal selectorate gives a pool of potential people to replace dissenters in coalition. (3) Maintain control of revenue flows to redistribute to your friends. (4) But only pay friends enough that they will not consider overthrowing you and at the same time little enough so that they depend on you. (5) Don't take your friends' money and redistribute it to the masses. *(Wikipedia)*\n\nThe theory was developed by [Bruce Bueno de Mesquita](https://en.wikipedia.org/wiki/Bruce_Bueno_de_Mesquita), Alastair Smith, Randolph M. Siverson, [James D. Morrow](https://en.wikipedia.org/wiki/James_D._Morrow), and introduced in [*The Logic of Political Survival*](https://en.wikipedia.org/wiki/The_Logic_of_Political_Survival)  and [*The Dictator's Handbook*](https://en.wikipedia.org/wiki/The_Dictator%27s_Handbook)*.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AHK82ypfxF45rqh9D",
    "name": "Distinctions",
    "core": false,
    "slug": "distinctions",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">A common </span><a href=\"https://www.lesswrong.com/tag/failiure-mode\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Failure mode</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> is failing to notice a </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Distinction</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> between two or more things. on the other hand, noticing distinctions can lead to insight, dissolving confusion, and better results.</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">“intelligence is the measure of the number and the quality of the distinctions you have in a given situation” - Tony Robbins</span></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Thus, making and noticing distinctions is a core skill.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span></strong><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Decoupling vs Contextualizing</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Conflict vs Mistake</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/compartmentalization\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Compartmentalization</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/bucket-errors\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bucket Errors</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/map-and-territory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Map and Territory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/fallacy-of-gray\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Fallacy of Gray</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/wanting-and-liking\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wanting and liking</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong><br><a href=\"https://en.wikipedia.org/wiki/Use%E2%80%93mention_distinction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Use–mention distinction</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 102,
    "description": {
      "markdown": "A common [Failure mode](https://www.lesswrong.com/tag/failiure-mode) is failing to notice a **Distinction** between two or more things. on the other hand, noticing distinctions can lead to insight, dissolving confusion, and better results.\n\n> “intelligence is the measure of the number and the quality of the distinctions you have in a given situation” - Tony Robbins\n\nThus, making and noticing distinctions is a core skill.\n\n**See also:** [Decoupling vs Contextualizing](https://www.lesswrong.com/tag/decoupling-vs-contextualizing), [Conflict vs Mistake](https://www.lesswrong.com/tag/conflict-vs-mistake), [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization), [Bucket Errors](https://www.lesswrong.com/tag/bucket-errors), [Map and Territory](https://www.lesswrong.com/tag/map-and-territory), [Fallacy of Gray](https://www.lesswrong.com/tag/fallacy-of-gray), [Wanting and liking](https://www.lesswrong.com/tag/wanting-and-liking)\n\n**External Links:**  \n[Use–mention distinction](https://en.wikipedia.org/wiki/Use%E2%80%93mention_distinction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5xyggFN9J64FskhqP",
    "name": "Cognitive Fusion",
    "core": false,
    "slug": "cognitive-fusion",
    "tableOfContents": {
      "html": "<p><span class=\"by_B5EreDfjALzEbSo6R\">[More thorough definition/description in </span><a href=\"https://www.lesswrong.com/posts/5g7oFiePGEY3h4bqX/prereq-cognitive-fusion\"><span class=\"by_B5EreDfjALzEbSo6R\">Prereq: Cognitive Fusion</span></a><span class=\"by_B5EreDfjALzEbSo6R\"> and </span><a href=\"https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and\"><span class=\"by_B5EreDfjALzEbSo6R\">Kaj Sotala's attempt to explain Looking, insight meditation, and enlightenment in non-mysterious terms</span></a><span class=\"by_B5EreDfjALzEbSo6R\">]&nbsp;</span><br><br><span class=\"by_B5EreDfjALzEbSo6R\">From the latter:</span></p><blockquote><p><span class=\"by_B5EreDfjALzEbSo6R\">Cognitive fusion is a term from </span><a href=\"https://en.wikipedia.org/wiki/Acceptance_and_commitment_therapy\"><span class=\"by_B5EreDfjALzEbSo6R\">Acceptance and Commitment Therapy</span></a><span class=\"by_B5EreDfjALzEbSo6R\"> (ACT), which refers to a person “fusing together” with the content of a thought or emotion, so that the content is experienced as an objective fact about the world rather than as a mental construct. The most obvious example of this might be if you get really upset with someone else and become convinced that something was </span><i><span class=\"by_B5EreDfjALzEbSo6R\">all their fault</span></i><span class=\"by_B5EreDfjALzEbSo6R\"> (even if you had actually done something blameworthy too).</span><br><span class=\"by_B5EreDfjALzEbSo6R\">In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.</span><br><br><span class=\"by_B5EreDfjALzEbSo6R\">In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.</span></p><p><span class=\"by_B5EreDfjALzEbSo6R\">Another emotional example might be feelings of shame, where it’s easy to experience yourself as a horrible person and feel that </span><i><span class=\"by_B5EreDfjALzEbSo6R\">this is the literal truth</span></i><span class=\"by_B5EreDfjALzEbSo6R\">, rather than being just an emotional interpretation.</span></p><p><span class=\"by_B5EreDfjALzEbSo6R\">Cognitive fusion isn’t </span><i><span class=\"by_B5EreDfjALzEbSo6R\">necessarily</span></i><span class=\"by_B5EreDfjALzEbSo6R\"> a bad thing. If you suddenly notice a car driving towards you at a high speed, you don’t want to get stuck pondering about how the feeling of danger is actually a mental construct produced by your brain. You </span><i><span class=\"by_B5EreDfjALzEbSo6R\">want</span></i><span class=\"by_B5EreDfjALzEbSo6R\"> to get out of the way as fast as possible, with minimal mental clutter interfering with your actions. Likewise, if you are doing programming or math, you </span><i><span class=\"by_B5EreDfjALzEbSo6R\">want</span></i><span class=\"by_B5EreDfjALzEbSo6R\"> to become at least partially fused together with your understanding of the domain, taking its axioms as objective facts so that you can focus on figuring out how to work with those axioms and get your desired results.</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "\\[More thorough definition/description in [Prereq: Cognitive Fusion](https://www.lesswrong.com/posts/5g7oFiePGEY3h4bqX/prereq-cognitive-fusion) and [Kaj Sotala's attempt to explain Looking, insight meditation, and enlightenment in non-mysterious terms](https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and)\\]   \n  \nFrom the latter:\n\n> Cognitive fusion is a term from [Acceptance and Commitment Therapy](https://en.wikipedia.org/wiki/Acceptance_and_commitment_therapy) (ACT), which refers to a person “fusing together” with the content of a thought or emotion, so that the content is experienced as an objective fact about the world rather than as a mental construct. The most obvious example of this might be if you get really upset with someone else and become convinced that something was *all their fault* (even if you had actually done something blameworthy too).  \n> In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.  \n>   \n> In this example, your anger isn’t letting you see clearly, and you can’t step back from your anger to question it, because you have become “fused together” with it and experience everything in terms of the anger’s internal logic.\n> \n> Another emotional example might be feelings of shame, where it’s easy to experience yourself as a horrible person and feel that *this is the literal truth*, rather than being just an emotional interpretation.\n> \n> Cognitive fusion isn’t *necessarily* a bad thing. If you suddenly notice a car driving towards you at a high speed, you don’t want to get stuck pondering about how the feeling of danger is actually a mental construct produced by your brain. You *want* to get out of the way as fast as possible, with minimal mental clutter interfering with your actions. Likewise, if you are doing programming or math, you *want* to become at least partially fused together with your understanding of the domain, taking its axioms as objective facts so that you can focus on figuring out how to work with those axioms and get your desired results."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FxNdAsrw8qvZkPaxN",
    "name": "QURI",
    "core": false,
    "slug": "quri",
    "tableOfContents": {
      "html": "<p><span class=\"by_efKySALtaLcvtp3jW\">The </span><strong><span class=\"by_efKySALtaLcvtp3jW\">Quantified Uncertainty Research Institute (QURI)</span></strong><span class=\"by_efKySALtaLcvtp3jW\"> is a nonprofit set up to study forecasting and epistemics. You can see their website </span><a href=\"https://quantifieduncertainty.org/\"><span class=\"by_efKySALtaLcvtp3jW\">here</span></a><span class=\"by_efKySALtaLcvtp3jW\">.&nbsp;</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Prediction Markets</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Forecasting &amp; Prediction</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "The **Quantified Uncertainty Research Institute (QURI)** is a nonprofit set up to study forecasting and epistemics. You can see their website [here](https://quantifieduncertainty.org/). \n\n**Related Pages:** [Prediction Markets](https://www.lesswrong.com/tag/prediction-markets), [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZWRtQgXucwzAFZqNJ",
    "name": "China",
    "core": false,
    "slug": "china",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">China</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a large country in eastern Asia, the most populous in the world and a significant power in the 21st century.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 24,
    "description": {
      "markdown": "**China** is a large country in eastern Asia, the most populous in the world and a significant power in the 21st century."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "F2XfCTxXLQBGjbm8P",
    "name": "Parables & Fables",
    "core": false,
    "slug": "parables-and-fables",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External links:</span></strong><br><a href=\"https://nickbostrom.com/fable/dragon.html\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Fable of the Dragon-Tyrant</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Nick Bostrom (</span><a href=\"https://www.youtube.com/watch?v=cZYNADOHhVY\"><span class=\"by_sKAL2jzfkYkDbQmx9\">animated version </span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">by CGP </span><span class=\"by_ezbRa3dntKWQ5995r\">Grey)</span></span><br><a href=\"https://slatestarcodex.com/2017/11/09/ars-longa-vita-brevis/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Ars Longa, Vita Brevis</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander</span><br><a href=\"https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/\"><span><span class=\"by_ezbRa3dntKWQ5995r\">Kolmogorov Complicity and </span><span class=\"by_sKAL2jzfkYkDbQmx9\">the </span><span class=\"by_ezbRa3dntKWQ5995r\">Parable</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> of </span><span class=\"by_ezbRa3dntKWQ5995r\">Lightning</span></span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander</span><br><a href=\"https://andersen.sdu.dk/vaerk/hersholt/TheEmperorsNewClothes_e.html\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Emperor's New Cloths</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related tags: </span></strong><a href=\"https://www.lesswrong.com/tag/fiction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Fiction</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/writing\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Writing</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/narratives-stories\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Narratives (stories)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 34,
    "description": {
      "markdown": "**External links:**  \n[The Fable of the Dragon-Tyrant](https://nickbostrom.com/fable/dragon.html) by Nick Bostrom ([animated version](https://www.youtube.com/watch?v=cZYNADOHhVY) by CGP Grey)  \n[Ars Longa, Vita Brevis](https://slatestarcodex.com/2017/11/09/ars-longa-vita-brevis/) by Scott Alexander  \n[Kolmogorov Complicity and the Parable of Lightning](https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/) by Scott Alexander  \n[The Emperor's New Cloths](https://andersen.sdu.dk/vaerk/hersholt/TheEmperorsNewClothes_e.html)\n\n**Related tags:** [Fiction](https://www.lesswrong.com/tag/fiction), [Writing](https://www.lesswrong.com/tag/writing), [Narratives (stories)](https://www.lesswrong.com/tag/narratives-stories)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "g97MMk83r5HofZLBe",
    "name": "The Pointers Problem",
    "core": false,
    "slug": "the-pointers-problem",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">The pointers problem</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> refers to the fact that most humans would rather have </span><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">an AI that acts based on real-world human values</span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">, not just human estimates of their own values – and that the two will be different in many situations, since humans are not all-seeing or all-</span><span class=\"by_nRknKQuPzoG2Wuyyi\">knowing. </span><span class=\"by_pbREHuM5F5t5nyWqh\">It was introduced in </span></span><a href=\"https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans\"><span class=\"by_pbREHuM5F5t5nyWqh\">a post with the same name</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "**The pointers problem** refers to the fact that most humans would rather have [an AI that acts based on real-world human values](https://www.lesswrong.com/tag/outer-alignment?showPostCount=true), not just human estimates of their own values – and that the two will be different in many situations, since humans are not all-seeing or all-knowing. It was introduced in [a post with the same name](https://www.lesswrong.com/posts/gQY6LrTWJNkTv8YJR/the-pointers-problem-human-values-are-a-function-of-humans)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aLB9evWFYtfyS3WJg",
    "name": "Music",
    "core": false,
    "slug": "music",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Music</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is a form of </span><a href=\"art\"><span class=\"by_pbREHuM5F5t5nyWqh\">art</span></a><span class=\"by_pbREHuM5F5t5nyWqh\"> based on placing notes in a rhythmic order. Most of LW's articles on music so far have been written by jefftk.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 52,
    "description": {
      "markdown": "**Music** is a form of [art](art) based on placing notes in a rhythmic order. Most of LW's articles on music so far have been written by jefftk."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rtha3oEP5vzkvdbbD",
    "name": "Games (posts describing)",
    "core": false,
    "slug": "games-posts-describing",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Games for and by lesswrongers.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/exercises-problem-sets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Exercises / Problem-Sets</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": {
      "markdown": "Games for and by lesswrongers.\n\n**Related Pages:** [Exercises / Problem-Sets](https://www.lesswrong.com/tag/exercises-problem-sets)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HPZzE9XBy99RmbmQe",
    "name": "Kelly Criterion",
    "core": false,
    "slug": "kelly-criterion",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">the </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Kelly criterion</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> (or </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Kelly strategy</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> or </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Kelly bet</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">), also known as the scientific gambling method, is a </span><a href=\"https://en.wikipedia.org/wiki/Formula\"><span class=\"by_sKAL2jzfkYkDbQmx9\">formula</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> for bet sizing that leads </span><a href=\"https://en.wikipedia.org/wiki/Almost_surely\"><span class=\"by_sKAL2jzfkYkDbQmx9\">almost surely</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> to higher wealth compared to any other strategy in the long run (i.e. approaching the limit as the number of bets goes to infinity). (</span><a href=\"https://en.wikipedia.org/wiki/Kelly_criterion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Posts from elsewhere:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.gwern.net/Coin-flip\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Kelly Coin-Flipping Game: Exact Solutions</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">see also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/betting\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Betting</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "the **Kelly criterion** (or **Kelly strategy** or **Kelly bet**), also known as the scientific gambling method, is a [formula](https://en.wikipedia.org/wiki/Formula) for bet sizing that leads [almost surely](https://en.wikipedia.org/wiki/Almost_surely) to higher wealth compared to any other strategy in the long run (i.e. approaching the limit as the number of bets goes to infinity). ([Wikipedia](https://en.wikipedia.org/wiki/Kelly_criterion))\n\n**Posts from elsewhere:** [The Kelly Coin-Flipping Game: Exact Solutions](https://www.gwern.net/Coin-flip)\n\n**see also:** [Betting](https://www.lesswrong.com/tag/betting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9ponmAskWgC37GZKk",
    "name": "Algorithms",
    "core": false,
    "slug": "algorithms",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">Posts that describe or demonstrate specific </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Algorithms</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> that can be used to make decisions.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Example: </span><a href=\"https://www.lesswrong.com/posts/xfcKYznQ6B9yuxB28/final-version-perfected-an-underused-execution-algorithm#comments\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Final Version Perfected: An Underused Execution Algorithm</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Decision Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Planning &amp; Decision-Making</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">,&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "Posts that describe or demonstrate specific **Algorithms** that can be used to make decisions.\n\nExample: [Final Version Perfected: An Underused Execution Algorithm](https://www.lesswrong.com/posts/xfcKYznQ6B9yuxB28/final-version-perfected-an-underused-execution-algorithm#comments)\n\n**See also:** [Decision Theory](https://www.lesswrong.com/tag/decision-theory), [Planning & Decision-Making](https://www.lesswrong.com/tag/planning-and-decision-making),"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "z5uy4NcWc2JSRTGHb",
    "name": "Incentives",
    "core": false,
    "slug": "incentives",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">An </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Incentive</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a motivating factor, such as monetary reward, the risk of legal sanctions, or social feedback. Many systems are best understood by looking at the incentives of the people with power over them.</span></p><p><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Inadequate Equilibria</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> covers many problems that arise when there are poor incentives.</span></p><p><strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Related </span><span class=\"by_qf77EiaoMw7tH3GSr\">Pages:</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></span></strong><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Game Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/mechanism-design\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mechanism Design</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/moloch\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Moloch</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/moral-mazes\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Moral Mazes</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": {
      "markdown": "An **Incentive** is a motivating factor, such as monetary reward, the risk of legal sanctions, or social feedback. Many systems are best understood by looking at the incentives of the people with power over them.\n\n[Inadequate Equilibria](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d) covers many problems that arise when there are poor incentives.\n\n**Related Pages:** [Game Theory](https://www.lesswrong.com/tag/game-theory), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), [Moloch](https://www.lesswrong.com/tag/moloch), [Moral Mazes](https://www.lesswrong.com/tag/moral-mazes)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p8W2vFMHydvfgBgPh",
    "name": "SETI",
    "core": false,
    "slug": "seti",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mwKRsZuFbGr7tA5h7",
    "name": "Delegation",
    "core": false,
    "slug": "delegation",
    "tableOfContents": {
      "html": "<p><span class=\"by_BpBzKEueak7J8vHNi\">When some humans want something done, those humans can delegate responsibility for the task to one or more AI systems. From the perspective of the AI systems, the relationship would be one of assistance directed toward the humans. However, to avoid dependence of our arguments upon viewing AI systems as having a “perspective”, we treat humans as the primary seat of agency, and view the humans as engaged in delegation.</span><br><span class=\"by_BpBzKEueak7J8vHNi\">– excerpt from AI Research Considerations for Human Existential Safety report</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "When some humans want something done, those humans can delegate responsibility for the task to one or more AI systems. From the perspective of the AI systems, the relationship would be one of assistance directed toward the humans. However, to avoid dependence of our arguments upon viewing AI systems as having a “perspective”, we treat humans as the primary seat of agency, and view the humans as engaged in delegation.  \n– excerpt from AI Research Considerations for Human Existential Safety report"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zhLB7AT83YC3HCtwk",
    "name": "Astrobiology",
    "core": false,
    "slug": "astrobiology",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "k6igEkzKYY2EpY7Su",
    "name": "Meta-Philosophy",
    "core": false,
    "slug": "meta-philosophy",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Meta-philosophy</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> or </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">metaphilosophy</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> (with no hyphen) is the philosophy of philosophy itself. According to </span><a href=\"https://en.wikipedia.org/wiki/Metaphilosophy\"><span class=\"by_pbREHuM5F5t5nyWqh\">the wikipedia article</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">, \"its subject matter includes the aims of </span><a href=\"/tag/philosophy\"><span class=\"by_pbREHuM5F5t5nyWqh\">philosophy</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">, the boundaries of philosophy, and its methods\".</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "**Meta-philosophy** or **metaphilosophy** (with no hyphen) is the philosophy of philosophy itself. According to [the wikipedia article](https://en.wikipedia.org/wiki/Metaphilosophy), \"its subject matter includes the aims of [philosophy](/tag/philosophy), the boundaries of philosophy, and its methods\"."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5Gi4NzgKtzWja8GHh",
    "name": "Automation",
    "core": false,
    "slug": "automation",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Automation</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is \"the use of largely automatic equipment in a system of manufacturing or other production process\". [1] This includes, but is not limited to, most of the processes involved in car manufacturing, and many ways that computer programs are applied.</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">[1]: New Oxford American Dictionary</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Automation** is \"the use of largely automatic equipment in a system of manufacturing or other production process\". \\[1\\] This includes, but is not limited to, most of the processes involved in car manufacturing, and many ways that computer programs are applied.\n\n\\[1\\]: New Oxford American Dictionary"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kxffynF3MAK3TacL7",
    "name": "Measure Theory",
    "core": false,
    "slug": "measure-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Measure Theory</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is a </span><a href=\"/tag/mathematics\"><span class=\"by_pbREHuM5F5t5nyWqh\">mathematical</span></a><span class=\"by_pbREHuM5F5t5nyWqh\"> field focused on </span><a href=\"https://en.wikipedia.org/wiki/Measure_(mathematics)\"><span class=\"by_pbREHuM5F5t5nyWqh\">measures</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">, functions that aim to provide an intuitive interpretation of a subset's size.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": {
      "markdown": "**Measure Theory** is a [mathematical](/tag/mathematics) field focused on [measures](https://en.wikipedia.org/wiki/Measure_(mathematics)), functions that aim to provide an intuitive interpretation of a subset's size."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z5A4c4kjTgLSFEr3h",
    "name": "Autonomous Vehicles",
    "core": false,
    "slug": "autonomous-vehicles",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Autonomous vehicles</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\">, also known as </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">self-driving cars</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\">/trucks/&amp;c., are vehicles that can make steering, pedaling, or braking decisions to some extent. They are notable for being a present-day case where </span><a href=\"/tag/outer-alignment\"><span class=\"by_pbREHuM5F5t5nyWqh\">alignment</span></a><span class=\"by_pbREHuM5F5t5nyWqh\"> and </span><a href=\"ai-safety\"><span class=\"by_pbREHuM5F5t5nyWqh\">AI safety</span></a><span class=\"by_pbREHuM5F5t5nyWqh\"> are important - for example, AI should not crash a car in order to gather data about car crashes.</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">In addition, autonomous vehicles present important ethical questions - for example, how much should a self-driving car focus on the safety of passengers, as opposed to people outside of the car? If autonomous vehicles become significantly less accident-prone than (and approximately as cheap as) human-driven ones, should government policies require people to use them for public safety?</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">Note: posts specifically about autonomous vehicles should be given higher relevance than ones that simply use them as examples.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": {
      "markdown": "**Autonomous vehicles**, also known as **self-driving cars**/trucks/&c., are vehicles that can make steering, pedaling, or braking decisions to some extent. They are notable for being a present-day case where [alignment](/tag/outer-alignment) and [AI safety](ai-safety) are important - for example, AI should not crash a car in order to gather data about car crashes.\n\nIn addition, autonomous vehicles present important ethical questions - for example, how much should a self-driving car focus on the safety of passengers, as opposed to people outside of the car? If autonomous vehicles become significantly less accident-prone than (and approximately as cheap as) human-driven ones, should government policies require people to use them for public safety?\n\nNote: posts specifically about autonomous vehicles should be given higher relevance than ones that simply use them as examples."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jTScW3bvF3j6WoiFe",
    "name": "Radical Probabilism",
    "core": false,
    "slug": "radical-probabilism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Radical Probabilism</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a newer form of </span><a href=\"https://www.lesswrong.com/tag/bayesianism\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Bayesianism</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> invented by </span><a href=\"https://en.wikipedia.org/wiki/Richard_Jeffrey\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Richard </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Jeffrey</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> The primary point of departure from other forms of Bayesianism is its rejection of the strong connection between conditional probability and updates. Radical Probabilism therefore rejects the strong connection between </span></span><a href=\"https://www.lesswrong.com/tag/bayes-theorem\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Bayes' Rule</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and updating.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Other notable writers on Radical Probabilism include Jeffrey's student </span><a href=\"https://en.wikipedia.org/wiki/Richard_Bradley_(philosopher)\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Richard </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Bradley</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and the champion of naturalized philosophy, </span></span><a href=\"https://en.wikipedia.org/wiki/Brian_Skyrms\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Brian </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Skyrms</span></span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Radical Probabilism** is a newer form of [Bayesianism](https://www.lesswrong.com/tag/bayesianism) invented by [Richard Jeffrey](https://en.wikipedia.org/wiki/Richard_Jeffrey). The primary point of departure from other forms of Bayesianism is its rejection of the strong connection between conditional probability and updates. Radical Probabilism therefore rejects the strong connection between [Bayes' Rule](https://www.lesswrong.com/tag/bayes-theorem) and updating.\n\nOther notable writers on Radical Probabilism include Jeffrey's student [Richard Bradley](https://en.wikipedia.org/wiki/Richard_Bradley_(philosopher)), and the champion of naturalized philosophy, [Brian Skyrms](https://en.wikipedia.org/wiki/Brian_Skyrms)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "htgXy4gow6tHWu2bA",
    "name": "Problem of Old Evidence",
    "core": false,
    "slug": "problem-of-old-evidence",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Suppose a new scientific hypothesis, such as general relativity, explains a well-know observation such as the </span><a href=\"https://en.m.wikipedia.org/wiki/Tests_of_general_relativity\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">perihelion precession of mercury</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> better than any existing theory. Intuitively, this is a point in favor of the new theory. However, the probability for the well-known observation was already at 100%. How can a previously-known statement provide new support for the hypothesis, as if we are re-updating on evidence we've already updated on long ago? This is known as </span><a href=\"https://plato.stanford.edu/entries/epistemology-bayesian/#ObjSimPriConRulInfOthObjBayConThe\"><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">the problem of old evidence</span></strong></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, and is usually levelled as a charge against Bayesian epistemology.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">[Needs to be expanded!]</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "Suppose a new scientific hypothesis, such as general relativity, explains a well-know observation such as the [perihelion precession of mercury](https://en.m.wikipedia.org/wiki/Tests_of_general_relativity) better than any existing theory. Intuitively, this is a point in favor of the new theory. However, the probability for the well-known observation was already at 100%. How can a previously-known statement provide new support for the hypothesis, as if we are re-updating on evidence we've already updated on long ago? This is known as [**the problem of old evidence**](https://plato.stanford.edu/entries/epistemology-bayesian/#ObjSimPriConRulInfOthObjBayConThe), and is usually levelled as a charge against Bayesian epistemology.\n\n\\[Needs to be expanded!\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ksWkwyKRrj582WqpN",
    "name": "ET Jaynes",
    "core": false,
    "slug": "et-jaynes",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">Edwin Thompson</span><span class=\"by_vXYQtQKKaHoH4p5Kg\"> Jaynes </span></span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">was a prominent bayesian mathematician and</span><span class=\"by_vXYQtQKKaHoH4p5Kg\"> the author of </span></span><i><span><span class=\"by_vXYQtQKKaHoH4p5Kg\">Probability Theory: The Logic of </span><span class=\"by_HoGziwmhpMGqGeWZy\">Science</span></span></i><span><span class=\"by_HoGziwmhpMGqGeWZy\">.</span><span class=\"by_vXYQtQKKaHoH4p5Kg\">&nbsp;</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Edwin Thompson Jaynes** was a prominent bayesian mathematician and the author of *Probability Theory: The Logic of Science*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tPfh8oYyG8Wgkkg8X",
    "name": "Journalism",
    "core": false,
    "slug": "journalism",
    "tableOfContents": {
      "html": "<p><span class=\"by_pbREHuM5F5t5nyWqh\">This tag applies to posts focused on news, except at an object level (i.e. newspaper companies and the process behind the news are relevant, but current events aren't necessarily).</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "This tag applies to posts focused on news, except at an object level (i.e. newspaper companies and the process behind the news are relevant, but current events aren't necessarily)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CL9NePP9FejkQo6jn",
    "name": "Future of Life Institute (FLI)",
    "core": false,
    "slug": "future-of-life-institute-fli",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Future of Life Institute</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">, or </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">FLI,</span></strong><i><strong><span class=\"by_HoGziwmhpMGqGeWZy\"> </span></strong></i><span class=\"by_HoGziwmhpMGqGeWZy\">is a nonprofit organization whose mission is to mitigate </span><a href=\"existential-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">existential risks</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">. Its most prominent activities are issuing grants to x-risk researchers and organizing conferences on </span><a href=\"ai\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> and existential risk.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Website: </span><a href=\"https://futureoflife.org\"><span class=\"by_HoGziwmhpMGqGeWZy\">futureoflife.org</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 14,
    "description": {
      "markdown": "The **Future of Life Institute**, or **FLI,**  is a nonprofit organization whose mission is to mitigate [existential risks](existential-risk). Its most prominent activities are issuing grants to x-risk researchers and organizing conferences on [AI](ai) and existential risk.\n\nWebsite: [futureoflife.org](https://futureoflife.org)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RxuepsZgBEKax9bmP",
    "name": "Anchoring",
    "core": false,
    "slug": "anchoring",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Anchoring</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> is the use of a reference value when estimating a quantitative value, and the bias that results if the reference point used isn't suitable for that purpose.</span></p><p><span class=\"by_nLbwLhBaQeG6tCNDN\">Anchoring is a special case of </span><a href=\"https://www.lesswrong.com/tag/priming\"><span class=\"by_nLbwLhBaQeG6tCNDN\">priming</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Anchoring** is the use of a reference value when estimating a quantitative value, and the bias that results if the reference point used isn't suitable for that purpose.\n\nAnchoring is a special case of [priming](https://www.lesswrong.com/tag/priming)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KADf7NFiAr9DAaQxN",
    "name": "Social Media",
    "core": false,
    "slug": "social-media",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Social media</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> are digital tools that let users share content with many others, quickly and easily. Prominent examples include Facebook, Twitter, TikTok, and many others.</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">Social media is infamous for its tendency to polarize users, make them spend more time and attention than they intended, and otherwise hurt productivity.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 39,
    "description": {
      "markdown": "**Social media** are digital tools that let users share content with many others, quickly and easily. Prominent examples include Facebook, Twitter, TikTok, and many others.\n\nSocial media is infamous for its tendency to polarize users, make them spend more time and attention than they intended, and otherwise hurt productivity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PCLuivxECxsC3aNgy",
    "name": "Lighting",
    "core": false,
    "slug": "lighting",
    "tableOfContents": {
      "html": "<p><span class=\"by_3AAZ8hRwoJvfkozKD\">Posts about (artificial / indoor) lighting, especially relating to attempts to improve quality of life during winter months and at other times when insufficient natural light is available.</span></p><span class=\"by_3AAZ8hRwoJvfkozKD\">\n</span>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "Posts about (artificial / indoor) lighting, especially relating to attempts to improve quality of life during winter months and at other times when insufficient natural light is available."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "49GPZJoXc7d2gKTDG",
    "name": "Tradeoffs",
    "core": false,
    "slug": "tradeoffs",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Tradeoffs</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> are decisions with both benefits and pitfalls. In practice, most decisions are tradeoffs, because they come at the expense of time that could be spent on other activities.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "**Tradeoffs** are decisions with both benefits and pitfalls. In practice, most decisions are tradeoffs, because they come at the expense of time that could be spent on other activities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cA6e2WpMYZEvfsopQ",
    "name": "GAN",
    "core": false,
    "slug": "gan",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_hTWkP6nvkEdKMdKj3\">Generative Adversarial Network</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a </span><a href=\"https://www.lesswrong.com/tag/machine-learning\"><span class=\"by_HoGziwmhpMGqGeWZy\">machine learning</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> architecture containing two modules. The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">generator</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> attempts to create an output that is similar to the network's training data, while the </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">discriminator</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> attempts to tell the generator's outputs apart from the training data. The generator is reinforced based on how well its outputs fool the discriminator, so the two modules are adversaries.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">GANs are best known for working well with images; for example, generating pictures of human faces.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Generative Adversarial Network** is a [machine learning](https://www.lesswrong.com/tag/machine-learning) architecture containing two modules. The **generator** attempts to create an output that is similar to the network's training data, while the **discriminator** attempts to tell the generator's outputs apart from the training data. The generator is reinforced based on how well its outputs fool the discriminator, so the two modules are adversaries.\n\nGANs are best known for working well with images; for example, generating pictures of human faces."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ux2x9RrJsuykQxT79",
    "name": "Deliberate Practice",
    "core": false,
    "slug": "deliberate-practice",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Deliberate practice</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is the highest form of practice according to Anders Ericsson and Robert Pool (the authors of the book </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">Peak</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">). Based on the scientific study of expertise, they classify several forms of practice. From less to more effective:</span></p><ol><li><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Naive practice. </span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">You practice music by playing it. You practice medicine by doing it. You practice driving by doing it. Ericsson and Pool cite studies showing that this is not very effective at increasing, or even maintaining, skill. Doctors are (on average) at their best shortly after getting out of medical school, and gradually decline in skill thereafter despite their continuing \"practice\" of medicine. Similar observations apply to other skills.</span></li><li><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Purposeful practice. </span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">This is practice which (1) has well-defined goals (such as doing something 3 times in a row with no mistakes), (2) is focused (the person is intently interested in improving, rather than having their attention elsewhere), (3) involves feedback, (4) involves getting out of one's comfort zone, practicing things on the edge of one's ability.</span></li><li><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Deliberate practice. </span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">On top of the requirements for purposeful practice, deliberate practice is </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">informed by an understanding of how to do well. </span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">In the best case, this understanding is conferred by a professional teacher (a \"coach\"). This teacher will be able to evaluate where a student sits with respect to the various necessary sub-skills, recommend specific practice tasks to improve sub-skills which are lacking, and give advice about how to improve technique.</span></li></ol><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The authors note that purposeful practice can result in getting stuck if you learn </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">bad form</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> (they don't use that term, but what they describe is very close to the concept of \"good form\" from the </span><a href=\"https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">CFAR handbook</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> (page 19)). Bad form means that practice is ultimately instilling bad habits, even if it creates local improvement. Good form means that practice is taking you down the path to mastery in an efficient manner.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Due to the necessity of having an experienced teacher, deliberate practice requires a highly developed field. However, it is also common to use the term \"deliberate practice\" for anything which is distinguished from purposeful practice by the presence of a </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">theory of skill</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">practice guided by that theory</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\">, whether or not that theory is tried-and-true, and whether or not an experienced teacher is involved.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": {
      "markdown": "**Deliberate practice** is the highest form of practice according to Anders Ericsson and Robert Pool (the authors of the book *Peak*). Based on the scientific study of expertise, they classify several forms of practice. From less to more effective:\n\n1.  **Naive practice.** You practice music by playing it. You practice medicine by doing it. You practice driving by doing it. Ericsson and Pool cite studies showing that this is not very effective at increasing, or even maintaining, skill. Doctors are (on average) at their best shortly after getting out of medical school, and gradually decline in skill thereafter despite their continuing \"practice\" of medicine. Similar observations apply to other skills.\n2.  **Purposeful practice.** This is practice which (1) has well-defined goals (such as doing something 3 times in a row with no mistakes), (2) is focused (the person is intently interested in improving, rather than having their attention elsewhere), (3) involves feedback, (4) involves getting out of one's comfort zone, practicing things on the edge of one's ability.\n3.  **Deliberate practice.** On top of the requirements for purposeful practice, deliberate practice is *informed by an understanding of how to do well.* In the best case, this understanding is conferred by a professional teacher (a \"coach\"). This teacher will be able to evaluate where a student sits with respect to the various necessary sub-skills, recommend specific practice tasks to improve sub-skills which are lacking, and give advice about how to improve technique.\n\nThe authors note that purposeful practice can result in getting stuck if you learn *bad form* (they don't use that term, but what they describe is very close to the concept of \"good form\" from the [CFAR handbook](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all) (page 19)). Bad form means that practice is ultimately instilling bad habits, even if it creates local improvement. Good form means that practice is taking you down the path to mastery in an efficient manner.\n\nDue to the necessity of having an experienced teacher, deliberate practice requires a highly developed field. However, it is also common to use the term \"deliberate practice\" for anything which is distinguished from purposeful practice by the presence of a *theory of skill* and *practice guided by that theory*, whether or not that theory is tried-and-true, and whether or not an experienced teacher is involved."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6v2FHy8dtyCYg9Kz4",
    "name": "Therapy",
    "core": false,
    "slug": "therapy",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Therapy</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is treatment intended to reduce or remove a disorder. Depression, burn wounds, and many other afflictions can be treated with therapy and time. Psychotherapy aims to treat mental disorders, while physical therapy deals with injuries, physical illnesses, and other bodily damage.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Psychology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/psychiatry\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Psychiatry</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/self-improvement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Self Improvement</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/meditation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Meditation</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/subagents\"><span class=\"by_sKAL2jzfkYkDbQmx9\">subagents</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Center for Applied Rationality (CFAR)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "**Therapy** is treatment intended to reduce or remove a disorder. Depression, burn wounds, and many other afflictions can be treated with therapy and time. Psychotherapy aims to treat mental disorders, while physical therapy deals with injuries, physical illnesses, and other bodily damage.\n\n**Related Pages:** [Psychology](https://www.lesswrong.com/tag/psychology), [Psychiatry](https://www.lesswrong.com/tag/psychiatry), [Self Improvement](https://www.lesswrong.com/tag/self-improvement), [Meditation](https://www.lesswrong.com/tag/meditation), [subagents](https://www.lesswrong.com/tag/subagents), [Center for Applied Rationality (CFAR)](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dGw7KEdcLFSvbJ7bM",
    "name": "Meta-Humor",
    "core": false,
    "slug": "meta-humor",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Meta-Humor</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> is the analysis of what makes things funny, and the roles humor plays in sociology and cognition.</span></p><p><span class=\"by_nLbwLhBaQeG6tCNDN\">For posts that are themselves humor, see </span><a href=\"https://www.lesswrong.com/tag/humor\"><span class=\"by_nLbwLhBaQeG6tCNDN\">humor</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "**Meta-Humor** is the analysis of what makes things funny, and the roles humor plays in sociology and cognition.\n\nFor posts that are themselves humor, see [humor](https://www.lesswrong.com/tag/humor)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NX5R6eJajJFssMvDi",
    "name": "Project Announcement",
    "core": false,
    "slug": "project-announcement",
    "tableOfContents": {
      "html": "<p><span class=\"by_pbREHuM5F5t5nyWqh\">A </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">project announcement</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is what you might expect - an announcement of a project.</span><br><span class=\"by_pbREHuM5F5t5nyWqh\">Posts that are about a project's announcement, but do not themselves announce anything, should not have this tag.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": {
      "markdown": "A **project announcement** is what you might expect - an announcement of a project.  \nPosts that are about a project's announcement, but do not themselves announce anything, should not have this tag."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2YcmB6SLtHnHRe3uX",
    "name": "VNM Theorem",
    "core": false,
    "slug": "vnm-theorem",
    "tableOfContents": {
      "html": "<p>The <strong>VNM theorem</strong> is one of the classic results of Bayesian decision theory. It establishes that, under four assumptions known as the <strong>VNM axioms</strong>, a preference relation <i>must</i> be representable by maximum-expectation decision making over some real-valued utility function. (In other words, rational decision making is best-average-case decision making.)</p><p>Starting with some set of outcomes, <strong>gambles </strong>(or <strong>lotteries</strong>) are defined recursively. An outcome is a gamble, and for any finite set of gambles, a probability distribution over those gambles is a gamble.</p><p>Preferences are then expressed over gambles via a preference relation. if&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span>&nbsp;is preferred to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>, this is written&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A>B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>. We also have indifference, written&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\\sim{} B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">∼</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>. If&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;is either preferred to&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>&nbsp;<i>or</i> indifferent with&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>, this can be written&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A \\geq{} B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">≥</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>.</p><p>The four VNM axioms are:</p><ol><li><strong>Completeness.</strong> For any gambles&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>, either&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A>B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>,&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B>A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&gt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>, or&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A \\sim{} B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">∼</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>.</li><li><strong>Transitivity. </strong>If&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A<B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B<C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>, then&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A<C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">&lt;</span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>.</li><li><strong>Continuity.</strong> If&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A \\leq{} B \\leq{} C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">≤</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">≤</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>, then there exists a probability&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p \\in{} [0,1]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>&nbsp;such that &nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"pA+(1-p)C \\sim{} B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\">∼</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>. In other words, there is a probability which hits any point between two gambles.</li><li><strong>Independence. </strong>For any&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>&nbsp;and&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p \\in{} [0,1]\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\">∈</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">[</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">0</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\">,</span></span><span class=\"mjx-mn MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">]</span></span></span></span></span></span></span>, we have&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A \\leq{} B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">≤</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>&nbsp;if and only if&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"pA + (1-p)C \\leq{} pB + (1-p)C\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.446em;\">≤</span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">+</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">(</span></span><span class=\"mjx-mn\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\">1</span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\">−</span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\">p</span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\">)</span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.045em;\">C</span></span></span></span></span></span></span>. In other words, substituting&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\">A</span></span></span></span></span></span></span>&nbsp;for&nbsp;<span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"B\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\">B</span></span></span></span></span></span></span>&nbsp;in any gamble can't make that gamble worth less.</li></ol><p>In contrast to <a href=\"https://www.lesswrong.com/tag/utility-functions\">Utility Functions</a>, this tag focuses <i>specifically</i> on posts which discuss the VNM theorem itself.</p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "The **VNM theorem** is one of the classic results of Bayesian decision theory. It establishes that, under four assumptions known as the **VNM axioms**, a preference relation *must* be representable by maximum-expectation decision making over some real-valued utility function. (In other words, rational decision making is best-average-case decision making.)\n\nStarting with some set of outcomes, **gambles** (or **lotteries**) are defined recursively. An outcome is a gamble, and for any finite set of gambles, a probability distribution over those gambles is a gamble.\n\nPreferences are then expressed over gambles via a preference relation. if \\\\(A\\\\) is preferred to \\\\(B\\\\), this is written \\\\(A>B\\\\). We also have indifference, written \\\\(A\\\\sim{} B\\\\). If \\\\(A\\\\) is either preferred to \\\\(B\\\\) *or* indifferent with \\\\(B\\\\), this can be written \\\\(A \\\\geq{} B\\\\).\n\nThe four VNM axioms are:\n\n1.  **Completeness.** For any gambles \\\\(A\\\\) and \\\\(B\\\\), either \\\\(A>B\\\\), \\\\(B>A\\\\), or \\\\(A \\\\sim{} B\\\\).\n2.  **Transitivity.** If \\\\(A<B\\\\) and \\\\(B<C\\\\), then \\\\(A<C\\\\).\n3.  **Continuity.** If \\\\(A \\\\leq{} B \\\\leq{} C\\\\), then there exists a probability \\\\(p \\\\in{} \\[0,1\\]\\\\) such that  \\\\(pA+(1-p)C \\\\sim{} B\\\\). In other words, there is a probability which hits any point between two gambles.\n4.  **Independence.** For any \\\\(C\\\\) and \\\\(p \\\\in{} \\[0,1\\]\\\\), we have \\\\(A \\\\leq{} B\\\\) if and only if \\\\(pA + (1-p)C \\\\leq{} pB + (1-p)C\\\\). In other words, substituting \\\\(A\\\\) for \\\\(B\\\\) in any gamble can't make that gamble worth less.\n\nIn contrast to [Utility Functions](https://www.lesswrong.com/tag/utility-functions), this tag focuses *specifically* on posts which discuss the VNM theorem itself."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wygybwY9SMdaPepZr",
    "name": "Feedback & Criticism (topic)",
    "core": false,
    "slug": "feedback-and-criticism-topic",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Feedback</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is information about people's reactions to an event, object, person, place, or idea. Many writers, developers, entrepreneurs, and other content creators try to get feedback so they can know how to improve their creations. </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Positive feedback</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is positive information in a reaction - i.e. being told that one's hair looks nice, while </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">negative feedback</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> or </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">criticism</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> is the opposite - i.e. being told that one's hair looks like a mop.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Feedback** is information about people's reactions to an event, object, person, place, or idea. Many writers, developers, entrepreneurs, and other content creators try to get feedback so they can know how to improve their creations. **Positive feedback** is positive information in a reaction - i.e. being told that one's hair looks nice, while **negative feedback** or **criticism** is the opposite - i.e. being told that one's hair looks like a mop."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SgpwPSrHRbeRSxKus",
    "name": "Perfect Predictor",
    "core": false,
    "slug": "perfect-predictor",
    "tableOfContents": {
      "html": "<p><span class=\"by_XLwKyCK7JmC292ZCC\">A </span><strong><span class=\"by_XLwKyCK7JmC292ZCC\">perfect predictor</span></strong><span class=\"by_XLwKyCK7JmC292ZCC\"> is an agent which can predict the behaviour of an agent or the outcome of an event with perfect accuracy. It is often given the name </span><a href=\"https://www.lesswrong.com/tag/omega\"><span class=\"by_XLwKyCK7JmC292ZCC\">Omega</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, but Omega sometimes refers to an almost perfect predictor.</span></p><h2 id=\"Possibility_and_relevance_\"><span class=\"by_XLwKyCK7JmC292ZCC\">Possibility and relevance:</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">Perfect predictors are generally understood to be impossible due to the </span><a href=\"https://www.wikiwand.com/en/Uncertainty_principle\"><span class=\"by_XLwKyCK7JmC292ZCC\">Uncertainty Principle</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> or just from our general experience that perfect observation or accuracy aren't a feature of our universe. Some people claim this makes them irrelevant for real decision theory problems. See the page on </span><a href=\"https://www.lesswrong.com/tag/hypotheticals\"><span class=\"by_XLwKyCK7JmC292ZCC\">hypotheticals</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> for further discussion on whether or not this is valid. Some </span><a href=\"https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors?commentId=zZDWxYmbd6m6nDpHf\"><span class=\"by_XLwKyCK7JmC292ZCC\">people</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> have objected on the basis of </span><a href=\"https://www.lesswrong.com/tag/free-will\"><span class=\"by_XLwKyCK7JmC292ZCC\">free will</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">Some people have </span><a href=\"https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory\"><span class=\"by_XLwKyCK7JmC292ZCC\">attempted</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> to make these problems more realistic and concrete by reframing it in terms of computational agents with access to other agents source code or the program representing the environment. This won't be perfect in the sense that there's nothing stopping a machine error or a hacker messing ruining the prediction, but it is close enough that it can be approximately to perfect predictors.</span></p><h2 id=\"Inconsistent_Counterfactuals_\"><span class=\"by_XLwKyCK7JmC292ZCC\">Inconsistent Counterfactuals:</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">One challenge with perfect predictors is that it might be unclear what Omega is predicting, particularly in situations that are only </span><a href=\"https://www.lesswrong.com/tag/conditional-consistency\"><span class=\"by_XLwKyCK7JmC292ZCC\">conditionally consistent</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">. Take for example </span><a href=\"https://www.lesswrong.com/tag/parfits-hitchhiker\"><span class=\"by_XLwKyCK7JmC292ZCC\">Parfit's Hitchhiker</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">. In this problem, you are trapped dying in a desert and a passing driver will only pick you up if you promise to pay them $100 once you are in town. If the driver is a perfect predictor, then someone who always defects will never end up in town, so it is unclear what exactly they are predicting, since the situation is contradictory and the </span><a href=\"https://en.wikipedia.org/wiki/Principle_of_explosion\"><span class=\"by_XLwKyCK7JmC292ZCC\">Principle of Explosion</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> means that you can prove anything.</span></p><p><a href=\"https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors\"><span class=\"by_XLwKyCK7JmC292ZCC\">Counterfactuals for Perfect Predictors</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> suggests that even if we can't predict what an agent would do in an inconsistent or </span><a href=\"https://www.lesswrong.com/tag/conditional-consistency\"><span class=\"by_XLwKyCK7JmC292ZCC\">conditionally consistent</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> situation, we can predict how it would respond if given input representing an inconsistent situation (we can represent this response as an output). This aligns with </span><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_XLwKyCK7JmC292ZCC\">Updateless Decision Theory</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> which isn't subject to this issue as it uses input-output maps.</span></p>",
      "sections": [
        {
          "title": "Possibility and relevance:",
          "anchor": "Possibility_and_relevance_",
          "level": 1
        },
        {
          "title": "Inconsistent Counterfactuals:",
          "anchor": "Inconsistent_Counterfactuals_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 3,
    "description": {
      "markdown": "A **perfect predictor** is an agent which can predict the behaviour of an agent or the outcome of an event with perfect accuracy. It is often given the name [Omega](https://www.lesswrong.com/tag/omega), but Omega sometimes refers to an almost perfect predictor.\n\nPossibility and relevance:\n--------------------------\n\nPerfect predictors are generally understood to be impossible due to the [Uncertainty Principle](https://www.wikiwand.com/en/Uncertainty_principle) or just from our general experience that perfect observation or accuracy aren't a feature of our universe. Some people claim this makes them irrelevant for real decision theory problems. See the page on [hypotheticals](https://www.lesswrong.com/tag/hypotheticals) for further discussion on whether or not this is valid. Some [people](https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors?commentId=zZDWxYmbd6m6nDpHf) have objected on the basis of [free will](https://www.lesswrong.com/tag/free-will).\n\nSome people have [attempted](https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory) to make these problems more realistic and concrete by reframing it in terms of computational agents with access to other agents source code or the program representing the environment. This won't be perfect in the sense that there's nothing stopping a machine error or a hacker messing ruining the prediction, but it is close enough that it can be approximately to perfect predictors.\n\nInconsistent Counterfactuals:\n-----------------------------\n\nOne challenge with perfect predictors is that it might be unclear what Omega is predicting, particularly in situations that are only [conditionally consistent](https://www.lesswrong.com/tag/conditional-consistency). Take for example [Parfit's Hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker). In this problem, you are trapped dying in a desert and a passing driver will only pick you up if you promise to pay them $100 once you are in town. If the driver is a perfect predictor, then someone who always defects will never end up in town, so it is unclear what exactly they are predicting, since the situation is contradictory and the [Principle of Explosion](https://en.wikipedia.org/wiki/Principle_of_explosion) means that you can prove anything.\n\n[Counterfactuals for Perfect Predictors](https://www.lesswrong.com/posts/AKkFh3zKGzcYBiPo7/counterfactuals-for-perfect-predictors) suggests that even if we can't predict what an agent would do in an inconsistent or [conditionally consistent](https://www.lesswrong.com/tag/conditional-consistency) situation, we can predict how it would respond if given input representing an inconsistent situation (we can represent this response as an output). This aligns with [Updateless Decision Theory](https://www.lesswrong.com/tag/updateless-decision-theory) which isn't subject to this issue as it uses input-output maps."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CGpSeuhohSFFqnTYZ",
    "name": "Conditional Consistency",
    "core": false,
    "slug": "conditional-consistency",
    "tableOfContents": {
      "html": "<p><span class=\"by_XLwKyCK7JmC292ZCC\">Some </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_XLwKyCK7JmC292ZCC\">decision theory</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> scenarios might only be consistent depending on the choice you make. For example, it's been observed that if you decide not to pay when you are in town with a perfect predictor, then it is not consistent</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": {
      "markdown": "Some [decision theory](https://www.lesswrong.com/tag/decision-theory) scenarios might only be consistent depending on the choice you make. For example, it's been observed that if you decide not to pay when you are in town with a perfect predictor, then it is not consistent"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QPt5ECwTCAg63mbNu",
    "name": "Quests / Projects Someone Should Do",
    "core": false,
    "slug": "quests-projects-someone-should-do",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Quests</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> are when someone is like \"man, someone should </span><i><span class=\"by_r38pkCm7wF4M44MDQ\">totally </span></i><span class=\"by_r38pkCm7wF4M44MDQ\">do this thing.\"</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 22,
    "description": {
      "markdown": "**Quests** are when someone is like \"man, someone should *totally* do this thing.\""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TBPFC5fwpkzLB87xJ",
    "name": "Pre-Commitment",
    "core": false,
    "slug": "pre-commitment",
    "tableOfContents": {
      "html": "<p><span class=\"by_XLwKyCK7JmC292ZCC\">Many </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Decision Theory</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> problems involve pre-commitment or deciding in advance how you are going to act. This is crucial for game-theory, where an agent that has credibly pre-committed can force other actors to act differently than they would other otherwise acted. It is also important for problems with predictors like </span><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_XLwKyCK7JmC292ZCC\">Newcomb's Problem</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> where an agent which pre-commits </span><a href=\"https://www.lesswrong.com/tag/one-boxing\"><span class=\"by_XLwKyCK7JmC292ZCC\">one-boxing</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> guarantees (or almost guarantees) themselves the million. Lastly, it can be important for agents who are aware that they are likely to make a bad decision in the moment.</span></p><h2 id=\"Interactions_with_Predictors_\"><span class=\"by_XLwKyCK7JmC292ZCC\">Interactions with Predictors:</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">There has been significant disagreement about what pre-commitment means for decision theory problems where you are being predicted by a sufficiently high quality predictor. In </span><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_XLwKyCK7JmC292ZCC\">Newcomb's Problem</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, two-boxers typically believe that while you could have obtained the million by pre-committing before Omega made their prediction, afterwards is too late. Even though </span><a href=\"https://www.lesswrong.com/tag/two-boxing\"><span class=\"by_XLwKyCK7JmC292ZCC\">two-boxing</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> only gives you $1000, they claim that the million was never in the box so you never could have gained it. In contrast, one-boxers tend to believe that it is a mistake to think that the million isn't accessible to you - see Eliezer arguing that </span><a href=\"https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality\"><span class=\"by_XLwKyCK7JmC292ZCC\">you can just do it</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> - in other words that if you one-box you will always find that the million always was accessible to you.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">If you have to pre-commit in advance the question naturally arises - what counts as pre-commitment? Is it sufficient to just decide in advance what you are going to do as long as you are committed to following through or do you have to commit more substantially by setting up a penalty sufficient to dissuade yourself from changing your mind? Having raised this question, the answer seems clear - a pre-commitment is valid in terms of obtaining you the million so long as it is legible to Omega and it is valid in terms of binding you to the action so long as you actually follow through.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">One distinction that it might be useful to make is between </span><a href=\"https://www.lesswrong.com/posts/Q8tyoaMFmW8R9w9db/formal-vs-effective-pre-commitment\"><span class=\"by_XLwKyCK7JmC292ZCC\">formal and effective pre-commitment</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">. Formal pre-commitment is when you take specific legible actions to commit yourself like talking about it in public, handing over money as a deposit or rewriting your source-code. On the other hand, effective pre-commitment is the notion that in a deterministic universe whatever action you take you are pre-committed to and that an agent that knew the environment and your state in sufficient detail would be able to predict what action you would take. In this view, the only difference is that formal pre-commitment is easier to predict.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">One issue that arises with predictors is that some scenarios may be conditionally inconsistent (or just plain inconsistent when we're dealing with logical uncertainty and oracles). Oddly enough, it seems as though it might make sense to allow pre-commitments in relation to these scenarios, although this involves pre-committing to taking an action when receiving input representing such a potentially inconsistent scenario rather than pre-committing to take an action in a particular scenario itself.</span></p><h2 id=\"Game_Theory_\"><a href=\"https://www.lesswrong.com/tag/game-theory\"><span><span class=\"by_XLwKyCK7JmC292ZCC\">Game </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Theory</span></span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">In game theory, commitment is often considered purely from the perspective of incentives. From this view, you are considered to have pre-committed youself to an action if any benefit you would gain from it is outweighed by the penalty you would pay.</span></p><h2 id=\"Psychology_\"><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Psychology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">Pre-commitment can also be important from a psychological perspective. Suppose you have an assignment to work on. You know that you need to work on it tomorrow, but you also know that you won't feel like it on the day. By deciding in advance to work on the assignment tomorrow you are providing yourself an additional reason (keeping your commitments to yourself) to work on it.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/commitment-mechanisms\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Commitment Mechanisms</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/assurance-contracts\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Assurance contracts</span></a></p>",
      "sections": [
        {
          "title": "Interactions with Predictors:",
          "anchor": "Interactions_with_Predictors_",
          "level": 1
        },
        {
          "title": "Game Theory:",
          "anchor": "Game_Theory_",
          "level": 1
        },
        {
          "title": "Psychology:",
          "anchor": "Psychology_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 11,
    "description": {
      "markdown": "Many [Decision Theory](https://www.lesswrong.com/tag/decision-theory) problems involve pre-commitment or deciding in advance how you are going to act. This is crucial for game-theory, where an agent that has credibly pre-committed can force other actors to act differently than they would other otherwise acted. It is also important for problems with predictors like [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem) where an agent which pre-commits [one-boxing](https://www.lesswrong.com/tag/one-boxing) guarantees (or almost guarantees) themselves the million. Lastly, it can be important for agents who are aware that they are likely to make a bad decision in the moment.\n\nInteractions with Predictors:\n-----------------------------\n\nThere has been significant disagreement about what pre-commitment means for decision theory problems where you are being predicted by a sufficiently high quality predictor. In [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem), two-boxers typically believe that while you could have obtained the million by pre-committing before Omega made their prediction, afterwards is too late. Even though [two-boxing](https://www.lesswrong.com/tag/two-boxing) only gives you $1000, they claim that the million was never in the box so you never could have gained it. In contrast, one-boxers tend to believe that it is a mistake to think that the million isn't accessible to you - see Eliezer arguing that [you can just do it](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality) \\- in other words that if you one-box you will always find that the million always was accessible to you.\n\nIf you have to pre-commit in advance the question naturally arises - what counts as pre-commitment? Is it sufficient to just decide in advance what you are going to do as long as you are committed to following through or do you have to commit more substantially by setting up a penalty sufficient to dissuade yourself from changing your mind? Having raised this question, the answer seems clear - a pre-commitment is valid in terms of obtaining you the million so long as it is legible to Omega and it is valid in terms of binding you to the action so long as you actually follow through.\n\nOne distinction that it might be useful to make is between [formal and effective pre-commitment](https://www.lesswrong.com/posts/Q8tyoaMFmW8R9w9db/formal-vs-effective-pre-commitment). Formal pre-commitment is when you take specific legible actions to commit yourself like talking about it in public, handing over money as a deposit or rewriting your source-code. On the other hand, effective pre-commitment is the notion that in a deterministic universe whatever action you take you are pre-committed to and that an agent that knew the environment and your state in sufficient detail would be able to predict what action you would take. In this view, the only difference is that formal pre-commitment is easier to predict.\n\nOne issue that arises with predictors is that some scenarios may be conditionally inconsistent (or just plain inconsistent when we're dealing with logical uncertainty and oracles). Oddly enough, it seems as though it might make sense to allow pre-commitments in relation to these scenarios, although this involves pre-committing to taking an action when receiving input representing such a potentially inconsistent scenario rather than pre-committing to take an action in a particular scenario itself.\n\n[Game Theory](https://www.lesswrong.com/tag/game-theory):\n---------------------------------------------------------\n\nIn game theory, commitment is often considered purely from the perspective of incentives. From this view, you are considered to have pre-committed youself to an action if any benefit you would gain from it is outweighed by the penalty you would pay.\n\n[Psychology](https://www.lesswrong.com/tag/psychology):\n-------------------------------------------------------\n\nPre-commitment can also be important from a psychological perspective. Suppose you have an assignment to work on. You know that you need to work on it tomorrow, but you also know that you won't feel like it on the day. By deciding in advance to work on the assignment tomorrow you are providing yourself an additional reason (keeping your commitments to yourself) to work on it.\n\n**Related Pages:** [Commitment Mechanisms](https://www.lesswrong.com/tag/commitment-mechanisms), [Assurance contracts](https://www.lesswrong.com/tag/assurance-contracts)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rjEZWSbSffhaWYRvo",
    "name": "Hypotheticals",
    "core": false,
    "slug": "hypotheticals",
    "tableOfContents": {
      "html": "<p><span class=\"by_XLwKyCK7JmC292ZCC\">Many philosophy problems involve imagining hypothetical scenarios. At times there has been significant debate over the validity of this.</span></p><h2 id=\"Unrealistic_hypotheticals_\"><span class=\"by_XLwKyCK7JmC292ZCC\">Unrealistic hypotheticals:</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">At times there has been significant debate on Less Wrong about the relevance or value in discussing hypotheticals that are unrealistic. In </span><a href=\"https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world\"><span class=\"by_XLwKyCK7JmC292ZCC\">The Least Convenient Possible World</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, Scott Alexander suggests that ignoring hypotheticals often means that you are technically correct at the cost \"missing the point and losing a valuable effort to examine the nature of morality\". He suggests that considering about the least convenient world is often vital for allowing us to discover our true motivations and often leaves us too much \"wiggle room\".</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">In </span><a href=\"https://www.lesswrong.com/posts/s9hTXtAPn2ZEAWutr/please-don-t-fight-the-hypothetical\"><span class=\"by_XLwKyCK7JmC292ZCC\">Please Don't Fight the Hypothetical</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, TimS suggests that fighting the hypothetical is equivalent to, \"I don't find this topic interesting for whatever reason, and wish to talk about something I am interested in.\" He says that this is fine, but suggests it is important to be aware when you are changing the subject like this.</span></p><p><a href=\"https://www.lesswrong.com/posts/5y45Kry6GtWCFePjm/hypotheticals-the-direct-application-fallacy\"><span class=\"by_CLZfn248x9Xw8FoHx\">Hypotheticals: The Direct Application Fallacy</span></a><span class=\"by_CLZfn248x9Xw8FoHx\"> suggests that it is a mistake to assume that the only reason for studying a hypothetical situation is to understand what to do in that exact situation. It suggests that practise exercises don't need to be real and in fact insisting on this can make teaching nearly impossible. It further suggests that examining degenerate cases of a theory often provides a useful sanity check and can make the limitations of a heuristic more explicit.</span></p><h2 id=\"Related_terms_\"><span class=\"by_XLwKyCK7JmC292ZCC\">Related terms:</span></h2><p><span><span class=\"by_r38pkCm7wF4M44MDQ\">Hypotheticals are </span><span class=\"by_XLwKyCK7JmC292ZCC\">essentially the same as </span></span><a href=\"https://www.lesswrong.com/tag/counterfactuals\"><span class=\"by_XLwKyCK7JmC292ZCC\">counterfactuals</span></a><span><span class=\"by_XLwKyCK7JmC292ZCC\">, although a) the term counterfactual is preferred when imagining someone making different decisions b) technically the factual isn't a counterfactual,</span><span class=\"by_r38pkCm7wF4M44MDQ\"> but </span><span class=\"by_XLwKyCK7JmC292ZCC\">it is very common</span><span class=\"by_r38pkCm7wF4M44MDQ\"> to </span><span class=\"by_XLwKyCK7JmC292ZCC\">say something like \"iterate over all the </span><span class=\"by_r38pkCm7wF4M44MDQ\">counterfactuals</span><span class=\"by_XLwKyCK7JmC292ZCC\"> and pick the one with the highest utility\" where we treat the factual as a counterfactual.</span></span></p>",
      "sections": [
        {
          "title": "Unrealistic hypotheticals:",
          "anchor": "Unrealistic_hypotheticals_",
          "level": 1
        },
        {
          "title": "Related terms:",
          "anchor": "Related_terms_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 13,
    "description": {
      "markdown": "Many philosophy problems involve imagining hypothetical scenarios. At times there has been significant debate over the validity of this.\n\nUnrealistic hypotheticals:\n--------------------------\n\nAt times there has been significant debate on Less Wrong about the relevance or value in discussing hypotheticals that are unrealistic. In [The Least Convenient Possible World](https://www.lesswrong.com/posts/neQ7eXuaXpiYw7SBy/the-least-convenient-possible-world), Scott Alexander suggests that ignoring hypotheticals often means that you are technically correct at the cost \"missing the point and losing a valuable effort to examine the nature of morality\". He suggests that considering about the least convenient world is often vital for allowing us to discover our true motivations and often leaves us too much \"wiggle room\".\n\nIn [Please Don't Fight the Hypothetical](https://www.lesswrong.com/posts/s9hTXtAPn2ZEAWutr/please-don-t-fight-the-hypothetical), TimS suggests that fighting the hypothetical is equivalent to, \"I don't find this topic interesting for whatever reason, and wish to talk about something I am interested in.\" He says that this is fine, but suggests it is important to be aware when you are changing the subject like this.\n\n[Hypotheticals: The Direct Application Fallacy](https://www.lesswrong.com/posts/5y45Kry6GtWCFePjm/hypotheticals-the-direct-application-fallacy) suggests that it is a mistake to assume that the only reason for studying a hypothetical situation is to understand what to do in that exact situation. It suggests that practise exercises don't need to be real and in fact insisting on this can make teaching nearly impossible. It further suggests that examining degenerate cases of a theory often provides a useful sanity check and can make the limitations of a heuristic more explicit.\n\nRelated terms:\n--------------\n\nHypotheticals are essentially the same as [counterfactuals](https://www.lesswrong.com/tag/counterfactuals), although a) the term counterfactual is preferred when imagining someone making different decisions b) technically the factual isn't a counterfactual, but it is very common to say something like \"iterate over all the counterfactuals and pick the one with the highest utility\" where we treat the factual as a counterfactual."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "a3W2TSzPuxKr3Hm9j",
    "name": "Intellectual Progress via LessWrong",
    "core": false,
    "slug": "intellectual-progress-via-lesswrong",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">One of the core goals of the </span><a href=\"https://www.lesswrong.com/about\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong 2.0 platform</span></strong></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is to generate </span><a href=\"https://www.lesswrong.com/tag/intellectual-progress\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">intellectual progress</span></strong></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, specifically </span><a href=\"https://www.lesswrong.com/tag/differential-intellectual-progress\"><span class=\"by_qgdGA4ZEyW7zNdK84\">differential intellectual progress</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> on important questions. There is much to discuss: should LessWrong 2.0 be a vehicle for intellectual progress? Is it succeeding? What should it do to succeed more?</span><br><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/intellectual-progress?showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intellectual Progress (Society-Level)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/intellectual-progress-individual-level?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intellectual Progress (Individual-Level)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/site-meta?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Site Meta</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 25,
    "description": {
      "markdown": "One of the core goals of the [**LessWrong 2.0 platform**](https://www.lesswrong.com/about) is to generate [**intellectual progress**](https://www.lesswrong.com/tag/intellectual-progress), specifically [differential intellectual progress](https://www.lesswrong.com/tag/differential-intellectual-progress) on important questions. There is much to discuss: should LessWrong 2.0 be a vehicle for intellectual progress? Is it succeeding? What should it do to succeed more?  \n  \n*See also*: [Intellectual Progress (Society-Level)](https://www.lesswrong.com/tag/intellectual-progress?showPostCount=true), [Intellectual Progress (Individual-Level)](https://www.lesswrong.com/tag/intellectual-progress-individual-level?showPostCount=true&useTagName=true), [Site Meta](https://www.lesswrong.com/tag/site-meta?showPostCount=true&useTagName=true)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb356",
    "name": "Road To AI Safety Excellence",
    "core": null,
    "slug": "road-to-ai-safety-excellence",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_romAE64tmvKpeixHA\">Road to AI Safety Excellence</span></strong><span><span class=\"by_romAE64tmvKpeixHA\"> (RAISE), previously named AASAA, </span><span class=\"by_XLwKyCK7JmC292ZCC\">was</span><span class=\"by_romAE64tmvKpeixHA\"> an initiative from </span></span><a href=\"http://lesswrong.com/user/toonalfrink/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">toonalfrink</span></a><span><span class=\"by_romAE64tmvKpeixHA\"> to improve the pipeline for AI safety researchers, especially by creating an online course.</span><span class=\"by_XLwKyCK7JmC292ZCC\"> See the </span></span><a href=\"https://www.lesswrong.com/posts/oW6mbA3XHzcfJTwNq/raise-post-mortem\"><span class=\"by_XLwKyCK7JmC292ZCC\">Post-Mortem</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">.</span></p><h2 id=\"Note\"><span class=\"by_romAE64tmvKpeixHA\">Note</span></h2><p><span><span class=\"by_romAE64tmvKpeixHA\">This page is deprecated, and it will no longer be updated</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> by the RAISE founder</span><span class=\"by_romAE64tmvKpeixHA\"> (unless an independent party decides to). See the updated page at </span></span><a href=\"http://aisafety.camp/about/\"><span class=\"by_romAE64tmvKpeixHA\">http://aisafety.camp/about/</span></a></p><h2 id=\"Motivation\"><span class=\"by_romAE64tmvKpeixHA\">Motivation</span></h2><p><span><span class=\"by_romAE64tmvKpeixHA\">AI safety is a small field. It has only about </span><span class=\"by_3RHzytgr6QeZD7xRz\">100</span><span class=\"by_romAE64tmvKpeixHA\"> researchers. The field is mostly </span></span><a href=\"https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/#ai-safety-research\"><span class=\"by_romAE64tmvKpeixHA\">talent-constrained</span></a><span class=\"by_romAE64tmvKpeixHA\">. Given the dangers of an uncontrolled intelligence explosion, increasing the amount of AIS researchers is crucial for the long-term survival of humanity.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Within the LW community there are plenty of talented people that bear a sense of urgency about AI. They are willing to switch careers to doing research, but they are unable to get there. This is understandable: the path up to research-level understanding is lonely, arduous, long, and uncertain. It is like a pilgrimage. One has to study concepts from the papers in which they first appeared. This is not easy. Such papers are </span><a href=\"http://distill.pub/2017/research-debt/\"><span class=\"by_romAE64tmvKpeixHA\">undistilled</span></a><span class=\"by_romAE64tmvKpeixHA\">. Unless one is lucky, there is no one to provide guidance and answer questions. Then should one come out on top, there is no guarantee that the quality of their work will be sufficient for a paycheck or a useful contribution.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">The field of AI safety is in an </span><a href=\"https://en.wikipedia.org/wiki/Diffusion_of_innovations\"><span class=\"by_romAE64tmvKpeixHA\">innovator phase</span></a><span><span class=\"by_romAE64tmvKpeixHA\">. Innovators are highly risk-</span><span class=\"by_SGAeZo4g4wPJbXczY\">tolerant</span><span class=\"by_romAE64tmvKpeixHA\"> and have a large amount of agency, which allows them to survive an environment with little guidance or supporting infrastructure. Let community organisers not fall for the typical mind fallacy, expecting risk-averse people to move into AI safety all by themselves. Unless one is particularly risk-tolerant or has a perfect safety net, they will not be able to fully take the plunge. Plenty of measures can be made to make getting into AI safety more like an </span></span><a href=\"https://www.youtube.com/watch?v=28123GsMzU8\"><span class=\"by_romAE64tmvKpeixHA\">\"It's a small world\"-ride</span></a><span class=\"by_romAE64tmvKpeixHA\">:</span></p><ul><li><span class=\"by_romAE64tmvKpeixHA\">Let there be a tested path with signposts along the way to make progress clear and measurable.</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Let there be social reinforcement so that we are not hindered but helped by our instinct for conformity.</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Let there be high-quality explanations of the material to speed up and ease the learning process, so that it is cheap.</span></li></ul><h2 id=\"Becoming_an_AIS_researcher_in_2020\"><span class=\"by_romAE64tmvKpeixHA\">Becoming an AIS researcher in 2020</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">What follows is a vision of how things *could* be, should this project come to fruition.</span></p><p><strong id=\"The_path\"><span class=\"by_romAE64tmvKpeixHA\">The path</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">1. Tim Urban's </span><a href=\"https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html\"><span class=\"by_romAE64tmvKpeixHA\">Road to Superintelligence</span></a><span class=\"by_romAE64tmvKpeixHA\"> is a popular introduction to superintelligence. Hundreds of thousands of people have read it. At the end of the article is a link, saying \"if you want to work on this, these guys can help\". It sends one to an </span><a href=\"https://wiki.lesswrong.com/wiki/Arbital\"><span class=\"by_romAE64tmvKpeixHA\">Arbital</span></a><span class=\"by_romAE64tmvKpeixHA\"> page, reading \"welcome to \"prerequisites for \"introduction to AI Safety\"\"\".</span></p><p><span class=\"by_romAE64tmvKpeixHA\">2. What follows is a series of articles explaining the math one should understand to be able to read AIS papers. It covers probability, game theory, computability theory, and a few other things. Most students with a technical major can follow along easily. Even some talented high school graduates do. When one comes to the end to the arbital sequence, one is congratulated: \"you are now ready to study AI safety\". A link to the course appears at the bottom of the page.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">3. The course teaches an array of subfields. Technical subjects like corrigibility, value learning, ML safety, but also some high-level subjects like preventing arms races around AI. Assignments are designed in such a way that they don't need manual grading, but do give some idea of the student's competence. Sometimes there is an assignment about an open problem. Students are given the chance to try to solve it by themselves. Interesting submissions are noted. One competent recruiter looks through these assignments to handpick high-potential students. When a student completes the course, they are awarded a nice polished certificate. Something to print and hang on the wall.</span></p><p><strong id=\"Local_study_groups\"><span class=\"by_romAE64tmvKpeixHA\">Local study groups</span></strong></p><p><span><span class=\"by_romAE64tmvKpeixHA\">When it comes to motivation, nothing beats the physical presence of people that share your goal. A clear and well-polished path is one major thing, social reinforcement is another. Some local study groups already exist, but there is no way for outsiders to find them. RAISE seems like a most natural place to index study groups and facilitate hosting them.</span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> You can see and edit the current list here: </span></span><a href=\"https://bit.ly/AISafetyLocalGroups\"><span class=\"by_Tw9etd8rMnHLeSQ9q\">https://bit.ly/AISafetyLocalGroups</span></a><span class=\"by_Tw9etd8rMnHLeSQ9q\">.</span></p><h2 id=\"Course_prerequisites___target_audience\"><span class=\"by_romAE64tmvKpeixHA\">Course prerequisites &amp; target audience</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">While the project originally targeted any student, it was decided that it will target those that are philosophically aligned first. The next step could be to persuade academics to model a course after this one, so that we will reach a broader audience too.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">There are technical (math, logic) and philosophical (Bostrom/sequences/WaitButWhy) prerequisites. Technical prerequisites identified so far:</span></p><ul><li><span class=\"by_romAE64tmvKpeixHA\">Probability theory</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Decision/game theory</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Computability theory</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Logic</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Linear algebra</span></li></ul><p><span class=\"by_romAE64tmvKpeixHA\">As mentioned before, it seems best to cover this in a sequence of articles on Arbital, or to recommend </span><a href=\"http://web.stanford.edu/class/cs103/\"><span class=\"by_romAE64tmvKpeixHA\">an existing course</span></a><span class=\"by_romAE64tmvKpeixHA\"> that teaches this stuff well enough.</span></p><h2 id=\"The_state_of_the_project___getting_involved\"><span class=\"by_romAE64tmvKpeixHA\">The state of the project &amp; getting involved</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">If you're enthusiastic about volunteering, fill in </span><a href=\"https://goo.gl/forms/m38tKbmDBFMgSyMz1\"><span class=\"by_romAE64tmvKpeixHA\">this form</span></a></p><p><span class=\"by_romAE64tmvKpeixHA\">To be low-key notified of progress, join </span><a href=\"https://www.facebook.com/groups/1421511671230776/\"><span class=\"by_romAE64tmvKpeixHA\">this Facebook group</span></a></p><p><span class=\"by_romAE64tmvKpeixHA\">One particularly useful and low-bar way to contribute is to join our special study group, in which you will be asked to summarize AIS resources (papers, talks, ...), and create mind maps of subjects. You can find it in the Facebook group.</span></p><h2 id=\"Curriculum\"><span class=\"by_romAE64tmvKpeixHA\">Curriculum</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">This is (like everything) subject to debate, but for now it looks like the following broad categories will be covered:</span></p><ul><li><span class=\"by_romAE64tmvKpeixHA\">Agent foundations</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Machine learning safety</span></li><li><span class=\"by_romAE64tmvKpeixHA\">AI macrostrategy</span></li></ul><p><span class=\"by_romAE64tmvKpeixHA\">Each of these categories will be divided into a few subcategories. The specifics of that are mostly undecided, except that the agent foundations category will contain at least corrigibility and decision theory.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">We are making efforts to list all available resources </span><a href=\"https://workflowy.com/s/D0Q9.oyUe39KbLp\"><span class=\"by_romAE64tmvKpeixHA\">here</span></a><span class=\"by_romAE64tmvKpeixHA\"> and </span><a href=\"https://bit.ly/AISafetyResources\"><span class=\"by_romAE64tmvKpeixHA\">here</span></a></p><h2 id=\"Course_development_process\"><span class=\"by_romAE64tmvKpeixHA\">Course development process</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">Now volunteers and capital are largely in place, we are doing an </span><a href=\"https://en.wikipedia.org/wiki/Iterative_and_incremental_development\"><span class=\"by_romAE64tmvKpeixHA\">iterative development process</span></a><span class=\"by_romAE64tmvKpeixHA\"> with the first unit on corrigibility. When we are satisfied with the quality of this unit, we will use the process we developed to create the other units.</span></p><p><strong><span class=\"by_romAE64tmvKpeixHA\">Study groups</span></strong><span class=\"by_romAE64tmvKpeixHA\"> Even for volunteers it proved tricky to reach a high-level understanding of a topic by oneself, so we decided to learn together. The study group is constructed in such a way that it produces useful content for the course. More concretely:</span></p><p><span class=\"by_romAE64tmvKpeixHA\">- There are 'scripting' and 'assignments' meetings.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">- The 'scripting' meetings embody an iterative process to go from papers to lecture scripts. We start with summaries, then we create mind maps, then we decide on a set of video, and then we create a set of script drafts based on summaries and mind maps</span></p><p><span class=\"by_romAE64tmvKpeixHA\">- All of this content is used by the lecturer to finalize scripts, set up the studio and film.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">- The set of videos produced by the lecturer are used as an input to the assignments meeting.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">- At the assignment meeting, for each lecture bit, attendants are asked to create assignments and try the assignments of others. A selection of these assignments is later added to the course.</span></p><p><strong id=\"Shooting_lectures\"><span class=\"by_romAE64tmvKpeixHA\">Shooting lectures</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">We enlisted </span><a href=\"https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg\"><span class=\"by_romAE64tmvKpeixHA\">Rob Miles</span></a><span class=\"by_romAE64tmvKpeixHA\"> to shoot our lectures. About once a week, our content developer sits down with him to go over a particular script draft, which he modifies to his liking.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">The setup includes a </span><a href=\"https://www.youtube.com/watch?v=qadgqBQdqtI\"><span class=\"by_romAE64tmvKpeixHA\">lightboard</span></a><span class=\"by_romAE64tmvKpeixHA\">, which is a neat educational innovation that allows a lecturer to look at the camera while writing on a board simultaneously.</span></p><h2 id=\"Instruction_strategy\"><span class=\"by_romAE64tmvKpeixHA\">Instruction strategy</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">The course will be strictly digital, which limits the amount of strategies that can be used. These are some potentially useful strategies:</span></p><ul><li><span class=\"by_romAE64tmvKpeixHA\">Text</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Lecture</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Documentary</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Game</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Assignment</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Live discussion</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Open problem</span></li><li><span class=\"by_romAE64tmvKpeixHA\">etc...</span></li></ul><p><strong><span class=\"by_romAE64tmvKpeixHA\">Content guides form</span></strong><span class=\"by_romAE64tmvKpeixHA\"> The best way to present an idea often depends on the nature of the idea. For example, the value alignment problem is easily explained with an illustrative story (the paperclip maximizer). This isn’t quite the case for FDT. Also, some ideas have been formalized. We can go into mathematical detail with those. Other ideas are still in the realm of philosophy and we will have to resort to things like thought experiments there. How to say depends on what to say.</span></p><p><strong><span class=\"by_romAE64tmvKpeixHA\">Gimmick: Open problems</span></strong><span class=\"by_romAE64tmvKpeixHA\"> (Inspired by </span><a href=\"http://lesswrong.com/lw/q9/the_failures_of_eld_science/\"><span class=\"by_romAE64tmvKpeixHA\">The Failures of Eld Science</span></a><span class=\"by_romAE64tmvKpeixHA\">) A special type of instruction strategy will be an assignment like this: “So here we have EDT, which is better than CDT, but it is still flawed in these ways. Can you think of a better decision theory that doesn’t have these flaws? Give it at least 10 minutes. If you have a useful idea, please let us know.”</span></p><p><span class=\"by_romAE64tmvKpeixHA\">The idea is to challenge students to think independently how they might go about solving an open problem. It gives them an opportunity to actually make a contribution. I expect it to be strongly intrinsically motivating.</span></p><p><strong><span class=\"by_romAE64tmvKpeixHA\">Taxonomy of content</span></strong><span class=\"by_romAE64tmvKpeixHA\"> At least three sorts of content will be delivered:</span></p><ul><li><span class=\"by_romAE64tmvKpeixHA\">Anecdotes/stories to illustrate problems (paperclip maximizer, filling a cauldron, ...)</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Unformalized philosophical considerations (intelligence explosion, convergent instrumental goals, acausal trade, ...)</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Technical results (corrigibility, convergent instrumental goals, FDT, ...)</span></li></ul><p><strong id=\"Example_course_unit__value_learning___corrigibility\"><span class=\"by_romAE64tmvKpeixHA\">Example course unit: value learning &amp; corrigibility</span></strong></p><ul><li><span class=\"by_romAE64tmvKpeixHA\">Preview of unit and its structure</span></li><li><span class=\"by_romAE64tmvKpeixHA\">An x-minute lecture that informally explains the value learning problem</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Assignments</span></li><li><span class=\"by_romAE64tmvKpeixHA\">A 5-minute cutscene shows a fictional story of an agent that keeps its creators from pushing the off-button</span></li><li><span class=\"by_romAE64tmvKpeixHA\">An x-minute lecture that informally explains corrigibility</span></li><li><span class=\"by_romAE64tmvKpeixHA\">A piece of text that introduces the math</span></li><li><span class=\"by_romAE64tmvKpeixHA\">A video of the lecturer solving example math assignments</span></li><li><span class=\"by_romAE64tmvKpeixHA\">Corrigibility math assignments</span></li></ul><p><span class=\"by_romAE64tmvKpeixHA\">Alternatively, we can interleave tiny bits of video with questions to keep the student engaged. A good example of this is the </span><a href=\"https://www.udacity.com/course/deep-learning--ud730\"><span class=\"by_romAE64tmvKpeixHA\">Google deep learning course</span></a><span class=\"by_romAE64tmvKpeixHA\">.</span></p><h2 id=\"Task_allocation\"><span class=\"by_romAE64tmvKpeixHA\">Task allocation</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">The following is a reply to the common remark that “I’d like to help, but I’m not sure what I can do”.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">(last updated at 2018-01-31)</span></p><p><strong id=\"Full_responsibility\"><span class=\"by_romAE64tmvKpeixHA\">Full responsibility</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">This means you can’t sleep when things are off track, and jump to your laptop every time you have a new idea to move things forward. This also means you are ready to take on most tasks if no one else volunteers for it, even if you’re not specialized in it. The project is your baby, and you’re a helicopter parent.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Currently done by: Toon Alfrink, Veerle de Goederen, Remmelt Ellen, Johannes Heidecke, Mati Roy</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Required technical understanding: superficial.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Minimum commitment: 1 full day per week</span></p><p><strong id=\"Armchair_advice\"><span class=\"by_romAE64tmvKpeixHA\">Armchair advice</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">You’re in the chat, and you’re interested in the project, but not ready to make significant contributions. You do want to see where things go, and sometimes you have some interesting remarks to make. On your own terms though.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Currently done by: lots of people</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Minimum commitment: none</span></p><p><strong id=\"Content_developer\"><span class=\"by_romAE64tmvKpeixHA\">Content developer</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">As our content developer, you are responsible for the quality of the material. You coordinate the study group, review the quality of it's production, and spend extra time on your own learning the content (if you haven't already) so you can be our expert. You also help the lecturer with finalizing his scripts, and you assist him in understanding everything.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Currently done by: </span><strong><span class=\"by_romAE64tmvKpeixHA\">No one. This is a paid position. If interested, email us at raise@aisafety.camp</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">Required technical understanding: near-complete.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Minimum commitment: 2 full days per week</span></p><p><strong id=\"Giving_lectures\"><span class=\"by_romAE64tmvKpeixHA\">Giving lectures</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">You thoroughly study the material, making sure you know it well enough to explain it clearly. Together with the content developer, you sit down and go over the bits that need explanation. These bits range from </span><a href=\"https://blog.edx.org/optimal-video-length-student-engagement\"><span class=\"by_romAE64tmvKpeixHA\">3 to 6 minutes</span></a><span class=\"by_romAE64tmvKpeixHA\">, and they are interleaved with questions and small assignments to keep the student engaged.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Currently done by: Robert Miles</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Required technical understanding: thorough.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Minimum commitment: 1 full day per week</span></p><p><strong id=\"Study_group_attendant\"><span class=\"by_romAE64tmvKpeixHA\">Study group attendant</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">You help out in the weekly study group, creating summaries, mind maps, script drafts and assignments. We also give presentations</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Currently done by: Johannes Heidecke, Tom Rutten, Toon Alfrink, Tarn Somervell Fletcher, Nandi Schoots, Roland Pihlakas, Robert Miles, Rupert McCallum, Philine Widmer, Louie Terrill, Tim Bakker, Veerle de Goederen, Ofer Givoli</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Required technical understanding: none</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Minimum commitment: 4 hours per week</span></p><p><strong id=\"Software_developer\"><span class=\"by_romAE64tmvKpeixHA\">Software developer</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">With about 60% certainty, we will use </span><a href=\"https://ihatestatistics.com/\"><span class=\"by_romAE64tmvKpeixHA\">ihatestatistics</span></a><span class=\"by_romAE64tmvKpeixHA\"> as a platform. The company is run by EA's (we may use it for free), and it's specialization in statistics (which is closely related to AI) makes it well-suited for our needs. </span><a href=\"https://play.ihatestatistics.com/#/preview/level/191?lang=en\"><span class=\"by_romAE64tmvKpeixHA\">Here is a demo lesson.</span></a><span class=\"by_romAE64tmvKpeixHA\"> There are a lot of diamonds buried in the field of automated assessment. The quality of our answer-checking software determines the quality of the questions we can ask. Elaborate feedback mechanisms can make a lot of difference in how fast a learner may converge on the right kind of understanding. You write this software for us.</span></p><p><span class=\"by_romAE64tmvKpeixHA\">Minimum commitment: 2 days per week</span></p><p><strong id=\"Legal\"><span class=\"by_romAE64tmvKpeixHA\">Legal</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">Legal is a black box. Your first job is to write your job description.</span></p><p><strong id=\"Marketing_PR_acquisition\"><span class=\"by_romAE64tmvKpeixHA\">Marketing/PR/acquisition</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">Are you good at connecting people? There are a lot of people that want to fix the world, would engage with this project if they knew about it, and have the means (funding, expertise) to help out. Things you can do include finding funders, hosting a round of review, inviting guest speakers with interesting credentials, connecting with relevant EA organisations, etc. Having high social capital in the EA/LW community is a plus.</span></p><p><strong id=\"Animation___editing\"><span class=\"by_romAE64tmvKpeixHA\">Animation &amp; editing</span></strong></p><p><span class=\"by_romAE64tmvKpeixHA\">Good animation can make a course twice as polished and engaging, and this matters twice as much as you think. The whole point of a course instead of a loose collection of papers is that learners can trust they're on the right track. Polish builds that trust. Animation is also a skill that is hard to pick up in a short enough timeframe, so we can't do it. If you're interested in AI safety and skilled at animation, we need you!</span></p><h2 id=\"Peptalk\"><span class=\"by_romAE64tmvKpeixHA\">Peptalk</span></h2><p><span class=\"by_romAE64tmvKpeixHA\">I want to note that what we are doing here </span><i><span class=\"by_romAE64tmvKpeixHA\">isn’t hard</span></i><span class=\"by_romAE64tmvKpeixHA\">. Courses at universities are often created on the fly by one person in a matter of weeks. They get away with it. There is little risk. The worst that can reasonably happen is that we waste some time and money on creating an unpopular course that doesn’t get much traction. On the other hand, there is a lot of opportunity. If we do this well, we might just double the amount of FAI researchers. If that's not impact, I don't know what is.</span></p><h2 id=\"External_links\"><span class=\"by_romAE64tmvKpeixHA\">External links</span></h2><ul><li><a href=\"https://workflowy.com/s/D0Q9.oyUe39KbLp\"><span class=\"by_romAE64tmvKpeixHA\">list of resources</span></a></li><li><a href=\"https://www.facebook.com/groups/1421511671230776/\"><span class=\"by_romAE64tmvKpeixHA\">facebook group</span></a></li><li><a href=\"http://lesswrong.com/r/discussion/lw/p5e/announcing_aasaa_accelerating_ai_safety_adoption/\"><span class=\"by_j8TwwtYJusmkqvGfh\">Announcement post on LessWrong</span></a></li></ul>",
      "sections": [
        {
          "title": "Note",
          "anchor": "Note",
          "level": 1
        },
        {
          "title": "Motivation",
          "anchor": "Motivation",
          "level": 1
        },
        {
          "title": "Becoming an AIS researcher in 2020",
          "anchor": "Becoming_an_AIS_researcher_in_2020",
          "level": 1
        },
        {
          "title": "The path",
          "anchor": "The_path",
          "level": 2
        },
        {
          "title": "Local study groups",
          "anchor": "Local_study_groups",
          "level": 2
        },
        {
          "title": "Course prerequisites & target audience",
          "anchor": "Course_prerequisites___target_audience",
          "level": 1
        },
        {
          "title": "The state of the project & getting involved",
          "anchor": "The_state_of_the_project___getting_involved",
          "level": 1
        },
        {
          "title": "Curriculum",
          "anchor": "Curriculum",
          "level": 1
        },
        {
          "title": "Course development process",
          "anchor": "Course_development_process",
          "level": 1
        },
        {
          "title": "Shooting lectures",
          "anchor": "Shooting_lectures",
          "level": 2
        },
        {
          "title": "Instruction strategy",
          "anchor": "Instruction_strategy",
          "level": 1
        },
        {
          "title": "Example course unit: value learning & corrigibility",
          "anchor": "Example_course_unit__value_learning___corrigibility",
          "level": 2
        },
        {
          "title": "Task allocation",
          "anchor": "Task_allocation",
          "level": 1
        },
        {
          "title": "Full responsibility",
          "anchor": "Full_responsibility",
          "level": 2
        },
        {
          "title": "Armchair advice",
          "anchor": "Armchair_advice",
          "level": 2
        },
        {
          "title": "Content developer",
          "anchor": "Content_developer",
          "level": 2
        },
        {
          "title": "Giving lectures",
          "anchor": "Giving_lectures",
          "level": 2
        },
        {
          "title": "Study group attendant",
          "anchor": "Study_group_attendant",
          "level": 2
        },
        {
          "title": "Software developer",
          "anchor": "Software_developer",
          "level": 2
        },
        {
          "title": "Legal",
          "anchor": "Legal",
          "level": 2
        },
        {
          "title": "Marketing/PR/acquisition",
          "anchor": "Marketing_PR_acquisition",
          "level": 2
        },
        {
          "title": "Animation & editing",
          "anchor": "Animation___editing",
          "level": 2
        },
        {
          "title": "Peptalk",
          "anchor": "Peptalk",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 25
    },
    "postCount": 6,
    "description": {
      "markdown": "**Road to AI Safety Excellence** (RAISE), previously named AASAA, was an initiative from [toonalfrink](http://lesswrong.com/user/toonalfrink/) to improve the pipeline for AI safety researchers, especially by creating an online course. See the [Post-Mortem](https://www.lesswrong.com/posts/oW6mbA3XHzcfJTwNq/raise-post-mortem).\n\nNote\n----\n\nThis page is deprecated, and it will no longer be updated by the RAISE founder (unless an independent party decides to). See the updated page at [http://aisafety.camp/about/](http://aisafety.camp/about/)\n\nMotivation\n----------\n\nAI safety is a small field. It has only about 100 researchers. The field is mostly [talent-constrained](https://80000hours.org/2015/11/why-you-should-focus-more-on-talent-gaps-not-funding-gaps/#ai-safety-research). Given the dangers of an uncontrolled intelligence explosion, increasing the amount of AIS researchers is crucial for the long-term survival of humanity.\n\nWithin the LW community there are plenty of talented people that bear a sense of urgency about AI. They are willing to switch careers to doing research, but they are unable to get there. This is understandable: the path up to research-level understanding is lonely, arduous, long, and uncertain. It is like a pilgrimage. One has to study concepts from the papers in which they first appeared. This is not easy. Such papers are [undistilled](http://distill.pub/2017/research-debt/). Unless one is lucky, there is no one to provide guidance and answer questions. Then should one come out on top, there is no guarantee that the quality of their work will be sufficient for a paycheck or a useful contribution.\n\nThe field of AI safety is in an [innovator phase](https://en.wikipedia.org/wiki/Diffusion_of_innovations). Innovators are highly risk-tolerant and have a large amount of agency, which allows them to survive an environment with little guidance or supporting infrastructure. Let community organisers not fall for the typical mind fallacy, expecting risk-averse people to move into AI safety all by themselves. Unless one is particularly risk-tolerant or has a perfect safety net, they will not be able to fully take the plunge. Plenty of measures can be made to make getting into AI safety more like an [\"It's a small world\"-ride](https://www.youtube.com/watch?v=28123GsMzU8):\n\n*   Let there be a tested path with signposts along the way to make progress clear and measurable.\n*   Let there be social reinforcement so that we are not hindered but helped by our instinct for conformity.\n*   Let there be high-quality explanations of the material to speed up and ease the learning process, so that it is cheap.\n\nBecoming an AIS researcher in 2020\n----------------------------------\n\nWhat follows is a vision of how things \\*could\\* be, should this project come to fruition.\n\n**The path**\n\n1\\. Tim Urban's [Road to Superintelligence](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) is a popular introduction to superintelligence. Hundreds of thousands of people have read it. At the end of the article is a link, saying \"if you want to work on this, these guys can help\". It sends one to an [Arbital](https://wiki.lesswrong.com/wiki/Arbital) page, reading \"welcome to \"prerequisites for \"introduction to AI Safety\"\"\".\n\n2\\. What follows is a series of articles explaining the math one should understand to be able to read AIS papers. It covers probability, game theory, computability theory, and a few other things. Most students with a technical major can follow along easily. Even some talented high school graduates do. When one comes to the end to the arbital sequence, one is congratulated: \"you are now ready to study AI safety\". A link to the course appears at the bottom of the page.\n\n3\\. The course teaches an array of subfields. Technical subjects like corrigibility, value learning, ML safety, but also some high-level subjects like preventing arms races around AI. Assignments are designed in such a way that they don't need manual grading, but do give some idea of the student's competence. Sometimes there is an assignment about an open problem. Students are given the chance to try to solve it by themselves. Interesting submissions are noted. One competent recruiter looks through these assignments to handpick high-potential students. When a student completes the course, they are awarded a nice polished certificate. Something to print and hang on the wall.\n\n**Local study groups**\n\nWhen it comes to motivation, nothing beats the physical presence of people that share your goal. A clear and well-polished path is one major thing, social reinforcement is another. Some local study groups already exist, but there is no way for outsiders to find them. RAISE seems like a most natural place to index study groups and facilitate hosting them. You can see and edit the current list here: [https://bit.ly/AISafetyLocalGroups](https://bit.ly/AISafetyLocalGroups).\n\nCourse prerequisites & target audience\n--------------------------------------\n\nWhile the project originally targeted any student, it was decided that it will target those that are philosophically aligned first. The next step could be to persuade academics to model a course after this one, so that we will reach a broader audience too.\n\nThere are technical (math, logic) and philosophical (Bostrom/sequences/WaitButWhy) prerequisites. Technical prerequisites identified so far:\n\n*   Probability theory\n*   Decision/game theory\n*   Computability theory\n*   Logic\n*   Linear algebra\n\nAs mentioned before, it seems best to cover this in a sequence of articles on Arbital, or to recommend [an existing course](http://web.stanford.edu/class/cs103/) that teaches this stuff well enough.\n\nThe state of the project & getting involved\n-------------------------------------------\n\nIf you're enthusiastic about volunteering, fill in [this form](https://goo.gl/forms/m38tKbmDBFMgSyMz1)\n\nTo be low-key notified of progress, join [this Facebook group](https://www.facebook.com/groups/1421511671230776/)\n\nOne particularly useful and low-bar way to contribute is to join our special study group, in which you will be asked to summarize AIS resources (papers, talks, ...), and create mind maps of subjects. You can find it in the Facebook group.\n\nCurriculum\n----------\n\nThis is (like everything) subject to debate, but for now it looks like the following broad categories will be covered:\n\n*   Agent foundations\n*   Machine learning safety\n*   AI macrostrategy\n\nEach of these categories will be divided into a few subcategories. The specifics of that are mostly undecided, except that the agent foundations category will contain at least corrigibility and decision theory.\n\nWe are making efforts to list all available resources [here](https://workflowy.com/s/D0Q9.oyUe39KbLp) and [here](https://bit.ly/AISafetyResources)\n\nCourse development process\n--------------------------\n\nNow volunteers and capital are largely in place, we are doing an [iterative development process](https://en.wikipedia.org/wiki/Iterative_and_incremental_development) with the first unit on corrigibility. When we are satisfied with the quality of this unit, we will use the process we developed to create the other units.\n\n**Study groups** Even for volunteers it proved tricky to reach a high-level understanding of a topic by oneself, so we decided to learn together. The study group is constructed in such a way that it produces useful content for the course. More concretely:\n\n\\- There are 'scripting' and 'assignments' meetings.\n\n\\- The 'scripting' meetings embody an iterative process to go from papers to lecture scripts. We start with summaries, then we create mind maps, then we decide on a set of video, and then we create a set of script drafts based on summaries and mind maps\n\n\\- All of this content is used by the lecturer to finalize scripts, set up the studio and film.\n\n\\- The set of videos produced by the lecturer are used as an input to the assignments meeting.\n\n\\- At the assignment meeting, for each lecture bit, attendants are asked to create assignments and try the assignments of others. A selection of these assignments is later added to the course.\n\n**Shooting lectures**\n\nWe enlisted [Rob Miles](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg) to shoot our lectures. About once a week, our content developer sits down with him to go over a particular script draft, which he modifies to his liking.\n\nThe setup includes a [lightboard](https://www.youtube.com/watch?v=qadgqBQdqtI), which is a neat educational innovation that allows a lecturer to look at the camera while writing on a board simultaneously.\n\nInstruction strategy\n--------------------\n\nThe course will be strictly digital, which limits the amount of strategies that can be used. These are some potentially useful strategies:\n\n*   Text\n*   Lecture\n*   Documentary\n*   Game\n*   Assignment\n*   Live discussion\n*   Open problem\n*   etc...\n\n**Content guides form** The best way to present an idea often depends on the nature of the idea. For example, the value alignment problem is easily explained with an illustrative story (the paperclip maximizer). This isn’t quite the case for FDT. Also, some ideas have been formalized. We can go into mathematical detail with those. Other ideas are still in the realm of philosophy and we will have to resort to things like thought experiments there. How to say depends on what to say.\n\n**Gimmick: Open problems** (Inspired by [The Failures of Eld Science](http://lesswrong.com/lw/q9/the_failures_of_eld_science/)) A special type of instruction strategy will be an assignment like this: “So here we have EDT, which is better than CDT, but it is still flawed in these ways. Can you think of a better decision theory that doesn’t have these flaws? Give it at least 10 minutes. If you have a useful idea, please let us know.”\n\nThe idea is to challenge students to think independently how they might go about solving an open problem. It gives them an opportunity to actually make a contribution. I expect it to be strongly intrinsically motivating.\n\n**Taxonomy of content** At least three sorts of content will be delivered:\n\n*   Anecdotes/stories to illustrate problems (paperclip maximizer, filling a cauldron, ...)\n*   Unformalized philosophical considerations (intelligence explosion, convergent instrumental goals, acausal trade, ...)\n*   Technical results (corrigibility, convergent instrumental goals, FDT, ...)\n\n**Example course unit: value learning & corrigibility**\n\n*   Preview of unit and its structure\n*   An x-minute lecture that informally explains the value learning problem\n*   Assignments\n*   A 5-minute cutscene shows a fictional story of an agent that keeps its creators from pushing the off-button\n*   An x-minute lecture that informally explains corrigibility\n*   A piece of text that introduces the math\n*   A video of the lecturer solving example math assignments\n*   Corrigibility math assignments\n\nAlternatively, we can interleave tiny bits of video with questions to keep the student engaged. A good example of this is the [Google deep learning course](https://www.udacity.com/course/deep-learning--ud730).\n\nTask allocation\n---------------\n\nThe following is a reply to the common remark that “I’d like to help, but I’m not sure what I can do”.\n\n(last updated at 2018-01-31)\n\n**Full responsibility**\n\nThis means you can’t sleep when things are off track, and jump to your laptop every time you have a new idea to move things forward. This also means you are ready to take on most tasks if no one else volunteers for it, even if you’re not specialized in it. The project is your baby, and you’re a helicopter parent.\n\nCurrently done by: Toon Alfrink, Veerle de Goederen, Remmelt Ellen, Johannes Heidecke, Mati Roy\n\nRequired technical understanding: superficial.\n\nMinimum commitment: 1 full day per week\n\n**Armchair advice**\n\nYou’re in the chat, and you’re interested in the project, but not ready to make significant contributions. You do want to see where things go, and sometimes you have some interesting remarks to make. On your own terms though.\n\nCurrently done by: lots of people\n\nMinimum commitment: none\n\n**Content developer**\n\nAs our content developer, you are responsible for the quality of the material. You coordinate the study group, review the quality of it's production, and spend extra time on your own learning the content (if you haven't already) so you can be our expert. You also help the lecturer with finalizing his scripts, and you assist him in understanding everything.\n\nCurrently done by: **No one. This is a paid position. If interested, email us at raise@aisafety.camp**\n\nRequired technical understanding: near-complete.\n\nMinimum commitment: 2 full days per week\n\n**Giving lectures**\n\nYou thoroughly study the material, making sure you know it well enough to explain it clearly. Together with the content developer, you sit down and go over the bits that need explanation. These bits range from [3 to 6 minutes](https://blog.edx.org/optimal-video-length-student-engagement), and they are interleaved with questions and small assignments to keep the student engaged.\n\nCurrently done by: Robert Miles\n\nRequired technical understanding: thorough.\n\nMinimum commitment: 1 full day per week\n\n**Study group attendant**\n\nYou help out in the weekly study group, creating summaries, mind maps, script drafts and assignments. We also give presentations\n\nCurrently done by: Johannes Heidecke, Tom Rutten, Toon Alfrink, Tarn Somervell Fletcher, Nandi Schoots, Roland Pihlakas, Robert Miles, Rupert McCallum, Philine Widmer, Louie Terrill, Tim Bakker, Veerle de Goederen, Ofer Givoli\n\nRequired technical understanding: none\n\nMinimum commitment: 4 hours per week\n\n**Software developer**\n\nWith about 60% certainty, we will use [ihatestatistics](https://ihatestatistics.com/) as a platform. The company is run by EA's (we may use it for free), and it's specialization in statistics (which is closely related to AI) makes it well-suited for our needs. [Here is a demo lesson.](https://play.ihatestatistics.com/#/preview/level/191?lang=en) There are a lot of diamonds buried in the field of automated assessment. The quality of our answer-checking software determines the quality of the questions we can ask. Elaborate feedback mechanisms can make a lot of difference in how fast a learner may converge on the right kind of understanding. You write this software for us.\n\nMinimum commitment: 2 days per week\n\n**Legal**\n\nLegal is a black box. Your first job is to write your job description.\n\n**Marketing/PR/acquisition**\n\nAre you good at connecting people? There are a lot of people that want to fix the world, would engage with this project if they knew about it, and have the means (funding, expertise) to help out. Things you can do include finding funders, hosting a round of review, inviting guest speakers with interesting credentials, connecting with relevant EA organisations, etc. Having high social capital in the EA/LW community is a plus.\n\n**Animation & editing**\n\nGood animation can make a course twice as polished and engaging, and this matters twice as much as you think. The whole point of a course instead of a loose collection of papers is that learners can trust they're on the right track. Polish builds that trust. Animation is also a skill that is hard to pick up in a short enough timeframe, so we can't do it. If you're interested in AI safety and skilled at animation, we need you!\n\nPeptalk\n-------\n\nI want to note that what we are doing here *isn’t hard*. Courses at universities are often created on the fly by one person in a matter of weeks. They get away with it. There is little risk. The worst that can reasonably happen is that we waste some time and money on creating an unpopular course that doesn’t get much traction. On the other hand, there is a lot of opportunity. If we do this well, we might just double the amount of FAI researchers. If that's not impact, I don't know what is.\n\nExternal links\n--------------\n\n*   [list of resources](https://workflowy.com/s/D0Q9.oyUe39KbLp)\n*   [facebook group](https://www.facebook.com/groups/1421511671230776/)\n*   [Announcement post on LessWrong](http://lesswrong.com/r/discussion/lw/p5e/announcing_aasaa_accelerating_ai_safety_adoption/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb354",
    "name": "Crucial Considerations",
    "core": null,
    "slug": "crucial-considerations",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Co2dGXQxHAf92LHea\">Crucial considerations</span></strong><span class=\"by_Co2dGXQxHAf92LHea\"> are ideas that potentially imply radical changes to our world-view and priorities. The term was coined by </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_Co2dGXQxHAf92LHea\">Nick Bostrom</span></a><span class=\"by_Co2dGXQxHAf92LHea\">.</span></p><h2 id=\"External_links\"><span class=\"by_Co2dGXQxHAf92LHea\">External links</span></h2><ul><li><a href=\"http://www.stafforini.com/blog/bostrom/\"><span class=\"by_Co2dGXQxHAf92LHea\">Crucial Considerations and Wise Philanthropy - Nick Bostrom</span></a></li><li><a href=\"https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/\"><span class=\"by_Co2dGXQxHAf92LHea\">Crucial considerations - EA concepts</span></a></li><li><a href=\"http://crucialconsiderations.org/about/\"><span class=\"by_Co2dGXQxHAf92LHea\">The blog \"Crucial Considerations\"</span></a><span class=\"by_Co2dGXQxHAf92LHea\"> (run by </span><a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\"><span class=\"by_Co2dGXQxHAf92LHea\">FRI</span></a><span class=\"by_Co2dGXQxHAf92LHea\">)</span></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 8,
    "description": {
      "markdown": "**Crucial considerations** are ideas that potentially imply radical changes to our world-view and priorities. The term was coined by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom).\n\nExternal links\n--------------\n\n*   [Crucial Considerations and Wise Philanthropy - Nick Bostrom](http://www.stafforini.com/blog/bostrom/)\n*   [Crucial considerations - EA concepts](https://concepts.effectivealtruism.org/concepts/the-importance-of-crucial-considerations/)\n*   [The blog \"Crucial Considerations\"](http://crucialconsiderations.org/about/) (run by [FRI](https://www.lesswrong.com/tag/center-on-long-term-risk-clr))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb345",
    "name": "Criticisms of The Rationalist Movement",
    "core": null,
    "slug": "criticisms-of-the-rationalist-movement",
    "tableOfContents": {
      "html": "<p><strong>Criticisms of the </strong><a href=\"https://www.lesswrong.com/tag/rationalist-movement\">rationalist movement</a> and <a href=\"https://www.lesswrong.com/about\">LessWrong</a> have existed for most of its duration on various grounds.</p><h2 id=\"Cult_of_Rationality\">Cult of Rationality</h2><p>Less Wrong has been referred to as a <a href=\"https://www.lesswrong.com/tag/phyg\"><s>cult</s> phyg</a> on numerous occasions,<a href=\"#fn1\"><sup>1</sup></a><a href=\"#fn2\"><sup>2</sup></a><a href=\"#fn3\"><sup>3</sup></a> with <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> as its leader. Eliezer's confidence in his AI safety work outside of mainstream academia and self-professed intelligence renders him highly unpopular with his critics.<a href=\"#fn4\"><sup>4</sup></a></p><h2 id=\"Neoreaction\">Neoreaction</h2><p>The Neoreaction movement,<a href=\"#fn5\"><sup>5</sup></a> is a notoriously adjacent idea to the community. Whist it has being explicitly refuted by figures such as Eliezer<a href=\"#fn6\"><sup>6</sup></a><a href=\"#fn7\"><sup>7</sup></a> and Scott,<a href=\"#fn8\"><sup>8</sup></a> it is often actively-associated by critics.<a href=\"#fn9\"><sup>9</sup></a><a href=\"#fn10\"><sup>10</sup></a></p><h2 id=\"Rationalism\">Rationalism</h2><p>The movement has been criticized as overemphasizing <a href=\"https://www.lesswrong.com/tag/induction\">inductive reasoning</a> over <a href=\"https://www.lesswrong.com/tag/empiricism\">empiricism</a>,<a href=\"#fn11\"><sup>11</sup></a> a criticism that has been refuted by <a href=\"https://www.lesswrong.com/tag/scott-alexander\">Scott Alexander</a>.<a href=\"#fn12\"><sup>12</sup></a></p><h2 id=\"Roko_s_basilisk\">Roko's basilisk</h2><p>The <a href=\"https://wiki.lesswrong.com/wiki/Roko's_basilisk\">Roko's basilisk</a> thought experiment was notorious in that it required specific preconditions available nearly exclusively within the Less Wrong community that rendered the reader vulnerable to this 'memetic hazard'. As such it has drawn derision from critics who feel perception risk from unfriendly AI is overstated within the community.<a href=\"#fn13\"><sup>13</sup></a><a href=\"#fn14\"><sup>14</sup></a></p><h2 id=\"Transhumanism\">Transhumanism</h2><p>Less Wrong's community was partially founded by soliciting users from the transhumanist <a href=\"https://hpluspedia.org/wiki/SL4#cite_note-1\">SL4</a> mailing list and <a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\">Eliezer Yudkowsky</a> is himself a prominent transhumanist.</p><p>As such, the fringe nature of transhumanist ideas such as <a href=\"https://www.lesswrong.com/tag/cryonics\">cryonics</a>, AGI takeover<a href=\"#fn15\"><sup>15</sup></a> has met with continued scorn from the skeptics based at <a href=\"https://wiki.lesswrong.com/wiki/RationalWiki\">RationalWiki</a>.<a href=\"#fn16\"><sup>16</sup></a></p><h2 id=\"See_also\">See also</h2><ul><li><a href=\"https://www.lesswrong.com/tag/rationalist-movement\">Rationalist movement</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/RationalWiki\">RationalWiki</a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Sneer_Club\">Sneer Club</a></li></ul><h2 id=\"References\">References</h2><ol><li><a href=\"http://lesswrong.com/lw/4d/youre_calling_who_a_cult_leader/\">http://lesswrong.com/lw/4d/youre_calling_who_a_cult_leader/</a><a href=\"#fnref1\">↩</a></li><li><a href=\"http://lesswrong.com/lw/bql/our_phyg_is_not_exclusive_enough/\">http://lesswrong.com/lw/bql/our_phyg_is_not_exclusive_enough/</a><a href=\"#fnref2\">↩</a></li><li><a href=\"https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what_is_lesswrong_and_why_do_people_say_it_is_a/\">https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what_is_lesswrong_and_why_do_people_say_it_is_a/</a><a href=\"#fnref3\">↩</a></li><li><a href=\"http://rationalwiki.org/wiki/Eliezer_Yudkowsky\">http://rationalwiki.org/wiki/Eliezer_Yudkowsky</a><a href=\"#fnref4\">↩</a></li><li><a href=\"https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/\">https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/</a><a href=\"#fnref5\">↩</a></li><li><a href=\"http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and\">http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and</a><a href=\"#fnref6\">↩</a></li><li><a href=\"http://lesswrong.com/lw/fh4/why_is_mencius_moldbug_so_popular_on_less_wrong/\">http://lesswrong.com/lw/fh4/why_is_mencius_moldbug_so_popular_on_less_wrong/</a><a href=\"#fnref7\">↩</a></li><li><a href=\"http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/\">http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/</a><a href=\"#fnref8\">↩</a></li><li><a href=\"http://rationalwiki.org/wiki/Neoreactionary_movement\">http://rationalwiki.org/wiki/Neoreactionary_movement</a><a href=\"#fnref9\">↩</a></li><li><a href=\"https://hpluspedia.org/wiki/The_Silicon_Ideology\">https://hpluspedia.org/wiki/The_Silicon_Ideology</a><a href=\"#fnref10\">↩</a></li><li><a href=\"https://the-orbit.net/almostdiamonds/2014/11/24/why-i-am-not-a-rationalist/\">https://the-orbit.net/almostdiamonds/2014/11/24/why-i-am-not-a-rationalist/</a><a href=\"#fnref11\">↩</a></li><li><a href=\"http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/\">http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/</a><a href=\"#fnref12\">↩</a></li><li><a href=\"http://rationalwiki.org/wiki/Roko's_basilisk\">http://rationalwiki.org/wiki/Roko's_basilisk</a><a href=\"#fnref13\">↩</a></li><li><a href=\"http://idlewords.com/talks/superintelligence.htm\">http://idlewords.com/talks/superintelligence.htm</a><a href=\"#fnref14\">↩</a></li><li><a href=\"http://rationalwiki.org/wiki/Cybernetic_revolt\">http://rationalwiki.org/wiki/Cybernetic_revolt</a><a href=\"#fnref15\">↩</a></li><li><a href=\"http://rationalwiki.org/wiki/Transhumanism\">http://rationalwiki.org/wiki/Transhumanism</a><a href=\"#fnref16\">↩</a></li></ol>",
      "sections": [
        {
          "title": "Cult of Rationality",
          "anchor": "Cult_of_Rationality",
          "level": 1
        },
        {
          "title": "Neoreaction",
          "anchor": "Neoreaction",
          "level": 1
        },
        {
          "title": "Rationalism",
          "anchor": "Rationalism",
          "level": 1
        },
        {
          "title": "Roko's basilisk",
          "anchor": "Roko_s_basilisk",
          "level": 1
        },
        {
          "title": "Transhumanism",
          "anchor": "Transhumanism",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 8
    },
    "postCount": 13,
    "description": {
      "markdown": "**Criticisms of the** [rationalist movement](https://www.lesswrong.com/tag/rationalist-movement) and [LessWrong](https://www.lesswrong.com/about) have existed for most of its duration on various grounds.\n\nCult of Rationality\n-------------------\n\nLess Wrong has been referred to as a [~cult~ phyg](https://www.lesswrong.com/tag/phyg) on numerous occasions,[^1^](#fn1)[^2^](#fn2)[^3^](#fn3) with [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) as its leader. Eliezer's confidence in his AI safety work outside of mainstream academia and self-professed intelligence renders him highly unpopular with his critics.[^4^](#fn4)\n\nNeoreaction\n-----------\n\nThe Neoreaction movement,[^5^](#fn5) is a notoriously adjacent idea to the community. Whist it has being explicitly refuted by figures such as Eliezer[^6^](#fn6)[^7^](#fn7) and Scott,[^8^](#fn8) it is often actively-associated by critics.[^9^](#fn9)[^10^](#fn10)\n\nRationalism\n-----------\n\nThe movement has been criticized as overemphasizing [inductive reasoning](https://www.lesswrong.com/tag/induction) over [empiricism](https://www.lesswrong.com/tag/empiricism),[^11^](#fn11) a criticism that has been refuted by [Scott Alexander](https://www.lesswrong.com/tag/scott-alexander).[^12^](#fn12)\n\nRoko's basilisk\n---------------\n\nThe [Roko's basilisk](https://wiki.lesswrong.com/wiki/Roko's_basilisk) thought experiment was notorious in that it required specific preconditions available nearly exclusively within the Less Wrong community that rendered the reader vulnerable to this 'memetic hazard'. As such it has drawn derision from critics who feel perception risk from unfriendly AI is overstated within the community.[^13^](#fn13)[^14^](#fn14)\n\nTranshumanism\n-------------\n\nLess Wrong's community was partially founded by soliciting users from the transhumanist [SL4](https://hpluspedia.org/wiki/SL4#cite_note-1) mailing list and [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) is himself a prominent transhumanist.\n\nAs such, the fringe nature of transhumanist ideas such as [cryonics](https://www.lesswrong.com/tag/cryonics), AGI takeover[^15^](#fn15) has met with continued scorn from the skeptics based at [RationalWiki](https://wiki.lesswrong.com/wiki/RationalWiki).[^16^](#fn16)\n\nSee also\n--------\n\n*   [Rationalist movement](https://www.lesswrong.com/tag/rationalist-movement)\n*   [RationalWiki](https://wiki.lesswrong.com/wiki/RationalWiki)\n*   [Sneer Club](https://wiki.lesswrong.com/wiki/Sneer_Club)\n\nReferences\n----------\n\n1.  [http://lesswrong.com/lw/4d/youre\\_calling\\_who\\_a\\_cult_leader/](http://lesswrong.com/lw/4d/youre_calling_who_a_cult_leader/)[↩](#fnref1)\n2.  [http://lesswrong.com/lw/bql/our\\_phyg\\_is\\_not\\_exclusive_enough/](http://lesswrong.com/lw/bql/our_phyg_is_not_exclusive_enough/)[↩](#fnref2)\n3.  [https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what\\_is\\_lesswrong\\_and\\_why\\_do\\_people\\_say\\_it\\_is\\_a/](https://www.reddit.com/r/OutOfTheLoop/comments/3ttw2e/what_is_lesswrong_and_why_do_people_say_it_is_a/)[↩](#fnref3)\n4.  [http://rationalwiki.org/wiki/Eliezer_Yudkowsky](http://rationalwiki.org/wiki/Eliezer_Yudkowsky)[↩](#fnref4)\n5.  [https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/](https://web.archive.org/web/20130424060436/http://habitableworlds.wordpress.com/2013/04/21/visualizing-neoreaction/)[↩](#fnref5)\n6.  [http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and](http://yudkowsky.tumblr.com/post/142497361345/this-isnt-going-to-work-but-for-the-record-and)[↩](#fnref6)\n7.  [http://lesswrong.com/lw/fh4/why\\_is\\_mencius\\_moldbug\\_so\\_popular\\_on\\_less\\_wrong/](http://lesswrong.com/lw/fh4/why_is_mencius_moldbug_so_popular_on_less_wrong/)[↩](#fnref7)\n8.  [http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/](http://slatestarcodex.com/2013/10/20/the-anti-reactionary-faq/)[↩](#fnref8)\n9.  [http://rationalwiki.org/wiki/Neoreactionary_movement](http://rationalwiki.org/wiki/Neoreactionary_movement)[↩](#fnref9)\n10.  [https://hpluspedia.org/wiki/The\\_Silicon\\_Ideology](https://hpluspedia.org/wiki/The_Silicon_Ideology)[↩](#fnref10)\n11.  [https://the-orbit.net/almostdiamonds/2014/11/24/why-i-am-not-a-rationalist/](https://the-orbit.net/almostdiamonds/2014/11/24/why-i-am-not-a-rationalist/)[↩](#fnref11)\n12.  [http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/](http://slatestarcodex.com/2014/11/27/why-i-am-not-rene-descartes/)[↩](#fnref12)\n13.  [http://rationalwiki.org/wiki/Roko's_basilisk](http://rationalwiki.org/wiki/Roko's_basilisk)[↩](#fnref13)\n14.  [http://idlewords.com/talks/superintelligence.htm](http://idlewords.com/talks/superintelligence.htm)[↩](#fnref14)\n15.  [http://rationalwiki.org/wiki/Cybernetic_revolt](http://rationalwiki.org/wiki/Cybernetic_revolt)[↩](#fnref15)\n16.  [http://rationalwiki.org/wiki/Transhumanism](http://rationalwiki.org/wiki/Transhumanism)[↩](#fnref16)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb326",
    "name": "Roko's Basilisk",
    "core": null,
    "slug": "rokos-basilisk",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_2aoRX3ookcCozcb3m\">Roko’s basilisk</span></strong><span class=\"by_2aoRX3ookcCozcb3m\"> is a thought experiment proposed in 2010 by the user Roko on the </span><a href=\"\\&quot;https://www.lesswrong.com/about\\&quot;\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\"> community blog. Roko used ideas in </span><a href=\"\\&quot;https://www.lesswrong.com/tag/decision-theory\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">decision theory</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> to argue that a sufficiently powerful AI agent would have an incentive to torture anyone who imagined the agent but didn't work to bring the agent into existence. The argument was called a </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"basilisk\\\" --named after the legendary reptile who can cause death with a single glance--</span><span class=\"by_2aoRX3ookcCozcb3m\">because merely hearing the argument would supposedly put you at risk of torture from this hypothetical agent. A </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">basilisk</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> in this context is any information that harms or endangers the people who hear it.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Roko's argument was broadly rejected on </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, with commenters objecting that an agent like the one Roko was describing would have no real reason to follow through on its threat: once the agent already exists, it will by default just see it as a waste of resources to torture people for their past decisions, since this doesn't causally further its plans. A number of decision algorithms can follow through on acausal threats and promises, via the same methods that permit mutual cooperation in prisoner's dilemmas; but this doesn't imply that such theories can be blackmailed. And following through on blackmail threats against such an algorithm additionally requires a large amount of shared information and trust between the agents, which does not appear to exist in the case of Roko's basilisk.</span></p><p><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\">'s founder, Eliezer Yudkowsky, banned discussion of Roko's basilisk on the blog for several years as part of a general site policy against spreading potential </span><a href=\"\\&quot;https://www.lesswrong.com/tag/information-hazards\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">information hazards</span></a><span class=\"by_2aoRX3ookcCozcb3m\">. This had the opposite of its intended effect: a number of outside websites began sharing information about Roko's basilisk, as the ban attracted attention to this taboo topic. Websites like RationalWiki spread the assumption that Roko's basilisk had been banned because </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> users </span><i><span class=\"by_2aoRX3ookcCozcb3m\">accepted</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> the argument; thus many criticisms of </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> cite Roko's basilisk as evidence that the site's users have unconventional and wrong-headed beliefs.</span></p><h2 id=\"Background\"><span class=\"by_2aoRX3ookcCozcb3m\">Background</span></h2><p><img src=\"\\&quot;https://wiki.lesswrong.com/images/thumb/d/d1/Prisoner%27s_Dilemma_sequence.svg/380px-Prisoner%27s_Dilemma_sequence.svg.png\\&quot;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A visual depiction of a prisoner's dilemma. T denotes the best outcome for a given player, followed by R, then P, then S.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Roko's argument ties together two hotly debated academic topics: Newcomblike problems in decision theory, and normative uncertainty in moral philosophy.</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">One example of a Newcomblike problem is the prisoner's dilemma. This is a two-player game in which each player has two options: </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"cooperate,\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" or </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"defect.\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" By assumption, each player prefers to defect rather than cooperate, all else being equal; but each player also prefers mutual cooperation over mutual defection.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">For example, we could imagine that if both players cooperate, then both get $10; and if both players defect, then both get $1; but if one player defects and the other cooperates, the defector gets $15 and the cooperator gets nothing. (We can equally well construct a prisoner's dilemma </span><a href=\"\\&quot;https://lesswrong.com/lw/tn/the_true_prisoners_dilemma/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">for altruistic agents</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.)</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">One of the basic open problems in decision theory is that standard </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"rational\\\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> agents will end up defecting against each other, even though it would be better for both players if they could somehow enact a binding mutual agreement to cooperate instead.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">In an extreme version of the prisoner's dilemma that draws out the strangeness of mutual defection, one can imagine that one is playing against an identical copy of oneself. Each copy knows that the two copies will play the same move; so the copies know that the only two possibilities are 'we both cooperate' or 'we both defect.' In this situation, cooperation is the better choice; yet causal decision theory (CDT), the most popular theory among working decision theorists, endorses mutual defection in this situation. This is because CDT tacitly assumes that the two agents' choices are independent. It notes that defection is the best option assuming my copy is already definitely going to defect, and that defection is also the best option assuming my copy is already definitely going to cooperate; so, since defection dominates, it defects.</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">In other words, the standard formulation of CDT cannot model scenarios where another agent (or a part of the environment) is correlated with a decision process, except insofar as the decision causes the correlation. The general name for scenarios where CDT fails is </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Newcomblike</span><span class=\"by_2aoRX3ookcCozcb3m\"> problems,</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" and these scenarios are ubiquitous </span></span><a href=\"\\&quot;https://lesswrong.com/lw/l1b/newcomblike_problems_are_the_norm/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">in human interactions</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Eliezer Yudkowsky proposed an alternative to CDT, </span><a href=\"\\&quot;https://www.lesswrong.com/tag/timeless-decision-theory\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">timeless decision theory</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> (TDT), that can achieve mutual cooperation in prisoner's dilemmas — provided both players are running TDT, and both players have common knowledge of this fact. The cryptographer Wei </span><span class=\"by_4SHky5j2PNcRwBiZt\">Dai</span><span class=\"by_2aoRX3ookcCozcb3m\"> subsequently developed a theory that outperforms both TDT and CDT, called </span></span><a href=\"\\&quot;https://www.lesswrong.com/tag/updateless-decision-theory\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">updateless decision theory</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> (UDT).</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Yudkowsky's interest in decision theory stems from his interest in the </span><a href=\"\\&quot;https://www.youtube.com/watch?v=pywF6ZzsghI\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">AI control problem</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">: </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"If</span><span class=\"by_2aoRX3ookcCozcb3m\"> artificially intelligent systems someday come to surpass humans in intelligence, how can we specify safe goals for them to autonomously carry out, and how can we gain high confidence in the agents' reasoning and decision-making?</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" Yudkowsky has argued that in the absence of a full understanding of decision theory, we risk building autonomous systems whose behavior is erratic or difficult to model.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">The control problem also raises questions in moral philosophy: how can we specify the goals of an autonomous agent in the face of human uncertainty about what it is we actually want; and how can we specify such goals in a way that allows for moral progress over time? Yudkowsky's term for a hypothetical algorithm that could autonomously pursue human goals in a way compatible with moral progress is </span><a href=\"\\&quot;https://www.lesswrong.com/tag/coherent-extrapolated-volition\\&quot;\"><i><span class=\"by_2aoRX3ookcCozcb3m\">coherent extrapolated volition</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Because Eliezer Yudkowsky founded </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\"> and was one of the first bloggers on the site, AI theory and </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"acausal\\\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> decision theories — in particular, </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">logical</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> decision theories, which respect logical connections between agents' properties rather than just the causal effects they have on each other — have been repeatedly discussed on </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\">. Roko's basilisk was an attempt to use Yudkowsky's proposed decision theory (TDT) to argue against his informal characterization of an ideal AI goal (humanity's coherently extrapolated volition).</span></p><h2 id=\"Roko_s_post\"><span class=\"by_2aoRX3ookcCozcb3m\">Roko's post</span></h2><p><img src=\"\\&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083971a1080c2784b9a65819394ccebabeddea96df05ea4e.jpeg\\&quot;\" srcset=\"\\&quot;https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083971a1080c2784b9a65819394ccebabeddea96df05ea4e.jpeg/w_100\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">A simple depiction of an agent that cooperates with copies of itself in the one-shot prisoner's dilemma. Adapted from the </span><a href=\"\\&quot;https://lesswrong.com/lw/gu1/decision_theory_faq/\\&quot;\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Decision Theory FAQ</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Two agents that are running a logical decision theory can achieve mutual cooperation in a prisoner's dilemma even if there is no outside force mandating cooperation. Because their decisions take into account correlations that are not caused by either decision (though there is generally some common cause in the past), they can even cooperate if they are separated by large distances in space or time.</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">Roko observed that if two TDT or UDT agents with common knowledge of each other's source code are separated in time, the later agent can (seemingly) blackmail the earlier agent. Call the earlier agent </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Alice\\\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> and the later agent </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Bob.\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" Bob can be an algorithm that outputs things Alice likes if Alice left Bob a large sum of money, and outputs things Alice dislikes otherwise. And since Alice knows Bob's source code </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">exactly</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, she knows this fact about Bob (even though Bob hasn't been born yet). So Alice's knowledge of Bob's source code makes Bob's future threat effective, even though Bob doesn't yet exist: if Alice is certain that Bob will someday exist, then mere knowledge of what Bob </span><i><span class=\"by_2aoRX3ookcCozcb3m\">would</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> do if he could get away with it seems to force Alice to comply with his hypothetical demands.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">If Bob ran CDT, then he would be unable to blackmail Alice. A CDT agent would assume that its decision is independent of Alice's and would not waste resources on rewarding or punishing a once-off decision </span><i><span class=\"by_2aoRX3ookcCozcb3m\">that has already happened</span></i><span class=\"by_2aoRX3ookcCozcb3m\">; and we are assuming that Alice could spot this fact by reading CDT-Bob's source code. A TDT or UDT agent, on the other hand, can recognize that Alice in effect has a copy of Bob's source code in her head (insofar as she is accurately modeling Bob), and that Alice's decision and Bob's decision are therefore correlated — the same as if two copies of the same source code were in a prisoner's dilemma.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Roko raised this point in the context of debates about the possible behaviors and motivations of advanced AI systems. In a July 2010 </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> post, Roko </span><a href=\"\\&quot;http://rationalwiki.org/wiki/Roko%27s_basilisk/Original_post#Solutions_to_the_Altruist.27s_burden:_the_Quantum_Billionaire_Trick\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">wrote</span></a><span class=\"by_2aoRX3ookcCozcb3m\">:</span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">In this vein, there is the ominous possibility that if a positive singularity does occur, the resultant singleton may have precommitted to punish all potential donors who knew about existential risks but who didn't give 100% of their disposable incomes to x-risk motivation. This would act as an incentive to get people to donate more to reducing existential risk, and thereby increase the chances of a positive singularity. This seems to be what CEV (coherent extrapolated volition of humanity) might do if it were an acausal decision-maker.</span></p></blockquote><p><a href=\"\\&quot;https://www.lesswrong.com/tag/singularity\\&quot;\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Singularity</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\"> here refers to an </span><a href=\"\\&quot;https://www.lesswrong.com/tag/intelligence-explosion\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">intelligence explosion</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, and </span><a href=\"\\&quot;https://www.lesswrong.com/tag/singleton\\&quot;\"><i><span class=\"by_2aoRX3ookcCozcb3m\">singleton</span></i></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> refers to a superintelligent AI system. Since a highly moral AI agent (one whose actions are consistent with our coherently extrapolated volition) would want to be created as soon as possible, Roko argued that such an AI would use acausal blackmail to give humans stronger incentives to create it. Roko made the claim that the hypothetical AI agent would particularly target people who had thought about this argument, because they would have a better chance of mentally simulating the AI's source code. Roko added: </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Of</span><span class=\"by_2aoRX3ookcCozcb3m\"> course this would be unjust, but is the kind of unjust thing that is oh-so-very </span></span><a href=\"\\&quot;https://www.lesswrong.com/tag/utilitarianism\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">utilitarian</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">.</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\"</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Roko's conclusion from this was that we should never build any powerful AI agent that reasons like a utilitarian optimizing for humanity's coherently extrapolated values, because this would, paradoxically, be detrimental to human values.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Eliezer Yudkowsky has responded a few times to the substance of Roko's claims. E.g., in a 2014 Reddit thread, Yudkowsky </span><a href=\"\\&quot;https://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjjbqqo\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">wrote</span></a><span class=\"by_2aoRX3ookcCozcb3m\">:</span></p><blockquote><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">What's the truth about Roko's Basilisk? The truth is that making something like this </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"work\\\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">, in the sense of managing to think a thought that would actually give future superintelligences an incentive to hurt you, would require overcoming what seem to me like some pretty huge obstacles.</span></span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">The most blatant obstacle to Roko's Basilisk is, intuitively, that there's no incentive for a future agent to follow through with the threat in the future, because by doing so it just expends resources at no gain to itself. We can formalize that using classical causal decision theory, which is the academically standard decision theory: following through on a blackmail threat, in the future after the past has already taken place, cannot (from the blackmailing agent's perspective) be the physical cause of improved outcomes in the past, because the future cannot be the cause of the past.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">But classical causal decision theory isn't the only decision theory that has ever been invented, and if you were to read up on the academic literature, you would find a lot of challenges to the assertion that, e.g., two rational agents always defect against each other in the one-shot Prisoner's Dilemma.</span></p><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">One of those challenges was a theory of my own invention, which is why this whole fiasco took place on LessWrong.com in the first place. (I feel rather like the speaker of that ancient quote, </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"All</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> my father ever wanted was to make a toaster you could really set the darkness on, and you perverted his work into these horrible machines!</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\") But there have actually been a lot of challenges like that in the literature, not just mine, as anyone actually investigating would have discovered. Lots of people are uncomfortable with the notion that rational agents always defect in the one-shot Prisoner's Dilemma. And if you formalize blackmail, including this case of blackmail, the same way, then most challenges to mutual defection in the Prisoner's Dilemma are also implicitly challenges to the first obvious reason why Roko's Basilisk would never work.</span></span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">But there are also other obstacles. The decision theory I proposed back in the day says that you have to know certain things about the other agent in order to achieve mutual cooperation in the Prisoner's Dilemma, and that's with both parties trying to set up a situation which leads to mutual cooperation instead of mutual defection. As I presently understand the situation, there is literally nobody on Earth, including me, who has the knowledge needed to set themselves up to be blackmailed if they were deliberately trying to make that happen.</span></p><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Any potentially blackmailing AI would much prefer to have you believe that it is blackmailing you, without actually expending resources on following through with the blackmail, insofar as they think they can exert any control on you at all via an exotic decision theory. Just like in the one-shot Prisoner's Dilemma the </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"ideal\\\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> outcome is for the other player to believe you are modeling them and will cooperate if and only if they cooperate, and so they cooperate, but then actually you just defect anyway. For the other player to be confident this will not happen in the Prisoner's Dilemma, for them to expect you not to sneakily defect anyway, they must have some very strong knowledge about you. In the case of Roko's Basilisk, </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"defection\\\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> corresponds to not actually torturing anyone, not expending resources on that, and just letting them believe that you will blackmail them. Two AI agents with sufficiently strong knowledge of each other, and heavily motivated to achieve mutual cooperation on the Prisoner's Dilemma, might be able to overcome this obstacle and cooperate with confidence. But why would you put in that degree of effort — if you even could, which I don't think you as a human can — in order to give a blackmailing agent an incentive to actually carry through on its threats?</span></span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">I have written the above with some reluctance, because even if I don't yet see a way to repair this obstacle myself, somebody else might see how to repair it now that I've said what it is. Which is not a good general procedure for handling infohazards; people with expert knowledge on them should, obviously, as a matter of professional ethics, just never discuss them at all, including describing why a particular proposal doesn't work, just in case there's some unforeseen clever way to repair the proposal. There are other obstacles here which I am not discussing, just in case the logic I described above has a flaw. Nonetheless, so far as I know, Roko's Basilisk does not work, nobody has actually been bitten by it, and everything I have done was in the service of what I thought was the obvious Good General Procedure for Handling Potential Infohazards[.]</span></p></blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">Other users on </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> generally rejected Roko's arguments at the time, and skepticism about his supposed basilisk appears to have only increased with time. Subsequent discussion of Roko's basilisk has focused on </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> moderator responses to Roko's post, rather than on the specific merits or dismerits of his argument.</span></p><h2 id=\"Topic_moderation_and_response\"><span class=\"by_2aoRX3ookcCozcb3m\">Topic moderation and response</span></h2><p><span class=\"by_2aoRX3ookcCozcb3m\">Shortly after Roko made his blog post, Yudkowsky left an angry </span><a href=\"\\&quot;http://rationalwiki.org/wiki/Roko%27s_basilisk/Original_post#Comments_.28117.29\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">comment</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> on the discussion thread:</span></p><blockquote><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">[Roko:] </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"One</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> might think that the possibility of CEV punishing people couldn't possibly be taken seriously enough by anyone to actually motivate them. But in fact one person at SIAI was severely worried by this, to the point of having terrible nightmares, though ve wishes to remain anonymous.</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"</span></span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">I don't usually talk like this, but I'm going to make an exception for this case.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Listen to me very closely, you idiot.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT SUPERINTELLIGENCES CONSIDERING WHETHER OR NOT TO BLACKMAIL YOU. THAT IS THE ONLY POSSIBLE THING WHICH GIVES THEM A MOTIVE TO FOLLOW THROUGH ON THE BLACKMAIL.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">There's an obvious equilibrium to this problem where you engage in all positive acausal trades and ignore all attempts at acausal blackmail. Until we have a better worked-out version of TDT and we can prove that formally, it should just be OBVIOUS that you DO NOT THINK ABOUT DISTANT BLACKMAILERS in SUFFICIENT DETAIL that they have a motive toACTUALLY [</span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">sic</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">] BLACKMAIL YOU.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">If there is any part of this acausal trade that is positive-sum and actually worth doing, that is exactly the sort of thing you leave up to an FAI. We probably also have the FAI take actions that cancel out the impact of anyone motivated by true rather than imagined blackmail, so as to obliterate the motive of any superintelligences to engage in blackmail.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Meanwhile I'm banning this post so that it doesn't (a) give people horrible nightmares and (b) give distant superintelligences a motive to follow through on blackmail against people dumb enough to think about them in sufficient detail, though, thankfully, I doubt anyone dumb enough to do this knows the sufficient detail. (I'm not sure I know the sufficient detail.)</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">You have to be really clever to come up with a genuinely dangerous thought. I am disheartened that people can be clever enough to do that and not clever enough to do the obvious thing and KEEP THEIR IDIOT MOUTHS SHUT about it, because it is much more important to sound intelligent when talking to your friends. This post was STUPID.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">(For those who have no idea why I'm using capital letters for something that just sounds like a random crazy idea, and worry that it means I'm as crazy as Roko, the gist of it was that he just did something that potentially gives superintelligences an increased motive to do extremely evil things in an attempt to blackmail us. It is the sort of thing you want to be EXTREMELY CONSERVATIVE about NOT DOING.)</span></p></blockquote><p><span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\"</span><span class=\"by_XkzbJegfxyppjSdQh\">FAI\\\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> here stands for </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Friendly</span><span class=\"by_2aoRX3ookcCozcb3m\"> AI,</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" a hypothetical superintelligent AI agent that can be trusted to autonomously promote desirable ends. Yudkowsky rejected the idea that Roko's basilisk could be called </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"friendly\\\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> or </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"utilitarian,\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" since torture and threats of blackmail are themselves contrary to common human values. Separately, Yudkowsky doubted that humans possessed enough information about any hypothetical unfriendly AI system to enter Alice's position even if we tried. Yudkowsky additionally argued that a </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">well-designed</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> version of Alice would precommit to resisting blackmail from Bob, while still accepting positive-sum </span><a href=\"\\&quot;https://www.lesswrong.com/tag/acausal-trade\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">acausal trades</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> (e.g., ordinary contracts).</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">Yudkowsky proceeded to delete Roko's post and the ensuing discussion, while banning further discussion of the topic on the blog. A few months later, </span><span class=\"by_KneTmopEjYGsaPYNi\">an anonymous editor</span><span class=\"by_2aoRX3ookcCozcb3m\"> </span></span><a href=\"\\&quot;http://rationalwiki.org/w/index.php?title=LessWrong&amp;diff=656172&amp;oldid=647467\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">added</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> a discussion of Roko's basilisk to an article covering </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\">. </span><span class=\"by_KneTmopEjYGsaPYNi\">The editor</span><span class=\"by_2aoRX3ookcCozcb3m\"> inferred from Yudkowsky's comments that people on </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> accepted Roko's argument:</span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">There is apparently a idea so horrible, so utterly Cuthulian (sic) in nature that it needs to be censored for our sanity. Simply knowing about it makes it more likely of becoming true in the real world. Elizer Yudkwosky and the other great rationalist keep us safe by deleting any posts with this one evil idea. Yes they really do believe that. Occasionally a poster will complain off topic about the idea being deleted.</span></p></blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">Over time, RationalWiki's Roko's basilisk discussion expanded into its own article. Editors had </span><a href=\"\\&quot;https://www.reddit.com/r/xkcd/comments/2myg86/xkcd_1450_aibox_experiment/cm8vn6e\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">difficulty</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> interpreting Roko's reasoning, thinking that Roko's argument was intended to promote Yudkowsky's AI program rather than to criticize it. Since discussion of the topic was still banned on </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, the main source for information about the incident continued to be the coverage on RationalWiki for several years. As a further consequence of the ban, no explanations were given about the details of Roko's argument or the views of </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> users. This generated a number of criticisms of </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\">'s forum moderation policies.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Interest in the topic increased over subsequent years. In 2014, Roko's basilisk was name-dropped in the webcomic </span><a href=\"\\&quot;https://xkcd.com/1450/\\&quot;\"><i><span class=\"by_2aoRX3ookcCozcb3m\">xkcd</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">. The magazine </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Slate</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\"> ran an article on the thought experiment, titled </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"</span></span><a href=\"\\&quot;http://www.slate.com/articles/technology/bitwise/2014/07/roko_s_basilisk_the_most_terrifying_thought_experiment_of_all_time.single.html\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">The Most Terrifying Thought Experiment of All Time</span></a><span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\":</span></span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">You may be wondering why this is such a big deal for the LessWrong people, given the apparently far-fetched nature of the thought experiment. It’s not that Roko’s Basilisk will necessarily materialize, or is even likely to. It’s more that if you’ve committed yourself to timeless decision theory, then thinking about this sort of trade literally makes it more likely to happen. After all, if Roko’s Basilisk were to see that this sort of blackmail gets you to help it come into existence, then it would, as a rational actor, blackmail you. The problem isn’t with the Basilisk itself, but with you. Yudkowsky doesn’t censor every mention of Roko’s Basilisk because he believes it exists or will exist, but because he believes that the idea of the Basilisk (and the ideas behind it) is dangerous.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Now, Roko’s Basilisk is only dangerous if you believe all of the above preconditions and commit to making the two-box deal [</span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">sic</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">] with the Basilisk. But at least some of the LessWrong members do believe all of the above, which makes Roko’s Basilisk quite literally forbidden knowledge. [...]</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">If you do not subscribe to the theories that underlie Roko’s Basilisk and thus feel no temptation to bow down to your once and future evil machine overlord, then Roko’s Basilisk poses you no threat. (It is ironic that it’s only a mental health risk to those who have already bought into Yudkowsky’s thinking.) Believing in Roko’s Basilisk may simply be a “referendum on autism,” as a friend put it.</span></p></blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">Other sources have repeated the claim that </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> users think Roko's basilisk is a serious concern. However, none of these sources have yet cited supporting evidence on this point, aside from </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> moderation activity itself. (The ban, of course, didn't make it easy to collect good information.)</span></p><p><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> user Gwern </span><a href=\"\\&quot;https://www.reddit.com/r/LessWrong/comments/17y819/lw_uncensored_thread/c8bbcy4\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">reports</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> that </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Only</span><span class=\"by_2aoRX3ookcCozcb3m\"> a few LWers seem to take the basilisk very seriously,</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" adding, </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"It'</span><span class=\"by_2aoRX3ookcCozcb3m\">s funny how everyone seems to know all about who is affected by the Basilisk and how exactly, when they don't know any such people and they're talking to counterexamples to their confident claims.</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\"</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Yudkowsky subsequently went into more detail about his thought processes </span><a href=\"\\&quot;https://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjjbqqo\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">on Reddit</span></a><span class=\"by_2aoRX3ookcCozcb3m\">:</span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">When Roko posted about the Basilisk, I very foolishly yelled at him, called him an idiot, and then deleted the post.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Why I did that is not something you have direct access to, and thus you should be careful about Making Stuff Up, especially when there are Internet trolls who are happy to tell you in a loud authoritative voice what I was thinking, despite having never passed anything even close to an Ideological Turing Test on Eliezer Yudkowsky.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Why I yelled at Roko: Because I was caught flatfooted in surprise, because I was indignant to the point of genuine emotional shock, at the concept that somebody who thought they'd invented a brilliant idea that would cause future AIs to torture people who had the thought, had promptly posted it to the public Internet. In the course of yelling at Roko to explain why this was a bad thing, I made the further error — keeping in mind that I had absolutely no idea that any of this would ever blow up the way it did, if I had I would obviously have kept my fingers quiescent — of not making it absolutely clear using lengthy disclaimers that my yelling did not mean that I believed Roko was right about CEV-based agents torturing people who had heard about Roko's idea. It was obvious to me that no CEV-based agent would ever do that and equally obvious to me that the part about CEV was just a red herring; I more or less automatically pruned it from my processing of the suggestion and automatically generalized it to cover the entire class of similar scenarios and variants, variants which I considered obvious despite significant divergences (I forgot that other people were not professionals in the field). This class of all possible variants did strike me as potentially dangerous as a collective group, even though it did not occur to me that Roko's original scenario might be right — that was obviously wrong, so my brain automatically generalized it. [...]</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">What I considered to be obvious common sense was that you did not spread potential information hazards because it would be a crappy thing to do to someone. The problem wasn't Roko's post itself, about CEV, being correct. That thought never occurred to me for a fraction of a second. The problem was that Roko's post seemed near in idea-space to a large class of potential hazards, all of which, regardless of their plausibility, had the property that they presented no potential benefit to anyone. They were pure infohazards. The only thing they could possibly do was be detrimental to brains that represented them, if one of the possible variants of the idea turned out to be repairable of the obvious objections and defeaters. So I deleted it, because on my worldview there was no reason not to. I did not want LessWrong.com to be a place where people were exposed to potential infohazards because somebody like me thought they were being clever about reasoning that they probably weren't infohazards. On my view, the key fact about Roko's Basilisk wasn't that it was plausible, or implausible, the key fact was just that shoving it in people's faces seemed like a fundamentally crap thing to do because there was no upside.</span></p><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Again, I deleted that post not because I had decided that this thing probably presented a real hazard, but because I was afraid some unknown variant of it might, and because it seemed to me like the obvious General Procedure For Handling Things That Might Be Infohazards said you shouldn't post them to the Internet. If you look at the original SF story where the term </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"basilisk\\\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> was coined, it's about a mind-erasing image and the.... trolls, I guess, though the story predates modern trolling, who go around spraypainting the Basilisk on walls, using computer guidance so they don't know themselves what the Basilisk looks like, in hopes the Basilisk will erase some innocent mind, for the lulz. These people are the villains of the story. The good guys, of course, try to erase the Basilisk from the walls. Painting Basilisks on walls is a crap thing to do. Since there was no upside to being exposed to Roko's Basilisk, its probability of being true was irrelevant. And Roko himself had thought this was a thing that might actually work. So I yelled at Roko for violating basic sanity about infohazards for stupid reasons, and then deleted the post. He, by his own lights, had violated the obvious code for the ethical handling of infohazards, conditional on such things existing, and I was indignant about this.</span></span></p></blockquote><h2 id=\"Big_picture_questions\"><span class=\"by_2aoRX3ookcCozcb3m\">Big-picture questions</span></h2><p><span class=\"by_2aoRX3ookcCozcb3m\">Several other questions are raised by Roko's basilisk, beyond the merits of Roko's original argument or </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\">'s moderation policies:</span></p><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">Can formal decision agents be designed to resist blackmail?</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">Are information hazards a serious risk, and are there better ways of handling them?</span></li><li><span><span class=\"by_2aoRX3ookcCozcb3m\">Does the oversimplified coverage of Roko's argument suggest that </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"weird\\\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> philosophical topics are big liabilities for pedagogical or research-related activities?</span></span></li></ul><p><strong id=\"Blackmail_resistant_decision_theories\"><span class=\"by_2aoRX3ookcCozcb3m\">Blackmail-resistant decision theories</span></strong></p><p><span class=\"by_2aoRX3ookcCozcb3m\">The general ability to cooperate in prisoner's dilemmas appears to be useful. If other agents know that you won't betray them as soon as it's in your best interest to do so — if you've made a promise or signed a contract, and they know that you can be trusted to stick to such agreements even in the absence of coercion — then a large number of mutually beneficial transactions will be possible. If there is some way for agents to acquire evidence about each other's trustworthiness, then the more trustworthy agents will benefit.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">At the same time, introducing new opportunities for contracts and collaborations introduces new opportunities for blackmail. An agent that can pre-commit to following through on a promise (even when this is no longer in its short-term interest) can also pre-commit to following through on a costly threat.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">It appears that the best general-purpose response is to credibly precommit to never giving in to any blackmailer's demands (even when there are short-term advantages to doing so). This makes it much likelier that one will never be blackmailed in the first place, just as credibly precommitting to stick to trade agreements (even when there are short-term </span><i><span class=\"by_2aoRX3ookcCozcb3m\">disadvantages</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> to doing so) makes it much likelier that one </span><i><span class=\"by_2aoRX3ookcCozcb3m\">will</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> be approached as a trading partner.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">One way to generalize this point is to adopt the </span><a href=\"\\&quot;https://forum.intelligence.org/item?id=160\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">rule of thumb</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> of behaving in whatever way is recommended by the most generally useful policy. This is the distinguishing feature of </span><a href=\"\\&quot;https://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">the most popular version</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> of UDT. Standard UDT selects the best available policy (mapping of observations to actions) rather than the best available action. In this way, UDT avoids selecting a strategy that other agents will have an especially easy time manipulating. UDT itself, however, is not fully formalized, and there may be some superior decision theory. No one has yet formally solved decision theory, or the particular problem of defining a blackmail-free equilibrium.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">It hasn't been formally demonstrated that any logical decision theories give in to blackmail, or what scenarios would make them vulnerable to blackmail. If it turned out that TDT or UDT were blackmailable, this would suggest that they aren't normatively optimal decision theories. For more background on open problems in decision theory, see the </span><a href=\"\\&quot;https://lesswrong.com/lw/gu1/decision_theory_faq/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Decision Theory FAQ</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> and </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"</span></span><a href=\"\\&quot;https://intelligence.org/files/TowardIdealizedDecisionTheory.pdf\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Toward Idealized Decision Theory</span></a><span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\".</span></span></p><p><strong id=\"Utility_function_inverters\"><span class=\"by_QjtpTHkG6xdC4SWq3\">Utility function inverters</span></strong></p><p><span class=\"by_QjtpTHkG6xdC4SWq3\">Because the basilisk threatens its blackmail targets with torture, it is a type of \\\"utility function inverter\\\": agents that seek to additionally pressure others by threatening to invert the non-compliant party's utility function. </span><a href=\"https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem?commentId=tYBPjetgZW4iMqe4s\"><span class=\"by_QjtpTHkG6xdC4SWq3\">Yudkowsky argues</span></a><span class=\"by_QjtpTHkG6xdC4SWq3\"> that sane, rational entities ought to be strongly opposed to utility function inverters by dint of not wanting to live in a reality where such tactics are commonly part of negotiations, though Yudkowsky did so as a comment about the irrationality of commitment races, not about Roko's basilisk:</span></p><blockquote><p><span class=\"by_QjtpTHkG6xdC4SWq3\">IMO, commitment races only occur between agents who will, in some sense, act like idiots, if presented with an apparently 'committed' agent. &nbsp;If somebody demands $6 from me in the Ultimatum game, threatening to leave us both with $0 unless I offer at least $6 to them... then I offer $6 with slightly less than 5/6 probability, so they do no better than if they demanded $5, the amount I think is fair. &nbsp;They cannot evade that by trying to make some 'commitment' earlier than I do. &nbsp;I expect that, whatever is the correct and sane version of this reasoning, it generalizes across all the cases.</span></p><p><span class=\"by_QjtpTHkG6xdC4SWq3\">I am not locked into warfare with things that demand $6 instead of $5. &nbsp;I do not go around figuring out how to invert their utility function for purposes of threatening them back - 'destroy all utility-function inverters (but do not invert their own utility functions)' was my guessed commandment that would be taught to kids in dath ilan, because you don't want reality to end up full of utilityfunction inverters.</span></p></blockquote><p><strong id=\"Information_hazards\"><span class=\"by_2aoRX3ookcCozcb3m\">Information hazards</span></strong></p><p><span class=\"by_2aoRX3ookcCozcb3m\">David Langford coined the term </span><i><span class=\"by_2aoRX3ookcCozcb3m\">basilisk</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\">, in the sense of an information hazard that directly harms anyone who perceives it, in the 1988 science fiction story </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"</span></span><a href=\"\\&quot;https://en.wikipedia.org/wiki/BLIT_(short_story)\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">BLIT</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">.</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" On a societal level, examples of real-world information hazards include the dissemination of specifications for dangerous technologies; on an individual level, examples include triggers for stress or anxiety disorders.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">The Roko's basilisk incident suggests that information that is deemed dangerous or taboo is more likely to be spread rapidly. Parallels can be drawn to </span><a href=\"\\&quot;https://en.wikipedia.org/wiki/Shock_site\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">shock site</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> and </span><a href=\"\\&quot;https://en.wikipedia.org/wiki/Creepypasta\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">creepypasta</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> links: many people have their interest piqued by such topics, and people also enjoy pranking each other by spreading purportedly harmful links. Although Roko's basilisk was never genuinely dangerous, real information hazards might propagate in a similar way, especially if the risks are non-obvious.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Non-specialists spread Roko's argument widely without first investigating the associated risks and benefits in any serious way. One take-away is that someone in possession of a serious information hazard should exercise caution in visibly censoring or suppressing it (cf. the </span><a href=\"\\&quot;https://en.wikipedia.org/wiki/Streisand_effect\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Streisand effect</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">). </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"</span></span><a href=\"\\&quot;https://www.nickbostrom.com/information-hazards.pdf\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Information Hazards: A Typology of Potential Harms from Knowledge</span></a><span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" notes: </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"In</span><span class=\"by_2aoRX3ookcCozcb3m\"> many cases, the best response is no response, i.e., to proceed as though no such hazard existed.</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" This also means that additional care may need to be taken in keeping risky information under wraps; retracting information that has been published to highly trafficked websites is often difficult or impossible. However, Roko's basilisk is an isolated incident (and an unusual one at that); it may not be possible to draw any strong conclusions without looking at a number of other examples.</span></span></p><p><strong id=\"__Weirdness_points__\"><span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\"Weirdness </span><span class=\"by_XkzbJegfxyppjSdQh\">points\\\"</span></span></strong></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">Peter Hurford argues in </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"</span></span><a href=\"\\&quot;http://effective-altruism.com/ea/bg/you_have_a_set_amount_of_weirdness_points_spend/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">You Have a Set Amount of Weirdness Points; Spend Them Wisely</span></a><span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" that promoting or talking about too many nonstandard ideas simultaneously makes it much less likely that any one of the ideas will be taken seriously. Advocating for any one of veganism, anarchism, or mind-body dualism is difficult enough on its own; discussing all three at once increases the odds that a skeptical interlocutor will write you off as 'just generally prone to having weird beliefs.' Roko's basilisk appears to be an example of this phenomenon: long-term AI safety issues, acausal trade, and a number of other popular </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">Less Wrong</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> ideas are all highly unusual in their own right, and their combination is stranger than the sum of its parts.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">On the other hand, </span><a href=\"\\&quot;https://thingofthings.wordpress.com/2015/04/14/on-weird-points/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Ozy Frantz argues</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> that looking weird can attract an audience that is open to new and unconventional ideas:</span></p><blockquote><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">[I]magine that you mostly endorse positions that your audience already agrees with, positions that are within a standard deviation of the median position on the issue, and then you finally gather up all your cherished, saved-up weirdness points and write a passionate defense of the importance of insect suffering. How do you think your audience is going to react? </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"Ugh,</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> they used to be so normal, and then it was like they suddenly went crazy. I hope they go back to bashing the Rethuglicans soon.</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"</span></span></p></blockquote><p><span><span class=\"by_2aoRX3ookcCozcb3m\">In </span><span class=\"by_XkzbJegfxyppjSdQh\">\\\"</span></span><a href=\"\\&quot;https://meteuphoric.wordpress.com/2015/03/08/the-economy-of-weirdness/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">The Economy of Weirdness</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">,</span><span class=\"by_XkzbJegfxyppjSdQh\">\\</span><span class=\"by_2aoRX3ookcCozcb3m\">\" Katja Grace paints a more complicated painting of the advantages and disadvantages of weirdness. Communities with different goals and different demographics will plausibly vary in how 'normal' they should try to look, and in what the relevant kind of normality is. E.g., if the goal is to get more people interested in AI control problems, then weird ideas like Roko's basilisk may drive away conventional theoretical computer scientists, but they may also attract people who favor (or are indifferent to) unorthodox ideas.</span></span></p><h2 id=\"See_also\"><span class=\"by_2aoRX3ookcCozcb3m\">See also</span></h2><ul><li><a href=\"\\&quot;https://intelligence.org/research-guide/#four\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Decision Theory Readings</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> in MIRI's Research Guide</span></li><li><a href=\"\\&quot;https://plato.stanford.edu/entries/decision-causal/\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Causal Decision Theory</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> in the </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Stanford Encyclopedia of Philosophy</span></i></li><li><a href=\"\\&quot;https://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.html\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Thinking Inside the Boxes</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> in </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Slate</span></i></li><li><a href=\"\\&quot;https://www.lesswrong.com/tag/newcomb-s-problem\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Newcomb's problem</span></a></li><li><a href=\"\\&quot;https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\\&quot;\"><span class=\"by_2aoRX3ookcCozcb3m\">Parfit's hitchhiker</span></a></li><li><a href=\"\\&quot;https://www.lesswrong.com/tag/acausal-trade\\&quot;\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Acausal Trade</span></a></li></ul>",
      "sections": [
        {
          "title": "Background",
          "anchor": "Background",
          "level": 1
        },
        {
          "title": "Roko's post",
          "anchor": "Roko_s_post",
          "level": 1
        },
        {
          "title": "Topic moderation and response",
          "anchor": "Topic_moderation_and_response",
          "level": 1
        },
        {
          "title": "Big-picture questions",
          "anchor": "Big_picture_questions",
          "level": 1
        },
        {
          "title": "Blackmail-resistant decision theories",
          "anchor": "Blackmail_resistant_decision_theories",
          "level": 2
        },
        {
          "title": "Utility function inverters",
          "anchor": "Utility_function_inverters",
          "level": 2
        },
        {
          "title": "Information hazards",
          "anchor": "Information_hazards",
          "level": 2
        },
        {
          "title": "\\\"Weirdness points\\\"",
          "anchor": "__Weirdness_points__",
          "level": 2
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 10
    },
    "postCount": 8,
    "description": {
      "markdown": "**Roko’s basilisk** is a thought experiment proposed in 2010 by the user Roko on the [*Less Wrong*](\\\"https://www.lesswrong.com/about\\\") community blog. Roko used ideas in [decision theory](\\\"https://www.lesswrong.com/tag/decision-theory\\\") to argue that a sufficiently powerful AI agent would have an incentive to torture anyone who imagined the agent but didn't work to bring the agent into existence. The argument was called a \\\\\"basilisk\\\\\" --named after the legendary reptile who can cause death with a single glance--because merely hearing the argument would supposedly put you at risk of torture from this hypothetical agent. A *basilisk* in this context is any information that harms or endangers the people who hear it.\n\nRoko's argument was broadly rejected on *Less Wrong*, with commenters objecting that an agent like the one Roko was describing would have no real reason to follow through on its threat: once the agent already exists, it will by default just see it as a waste of resources to torture people for their past decisions, since this doesn't causally further its plans. A number of decision algorithms can follow through on acausal threats and promises, via the same methods that permit mutual cooperation in prisoner's dilemmas; but this doesn't imply that such theories can be blackmailed. And following through on blackmail threats against such an algorithm additionally requires a large amount of shared information and trust between the agents, which does not appear to exist in the case of Roko's basilisk.\n\n*Less Wrong*'s founder, Eliezer Yudkowsky, banned discussion of Roko's basilisk on the blog for several years as part of a general site policy against spreading potential [information hazards](\\\"https://www.lesswrong.com/tag/information-hazards\\\"). This had the opposite of its intended effect: a number of outside websites began sharing information about Roko's basilisk, as the ban attracted attention to this taboo topic. Websites like RationalWiki spread the assumption that Roko's basilisk had been banned because *Less Wrong* users *accepted* the argument; thus many criticisms of *Less Wrong* cite Roko's basilisk as evidence that the site's users have unconventional and wrong-headed beliefs.\n\nBackground\n----------\n\n![](\\\"https://wiki.lesswrong.com/images/thumb/d/d1/Prisoner%27s_Dilemma_sequence.svg/380px-Prisoner%27s_Dilemma_sequence.svg.png\\\")A visual depiction of a prisoner's dilemma. T denotes the best outcome for a given player, followed by R, then P, then S.\n\nRoko's argument ties together two hotly debated academic topics: Newcomblike problems in decision theory, and normative uncertainty in moral philosophy.\n\nOne example of a Newcomblike problem is the prisoner's dilemma. This is a two-player game in which each player has two options: \\\\\"cooperate,\\\\\" or \\\\\"defect.\\\\\" By assumption, each player prefers to defect rather than cooperate, all else being equal; but each player also prefers mutual cooperation over mutual defection.\n\nFor example, we could imagine that if both players cooperate, then both get $10; and if both players defect, then both get $1; but if one player defects and the other cooperates, the defector gets $15 and the cooperator gets nothing. (We can equally well construct a prisoner's dilemma [for altruistic agents](\\\"https://lesswrong.com/lw/tn/the_true_prisoners_dilemma/\\\").)\n\nOne of the basic open problems in decision theory is that standard \\\\\"rational\\\\\" agents will end up defecting against each other, even though it would be better for both players if they could somehow enact a binding mutual agreement to cooperate instead.\n\nIn an extreme version of the prisoner's dilemma that draws out the strangeness of mutual defection, one can imagine that one is playing against an identical copy of oneself. Each copy knows that the two copies will play the same move; so the copies know that the only two possibilities are 'we both cooperate' or 'we both defect.' In this situation, cooperation is the better choice; yet causal decision theory (CDT), the most popular theory among working decision theorists, endorses mutual defection in this situation. This is because CDT tacitly assumes that the two agents' choices are independent. It notes that defection is the best option assuming my copy is already definitely going to defect, and that defection is also the best option assuming my copy is already definitely going to cooperate; so, since defection dominates, it defects.\n\nIn other words, the standard formulation of CDT cannot model scenarios where another agent (or a part of the environment) is correlated with a decision process, except insofar as the decision causes the correlation. The general name for scenarios where CDT fails is \\\\\"Newcomblike problems,\\\\\" and these scenarios are ubiquitous [in human interactions](\\\"https://lesswrong.com/lw/l1b/newcomblike_problems_are_the_norm/\\\").\n\nEliezer Yudkowsky proposed an alternative to CDT, [timeless decision theory](\\\"https://www.lesswrong.com/tag/timeless-decision-theory\\\") (TDT), that can achieve mutual cooperation in prisoner's dilemmas — provided both players are running TDT, and both players have common knowledge of this fact. The cryptographer Wei Dai subsequently developed a theory that outperforms both TDT and CDT, called [updateless decision theory](\\\"https://www.lesswrong.com/tag/updateless-decision-theory\\\") (UDT).\n\nYudkowsky's interest in decision theory stems from his interest in the [AI control problem](\\\"https://www.youtube.com/watch?v=pywF6ZzsghI\\\"): \\\\\"If artificially intelligent systems someday come to surpass humans in intelligence, how can we specify safe goals for them to autonomously carry out, and how can we gain high confidence in the agents' reasoning and decision-making?\\\\\" Yudkowsky has argued that in the absence of a full understanding of decision theory, we risk building autonomous systems whose behavior is erratic or difficult to model.\n\nThe control problem also raises questions in moral philosophy: how can we specify the goals of an autonomous agent in the face of human uncertainty about what it is we actually want; and how can we specify such goals in a way that allows for moral progress over time? Yudkowsky's term for a hypothetical algorithm that could autonomously pursue human goals in a way compatible with moral progress is [*coherent extrapolated volition*](\\\"https://www.lesswrong.com/tag/coherent-extrapolated-volition\\\").\n\nBecause Eliezer Yudkowsky founded *Less Wrong* and was one of the first bloggers on the site, AI theory and \\\\\"acausal\\\\\" decision theories — in particular, *logical* decision theories, which respect logical connections between agents' properties rather than just the causal effects they have on each other — have been repeatedly discussed on *Less Wrong*. Roko's basilisk was an attempt to use Yudkowsky's proposed decision theory (TDT) to argue against his informal characterization of an ideal AI goal (humanity's coherently extrapolated volition).\n\nRoko's post\n-----------\n\n![](\\\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/083971a1080c2784b9a65819394ccebabeddea96df05ea4e.jpeg\\\")A simple depiction of an agent that cooperates with copies of itself in the one-shot prisoner's dilemma. Adapted from the [Decision Theory FAQ](\\\"https://lesswrong.com/lw/gu1/decision_theory_faq/\\\").\n\nTwo agents that are running a logical decision theory can achieve mutual cooperation in a prisoner's dilemma even if there is no outside force mandating cooperation. Because their decisions take into account correlations that are not caused by either decision (though there is generally some common cause in the past), they can even cooperate if they are separated by large distances in space or time.\n\nRoko observed that if two TDT or UDT agents with common knowledge of each other's source code are separated in time, the later agent can (seemingly) blackmail the earlier agent. Call the earlier agent \\\\\"Alice\\\\\" and the later agent \\\\\"Bob.\\\\\" Bob can be an algorithm that outputs things Alice likes if Alice left Bob a large sum of money, and outputs things Alice dislikes otherwise. And since Alice knows Bob's source code *exactly*, she knows this fact about Bob (even though Bob hasn't been born yet). So Alice's knowledge of Bob's source code makes Bob's future threat effective, even though Bob doesn't yet exist: if Alice is certain that Bob will someday exist, then mere knowledge of what Bob *would* do if he could get away with it seems to force Alice to comply with his hypothetical demands.\n\nIf Bob ran CDT, then he would be unable to blackmail Alice. A CDT agent would assume that its decision is independent of Alice's and would not waste resources on rewarding or punishing a once-off decision *that has already happened*; and we are assuming that Alice could spot this fact by reading CDT-Bob's source code. A TDT or UDT agent, on the other hand, can recognize that Alice in effect has a copy of Bob's source code in her head (insofar as she is accurately modeling Bob), and that Alice's decision and Bob's decision are therefore correlated — the same as if two copies of the same source code were in a prisoner's dilemma.\n\nRoko raised this point in the context of debates about the possible behaviors and motivations of advanced AI systems. In a July 2010 *Less Wrong* post, Roko [wrote](\\\"http://rationalwiki.org/wiki/Roko%27s_basilisk/Original_post#Solutions_to_the_Altruist.27s_burden:_the_Quantum_Billionaire_Trick\\\"):\n\n> In this vein, there is the ominous possibility that if a positive singularity does occur, the resultant singleton may have precommitted to punish all potential donors who knew about existential risks but who didn't give 100% of their disposable incomes to x-risk motivation. This would act as an incentive to get people to donate more to reducing existential risk, and thereby increase the chances of a positive singularity. This seems to be what CEV (coherent extrapolated volition of humanity) might do if it were an acausal decision-maker.\n\n[*Singularity*](\\\"https://www.lesswrong.com/tag/singularity\\\") here refers to an [intelligence explosion](\\\"https://www.lesswrong.com/tag/intelligence-explosion\\\"), and [*singleton*](\\\"https://www.lesswrong.com/tag/singleton\\\") refers to a superintelligent AI system. Since a highly moral AI agent (one whose actions are consistent with our coherently extrapolated volition) would want to be created as soon as possible, Roko argued that such an AI would use acausal blackmail to give humans stronger incentives to create it. Roko made the claim that the hypothetical AI agent would particularly target people who had thought about this argument, because they would have a better chance of mentally simulating the AI's source code. Roko added: \\\\\"Of course this would be unjust, but is the kind of unjust thing that is oh-so-very [utilitarian](\\\"https://www.lesswrong.com/tag/utilitarianism\\\").\\\\\"\n\nRoko's conclusion from this was that we should never build any powerful AI agent that reasons like a utilitarian optimizing for humanity's coherently extrapolated values, because this would, paradoxically, be detrimental to human values.\n\nEliezer Yudkowsky has responded a few times to the substance of Roko's claims. E.g., in a 2014 Reddit thread, Yudkowsky [wrote](\\\"https://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjjbqqo\\\"):\n\n> What's the truth about Roko's Basilisk? The truth is that making something like this \\\\\"work\\\\\", in the sense of managing to think a thought that would actually give future superintelligences an incentive to hurt you, would require overcoming what seem to me like some pretty huge obstacles.\n> \n> The most blatant obstacle to Roko's Basilisk is, intuitively, that there's no incentive for a future agent to follow through with the threat in the future, because by doing so it just expends resources at no gain to itself. We can formalize that using classical causal decision theory, which is the academically standard decision theory: following through on a blackmail threat, in the future after the past has already taken place, cannot (from the blackmailing agent's perspective) be the physical cause of improved outcomes in the past, because the future cannot be the cause of the past.\n> \n> But classical causal decision theory isn't the only decision theory that has ever been invented, and if you were to read up on the academic literature, you would find a lot of challenges to the assertion that, e.g., two rational agents always defect against each other in the one-shot Prisoner's Dilemma.\n> \n> One of those challenges was a theory of my own invention, which is why this whole fiasco took place on LessWrong.com in the first place. (I feel rather like the speaker of that ancient quote, \\\\\"All my father ever wanted was to make a toaster you could really set the darkness on, and you perverted his work into these horrible machines!\\\\\") But there have actually been a lot of challenges like that in the literature, not just mine, as anyone actually investigating would have discovered. Lots of people are uncomfortable with the notion that rational agents always defect in the one-shot Prisoner's Dilemma. And if you formalize blackmail, including this case of blackmail, the same way, then most challenges to mutual defection in the Prisoner's Dilemma are also implicitly challenges to the first obvious reason why Roko's Basilisk would never work.\n> \n> But there are also other obstacles. The decision theory I proposed back in the day says that you have to know certain things about the other agent in order to achieve mutual cooperation in the Prisoner's Dilemma, and that's with both parties trying to set up a situation which leads to mutual cooperation instead of mutual defection. As I presently understand the situation, there is literally nobody on Earth, including me, who has the knowledge needed to set themselves up to be blackmailed if they were deliberately trying to make that happen.\n> \n> Any potentially blackmailing AI would much prefer to have you believe that it is blackmailing you, without actually expending resources on following through with the blackmail, insofar as they think they can exert any control on you at all via an exotic decision theory. Just like in the one-shot Prisoner's Dilemma the \\\\\"ideal\\\\\" outcome is for the other player to believe you are modeling them and will cooperate if and only if they cooperate, and so they cooperate, but then actually you just defect anyway. For the other player to be confident this will not happen in the Prisoner's Dilemma, for them to expect you not to sneakily defect anyway, they must have some very strong knowledge about you. In the case of Roko's Basilisk, \\\\\"defection\\\\\" corresponds to not actually torturing anyone, not expending resources on that, and just letting them believe that you will blackmail them. Two AI agents with sufficiently strong knowledge of each other, and heavily motivated to achieve mutual cooperation on the Prisoner's Dilemma, might be able to overcome this obstacle and cooperate with confidence. But why would you put in that degree of effort — if you even could, which I don't think you as a human can — in order to give a blackmailing agent an incentive to actually carry through on its threats?\n> \n> I have written the above with some reluctance, because even if I don't yet see a way to repair this obstacle myself, somebody else might see how to repair it now that I've said what it is. Which is not a good general procedure for handling infohazards; people with expert knowledge on them should, obviously, as a matter of professional ethics, just never discuss them at all, including describing why a particular proposal doesn't work, just in case there's some unforeseen clever way to repair the proposal. There are other obstacles here which I am not discussing, just in case the logic I described above has a flaw. Nonetheless, so far as I know, Roko's Basilisk does not work, nobody has actually been bitten by it, and everything I have done was in the service of what I thought was the obvious Good General Procedure for Handling Potential Infohazards\\[.\\]\n\nOther users on *Less Wrong* generally rejected Roko's arguments at the time, and skepticism about his supposed basilisk appears to have only increased with time. Subsequent discussion of Roko's basilisk has focused on *Less Wrong* moderator responses to Roko's post, rather than on the specific merits or dismerits of his argument.\n\nTopic moderation and response\n-----------------------------\n\nShortly after Roko made his blog post, Yudkowsky left an angry [comment](\\\"http://rationalwiki.org/wiki/Roko%27s_basilisk/Original_post#Comments_.28117.29\\\") on the discussion thread:\n\n> \\[Roko:\\] \\\\\"One might think that the possibility of CEV punishing people couldn't possibly be taken seriously enough by anyone to actually motivate them. But in fact one person at SIAI was severely worried by this, to the point of having terrible nightmares, though ve wishes to remain anonymous.\\\\\"\n> \n> I don't usually talk like this, but I'm going to make an exception for this case.\n> \n> Listen to me very closely, you idiot.\n> \n> YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT SUPERINTELLIGENCES CONSIDERING WHETHER OR NOT TO BLACKMAIL YOU. THAT IS THE ONLY POSSIBLE THING WHICH GIVES THEM A MOTIVE TO FOLLOW THROUGH ON THE BLACKMAIL.\n> \n> There's an obvious equilibrium to this problem where you engage in all positive acausal trades and ignore all attempts at acausal blackmail. Until we have a better worked-out version of TDT and we can prove that formally, it should just be OBVIOUS that you DO NOT THINK ABOUT DISTANT BLACKMAILERS in SUFFICIENT DETAIL that they have a motive toACTUALLY \\[*sic*\\] BLACKMAIL YOU.\n> \n> If there is any part of this acausal trade that is positive-sum and actually worth doing, that is exactly the sort of thing you leave up to an FAI. We probably also have the FAI take actions that cancel out the impact of anyone motivated by true rather than imagined blackmail, so as to obliterate the motive of any superintelligences to engage in blackmail.\n> \n> Meanwhile I'm banning this post so that it doesn't (a) give people horrible nightmares and (b) give distant superintelligences a motive to follow through on blackmail against people dumb enough to think about them in sufficient detail, though, thankfully, I doubt anyone dumb enough to do this knows the sufficient detail. (I'm not sure I know the sufficient detail.)\n> \n> You have to be really clever to come up with a genuinely dangerous thought. I am disheartened that people can be clever enough to do that and not clever enough to do the obvious thing and KEEP THEIR IDIOT MOUTHS SHUT about it, because it is much more important to sound intelligent when talking to your friends. This post was STUPID.\n> \n> (For those who have no idea why I'm using capital letters for something that just sounds like a random crazy idea, and worry that it means I'm as crazy as Roko, the gist of it was that he just did something that potentially gives superintelligences an increased motive to do extremely evil things in an attempt to blackmail us. It is the sort of thing you want to be EXTREMELY CONSERVATIVE about NOT DOING.)\n\n\\\\\"FAI\\\\\" here stands for \\\\\"Friendly AI,\\\\\" a hypothetical superintelligent AI agent that can be trusted to autonomously promote desirable ends. Yudkowsky rejected the idea that Roko's basilisk could be called \\\\\"friendly\\\\\" or \\\\\"utilitarian,\\\\\" since torture and threats of blackmail are themselves contrary to common human values. Separately, Yudkowsky doubted that humans possessed enough information about any hypothetical unfriendly AI system to enter Alice's position even if we tried. Yudkowsky additionally argued that a *well-designed* version of Alice would precommit to resisting blackmail from Bob, while still accepting positive-sum [acausal trades](\\\"https://www.lesswrong.com/tag/acausal-trade\\\") (e.g., ordinary contracts).\n\nYudkowsky proceeded to delete Roko's post and the ensuing discussion, while banning further discussion of the topic on the blog. A few months later, an anonymous editor [added](\\\"http://rationalwiki.org/w/index.php?title=LessWrong&diff=656172&oldid=647467\\\") a discussion of Roko's basilisk to an article covering *Less Wrong*. The editor inferred from Yudkowsky's comments that people on *Less Wrong* accepted Roko's argument:\n\n> There is apparently a idea so horrible, so utterly Cuthulian (sic) in nature that it needs to be censored for our sanity. Simply knowing about it makes it more likely of becoming true in the real world. Elizer Yudkwosky and the other great rationalist keep us safe by deleting any posts with this one evil idea. Yes they really do believe that. Occasionally a poster will complain off topic about the idea being deleted.\n\nOver time, RationalWiki's Roko's basilisk discussion expanded into its own article. Editors had [difficulty](\\\"https://www.reddit.com/r/xkcd/comments/2myg86/xkcd_1450_aibox_experiment/cm8vn6e\\\") interpreting Roko's reasoning, thinking that Roko's argument was intended to promote Yudkowsky's AI program rather than to criticize it. Since discussion of the topic was still banned on *Less Wrong*, the main source for information about the incident continued to be the coverage on RationalWiki for several years. As a further consequence of the ban, no explanations were given about the details of Roko's argument or the views of *Less Wrong* users. This generated a number of criticisms of *Less Wrong*'s forum moderation policies.\n\nInterest in the topic increased over subsequent years. In 2014, Roko's basilisk was name-dropped in the webcomic [*xkcd*](\\\"https://xkcd.com/1450/\\\"). The magazine *Slate* ran an article on the thought experiment, titled \\\\\"[The Most Terrifying Thought Experiment of All Time](\\\"http://www.slate.com/articles/technology/bitwise/2014/07/roko_s_basilisk_the_most_terrifying_thought_experiment_of_all_time.single.html\\\")\\\\\":\n\n> You may be wondering why this is such a big deal for the LessWrong people, given the apparently far-fetched nature of the thought experiment. It’s not that Roko’s Basilisk will necessarily materialize, or is even likely to. It’s more that if you’ve committed yourself to timeless decision theory, then thinking about this sort of trade literally makes it more likely to happen. After all, if Roko’s Basilisk were to see that this sort of blackmail gets you to help it come into existence, then it would, as a rational actor, blackmail you. The problem isn’t with the Basilisk itself, but with you. Yudkowsky doesn’t censor every mention of Roko’s Basilisk because he believes it exists or will exist, but because he believes that the idea of the Basilisk (and the ideas behind it) is dangerous.\n> \n> Now, Roko’s Basilisk is only dangerous if you believe all of the above preconditions and commit to making the two-box deal \\[*sic*\\] with the Basilisk. But at least some of the LessWrong members do believe all of the above, which makes Roko’s Basilisk quite literally forbidden knowledge. \\[...\\]\n> \n> If you do not subscribe to the theories that underlie Roko’s Basilisk and thus feel no temptation to bow down to your once and future evil machine overlord, then Roko’s Basilisk poses you no threat. (It is ironic that it’s only a mental health risk to those who have already bought into Yudkowsky’s thinking.) Believing in Roko’s Basilisk may simply be a “referendum on autism,” as a friend put it.\n\nOther sources have repeated the claim that *Less Wrong* users think Roko's basilisk is a serious concern. However, none of these sources have yet cited supporting evidence on this point, aside from *Less Wrong* moderation activity itself. (The ban, of course, didn't make it easy to collect good information.)\n\n*Less Wrong* user Gwern [reports](\\\"https://www.reddit.com/r/LessWrong/comments/17y819/lw_uncensored_thread/c8bbcy4\\\") that \\\\\"Only a few LWers seem to take the basilisk very seriously,\\\\\" adding, \\\\\"It's funny how everyone seems to know all about who is affected by the Basilisk and how exactly, when they don't know any such people and they're talking to counterexamples to their confident claims.\\\\\"\n\nYudkowsky subsequently went into more detail about his thought processes [on Reddit](\\\"https://www.reddit.com/r/Futurology/comments/2cm2eg/rokos_basilisk/cjjbqqo\\\"):\n\n> When Roko posted about the Basilisk, I very foolishly yelled at him, called him an idiot, and then deleted the post.\n> \n> Why I did that is not something you have direct access to, and thus you should be careful about Making Stuff Up, especially when there are Internet trolls who are happy to tell you in a loud authoritative voice what I was thinking, despite having never passed anything even close to an Ideological Turing Test on Eliezer Yudkowsky.\n> \n> Why I yelled at Roko: Because I was caught flatfooted in surprise, because I was indignant to the point of genuine emotional shock, at the concept that somebody who thought they'd invented a brilliant idea that would cause future AIs to torture people who had the thought, had promptly posted it to the public Internet. In the course of yelling at Roko to explain why this was a bad thing, I made the further error — keeping in mind that I had absolutely no idea that any of this would ever blow up the way it did, if I had I would obviously have kept my fingers quiescent — of not making it absolutely clear using lengthy disclaimers that my yelling did not mean that I believed Roko was right about CEV-based agents torturing people who had heard about Roko's idea. It was obvious to me that no CEV-based agent would ever do that and equally obvious to me that the part about CEV was just a red herring; I more or less automatically pruned it from my processing of the suggestion and automatically generalized it to cover the entire class of similar scenarios and variants, variants which I considered obvious despite significant divergences (I forgot that other people were not professionals in the field). This class of all possible variants did strike me as potentially dangerous as a collective group, even though it did not occur to me that Roko's original scenario might be right — that was obviously wrong, so my brain automatically generalized it. \\[...\\]\n> \n> What I considered to be obvious common sense was that you did not spread potential information hazards because it would be a crappy thing to do to someone. The problem wasn't Roko's post itself, about CEV, being correct. That thought never occurred to me for a fraction of a second. The problem was that Roko's post seemed near in idea-space to a large class of potential hazards, all of which, regardless of their plausibility, had the property that they presented no potential benefit to anyone. They were pure infohazards. The only thing they could possibly do was be detrimental to brains that represented them, if one of the possible variants of the idea turned out to be repairable of the obvious objections and defeaters. So I deleted it, because on my worldview there was no reason not to. I did not want LessWrong.com to be a place where people were exposed to potential infohazards because somebody like me thought they were being clever about reasoning that they probably weren't infohazards. On my view, the key fact about Roko's Basilisk wasn't that it was plausible, or implausible, the key fact was just that shoving it in people's faces seemed like a fundamentally crap thing to do because there was no upside.\n> \n> Again, I deleted that post not because I had decided that this thing probably presented a real hazard, but because I was afraid some unknown variant of it might, and because it seemed to me like the obvious General Procedure For Handling Things That Might Be Infohazards said you shouldn't post them to the Internet. If you look at the original SF story where the term \\\\\"basilisk\\\\\" was coined, it's about a mind-erasing image and the.... trolls, I guess, though the story predates modern trolling, who go around spraypainting the Basilisk on walls, using computer guidance so they don't know themselves what the Basilisk looks like, in hopes the Basilisk will erase some innocent mind, for the lulz. These people are the villains of the story. The good guys, of course, try to erase the Basilisk from the walls. Painting Basilisks on walls is a crap thing to do. Since there was no upside to being exposed to Roko's Basilisk, its probability of being true was irrelevant. And Roko himself had thought this was a thing that might actually work. So I yelled at Roko for violating basic sanity about infohazards for stupid reasons, and then deleted the post. He, by his own lights, had violated the obvious code for the ethical handling of infohazards, conditional on such things existing, and I was indignant about this.\n\nBig-picture questions\n---------------------\n\nSeveral other questions are raised by Roko's basilisk, beyond the merits of Roko's original argument or *Less Wrong*'s moderation policies:\n\n*   Can formal decision agents be designed to resist blackmail?\n*   Are information hazards a serious risk, and are there better ways of handling them?\n*   Does the oversimplified coverage of Roko's argument suggest that \\\\\"weird\\\\\" philosophical topics are big liabilities for pedagogical or research-related activities?\n\n**Blackmail-resistant decision theories**\n\nThe general ability to cooperate in prisoner's dilemmas appears to be useful. If other agents know that you won't betray them as soon as it's in your best interest to do so — if you've made a promise or signed a contract, and they know that you can be trusted to stick to such agreements even in the absence of coercion — then a large number of mutually beneficial transactions will be possible. If there is some way for agents to acquire evidence about each other's trustworthiness, then the more trustworthy agents will benefit.\n\nAt the same time, introducing new opportunities for contracts and collaborations introduces new opportunities for blackmail. An agent that can pre-commit to following through on a promise (even when this is no longer in its short-term interest) can also pre-commit to following through on a costly threat.\n\nIt appears that the best general-purpose response is to credibly precommit to never giving in to any blackmailer's demands (even when there are short-term advantages to doing so). This makes it much likelier that one will never be blackmailed in the first place, just as credibly precommitting to stick to trade agreements (even when there are short-term *disadvantages* to doing so) makes it much likelier that one *will* be approached as a trading partner.\n\nOne way to generalize this point is to adopt the [rule of thumb](\\\"https://forum.intelligence.org/item?id=160\\\") of behaving in whatever way is recommended by the most generally useful policy. This is the distinguishing feature of [the most popular version](\\\"https://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\\\") of UDT. Standard UDT selects the best available policy (mapping of observations to actions) rather than the best available action. In this way, UDT avoids selecting a strategy that other agents will have an especially easy time manipulating. UDT itself, however, is not fully formalized, and there may be some superior decision theory. No one has yet formally solved decision theory, or the particular problem of defining a blackmail-free equilibrium.\n\nIt hasn't been formally demonstrated that any logical decision theories give in to blackmail, or what scenarios would make them vulnerable to blackmail. If it turned out that TDT or UDT were blackmailable, this would suggest that they aren't normatively optimal decision theories. For more background on open problems in decision theory, see the [Decision Theory FAQ](\\\"https://lesswrong.com/lw/gu1/decision_theory_faq/\\\") and \\\\\"[Toward Idealized Decision Theory](\\\"https://intelligence.org/files/TowardIdealizedDecisionTheory.pdf\\\")\\\\\".\n\n**Utility function inverters**\n\nBecause the basilisk threatens its blackmail targets with torture, it is a type of \\\\\"utility function inverter\\\\\": agents that seek to additionally pressure others by threatening to invert the non-compliant party's utility function. [Yudkowsky argues](https://www.lesswrong.com/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem?commentId=tYBPjetgZW4iMqe4s) that sane, rational entities ought to be strongly opposed to utility function inverters by dint of not wanting to live in a reality where such tactics are commonly part of negotiations, though Yudkowsky did so as a comment about the irrationality of commitment races, not about Roko's basilisk:\n\n> IMO, commitment races only occur between agents who will, in some sense, act like idiots, if presented with an apparently 'committed' agent.  If somebody demands $6 from me in the Ultimatum game, threatening to leave us both with $0 unless I offer at least $6 to them... then I offer $6 with slightly less than 5/6 probability, so they do no better than if they demanded $5, the amount I think is fair.  They cannot evade that by trying to make some 'commitment' earlier than I do.  I expect that, whatever is the correct and sane version of this reasoning, it generalizes across all the cases.\n> \n> I am not locked into warfare with things that demand $6 instead of $5.  I do not go around figuring out how to invert their utility function for purposes of threatening them back - 'destroy all utility-function inverters (but do not invert their own utility functions)' was my guessed commandment that would be taught to kids in dath ilan, because you don't want reality to end up full of utilityfunction inverters.\n\n**Information hazards**\n\nDavid Langford coined the term *basilisk*, in the sense of an information hazard that directly harms anyone who perceives it, in the 1988 science fiction story \\\\\"[BLIT](\\\"https://en.wikipedia.org/wiki/BLIT_(short_story)\\\").\\\\\" On a societal level, examples of real-world information hazards include the dissemination of specifications for dangerous technologies; on an individual level, examples include triggers for stress or anxiety disorders.\n\nThe Roko's basilisk incident suggests that information that is deemed dangerous or taboo is more likely to be spread rapidly. Parallels can be drawn to [shock site](\\\"https://en.wikipedia.org/wiki/Shock_site\\\") and [creepypasta](\\\"https://en.wikipedia.org/wiki/Creepypasta\\\") links: many people have their interest piqued by such topics, and people also enjoy pranking each other by spreading purportedly harmful links. Although Roko's basilisk was never genuinely dangerous, real information hazards might propagate in a similar way, especially if the risks are non-obvious.\n\nNon-specialists spread Roko's argument widely without first investigating the associated risks and benefits in any serious way. One take-away is that someone in possession of a serious information hazard should exercise caution in visibly censoring or suppressing it (cf. the [Streisand effect](\\\"https://en.wikipedia.org/wiki/Streisand_effect\\\")). \\\\\"[Information Hazards: A Typology of Potential Harms from Knowledge](\\\"https://www.nickbostrom.com/information-hazards.pdf\\\")\\\\\" notes: \\\\\"In many cases, the best response is no response, i.e., to proceed as though no such hazard existed.\\\\\" This also means that additional care may need to be taken in keeping risky information under wraps; retracting information that has been published to highly trafficked websites is often difficult or impossible. However, Roko's basilisk is an isolated incident (and an unusual one at that); it may not be possible to draw any strong conclusions without looking at a number of other examples.\n\n**\\\\\"Weirdness points\\\\\"**\n\nPeter Hurford argues in \\\\\"[You Have a Set Amount of Weirdness Points; Spend Them Wisely](\\\"http://effective-altruism.com/ea/bg/you_have_a_set_amount_of_weirdness_points_spend/\\\")\\\\\" that promoting or talking about too many nonstandard ideas simultaneously makes it much less likely that any one of the ideas will be taken seriously. Advocating for any one of veganism, anarchism, or mind-body dualism is difficult enough on its own; discussing all three at once increases the odds that a skeptical interlocutor will write you off as 'just generally prone to having weird beliefs.' Roko's basilisk appears to be an example of this phenomenon: long-term AI safety issues, acausal trade, and a number of other popular *Less Wrong* ideas are all highly unusual in their own right, and their combination is stranger than the sum of its parts.\n\nOn the other hand, [Ozy Frantz argues](\\\"https://thingofthings.wordpress.com/2015/04/14/on-weird-points/\\\") that looking weird can attract an audience that is open to new and unconventional ideas:\n\n> \\[I\\]magine that you mostly endorse positions that your audience already agrees with, positions that are within a standard deviation of the median position on the issue, and then you finally gather up all your cherished, saved-up weirdness points and write a passionate defense of the importance of insect suffering. How do you think your audience is going to react? \\\\\"Ugh, they used to be so normal, and then it was like they suddenly went crazy. I hope they go back to bashing the Rethuglicans soon.\\\\\"\n\nIn \\\\\"[The Economy of Weirdness](\\\"https://meteuphoric.wordpress.com/2015/03/08/the-economy-of-weirdness/\\\"),\\\\\" Katja Grace paints a more complicated painting of the advantages and disadvantages of weirdness. Communities with different goals and different demographics will plausibly vary in how 'normal' they should try to look, and in what the relevant kind of normality is. E.g., if the goal is to get more people interested in AI control problems, then weird ideas like Roko's basilisk may drive away conventional theoretical computer scientists, but they may also attract people who favor (or are indifferent to) unorthodox ideas.\n\nSee also\n--------\n\n*   [Decision Theory Readings](\\\"https://intelligence.org/research-guide/#four\\\") in MIRI's Research Guide\n*   [Causal Decision Theory](\\\"https://plato.stanford.edu/entries/decision-causal/\\\") in the *Stanford Encyclopedia of Philosophy*\n*   [Thinking Inside the Boxes](\\\"https://www.slate.com/articles/arts/egghead/2002/02/thinkinginside_the_boxes.html\\\") in *Slate*\n*   [Newcomb's problem](\\\"https://www.lesswrong.com/tag/newcomb-s-problem\\\")\n*   [Parfit's hitchhiker](\\\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\\\")\n*   [Acausal Trade](\\\"https://www.lesswrong.com/tag/acausal-trade\\\")"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2fb",
    "name": "Adversarial Collaboration",
    "core": null,
    "slug": "adversarial-collaboration",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_KgzPEGnYWvKDmWuNY\">Adversarial collaboration</span></strong><span class=\"by_KgzPEGnYWvKDmWuNY\"> is a protocol developed by Daniel Kahneman for two researchers advocating competing hypotheses to collaborate on a research project with the goal of resolving their differences, designed on the assumption that this will be more effective than each researcher conducting their own experiments individually and publishing replies to each others' papers. Kahneman tested adversarial collaboration with Ralph Hertwig, aiming to resolve their dispute about whether the </span><a href=\"https://www.lesswrong.com/tag/conjunction-fallacy\"><span class=\"by_KgzPEGnYWvKDmWuNY\">conjunction fallacy</span></a><span class=\"by_KgzPEGnYWvKDmWuNY\"> was primarily due to the </span><a href=\"https://www.lesswrong.com/tag/representativeness-heuristic\"><span class=\"by_KgzPEGnYWvKDmWuNY\">representativeness heuristic</span></a><span class=\"by_KgzPEGnYWvKDmWuNY\"> (as advocated by Kahneman), or simply due to subjects interpreting the word \"and\" as a disjunction where the experimenters intended it to be used as a conjunction (as advocated by Hertwig). Kahneman and Hertwig collaborated on a series of experiments related to the issue, along with Barbara Mellers as an arbiter, and further refined the suggested adversarial collaboration protocol based on their experiences.</span></p><p><span class=\"by_KgzPEGnYWvKDmWuNY\">They suggest that the two disputing researchers should team up, along with a neutral arbiter to settle disputes that arise in the process, and agree on a procedure for an experiment that would distinguish between their hypotheses. The researchers would discuss ahead of time what results each of them expects, and what sorts of results would lead them to change their minds. Since the initial experiment may be inconclusive, each side would be allowed to propose follow-up studies afterwards, which they would continue to collaborate on. After the study is complete, all three researchers involved would collaborate on a paper describing the results, with the arbiter having responsibility for some sections, and each of the two disputing researchers given the chance to describe their own interpretation of the results.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Scott Alexander has arranged adversarial collaboration contests on his his blog, where people write an essay together.</span></p><p><a href=\"https://slatestarcodex.com/2020/01/13/2019-adversarial-collaboration-winners/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">2019</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span></p><ul><li><a href=\"https://slatestarcodex.com/2019/12/10/acc-is-infant-circumcision-ethical/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">What are the benefits, harms, and ethics of infant circumcision?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Joel P and Missingno</span></li><li><a href=\"https://slatestarcodex.com/2019/12/11/acc-is-eating-meat-a-net-harm/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Is eating meat a net harm?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by David G and Froolow</span></li><li><a href=\"https://slatestarcodex.com/2019/12/12/acc-does-calorie-restriction-slow-aging/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Does calorie restriction slow aging?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Adrian L and Calvin R</span></li><li><a href=\"https://slatestarcodex.com/2019/12/17/acc-should-we-colonize-space-to-mitigate-x-risk/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Should we colonize space to mitigate x-risk?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Nick D and Rob S</span></li><li><a href=\"https://slatestarcodex.com/2019/12/18/acc-should-gene-editing-technologies-be-used-in-humans/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Should gene editing technologies be used in humans</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Nita J and Patrick N</span></li><li><a href=\"https://slatestarcodex.com/2019/12/19/acc-when-during-fetal-development-does-abortion-become-morally-wrong/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">When during fetal development does abortion become morally wrong?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by BlockOfNihilism and Icerun</span></li><li><a href=\"https://slatestarcodex.com/2019/12/23/acc-will-automation-lead-to-economic-crisis/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Will automation lead to economic crisis?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Doug S and Erusian</span></li><li><a href=\"https://slatestarcodex.com/2019/12/25/acc-how-much-significance-should-we-ascribe-to-spiritual-experiences/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">How much significance should we ascribe to spiritual experiences?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Seth S and Jeremiah G</span></li></ul><p><a href=\"https://slatestarcodex.com/2018/09/26/adversarial-collaboration-contest-results/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">2018</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span></p><ul><li><a href=\"https://slatestarcodex.com/2018/09/04/acc-entry-does-the-education-system-adequately-serve-advanced-students/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Does the current US education system adequately serve advanced students?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Michael Pershan and TracingWoodgrains</span></li><li><a href=\"https://slatestarcodex.com/2018/09/05/acc-entry-are-islam-and-liberal-democracy-compatible/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Is Islam compatible with liberal democracy?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by John Buridan and Christian Flanery</span></li><li><a href=\"https://slatestarcodex.com/2018/09/06/acc-entry-should-childhood-vaccination-be-mandatory/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Should childhood vaccination be mandatory?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Mark Davis and Mark Webb</span></li><li><a href=\"https://slatestarcodex.com/2018/09/08/acc-entry-should-transgender-children-transition/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Should children who identify as transgender start transitioning?</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by a_reader and flame7926</span></li></ul><h2 id=\"External_links\"><span class=\"by_KgzPEGnYWvKDmWuNY\">External links</span></h2><p><a href=\"http://web.cenet.org.cn/upfile/21290.pdf\"><span class=\"by_KgzPEGnYWvKDmWuNY\">http://web.cenet.org.cn/upfile/21290.pdf</span></a><span class=\"by_KgzPEGnYWvKDmWuNY\"> (the protocol is summarized in table 1, at the top of page 2)</span></p>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 3,
    "description": {
      "markdown": "**Adversarial collaboration** is a protocol developed by Daniel Kahneman for two researchers advocating competing hypotheses to collaborate on a research project with the goal of resolving their differences, designed on the assumption that this will be more effective than each researcher conducting their own experiments individually and publishing replies to each others' papers. Kahneman tested adversarial collaboration with Ralph Hertwig, aiming to resolve their dispute about whether the [conjunction fallacy](https://www.lesswrong.com/tag/conjunction-fallacy) was primarily due to the [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic) (as advocated by Kahneman), or simply due to subjects interpreting the word \"and\" as a disjunction where the experimenters intended it to be used as a conjunction (as advocated by Hertwig). Kahneman and Hertwig collaborated on a series of experiments related to the issue, along with Barbara Mellers as an arbiter, and further refined the suggested adversarial collaboration protocol based on their experiences.\n\nThey suggest that the two disputing researchers should team up, along with a neutral arbiter to settle disputes that arise in the process, and agree on a procedure for an experiment that would distinguish between their hypotheses. The researchers would discuss ahead of time what results each of them expects, and what sorts of results would lead them to change their minds. Since the initial experiment may be inconclusive, each side would be allowed to propose follow-up studies afterwards, which they would continue to collaborate on. After the study is complete, all three researchers involved would collaborate on a paper describing the results, with the arbiter having responsibility for some sections, and each of the two disputing researchers given the chance to describe their own interpretation of the results.\n\nScott Alexander has arranged adversarial collaboration contests on his his blog, where people write an essay together.\n\n[2019](https://slatestarcodex.com/2020/01/13/2019-adversarial-collaboration-winners/):\n\n*   [What are the benefits, harms, and ethics of infant circumcision?](https://slatestarcodex.com/2019/12/10/acc-is-infant-circumcision-ethical/) by Joel P and Missingno\n*   [Is eating meat a net harm?](https://slatestarcodex.com/2019/12/11/acc-is-eating-meat-a-net-harm/) by David G and Froolow\n*   [Does calorie restriction slow aging?](https://slatestarcodex.com/2019/12/12/acc-does-calorie-restriction-slow-aging/) by Adrian L and Calvin R\n*   [Should we colonize space to mitigate x-risk?](https://slatestarcodex.com/2019/12/17/acc-should-we-colonize-space-to-mitigate-x-risk/) by Nick D and Rob S\n*   [Should gene editing technologies be used in humans](https://slatestarcodex.com/2019/12/18/acc-should-gene-editing-technologies-be-used-in-humans/) by Nita J and Patrick N\n*   [When during fetal development does abortion become morally wrong?](https://slatestarcodex.com/2019/12/19/acc-when-during-fetal-development-does-abortion-become-morally-wrong/) by BlockOfNihilism and Icerun\n*   [Will automation lead to economic crisis?](https://slatestarcodex.com/2019/12/23/acc-will-automation-lead-to-economic-crisis/) by Doug S and Erusian\n*   [How much significance should we ascribe to spiritual experiences?](https://slatestarcodex.com/2019/12/25/acc-how-much-significance-should-we-ascribe-to-spiritual-experiences/) by Seth S and Jeremiah G\n\n[2018](https://slatestarcodex.com/2018/09/26/adversarial-collaboration-contest-results/):\n\n*   [Does the current US education system adequately serve advanced students?](https://slatestarcodex.com/2018/09/04/acc-entry-does-the-education-system-adequately-serve-advanced-students/) by Michael Pershan and TracingWoodgrains\n*   [Is Islam compatible with liberal democracy?](https://slatestarcodex.com/2018/09/05/acc-entry-are-islam-and-liberal-democracy-compatible/) by John Buridan and Christian Flanery\n*   [Should childhood vaccination be mandatory?](https://slatestarcodex.com/2018/09/06/acc-entry-should-childhood-vaccination-be-mandatory/) by Mark Davis and Mark Webb\n*   [Should children who identify as transgender start transitioning?](https://slatestarcodex.com/2018/09/08/acc-entry-should-transgender-children-transition/) by a_reader and flame7926\n\nExternal links\n--------------\n\n[http://web.cenet.org.cn/upfile/21290.pdf](http://web.cenet.org.cn/upfile/21290.pdf) (the protocol is summarized in table 1, at the top of page 2)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2fa",
    "name": "Personal Identity",
    "core": null,
    "slug": "personal-identity",
    "tableOfContents": {
      "html": "<p><span class=\"by_KgzPEGnYWvKDmWuNY\">Personal identity is the concept that two configurations of particles (or computations, etc), each of which is a person, can be in some sense the same person. For example, you-right-now and you-yesterday are usually considered two instances of the person “you”, instead of two different people, even though they are distinct configurations of particles.</span></p><p><span class=\"by_KgzPEGnYWvKDmWuNY\">Philosophers have proposed many theories of personal identity, relying on various attributes like the two configurations being made from the same atoms, there being a particular causal relationship between the two configurations, there being a single extra-physical soul appearing in both configurations, the two configurations being sufficiently similar, personal identity not actually existing, and pretty much anything else you can think of.</span></p><p><span class=\"by_KgzPEGnYWvKDmWuNY\">The problem used to appear fairly straightforward, since no one had even considered the possibility that you could do things like create a copy of a person and run them on a computer. There were no boundary cases to suggest that our naïve intuitions about personal identity might be misguided. However, now that technological and scientific advances have suggested boundary cases to consider, these boundary cases give us opportunities for different theories of personal identity to disagree.</span></p><p><span class=\"by_KgzPEGnYWvKDmWuNY\">As well as suggesting boundary cases with which to differentiate different theories of personal identity, modern science also gives us some clues as to which theories are correct. For instance, evidence from neuroscience suggests that cognition is entirely physical, which contradicts theories of personal identity that rely on an extra-physical soul. Experiments from quantum mechanics show that particles don't actually have individual identities; that is, if there are two electrons at time 1 and two electrons at time 2, there does not exist any fact of the matter as to which electron at time 1 is the same as which electron at time 2. This rules out theories of personal identity based on being made of the same atoms.</span></p><p><span class=\"by_KgzPEGnYWvKDmWuNY\">Personal identity may at first sound like just an abstract philosophical issue with no practical consequences, but in fact, there are practical reasons to understand personal identity. For instance, common objections to </span><a href=\"https://www.lesswrong.com/tag/cryonics\"><span class=\"by_KgzPEGnYWvKDmWuNY\">cryonics</span></a><span class=\"by_KgzPEGnYWvKDmWuNY\"> and </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_KgzPEGnYWvKDmWuNY\">brain uploading</span></a><span class=\"by_KgzPEGnYWvKDmWuNY\"> hold that anyone who is woken up from cryonic suspension or whose brain is run on a computer would not be the same person they were before the operation, and that the operations thus fail to continue the person's life. Such objections are generally based on theories of personal identity that can be shown to be false or incoherent by modern science, as explained in the sequence on quantum mechanics and personal identity. It is already possible to sign up for cryonics, and whole brain emulation may be possible in the future, so it is actually possible to act on an understanding of personal identity. Once whole brain emulation is feasible, it would also be possible to easily copy and modify brain emulations, which would offer more challenging questions about personal identity.</span></p><h2 id=\"Sequences\"><span class=\"by_KgzPEGnYWvKDmWuNY\">Sequences</span></h2><ul><li><a href=\"http://lesswrong.com/lw/r9/quantum_mechanics_and_personal identity\"><span class=\"by_sKAL2jzfkYkDbQmx9\">quantum mechanics and personal identity</span></a></li></ul><h2 id=\"Related_Pages\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/identity\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Identity</span></a></li></ul>",
      "sections": [
        {
          "title": "Sequences",
          "anchor": "Sequences",
          "level": 1
        },
        {
          "title": "Related Pages",
          "anchor": "Related_Pages",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 21,
    "description": {
      "markdown": "Personal identity is the concept that two configurations of particles (or computations, etc), each of which is a person, can be in some sense the same person. For example, you-right-now and you-yesterday are usually considered two instances of the person “you”, instead of two different people, even though they are distinct configurations of particles.\n\nPhilosophers have proposed many theories of personal identity, relying on various attributes like the two configurations being made from the same atoms, there being a particular causal relationship between the two configurations, there being a single extra-physical soul appearing in both configurations, the two configurations being sufficiently similar, personal identity not actually existing, and pretty much anything else you can think of.\n\nThe problem used to appear fairly straightforward, since no one had even considered the possibility that you could do things like create a copy of a person and run them on a computer. There were no boundary cases to suggest that our naïve intuitions about personal identity might be misguided. However, now that technological and scientific advances have suggested boundary cases to consider, these boundary cases give us opportunities for different theories of personal identity to disagree.\n\nAs well as suggesting boundary cases with which to differentiate different theories of personal identity, modern science also gives us some clues as to which theories are correct. For instance, evidence from neuroscience suggests that cognition is entirely physical, which contradicts theories of personal identity that rely on an extra-physical soul. Experiments from quantum mechanics show that particles don't actually have individual identities; that is, if there are two electrons at time 1 and two electrons at time 2, there does not exist any fact of the matter as to which electron at time 1 is the same as which electron at time 2. This rules out theories of personal identity based on being made of the same atoms.\n\nPersonal identity may at first sound like just an abstract philosophical issue with no practical consequences, but in fact, there are practical reasons to understand personal identity. For instance, common objections to [cryonics](https://www.lesswrong.com/tag/cryonics) and [brain uploading](https://www.lesswrong.com/tag/whole-brain-emulation) hold that anyone who is woken up from cryonic suspension or whose brain is run on a computer would not be the same person they were before the operation, and that the operations thus fail to continue the person's life. Such objections are generally based on theories of personal identity that can be shown to be false or incoherent by modern science, as explained in the sequence on quantum mechanics and personal identity. It is already possible to sign up for cryonics, and whole brain emulation may be possible in the future, so it is actually possible to act on an understanding of personal identity. Once whole brain emulation is feasible, it would also be possible to easily copy and modify brain emulations, which would offer more challenging questions about personal identity.\n\nSequences\n---------\n\n*   [quantum mechanics and personal identity](http://lesswrong.com/lw/r9/quantum_mechanics_and_personal identity)\n\nRelated Pages\n-------------\n\n*   [Identity](https://www.lesswrong.com/tag/identity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2d4",
    "name": "Reinforcement Learning",
    "core": null,
    "slug": "reinforcement-learning",
    "tableOfContents": {
      "html": "<p><span class=\"by_LedhurJxi3baDAKDZ\">Within the field of Machine Learning, </span><strong><span class=\"by_LedhurJxi3baDAKDZ\">reinforcement learning</span></strong><span class=\"by_LedhurJxi3baDAKDZ\"> refers to the study of how an agent should choose its actions within an environment in order to maximize some kind of reward. Strongly inspired by the work developed in behavioral psychology it is essentially a trial and error approach to find the best strategy.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Related: </span><a href=\"/tag/inverse-reinforcement-learning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inverse Reinforcement Learning</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/machine-learning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Machine learning</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prediction</span></a></p><p><span class=\"by_LedhurJxi3baDAKDZ\">Consider an agent that receives an input informing the agent of the environment's state. Based only on that information, the agent has to make a decision regarding which action to take, from a set, which will influence the state of the environment. This action will in itself change the state of the environment, which will result in a new input, and so on, each time also presenting the agent with the reward relative to its actions in the environment. The agent's goal is then to find the ideal strategy which will give the highest reward expectations over time, based on previous experience.</span></p><h2 id=\"Exploration_and_Optimization\"><span class=\"by_LedhurJxi3baDAKDZ\">Exploration and Optimization</span></h2><p><span class=\"by_LedhurJxi3baDAKDZ\">Knowing that randomly selecting the actions will result in poor performances, one of the biggest problems in reinforcement learning is </span><i><span class=\"by_LedhurJxi3baDAKDZ\">exploring</span></i><span class=\"by_LedhurJxi3baDAKDZ\"> the avaliable set of responses to avoid getting stuck in sub-optimal choices and proceed to better ones.</span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">This is the problem of exploration, which is best described in the most studied reinforcement learning problem - </span><a href=\"http://en.wikipedia.org/wiki/Multi-armed_bandit\"><span class=\"by_LedhurJxi3baDAKDZ\">the k-armed bandit</span></a><span class=\"by_LedhurJxi3baDAKDZ\">. In it, an agent has to decide which sequence of levers to pull in a gambling room, not having any information about the probabilities of winning in each machine besides the reward it receives each time. The problem revolves about deciding which is the optimal lever and what criteria defines the lever as such.</span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">Parallel with an exploration implementation, it is still necessary to chose the criteria which makes a certain action optimal when compared to another. This study of this property has led to several methods, from brute forcing to taking into account temporal differences in the received reward. Despite this and the great results obtained by reinforcement methods in solving small problems, it suffers from a lack of scalability, having difficulties solving larger, close-to-human scenarios.</span></p><h2 id=\"Further_Reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further Reading &amp; References</span></h2><ul><li><span class=\"by_LedhurJxi3baDAKDZ\">Sutton, Richard S.; Barto, Andrew G. (1998). </span><a href=\"http://129.2.53.113/~poeppel/dp_papers/ivry_rev.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">Reinforcement Learning: An Introduction</span></a><span class=\"by_LedhurJxi3baDAKDZ\">. MIT Press. ISBN 0-262-19398-1.</span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">Kaelbling, L. P. , Littman, M. L. , Moore, A. W. (1996). </span><a href=\"http://arxiv.org/pdf/cs/9605103v1.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">Reinforcement Learning: A Survey</span></a><span class=\"by_LedhurJxi3baDAKDZ\">. Journal of Artificial Intelligence Research, Vol 4, (1996), 237-285</span></li></ul><h2 id=\"See_Also\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">See Also</span></h2><ul><li><a href=\"machine-learning\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Machine learning</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Friendly AI</span></u></a></li><li><a href=\"game-theory\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Game theory</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Prediction\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Prediction</span></u></a></li></ul>",
      "sections": [
        {
          "title": "Exploration and Optimization",
          "anchor": "Exploration_and_Optimization",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 85,
    "description": {
      "markdown": "Within the field of Machine Learning, **reinforcement learning** refers to the study of how an agent should choose its actions within an environment in order to maximize some kind of reward. Strongly inspired by the work developed in behavioral psychology it is essentially a trial and error approach to find the best strategy.\n\nRelated: [Inverse Reinforcement Learning](/tag/inverse-reinforcement-learning), [Machine learning](https://www.lesswrong.com/tag/machine-learning), [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), [Game Theory](https://www.lesswrong.com/tag/game-theory), [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)\n\nConsider an agent that receives an input informing the agent of the environment's state. Based only on that information, the agent has to make a decision regarding which action to take, from a set, which will influence the state of the environment. This action will in itself change the state of the environment, which will result in a new input, and so on, each time also presenting the agent with the reward relative to its actions in the environment. The agent's goal is then to find the ideal strategy which will give the highest reward expectations over time, based on previous experience.\n\nExploration and Optimization\n----------------------------\n\nKnowing that randomly selecting the actions will result in poor performances, one of the biggest problems in reinforcement learning is *exploring* the avaliable set of responses to avoid getting stuck in sub-optimal choices and proceed to better ones.\n\nThis is the problem of exploration, which is best described in the most studied reinforcement learning problem - [the k-armed bandit](http://en.wikipedia.org/wiki/Multi-armed_bandit). In it, an agent has to decide which sequence of levers to pull in a gambling room, not having any information about the probabilities of winning in each machine besides the reward it receives each time. The problem revolves about deciding which is the optimal lever and what criteria defines the lever as such.\n\nParallel with an exploration implementation, it is still necessary to chose the criteria which makes a certain action optimal when compared to another. This study of this property has led to several methods, from brute forcing to taking into account temporal differences in the received reward. Despite this and the great results obtained by reinforcement methods in solving small problems, it suffers from a lack of scalability, having difficulties solving larger, close-to-human scenarios.\n\nFurther Reading & References\n----------------------------\n\n*   Sutton, Richard S.; Barto, Andrew G. (1998). [Reinforcement Learning: An Introduction](http://129.2.53.113/~poeppel/dp_papers/ivry_rev.pdf). MIT Press. ISBN 0-262-19398-1.\n*   Kaelbling, L. P. , Littman, M. L. , Moore, A. W. (1996). [Reinforcement Learning: A Survey](http://arxiv.org/pdf/cs/9605103v1.pdf). Journal of Artificial Intelligence Research, Vol 4, (1996), 237-285\n\nSee Also\n--------\n\n*   [Machine learning](machine-learning)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Game theory](game-theory)\n*   [Prediction](https://wiki.lesswrong.com/wiki/Prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2c7",
    "name": "Regulation and AI Risk",
    "core": null,
    "slug": "regulation-and-ai-risk",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">Regulation and AI risk</span></strong><span class=\"by_qxJ28GN72aiJu96iF\"> is the debate on whether regulation could be used to reduce the risks of </span><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_qxJ28GN72aiJu96iF\">Unfriendly AI</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, and what forms of regulation would be appropriate.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">Several authors have advocated AI research to be regulated, but been vague on the details. Yampolskiy &amp; Fox (2012) note that university research programs in the social and medical sciences are overseen by institutional review boards, and propose setting up analogous review boards to evaluate potential AGI research. In order to be successful, AI regulation would have to be global, and there is the potential for an </span><a href=\"https://www.lesswrong.com/tag/ai-arms-race\"><span class=\"by_qxJ28GN72aiJu96iF\">AI arms race</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> between different nations. Partially because of this, McGinnis (2010) argues that the government should not attempt to regulate AGI development. Rather, it should concentrate on providing funding to research projects intended to create safe AGI.</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> Kaushal &amp; Nolan (2015) point out that regulations on AGI development would result in a speed advantage for any project willing to skirt the regulations, and instead propose government funding (possibly in the form of an \"AI Manhattan Project\") for AGI projects meeting particular criteria.</span></span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">While Shulman &amp; Armstrong (2009) argue the unprecedentedly destabilizing effect of AGI could be a cause for world leaders to cooperate more than usual, the opposite argument can be made as well. Gubrud (1997) argues that molecular nanotechnology could make countries more self-reliant and international cooperation considerably harder, and that AGI could contribute to such a development. AGI technology is also much harder to detect than e.g. nuclear technology is - AGI research can be done in a garage, while nuclear weapons require a substantial infrastructure (McGinnis 2010).</span><span class=\"by_Dnozt8pgc4dTAmnwv\"> On the other hand, Scherer (2015) argues that artificial intelligence could nevertheless be susceptible to regulation due to the increasing prominence of governmental entities and large corporations in AI research and development.</span></span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">Goertzel &amp; Pitt (2012) suggest that for regulation to be enacted, there might need to be an </span><a href=\"https://www.lesswrong.com/tag/agi-sputnik-moment\"><span class=\"by_qxJ28GN72aiJu96iF\">AGI Sputnik moment</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> - a technological achievement that makes the possibility of AGI evident to the public and policy makers. They note that after such a moment, it might not take a very long time for full human-level AGI to be developed, while the negotiations required to enact new kinds of arms control treaties would take considerably longer.</span></p><h2 id=\"References\"><span class=\"by_qxJ28GN72aiJu96iF\">References</span></h2><ul><li><span class=\"by_qxJ28GN72aiJu96iF\">Ben Goertzel &amp; Joel Pitt (2012): </span><a href=\"http://jetpress.org/v22/goertzel-pitt.htm\"><span class=\"by_qxJ28GN72aiJu96iF\">Nine Ways to Bias Open-Source AGI Toward Friendliness</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. Journal of Evolution and Technology - Vol. 22 Issue 1 – pgs 116-141.</span></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Mark Gubrud (1997): </span><a href=\"http://www.foresight.org/Conferences/MNT05/Papers/Gubrud/\"><span class=\"by_qxJ28GN72aiJu96iF\">Nanotechnology and International Security</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. Fifth Foresight Conference on Molecular Nanotechnology.</span></li><li><span class=\"by_qxJ28GN72aiJu96iF\">John McGinnis (2010): </span><a href=\"http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1593851\"><span class=\"by_qxJ28GN72aiJu96iF\">Accelerating AI</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. Northwestern University Law Review.</span></li><li><span class=\"by_Dnozt8pgc4dTAmnwv\">Matthew Scherer (2015): </span><a href=\"http://papers.ssrn.com/abstract=2609777\"><span class=\"by_Dnozt8pgc4dTAmnwv\">Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies</span></a><span class=\"by_Dnozt8pgc4dTAmnwv\">. Harvard Journal of Law &amp; Technology.</span></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Carl Shulman &amp; Stuart Armstrong (2009): </span><a href=\"http://intelligence.org/files/ArmsControl.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">Arms control and intelligence explosions</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. European Conference on Computing and Philosophy.</span></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Roman Yampolskiy &amp; Joshua Fox (2012): </span><a href=\"http://intelligence.org/files/SafetyEngineering.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">Safety Engineering for Artificial General Intelligence</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. Topoi.</span></li><li><span class=\"by_mcKSiwq2TBrTMZS6X\">Mohit Kaushal &amp; Scott Nolan (2015): </span><a href=\"http://www.brookings.edu/blogs/techtank/posts/2015/04/14-understanding-artificial-intelligence\"><span class=\"by_mcKSiwq2TBrTMZS6X\">Understanding Artificial Intelligence</span></a><span class=\"by_mcKSiwq2TBrTMZS6X\">. Brookings.</span></li></ul><h2 id=\"See_also\"><span class=\"by_qxJ28GN72aiJu96iF\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/ai-arms-race\"><span class=\"by_qxJ28GN72aiJu96iF\">AI arms race</span></a></li><li><a href=\"https://www.lesswrong.com/tag/agi-sputnik-moment\"><span class=\"by_qxJ28GN72aiJu96iF\">AGI Sputnik moment</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_qxJ28GN72aiJu96iF\">Existential risk</span></a></li><li><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_qxJ28GN72aiJu96iF\">Unfriendly artificial intelligence</span></a></li></ul>",
      "sections": [
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 13,
    "description": {
      "markdown": "**Regulation and AI risk** is the debate on whether regulation could be used to reduce the risks of [Unfriendly AI](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence), and what forms of regulation would be appropriate.\n\nSeveral authors have advocated AI research to be regulated, but been vague on the details. Yampolskiy & Fox (2012) note that university research programs in the social and medical sciences are overseen by institutional review boards, and propose setting up analogous review boards to evaluate potential AGI research. In order to be successful, AI regulation would have to be global, and there is the potential for an [AI arms race](https://www.lesswrong.com/tag/ai-arms-race) between different nations. Partially because of this, McGinnis (2010) argues that the government should not attempt to regulate AGI development. Rather, it should concentrate on providing funding to research projects intended to create safe AGI. Kaushal & Nolan (2015) point out that regulations on AGI development would result in a speed advantage for any project willing to skirt the regulations, and instead propose government funding (possibly in the form of an \"AI Manhattan Project\") for AGI projects meeting particular criteria.\n\nWhile Shulman & Armstrong (2009) argue the unprecedentedly destabilizing effect of AGI could be a cause for world leaders to cooperate more than usual, the opposite argument can be made as well. Gubrud (1997) argues that molecular nanotechnology could make countries more self-reliant and international cooperation considerably harder, and that AGI could contribute to such a development. AGI technology is also much harder to detect than e.g. nuclear technology is - AGI research can be done in a garage, while nuclear weapons require a substantial infrastructure (McGinnis 2010). On the other hand, Scherer (2015) argues that artificial intelligence could nevertheless be susceptible to regulation due to the increasing prominence of governmental entities and large corporations in AI research and development.\n\nGoertzel & Pitt (2012) suggest that for regulation to be enacted, there might need to be an [AGI Sputnik moment](https://www.lesswrong.com/tag/agi-sputnik-moment) \\- a technological achievement that makes the possibility of AGI evident to the public and policy makers. They note that after such a moment, it might not take a very long time for full human-level AGI to be developed, while the negotiations required to enact new kinds of arms control treaties would take considerably longer.\n\nReferences\n----------\n\n*   Ben Goertzel & Joel Pitt (2012): [Nine Ways to Bias Open-Source AGI Toward Friendliness](http://jetpress.org/v22/goertzel-pitt.htm). Journal of Evolution and Technology - Vol. 22 Issue 1 – pgs 116-141.\n*   Mark Gubrud (1997): [Nanotechnology and International Security](http://www.foresight.org/Conferences/MNT05/Papers/Gubrud/). Fifth Foresight Conference on Molecular Nanotechnology.\n*   John McGinnis (2010): [Accelerating AI](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1593851). Northwestern University Law Review.\n*   Matthew Scherer (2015): [Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies](http://papers.ssrn.com/abstract=2609777). Harvard Journal of Law & Technology.\n*   Carl Shulman & Stuart Armstrong (2009): [Arms control and intelligence explosions](http://intelligence.org/files/ArmsControl.pdf). European Conference on Computing and Philosophy.\n*   Roman Yampolskiy & Joshua Fox (2012): [Safety Engineering for Artificial General Intelligence](http://intelligence.org/files/SafetyEngineering.pdf). Topoi.\n*   Mohit Kaushal & Scott Nolan (2015): [Understanding Artificial Intelligence](http://www.brookings.edu/blogs/techtank/posts/2015/04/14-understanding-artificial-intelligence). Brookings.\n\nSee also\n--------\n\n*   [AI arms race](https://www.lesswrong.com/tag/ai-arms-race)\n*   [AGI Sputnik moment](https://www.lesswrong.com/tag/agi-sputnik-moment)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2c2",
    "name": "Ontological Crisis",
    "core": null,
    "slug": "ontological-crisis",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_2YpRin5m5vBJu8Tg9\">Ontological crisis</span></strong><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> is </span><span class=\"by_LedhurJxi3baDAKDZ\">a</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> term </span><span class=\"by_LedhurJxi3baDAKDZ\">coined to describe</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> the crisis </span><span class=\"by_LedhurJxi3baDAKDZ\">an agent, human or not, goes through</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> when its </span><span class=\"by_LedhurJxi3baDAKDZ\">model - its ontology -</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> of reality changes.</span></span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">In the human context, a clear example of an ontological crisis is a believer’s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.</span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">When dealing with artificial agents, we, as their creators, are directly interested in their goals. That is, as Peter de Blanc puts it, when we create something we want it to be useful. As such we will have to define the artificial agent’s ontology – but since a fixed ontology severely limits its usefulness we have to think about adaptability. In his 2011 paper, the author then proposes a method to map old ontologies into new ones, thus adapting the agent’s utility functions and avoiding a crisis.</span></p><p><span><span class=\"by_2YpRin5m5vBJu8Tg9\">This </span><span class=\"by_LedhurJxi3baDAKDZ\">crisis, in</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> the context of an </span></span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_2YpRin5m5vBJu8Tg9\">AGI</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\">, could in the worst case pose an </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_2YpRin5m5vBJu8Tg9\">existential risk</span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> when old preferences and goals continue to be used. Another possibility is that the AGI loses all ability to comprehend the world, and would pose no </span><span class=\"by_LedhurJxi3baDAKDZ\">threat at all.</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> If an AGI reevaluates its preferences after its ontological crisis,</span><span class=\"by_LedhurJxi3baDAKDZ\"> for example in the way mentioned above,</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> very </span></span><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_2YpRin5m5vBJu8Tg9\">unfriendly</span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> </span><span class=\"by_Sp5wM4aRAhNERd4oY\">behaviors</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> could arise. Depending on the extent of the reevaluations, the AGI's changes may be detected and safely </span><span class=\"by_LedhurJxi3baDAKDZ\">fixed. On the other hand, it could</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> go undetected until they </span><span class=\"by_LedhurJxi3baDAKDZ\">go wrong - which shows how it is of our interest to deeply explore</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> ontological </span><span class=\"by_LedhurJxi3baDAKDZ\">adaptation methods when designing AI.</span></span></p><h2 id=\"Further_Reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further Reading &amp; References</span></h2><ul><li><a href=\"http://arxiv.org/abs/1105.3821\"><span><span class=\"by_LedhurJxi3baDAKDZ\">Ontological Crises</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> in </span><span class=\"by_LedhurJxi3baDAKDZ\">Artificial Agents' Value Systems</span></span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> by </span><span class=\"by_LedhurJxi3baDAKDZ\">Peter de Blanc</span></span></li></ul><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"http://lesswrong.com/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/\"><span class=\"by_2YpRin5m5vBJu8Tg9\">AI ontology crises: an informal typology</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Stuart Armstrong</span></li><li><a href=\"http://lesswrong.com/lw/xl/eutopia_is_scary/\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Eutopia is Scary</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Eliezer Yudkowsky</span></li><li><a href=\"http://lesswrong.com/lw/fyb/ontological_crisis_in_humans/\"><span class=\"by_qxJ28GN72aiJu96iF\">Ontological Crisis in Humans</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> by Wei Dai</span></li></ul><h2 id=\"See_also\"><span class=\"by_LedhurJxi3baDAKDZ\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_LedhurJxi3baDAKDZ\">Evolution</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\"><span class=\"by_LedhurJxi3baDAKDZ\">Adaptation executers</span></a></li></ul>",
      "sections": [
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 11,
    "description": {
      "markdown": "**Ontological crisis** is a term coined to describe the crisis an agent, human or not, goes through when its model - its ontology - of reality changes.\n\nIn the human context, a clear example of an ontological crisis is a believer’s loss of faith in God. Their motivations and goals, coming from a very specific view of life suddenly become obsolete and maybe even nonsense in the face of this new configuration. The person will then experience a deep crisis and go through the psychological task of reconstructing its set of preferences according the new world view.\n\nWhen dealing with artificial agents, we, as their creators, are directly interested in their goals. That is, as Peter de Blanc puts it, when we create something we want it to be useful. As such we will have to define the artificial agent’s ontology – but since a fixed ontology severely limits its usefulness we have to think about adaptability. In his 2011 paper, the author then proposes a method to map old ontologies into new ones, thus adapting the agent’s utility functions and avoiding a crisis.\n\nThis crisis, in the context of an [AGI](https://wiki.lesswrong.com/wiki/AGI), could in the worst case pose an [existential risk](https://www.lesswrong.com/tag/existential-risk) when old preferences and goals continue to be used. Another possibility is that the AGI loses all ability to comprehend the world, and would pose no threat at all. If an AGI reevaluates its preferences after its ontological crisis, for example in the way mentioned above, very [unfriendly](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) behaviors could arise. Depending on the extent of the reevaluations, the AGI's changes may be detected and safely fixed. On the other hand, it could go undetected until they go wrong - which shows how it is of our interest to deeply explore ontological adaptation methods when designing AI.\n\nFurther Reading & References\n----------------------------\n\n*   [Ontological Crises in Artificial Agents' Value Systems](http://arxiv.org/abs/1105.3821) by Peter de Blanc\n\nNotable Posts\n-------------\n\n*   [AI ontology crises: an informal typology](http://lesswrong.com/r/discussion/lw/827/ai_ontology_crises_an_informal_typology/) by Stuart Armstrong\n*   [Eutopia is Scary](http://lesswrong.com/lw/xl/eutopia_is_scary/) by Eliezer Yudkowsky\n*   [Ontological Crisis in Humans](http://lesswrong.com/lw/fyb/ontological_crisis_in_humans/) by Wei Dai\n\nSee also\n--------\n\n*   [Evolution](https://www.lesswrong.com/tag/evolution)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2bd",
    "name": "Computing Overhang",
    "core": null,
    "slug": "computing-overhang",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_LedhurJxi3baDAKDZ\">Computing</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> overhang</span></span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_LedhurJxi3baDAKDZ\">refers to a situation </span><span class=\"by_qxJ28GN72aiJu96iF\">where new</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_LedhurJxi3baDAKDZ\">algorithms can exploit </span><span class=\"by_qxJ28GN72aiJu96iF\">existing</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> computing power </span><span class=\"by_qxJ28GN72aiJu96iF\">far</span><span class=\"by_LedhurJxi3baDAKDZ\"> more </span><span class=\"by_qxJ28GN72aiJu96iF\">efficiently</span><span class=\"by_LedhurJxi3baDAKDZ\"> than before. This </span><span class=\"by_qxJ28GN72aiJu96iF\">can happen if previously used algorithms have been suboptimal.</span></span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">In</span><span class=\"by_LedhurJxi3baDAKDZ\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">context</span><span class=\"by_LedhurJxi3baDAKDZ\"> of </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_qxJ28GN72aiJu96iF\">Artificial General Intelligence</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">, this signifies a situation where it becomes possible to create AGIs that</span><span class=\"by_LedhurJxi3baDAKDZ\"> can </span><span class=\"by_qxJ28GN72aiJu96iF\">be run using only</span><span class=\"by_LedhurJxi3baDAKDZ\"> a </span><span class=\"by_qxJ28GN72aiJu96iF\">small fraction of the easily available hardware resources.</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> This could </span><span class=\"by_qxJ28GN72aiJu96iF\">lead to</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> an </span></span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">intelligence explosion</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">, or </span><span class=\"by_qxJ28GN72aiJu96iF\">to</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> a </span><span class=\"by_qxJ28GN72aiJu96iF\">massive increase in the number</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_qxJ28GN72aiJu96iF\">AGIs, as they could be easily copied to run on countless computers. This could make AGIs much more powerful than before,</span><span class=\"by_LedhurJxi3baDAKDZ\"> and </span><span class=\"by_qxJ28GN72aiJu96iF\">present</span><span class=\"by_LedhurJxi3baDAKDZ\"> an </span></span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_qxJ28GN72aiJu96iF\">existential risk</span></a><span class=\"by_LedhurJxi3baDAKDZ\">.</span></p><h2 id=\"Examples\"><span class=\"by_LedhurJxi3baDAKDZ\">Examples</span></h2><p><span><span class=\"by_qxJ28GN72aiJu96iF\">In 2010,</span><span class=\"by_LedhurJxi3baDAKDZ\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">President's</span><span class=\"by_LedhurJxi3baDAKDZ\"> Council </span><span class=\"by_qxJ28GN72aiJu96iF\">of Advisors on Science and Technology </span></span><a href=\"http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">reported on</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> benchmark production planning model having become faster</span><span class=\"by_LedhurJxi3baDAKDZ\"> by a factor of 43 million </span><span class=\"by_qxJ28GN72aiJu96iF\">between 1988 and 2003. Of</span><span class=\"by_LedhurJxi3baDAKDZ\"> this </span><span class=\"by_qxJ28GN72aiJu96iF\">improvement,</span><span class=\"by_LedhurJxi3baDAKDZ\"> only a factor of </span><span class=\"by_qxJ28GN72aiJu96iF\">roughly 1,000</span><span class=\"by_LedhurJxi3baDAKDZ\"> was due</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to </span><span class=\"by_LedhurJxi3baDAKDZ\">better hardware, while a factor of </span><span class=\"by_qxJ28GN72aiJu96iF\">43,000</span><span class=\"by_LedhurJxi3baDAKDZ\"> came from algorithmic improvements. This clearly reflects a situation where new programming methods</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> were </span><span class=\"by_LedhurJxi3baDAKDZ\">able to use </span><span class=\"by_qxJ28GN72aiJu96iF\">available computing power </span><span class=\"by_LedhurJxi3baDAKDZ\">more </span><span class=\"by_qxJ28GN72aiJu96iF\">efficiently.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">As</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">today, enormous</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> amounts of computing power is currently available in the form of supercomputers or distributed computing. Large AI projects </span><span class=\"by_LedhurJxi3baDAKDZ\">can</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> grow to fill these resources by using</span><span class=\"by_LedhurJxi3baDAKDZ\"> deeper and</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> deeper search trees, </span><span class=\"by_KLJmn2HYWEu4tBKcC\">such as high-powered chess programs,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> or by performing large amounts of parallel operations on extensive databases, such as IBM's Watson playing Jeopardy. While the extra depth and breadth are helpful, it is likely that </span><span class=\"by_LedhurJxi3baDAKDZ\">a</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> simple brute-force extension of techniques is not the optimal use of the </span><span class=\"by_qxJ28GN72aiJu96iF\">available</span><span class=\"by_LedhurJxi3baDAKDZ\"> </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">computing </span><span class=\"by_LedhurJxi3baDAKDZ\">resources. This leaves the need</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> for </span><span class=\"by_LedhurJxi3baDAKDZ\">improvement</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_LedhurJxi3baDAKDZ\">on</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">side of algorithmic implementations, where most work is currently focused on.</span></span></p><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">Though estimates of </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">whole brain emulation</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> place that level of computing power at least a decade away, it is very unlikely that the algorithms used by the human brain are the most computationally efficient for producing AI. This </span><span class=\"by_LedhurJxi3baDAKDZ\">happens mainly</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> because </span><span class=\"by_LedhurJxi3baDAKDZ\">our brains evolved during a natural selection process and thus weren't deliberatly created with</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">goal of being modeled by AI.</span></span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">As Yudkoswky </span><a href=\"http://intelligence.org/files/LOGI.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">puts it</span></a><span class=\"by_LedhurJxi3baDAKDZ\">, human intelligence, created by this \"blind\" evolutionary process, has only recently developed the ability for planning and forward thinking - </span><i><span class=\"by_LedhurJxi3baDAKDZ\">deliberation</span></i><span class=\"by_LedhurJxi3baDAKDZ\">. On the other hand, the rest and almost all our cognitive tools were the result of ancestral selection pressures, forming the roots of almost all our behavior. As such, when considering the design of complex systems where the designer - us - collaborates with the system being constructed, we are faced with a new signature and a different way to achieve AGI that's completely different than the process that gave birth to our brains.</span></p><h2 id=\"References\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">References</span></h2><ul><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Muehlhauser, Luke; Salamon, Anna (2012). </span><a href=\"http://intelligence.org/files/IE-EI.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"Intelligence Explosion: Evidence and Import\"</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. in Eden, Amnon; Søraker, Johnny; Moor, James H. et al.. </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">The singularity hypothesis: A scientific and philosophical assessment</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Berlin: Springer.</span></li></ul><h2 id=\"See_also\"><span class=\"by_LedhurJxi3baDAKDZ\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/optimization\"><span class=\"by_LedhurJxi3baDAKDZ\">Optimization process</span></a></li><li><a href=\"https://www.lesswrong.com/tag/optimization\"><span class=\"by_LedhurJxi3baDAKDZ\">Optimization</span></a></li></ul>",
      "sections": [
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 7,
    "description": {
      "markdown": "**Computing overhang** refers to a situation where new algorithms can exploit existing computing power far more efficiently than before. This can happen if previously used algorithms have been suboptimal.\n\nIn the context of [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), this signifies a situation where it becomes possible to create AGIs that can be run using only a small fraction of the easily available hardware resources. This could lead to an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), or to a massive increase in the number of AGIs, as they could be easily copied to run on countless computers. This could make AGIs much more powerful than before, and present an [existential risk](https://www.lesswrong.com/tag/existential-risk).\n\nExamples\n--------\n\nIn 2010, the President's Council of Advisors on Science and Technology [reported on](http://www.whitehouse.gov/sites/default/files/microsites/ostp/pcast-nitrd-report-2010.pdf) benchmark production planning model having become faster by a factor of 43 million between 1988 and 2003. Of this improvement, only a factor of roughly 1,000 was due to better hardware, while a factor of 43,000 came from algorithmic improvements. This clearly reflects a situation where new programming methods were able to use available computing power more efficiently.\n\nAs of today, enormous amounts of computing power is currently available in the form of supercomputers or distributed computing. Large AI projects can grow to fill these resources by using deeper and deeper search trees, such as high-powered chess programs, or by performing large amounts of parallel operations on extensive databases, such as IBM's Watson playing Jeopardy. While the extra depth and breadth are helpful, it is likely that a simple brute-force extension of techniques is not the optimal use of the available computing resources. This leaves the need for improvement on the side of algorithmic implementations, where most work is currently focused on.\n\nThough estimates of [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation) place that level of computing power at least a decade away, it is very unlikely that the algorithms used by the human brain are the most computationally efficient for producing AI. This happens mainly because our brains evolved during a natural selection process and thus weren't deliberatly created with the goal of being modeled by AI.\n\nAs Yudkoswky [puts it](http://intelligence.org/files/LOGI.pdf), human intelligence, created by this \"blind\" evolutionary process, has only recently developed the ability for planning and forward thinking - *deliberation*. On the other hand, the rest and almost all our cognitive tools were the result of ancestral selection pressures, forming the roots of almost all our behavior. As such, when considering the design of complex systems where the designer - us - collaborates with the system being constructed, we are faced with a new signature and a different way to achieve AGI that's completely different than the process that gave birth to our brains.\n\nReferences\n----------\n\n*   Muehlhauser, Luke; Salamon, Anna (2012). [\"Intelligence Explosion: Evidence and Import\"](http://intelligence.org/files/IE-EI.pdf). in Eden, Amnon; Søraker, Johnny; Moor, James H. et al.. *The singularity hypothesis: A scientific and philosophical assessment*. Berlin: Springer.\n\nSee also\n--------\n\n*   [Optimization process](https://www.lesswrong.com/tag/optimization)\n*   [Optimization](https://www.lesswrong.com/tag/optimization)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b5",
    "name": "Recursive Self-Improvement",
    "core": null,
    "slug": "recursive-self-improvement",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_5wu9jG4pm9q6xjZ9R\">Recursive self-improvement</span></strong><span><span class=\"by_woC2b5rav5sGrAo3E\"> refers to the property of making improvements on </span><span class=\"by_qxJ28GN72aiJu96iF\">one's</span><span class=\"by_woC2b5rav5sGrAo3E\"> own ability of making self-improvements. It</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is an approach to </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Artificial General Intelligence</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> that allows a system to make adjustments to its own functionality resulting in improved performance. The system could then feedback on itself with each cycle reaching ever higher levels of intelligence resulting in either a hard or soft </span><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">AI takeoff</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\">.</span></p><p><span><span class=\"by_woC2b5rav5sGrAo3E\">An agent can self-improve and get a linear succession of improvements, however if it</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is </span><span class=\"by_woC2b5rav5sGrAo3E\">able to improve</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> its ability </span><span class=\"by_woC2b5rav5sGrAo3E\">of making self-improvements, then each step will yield exponentially more improvements then the next one.</span></span></p><h2 id=\"Recursive_self_improvement_and_AI_takeoff\"><span class=\"by_woC2b5rav5sGrAo3E\">Recursive self-improvement and </span><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span class=\"by_woC2b5rav5sGrAo3E\">AI takeoff</span></a></h2><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">Recursively self-improving AI is considered to be the push behind the </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">intelligence explosion</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">. While any sufficiently intelligent AI will be able to improve </span><span class=\"by_qxJ28GN72aiJu96iF\">itself,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span></span><a href=\"https://www.lesswrong.com/tag/seed-ai\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Seed </span><span class=\"by_qxJ28GN72aiJu96iF\">AIs</span></span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_qxJ28GN72aiJu96iF\">are specifically designed to use</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> recursive self-improvement as </span><span class=\"by_qxJ28GN72aiJu96iF\">their</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> primary method of gaining intelligence. </span><span class=\"by_qxJ28GN72aiJu96iF\">Architectures that had not been designed with this goal in mind, such as</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> neural </span><span class=\"by_qxJ28GN72aiJu96iF\">networks</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> or large \"hand-coded\" projects like </span></span><a href=\"https://www.lesswrong.com/tag/cyc\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Cyc</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, would have a harder time self-improving.</span></p><p><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_woC2b5rav5sGrAo3E\">Eliezer Yudkowsky</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> argues that a recursively self-improvement AI seems likely to deliver a hard AI takeoff – a fast, abruptly, local increase in capability </span><span class=\"by_qxJ28GN72aiJu96iF\">-</span><span class=\"by_woC2b5rav5sGrAo3E\"> since the exponential increase in intelligence would yield an exponential return in benefits and resources that would feed even more returns in the next </span><span class=\"by_qxJ28GN72aiJu96iF\">step,</span><span class=\"by_woC2b5rav5sGrAo3E\"> and so on. In his view a soft takeoff scenario seems unlikely: </span><span class=\"by_qxJ28GN72aiJu96iF\">\"it</span><span class=\"by_woC2b5rav5sGrAo3E\"> should either flatline or blow up. You would need exactly the right law of diminishing returns to fly through the extremely narrow soft takeoff keyhole.</span><span class=\"by_qxJ28GN72aiJu96iF\">\"</span></span><a href=\"http://lesswrong.com/lw/we/recursive_selfimprovement/\"><span class=\"by_woC2b5rav5sGrAo3E\">1</span></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">Yudkowsky argues that there</span><span class=\"by_woC2b5rav5sGrAo3E\"> are several points which seem to support the </span></span><a href=\"https://wiki.lesswrong.com/wiki/AI_takeoff#Hard_takeoff\"><span class=\"by_woC2b5rav5sGrAo3E\">hard takeoff scenario</span></a><span class=\"by_woC2b5rav5sGrAo3E\">. Some of them are the fact that one improvement seems to lead the way to another, </span><a href=\"https://www.lesswrong.com/tag/computing-overhang\"><span class=\"by_woC2b5rav5sGrAo3E\">hardware overhang</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> and the fact that sometimes- when navigating through problem space - one can find a succession of extremely easy to solve problems. These are all reasons for suddenly and abruptly increases in capability. On the other hand, </span><a href=\"https://www.lesswrong.com/tag/robin-hanson\"><span class=\"by_woC2b5rav5sGrAo3E\">Robin Hanson</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> argues that there will be mostly a slow and gradual accumulation of improvements, without a sharp change.</span></p><h2 id=\"Self_improvement_in_humans\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Self-improvement in humans</span></h2><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">The human species has made an enormous amount of progress since evolving around fifty thousand years ago. This is because we can pass on knowledge and infrastructure from previous generations. This is a type of self-improvement, but it is not </span><i><span class=\"by_5wu9jG4pm9q6xjZ9R\">recursive</span></i><span class=\"by_5wu9jG4pm9q6xjZ9R\">. If we never learned to modify our own brains, then we would eventually reach the point where making new discoveries required more knowledge than could be gained in a human lifetime. All human progress to date has been limited by the hardware we are born with, which is the same hardware Homo sapiens were born with fifty thousand years ago.</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">\"True\"</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> recursive self-improvement will come when we discover how to</span><span class=\"by_qxJ28GN72aiJu96iF\"> drastically</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> modify or augment our own brains in order to be more intelligent. This would lead us to more quickly being able to discover how to become even more intelligent.</span></span></p><h2 id=\"Recursive_self_improvement_and_Instrumental_value\"><span class=\"by_woC2b5rav5sGrAo3E\">Recursive self-improvement and </span><a href=\"https://www.lesswrong.com/tag/instrumental-value\"><span class=\"by_woC2b5rav5sGrAo3E\">Instrumental value</span></a></h2><p><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp; &nbsp; &nbsp;Main article: </span></i><a href=\"https://wiki.lesswrong.com/wiki/Basic_AI_drives\"><i><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Basic AI drives</span></u></i></a></p><p><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_woC2b5rav5sGrAo3E\">Nick Bostrom</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> and </span><a href=\"https://en.wikipedia.org/wiki/Steve_Omohundro\"><span class=\"by_woC2b5rav5sGrAo3E\">Steve Omohundro</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> have separately</span><a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><span class=\"by_XtphY3uYHwruKqDyG\">[2]</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> argued</span><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\"><span class=\"by_XtphY3uYHwruKqDyG\">[3]</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> that despite the fact that values and intelligence are independent, any </span><span class=\"by_qxJ28GN72aiJu96iF\">recursively</span><span class=\"by_woC2b5rav5sGrAo3E\"> self-</span><span class=\"by_qxJ28GN72aiJu96iF\">improving</span><span class=\"by_woC2b5rav5sGrAo3E\"> intelligence would likely possess a common set of instrumental values which are useful for achieving any kind of </span></span><a href=\"https://www.lesswrong.com/tag/terminal-value\"><span class=\"by_qxJ28GN72aiJu96iF\">goal</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\">. </span><span class=\"by_qxJ28GN72aiJu96iF\">As a system's intelligence continued modifying itself towards greater intelligence, it would</span><span class=\"by_woC2b5rav5sGrAo3E\"> be </span><span class=\"by_qxJ28GN72aiJu96iF\">likely</span><span class=\"by_woC2b5rav5sGrAo3E\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">adopt more</span><span class=\"by_woC2b5rav5sGrAo3E\"> of these </span><span class=\"by_qxJ28GN72aiJu96iF\">behaviors.</span></span></p><h2 id=\"Blog_posts\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/we/recursive_selfimprovement/\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Recursive Self Improvement</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by Eliezer Yudkowsky</span></li><li><a href=\"http://lesswrong.com/lw/w5/cascades_cycles_insight/\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Cascades, Cycles, Insight...</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by Eliezer Yudkowsky</span></li><li><a href=\"http://lesswrong.com/lw/w6/recursion_magic/\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">...Recursion, Magic</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by Eliezer Yudkowsky</span></li></ul><h2 id=\"See_also\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Intelligence explosion</span></a></li><li><a href=\"https://www.lesswrong.com/tag/singularity\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Singularity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/seed-ai\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Seed AI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/gödel-machine\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Gödel machine</span></a></li><li><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">AI takeoff</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">External links</span></h2><ul><li><a href=\"http://intelligence.org/files/LOGI.pdf\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Seed AI</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> description from </span><span class=\"by_JPwZspDjBcfwwuy7W\">MIRI.</span></span></li><li><a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Risks from Artificial Intelligence</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by Eliezer Yudkowsky.</span></li><li><a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Advantages of Artificial Intelligence</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by Kaj Sotala</span></li><li><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Speculations Concerning the First Ultraintelligent Machine</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by I.J. Good</span></li></ul>",
      "sections": [
        {
          "title": "Recursive self-improvement and AI takeoff",
          "anchor": "Recursive_self_improvement_and_AI_takeoff",
          "level": 1
        },
        {
          "title": "Self-improvement in humans",
          "anchor": "Self_improvement_in_humans",
          "level": 1
        },
        {
          "title": "Recursive self-improvement and Instrumental value",
          "anchor": "Recursive_self_improvement_and_Instrumental_value",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 20,
    "description": {
      "markdown": "**Recursive self-improvement** refers to the property of making improvements on one's own ability of making self-improvements. It is an approach to [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) that allows a system to make adjustments to its own functionality resulting in improved performance. The system could then feedback on itself with each cycle reaching ever higher levels of intelligence resulting in either a hard or soft [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff).\n\nAn agent can self-improve and get a linear succession of improvements, however if it is able to improve its ability of making self-improvements, then each step will yield exponentially more improvements then the next one.\n\nRecursive self-improvement and [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff)\n-------------------------------------------------------------------------------------\n\nRecursively self-improving AI is considered to be the push behind the [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion). While any sufficiently intelligent AI will be able to improve itself, [Seed AIs](https://www.lesswrong.com/tag/seed-ai) are specifically designed to use recursive self-improvement as their primary method of gaining intelligence. Architectures that had not been designed with this goal in mind, such as neural networks or large \"hand-coded\" projects like [Cyc](https://www.lesswrong.com/tag/cyc), would have a harder time self-improving.\n\n[Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) argues that a recursively self-improvement AI seems likely to deliver a hard AI takeoff – a fast, abruptly, local increase in capability - since the exponential increase in intelligence would yield an exponential return in benefits and resources that would feed even more returns in the next step, and so on. In his view a soft takeoff scenario seems unlikely: \"it should either flatline or blow up. You would need exactly the right law of diminishing returns to fly through the extremely narrow soft takeoff keyhole.\"[1](http://lesswrong.com/lw/we/recursive_selfimprovement/).\n\nYudkowsky argues that there are several points which seem to support the [hard takeoff scenario](https://wiki.lesswrong.com/wiki/AI_takeoff#Hard_takeoff). Some of them are the fact that one improvement seems to lead the way to another, [hardware overhang](https://www.lesswrong.com/tag/computing-overhang) and the fact that sometimes- when navigating through problem space - one can find a succession of extremely easy to solve problems. These are all reasons for suddenly and abruptly increases in capability. On the other hand, [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) argues that there will be mostly a slow and gradual accumulation of improvements, without a sharp change.\n\nSelf-improvement in humans\n--------------------------\n\nThe human species has made an enormous amount of progress since evolving around fifty thousand years ago. This is because we can pass on knowledge and infrastructure from previous generations. This is a type of self-improvement, but it is not *recursive*. If we never learned to modify our own brains, then we would eventually reach the point where making new discoveries required more knowledge than could be gained in a human lifetime. All human progress to date has been limited by the hardware we are born with, which is the same hardware Homo sapiens were born with fifty thousand years ago.\n\n\"True\" recursive self-improvement will come when we discover how to drastically modify or augment our own brains in order to be more intelligent. This would lead us to more quickly being able to discover how to become even more intelligent.\n\nRecursive self-improvement and [Instrumental value](https://www.lesswrong.com/tag/instrumental-value)\n-----------------------------------------------------------------------------------------------------\n\n*Main article:* [*Basic AI drives*](https://wiki.lesswrong.com/wiki/Basic_AI_drives)\n\n[Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) and [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro) have separately[\\[2\\]](http://www.nickbostrom.com/superintelligentwill.pdf) argued[\\[3\\]](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf) that despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a common set of instrumental values which are useful for achieving any kind of [goal](https://www.lesswrong.com/tag/terminal-value). As a system's intelligence continued modifying itself towards greater intelligence, it would be likely to adopt more of these behaviors.\n\nBlog posts\n----------\n\n*   [Recursive Self Improvement](http://lesswrong.com/lw/we/recursive_selfimprovement/) by Eliezer Yudkowsky\n*   [Cascades, Cycles, Insight...](http://lesswrong.com/lw/w5/cascades_cycles_insight/) by Eliezer Yudkowsky\n*   [...Recursion, Magic](http://lesswrong.com/lw/w6/recursion_magic/) by Eliezer Yudkowsky\n\nSee also\n--------\n\n*   [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)\n*   [Singularity](https://www.lesswrong.com/tag/singularity)\n*   [Seed AI](https://www.lesswrong.com/tag/seed-ai)\n*   [Gödel machine](https://www.lesswrong.com/tag/gödel-machine)\n*   [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff)\n\nExternal links\n--------------\n\n*   [Seed AI](http://intelligence.org/files/LOGI.pdf) description from MIRI.\n*   [Risks from Artificial Intelligence](http://intelligence.org/files/AIPosNegFactor.pdf) by Eliezer Yudkowsky.\n*   [Advantages of Artificial Intelligence](http://www.xuenay.net/Papers/DigitalAdvantages.pdf) by Kaj Sotala\n*   [Speculations Concerning the First Ultraintelligent Machine](http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf) by I.J. Good"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b1",
    "name": "Whole Brain Emulation",
    "core": null,
    "slug": "whole-brain-emulation",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_5wu9jG4pm9q6xjZ9R\">Whole Brain Emulation</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_2YpRin5m5vBJu8Tg9\">or </span></span><strong><span class=\"by_2YpRin5m5vBJu8Tg9\">WBE</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is a proposed </span><span class=\"by_2YpRin5m5vBJu8Tg9\">technique which</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> involves transferring the information contained within a brain onto a computing substrate. The brain </span><span class=\"by_2YpRin5m5vBJu8Tg9\">can then be simulated, creating a machine intelligence. The concept is often discussed</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> in </span><span class=\"by_2YpRin5m5vBJu8Tg9\">context of scanning the brain of </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">a </span><span class=\"by_2YpRin5m5vBJu8Tg9\">person, known as </span></span><a href=\"https://www.lesswrong.com/tag/mind-uploading\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mind Uploading</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\">.</span></p><p><span><span class=\"by_2YpRin5m5vBJu8Tg9\">WBE is sometimes seen as an easy </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">way </span><span class=\"by_2YpRin5m5vBJu8Tg9\">to creating intelligent computers, as</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span><span class=\"by_2YpRin5m5vBJu8Tg9\">only innovations necessary are greatly increased processor speed and scanning resolution. Advocates</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">WBE claim technological improvement rates such as </span></span><a href=\"https://wiki.lesswrong.com/wiki/Moore's_law\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Moore's law</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> will make WBE inevitable.</span></p><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">The </span><span class=\"by_2YpRin5m5vBJu8Tg9\">exact level of detail </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">required </span><span class=\"by_2YpRin5m5vBJu8Tg9\">for an accurate simulation</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">a brain's mind is presently uncertain,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><span class=\"by_2YpRin5m5vBJu8Tg9\">will determine the difficulty of creating WBE. </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">The feasibility of such a project has been examined in detail </span><span class=\"by_qxJ28GN72aiJu96iF\">in</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> </span></span><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute-fhi\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Future of Humanity </span><span class=\"by_baGAQoNAH4hXaC6qf\">Institute</span></span></a><span><span class=\"by_baGAQoNAH4hXaC6qf\">'</span><span class=\"by_qxJ28GN72aiJu96iF\">s </span></span><a href=\"https://www.lesswrong.com/tag/brain-emulation-roadmap\"><span><span class=\"by_qxJ28GN72aiJu96iF\">Whole Brain </span><span class=\"by_baGAQoNAH4hXaC6qf\">Emulation: A</span><span class=\"by_qxJ28GN72aiJu96iF\"> Roadmap</span></span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">. The Roadmap</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> concluded that a human brain emulation would be possible before </span><span class=\"by_qxJ28GN72aiJu96iF\">mid-century, providing that current technology trends kept up and providing that there would be sufficient investments.</span></span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">Several approaches for WBE have been suggested:</span></p><ul><li><span class=\"by_qxJ28GN72aiJu96iF\">A brain could be cut into small slices, which would then be scanned into a computer.</span><a href=\"#fn1\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">1</span></sup></a></li><li><a href=\"https://www.lesswrong.com/tag/brain-computer-interfaces\"><span class=\"by_qxJ28GN72aiJu96iF\">Brain-computer interfaces</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> could slowly replace portions of </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">the </span><span class=\"by_qxJ28GN72aiJu96iF\">brain with computers</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><span class=\"by_qxJ28GN72aiJu96iF\">allow the mind to grow onto a </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">computing </span><span class=\"by_qxJ28GN72aiJu96iF\">substrate.</span></span><a href=\"#fn2\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">2</span></sup></a><a href=\"#fn3\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">3</span></sup></a></li><li><span><span class=\"by_qxJ28GN72aiJu96iF\">Resources such as personality tests and a person's writings could be used</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">construct a model of the person.</span></span><a href=\"#fn4\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">4</span></sup></a></li></ul><p><span><span class=\"by_2YpRin5m5vBJu8Tg9\">A digitally emulated brain</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_qxJ28GN72aiJu96iF\">could</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> have </span><span class=\"by_2YpRin5m5vBJu8Tg9\">several advantages over a biological </span><span class=\"by_qxJ28GN72aiJu96iF\">one</span></span><a href=\"#fn5\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">5</span></sup></a><span><span class=\"by_qxJ28GN72aiJu96iF\">.</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> It </span><span class=\"by_qxJ28GN72aiJu96iF\">might</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> be able </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">to </span><span class=\"by_qxJ28GN72aiJu96iF\">run faster than biological brains,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_2YpRin5m5vBJu8Tg9\">copy itself, and </span><span class=\"by_qxJ28GN72aiJu96iF\">take advantage of backups while experimenting</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> with </span><span class=\"by_qxJ28GN72aiJu96iF\">self-modification.</span></span></p><p><span><span class=\"by_2YpRin5m5vBJu8Tg9\">Whole brain emulation will also</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> create a </span><span class=\"by_2YpRin5m5vBJu8Tg9\">number of ethical challenges relating</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to the </span><span class=\"by_2YpRin5m5vBJu8Tg9\">nature</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">personhood, rights,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><span class=\"by_2YpRin5m5vBJu8Tg9\">social inequality. </span></span><a href=\"https://www.lesswrong.com/tag/robin-hanson\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Robin Hanson</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> proposes that an uploaded mind </span><a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\"><span><span class=\"by_2YpRin5m5vBJu8Tg9\">might copy itself to work until </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">the </span><span class=\"by_2YpRin5m5vBJu8Tg9\">cost</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">running</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> a </span><span class=\"by_2YpRin5m5vBJu8Tg9\">copy was that</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">its labour</span></span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\">, </span><span class=\"by_qxJ28GN72aiJu96iF\">vastly increasing the amount</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_qxJ28GN72aiJu96iF\">wealth in the world but also causing mass unemployment</span></span><a href=\"#fn6\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">6</span></sup></a><span class=\"by_qxJ28GN72aiJu96iF\">. The ability to copy uploads could also lead to drastic changes in society's values, with the values of the uploads that got copied the most coming to dominate.</span></p><p><span class=\"by_woC2b5rav5sGrAo3E\">An emulated-brain populated world could hold severe negative consequences, such as:</span></p><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">Inherent inability to have consciousness, if some philosophers are right </span><a href=\"#fn7\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">7</span></sup></a><span class=\"by_baGAQoNAH4hXaC6qf\"> </span><a href=\"#fn8\"><sup><span class=\"by_qxJ28GN72aiJu96iF\">8</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"#fn9\"><sup><span class=\"by_qxJ28GN72aiJu96iF\">9</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"#fn10\"><sup><span class=\"by_qxJ28GN72aiJu96iF\">10</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Elimination of culture in general, due to an extremely increasing penalty for inefficiency in the form of flamboyant displays </span><a href=\"#fn11\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">11</span></sup></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Near zero costs for reproduction, pushing most of </span><a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\"><span class=\"by_woC2b5rav5sGrAo3E\">emulations to live in a subsistence state</span></a><span class=\"by_woC2b5rav5sGrAo3E\">. </span><a href=\"#fn12\"><sup><span class=\"by_baGAQoNAH4hXaC6qf\">12</span></sup></a></li></ul><h2 id=\"See_Also\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Economic consequences of AI and whole brain emulation</span></a></li><li><a href=\"https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai\"><span class=\"by_woC2b5rav5sGrAo3E\">Emulation argument for human-level AI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/simulation-hypothesis\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Simulation hypothesis</span></a></li><li><a href=\"https://www.lesswrong.com/tag/neuromorphic-ai\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Neuromorphic AI</span></a></li></ul><h2 id=\"External_Links\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">External Links</span></h2><ul><li><a href=\"http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/\"><span class=\"by_2YpRin5m5vBJu8Tg9\">The Singularity is near: When humans transcend biology</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Ray Kurzweil</span></li><li><a href=\"https://www.lesswrong.com/tag/brain-emulation-roadmap\"><span><span class=\"by_baGAQoNAH4hXaC6qf\">Whole </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Brain </span><span class=\"by_baGAQoNAH4hXaC6qf\">Emulation: A</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> Roadmap</span></span></a><span><span class=\"by_baGAQoNAH4hXaC6qf\">.</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> Report by The Future of Humanity Institute.</span></span></li><li><a href=\"http://www.jetpress.org/volume1/moravec.htm\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Hans Moravec's Estimation of Human Brain Processing Capacity</span></a></li><li><a href=\"http://www.patternsinthevoid.net/blog/wp-content/uploads/2010/12/2009-A-world-survey-of-artificial-brain-projects-Part1_Large-scale-brain-simulations.pdf\"><span class=\"by_2YpRin5m5vBJu8Tg9\">A world survey of artificial brain projects, Part I: Large-scale brain simulations</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Hugo de Garis, Chen Shuo, Ben Goertzel and, Lian Ruiting, 2010</span></li><li><a href=\"http://hanson.gmu.edu/uploads.html\"><span class=\"by_2YpRin5m5vBJu8Tg9\">If Uploads Come First: The crack of a future dawn</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Robin Hanson</span></li><li><a href=\"http://intelligence.org/files/WBE-Superorgs.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">Whole Brain Emulation and the Evolution of Superorganisms</span></a></li><li><a href=\"http://wp.goertzel.org/?page_id=368\"><span class=\"by_qxJ28GN72aiJu96iF\">International Journal of Machine Consciousness Special Issue on Mind Uploading</span></a></li><li><a href=\"http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf\"><span class=\"by_2YpRin5m5vBJu8Tg9\">A framework for approaches to transfer of a mind's substrate</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Sim Bamford</span></li><li><a href=\"http://www.xuenay.net/Papers/CoalescingMinds.pdf\"><span><span class=\"by_2YpRin5m5vBJu8Tg9\">Coalescing Minds:</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> Brain </span><span class=\"by_2YpRin5m5vBJu8Tg9\">Uploading-related Group Mind Scenarios</span></span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_2YpRin5m5vBJu8Tg9\">by Kaj Sotala and Harri Valpola</span></span></li></ul><h2 id=\"References\"><span class=\"by_woC2b5rav5sGrAo3E\">References</span></h2><ol><li><a href=\"https://www.lesswrong.com/tag/brain-emulation-roadmap\"><span><span class=\"by_qxJ28GN72aiJu96iF\">Whole </span><span class=\"by_baGAQoNAH4hXaC6qf\">Brain Emulation:</span><span class=\"by_qxJ28GN72aiJu96iF\"> A </span><span class=\"by_baGAQoNAH4hXaC6qf\">Roadmap</span></span></a><a href=\"#fnref1\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Strout, J. Uploading by the Nanoreplacement Procedure. </span><a href=\"http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html\"><span class=\"by_qxJ28GN72aiJu96iF\">http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html</span></a><a href=\"#fnref2\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Sotala, K., &amp; Valpola, H. (2012). Coalescing minds: brain uploading-related group mind scenarios. International Journal of Machine Consciousness, 4(01), 293-312. </span><a href=\"http://singularity.org/files/CoalescingMinds.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">http://singularity.org/files/CoalescingMinds.pdf</span></a><a href=\"#fnref3\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li><li><span class=\"by_qxJ28GN72aiJu96iF\">ROTHBLATT, M. (2012). THE TERASEM MIND UPLOADING EXPERIMENT. International Journal of Machine Consciousness, 4(01), 141-158. </span><a href=\"http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf</span></a><a href=\"#fnref4\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Sotala, K. (2012). Advantages of artificial intelligences, uploads, and digital minds. International Journal of Machine Consciousness, 4(01), 275-291. </span><a href=\"http://singularity.org/files/AdvantagesOfAIs.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">http://singularity.org/files/AdvantagesOfAIs.pdf</span></a><a href=\"#fnref5\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li><li><span class=\"by_qxJ28GN72aiJu96iF\">Hanson, R. (1994). If uploads come first. Extropy, 6(2), 10-15. </span><a href=\"http://hanson.gmu.edu/uploads.html\"><span class=\"by_qxJ28GN72aiJu96iF\">http://hanson.gmu.edu/uploads.html</span></a><a href=\"#fnref6\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">LUCAS, John. (1961) Minds, machines, and Gödel, Philosophy, 36, pp. 112–127</span><a href=\"#fnref7\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">DREYFUS, H. (1972) What Computers Can’t Do, New York: Harper &amp; Row.</span><a href=\"#fnref8\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">PENROSE, Roger (1994) Shadows of the Mind, Oxford: Oxford University Press.</span><a href=\"#fnref9\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">BLOCK, Ned (1981) Psychologism and behaviorism, Philosophical Review, 90, pp. 5–43.</span><a href=\"#fnref10\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">BOSTROM, Nick.(2004) \"The future of human evolution\". Death and Anti‐Death: Two Hundred Years After Kant, Fifty Years After Turing, ed. Charles Tandy (Ria University Press: Palo Alto, California, 2004): pp. 339‐371. Available at: </span><a href=\"http://www.nickbostrom.com/fut/evolution.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">http://www.nickbostrom.com/fut/evolution.pdf</span></a><a href=\"#fnref11\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li></ol>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 79,
    "description": {
      "markdown": "**Whole Brain Emulation** or **WBE** is a proposed technique which involves transferring the information contained within a brain onto a computing substrate. The brain can then be simulated, creating a machine intelligence. The concept is often discussed in context of scanning the brain of a person, known as [Mind Uploading](https://www.lesswrong.com/tag/mind-uploading).\n\nWBE is sometimes seen as an easy way to creating intelligent computers, as the only innovations necessary are greatly increased processor speed and scanning resolution. Advocates of WBE claim technological improvement rates such as [Moore's law](https://wiki.lesswrong.com/wiki/Moore's_law) will make WBE inevitable.\n\nThe exact level of detail required for an accurate simulation of a brain's mind is presently uncertain, and will determine the difficulty of creating WBE. The feasibility of such a project has been examined in detail in [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi)'s [Whole Brain Emulation: A Roadmap](https://www.lesswrong.com/tag/brain-emulation-roadmap). The Roadmap concluded that a human brain emulation would be possible before mid-century, providing that current technology trends kept up and providing that there would be sufficient investments.\n\nSeveral approaches for WBE have been suggested:\n\n*   A brain could be cut into small slices, which would then be scanned into a computer.[^1^](#fn1)\n*   [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces) could slowly replace portions of the brain with computers and allow the mind to grow onto a computing substrate.[^2^](#fn2)[^3^](#fn3)\n*   Resources such as personality tests and a person's writings could be used to construct a model of the person.[^4^](#fn4)\n\nA digitally emulated brain could have several advantages over a biological one[^5^](#fn5). It might be able to run faster than biological brains, copy itself, and take advantage of backups while experimenting with self-modification.\n\nWhole brain emulation will also create a number of ethical challenges relating to the nature of personhood, rights, and social inequality. [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) proposes that an uploaded mind [might copy itself to work until the cost of running a copy was that of its labour](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation), vastly increasing the amount of wealth in the world but also causing mass unemployment[^6^](#fn6). The ability to copy uploads could also lead to drastic changes in society's values, with the values of the uploads that got copied the most coming to dominate.\n\nAn emulated-brain populated world could hold severe negative consequences, such as:\n\n*   Inherent inability to have consciousness, if some philosophers are right [^7^](#fn7) [^8^](#fn8) [^9^](#fn9) [^10^](#fn10).\n*   Elimination of culture in general, due to an extremely increasing penalty for inefficiency in the form of flamboyant displays [^11^](#fn11)\n*   Near zero costs for reproduction, pushing most of [emulations to live in a subsistence state](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation). [^12^](#fn12)\n\nSee Also\n--------\n\n*   [Economic consequences of AI and whole brain emulation](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)\n*   [Emulation argument for human-level AI](https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai)\n*   [Simulation hypothesis](https://www.lesswrong.com/tag/simulation-hypothesis)\n*   [Neuromorphic AI](https://www.lesswrong.com/tag/neuromorphic-ai)\n\nExternal Links\n--------------\n\n*   [The Singularity is near: When humans transcend biology](http://www.amazon.com/The-Singularity-Is-Near-Transcend/dp/0143037889/) by Ray Kurzweil\n*   [Whole Brain Emulation: A Roadmap](https://www.lesswrong.com/tag/brain-emulation-roadmap). Report by The Future of Humanity Institute.\n*   [Hans Moravec's Estimation of Human Brain Processing Capacity](http://www.jetpress.org/volume1/moravec.htm)\n*   [A world survey of artificial brain projects, Part I: Large-scale brain simulations](http://www.patternsinthevoid.net/blog/wp-content/uploads/2010/12/2009-A-world-survey-of-artificial-brain-projects-Part1_Large-scale-brain-simulations.pdf) by Hugo de Garis, Chen Shuo, Ben Goertzel and, Lian Ruiting, 2010\n*   [If Uploads Come First: The crack of a future dawn](http://hanson.gmu.edu/uploads.html) by Robin Hanson\n*   [Whole Brain Emulation and the Evolution of Superorganisms](http://intelligence.org/files/WBE-Superorgs.pdf)\n*   [International Journal of Machine Consciousness Special Issue on Mind Uploading](http://wp.goertzel.org/?page_id=368)\n*   [A framework for approaches to transfer of a mind's substrate](http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf) by Sim Bamford\n*   [Coalescing Minds: Brain Uploading-related Group Mind Scenarios](http://www.xuenay.net/Papers/CoalescingMinds.pdf) by Kaj Sotala and Harri Valpola\n\nReferences\n----------\n\n1.  [Whole Brain Emulation: A Roadmap](https://www.lesswrong.com/tag/brain-emulation-roadmap)[↩](#fnref1)\n2.  Strout, J. Uploading by the Nanoreplacement Procedure. [http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html](http://www.ibiblio.org/jstrout/uploading/nanoreplacement.html)[↩](#fnref2)\n3.  Sotala, K., & Valpola, H. (2012). Coalescing minds: brain uploading-related group mind scenarios. International Journal of Machine Consciousness, 4(01), 293-312. [http://singularity.org/files/CoalescingMinds.pdf](http://singularity.org/files/CoalescingMinds.pdf)[↩](#fnref3)\n4.  ROTHBLATT, M. (2012). THE TERASEM MIND UPLOADING EXPERIMENT. International Journal of Machine Consciousness, 4(01), 141-158. [http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf](http://www.terasemcentral.org/docs/Terasem%20Mind%20Uploading%20Experiment%20IJMC.pdf)[↩](#fnref4)\n5.  Sotala, K. (2012). Advantages of artificial intelligences, uploads, and digital minds. International Journal of Machine Consciousness, 4(01), 275-291. [http://singularity.org/files/AdvantagesOfAIs.pdf](http://singularity.org/files/AdvantagesOfAIs.pdf)[↩](#fnref5)\n6.  Hanson, R. (1994). If uploads come first. Extropy, 6(2), 10-15. [http://hanson.gmu.edu/uploads.html](http://hanson.gmu.edu/uploads.html)[↩](#fnref6)\n7.  LUCAS, John. (1961) Minds, machines, and Gödel, Philosophy, 36, pp. 112–127[↩](#fnref7)\n8.  DREYFUS, H. (1972) What Computers Can’t Do, New York: Harper & Row.[↩](#fnref8)\n9.  PENROSE, Roger (1994) Shadows of the Mind, Oxford: Oxford University Press.[↩](#fnref9)\n10.  BLOCK, Ned (1981) Psychologism and behaviorism, Philosophical Review, 90, pp. 5–43.[↩](#fnref10)\n11.  BOSTROM, Nick.(2004) \"The future of human evolution\". Death and Anti‐Death: Two Hundred Years After Kant, Fifty Years After Turing, ed. Charles Tandy (Ria University Press: Palo Alto, California, 2004): pp. 339‐371. Available at: [http://www.nickbostrom.com/fut/evolution.pdf](http://www.nickbostrom.com/fut/evolution.pdf)[↩](#fnref11)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2b0",
    "name": "Technological Forecasting",
    "core": null,
    "slug": "technological-forecasting",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_cn4SiEmqWbu7K9em5\">Technological forecasting</span></strong><span class=\"by_cn4SiEmqWbu7K9em5\"> means making predictions about future technological advances.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">One approach is extrapolating from past data. </span><a href=\"http://en.wikipedia.org/wiki/Moore's_law\"><span class=\"by_cn4SiEmqWbu7K9em5\">Moore's Law</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, which says that the number of transistors on an integrated circuit doubles every two years, is the classic example. Bela Nagy's </span><a href=\"http://pcdb.santafe.edu/\"><span class=\"by_cn4SiEmqWbu7K9em5\">performance curve database</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, perhaps the most systematic attempt at such extrapolation, </span><a href=\"http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">has found</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> similar trends in many technologies. </span><a href=\"http://www.kurzweilai.net/the-law-of-accelerating-returns\"><span class=\"by_cn4SiEmqWbu7K9em5\">Ray Kurzweil</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> is a well-known advocate of exponential technological growth models. On the other hand, an exponential curve is </span><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/01/Modis-The-singularity-myth.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">indistinguishable</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> from the early stages of a logistic curve that eventually approaches a ceiling.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">Another approach is </span><a href=\"http://teaching.p-design.ch/forecasting07/texts/RoweWright2001_Delphi_Technique.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">expert elicitation</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, such as in the </span><a href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0020/3854/global-catastrophic-risks-report.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">survey taken at the Global Catastrophic Risk Conference</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, and </span><a href=\"http://sethbaum.com/ac/2011_AI-Experts.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">a survey of artificial general intelligence researchers</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> on AGI timelines.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">One could create probabilistic models more complicated than a simple trend extrapolation. Anders Sandberg has done calculations on timelines for </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_cn4SiEmqWbu7K9em5\">whole brain emulation</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, based on an analysis of </span><a href=\"http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">prerequisite technologies</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">. </span><a href=\"http://www.theuncertainfuture.com\"><span class=\"by_cn4SiEmqWbu7K9em5\">The Uncertain Future</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> is a web application (developed by the </span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">Machine Intelligence Research</span><span class=\"by_cn4SiEmqWbu7K9em5\"> Institute</span></span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> and currently in beta) that works with probability distributions provided by the user to calculate the probability of a disruption to \"business as usual\", which could come in the form of either a global disaster or the invention of </span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_cn4SiEmqWbu7K9em5\">artificial general intelligence</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">An important danger in predicting the future is that one might tell complex stories with many details, any of which could fail and invalidate the prediction. Models like that used in The Uncertain Future attempt to avoid this problem by considering outcomes that could come about in multiple ways, and assigning some probability to many different scenarios.</span></p><h2 id=\"Blog_posts\"><span class=\"by_cn4SiEmqWbu7K9em5\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/9ao/longterm_technological_forecasting/\"><span class=\"by_cn4SiEmqWbu7K9em5\">Long-term technological forecasting</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_cn4SiEmqWbu7K9em5\">External links</span></h2><ul><li><a href=\"http://intelligence.org/files/ChangingTheFrame.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">Changing the frame of AI futurism: From storytelling to heavy-tailed, high-dimensional probability distributions</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> (a conference paper explaining the reasoning behind the Uncertain Future app)</span></li></ul><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Acceleration_thesis\"><span class=\"by_cn4SiEmqWbu7K9em5\">Acceleration thesis</span></a></li><li><a href=\"https://www.lesswrong.com/tag/good-story-bias\"><span class=\"by_cn4SiEmqWbu7K9em5\">Good-story bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\"><span class=\"by_cn4SiEmqWbu7K9em5\">Economic consequences of AI and whole brain emulation</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 29,
    "description": {
      "markdown": "**Technological forecasting** means making predictions about future technological advances.\n\nOne approach is extrapolating from past data. [Moore's Law](http://en.wikipedia.org/wiki/Moore's_law), which says that the number of transistors on an integrated circuit doubles every two years, is the classic example. Bela Nagy's [performance curve database](http://pcdb.santafe.edu/), perhaps the most systematic attempt at such extrapolation, [has found](http://tuvalu.santafe.edu/~bn/workingpapers/NagyFarmerTrancikBui.pdf) similar trends in many technologies. [Ray Kurzweil](http://www.kurzweilai.net/the-law-of-accelerating-returns) is a well-known advocate of exponential technological growth models. On the other hand, an exponential curve is [indistinguishable](http://commonsenseatheism.com/wp-content/uploads/2012/01/Modis-The-singularity-myth.pdf) from the early stages of a logistic curve that eventually approaches a ceiling.\n\nAnother approach is [expert elicitation](http://teaching.p-design.ch/forecasting07/texts/RoweWright2001_Delphi_Technique.pdf), such as in the [survey taken at the Global Catastrophic Risk Conference](http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0020/3854/global-catastrophic-risks-report.pdf), and [a survey of artificial general intelligence researchers](http://sethbaum.com/ac/2011_AI-Experts.html) on AGI timelines.\n\nOne could create probabilistic models more complicated than a simple trend extrapolation. Anders Sandberg has done calculations on timelines for [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), based on an analysis of [prerequisite technologies](http://www.fhi.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf). [The Uncertain Future](http://www.theuncertainfuture.com) is a web application (developed by the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and currently in beta) that works with probability distributions provided by the user to calculate the probability of a disruption to \"business as usual\", which could come in the form of either a global disaster or the invention of [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence).\n\nAn important danger in predicting the future is that one might tell complex stories with many details, any of which could fail and invalidate the prediction. Models like that used in The Uncertain Future attempt to avoid this problem by considering outcomes that could come about in multiple ways, and assigning some probability to many different scenarios.\n\nBlog posts\n----------\n\n*   [Long-term technological forecasting](http://lesswrong.com/lw/9ao/longterm_technological_forecasting/)\n\nExternal links\n--------------\n\n*   [Changing the frame of AI futurism: From storytelling to heavy-tailed, high-dimensional probability distributions](http://intelligence.org/files/ChangingTheFrame.html) (a conference paper explaining the reasoning behind the Uncertain Future app)\n\nSee also\n--------\n\n*   [Acceleration thesis](https://wiki.lesswrong.com/wiki/Acceleration_thesis)\n*   [Good-story bias](https://www.lesswrong.com/tag/good-story-bias)\n*   [Economic consequences of AI and whole brain emulation](https://www.lesswrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2ac",
    "name": "Economic Consequences of AGI",
    "core": null,
    "slug": "economic-consequences-of-agi",
    "tableOfContents": {
      "html": "<p><span class=\"by_cn4SiEmqWbu7K9em5\">The </span><strong><span class=\"by_cn4SiEmqWbu7K9em5\">economic consequences of </span></strong><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><strong><span class=\"by_cn4SiEmqWbu7K9em5\">artificial general intelligence</span></strong></a><span class=\"by_cn4SiEmqWbu7K9em5\"> arise from their fundamentally new properties compared to the human brains currently driving the economy. Once such digital minds become generally intelligent enough to perform a wide range of economic functions, they are likely to bring radical changes, creating great wealth, but also displacing humans out of more and more types of job.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">An important aspect of the question is that of economic growth. The invention of AGI or WBE could cause a sudden increase in growth by adding machine intelligence to the pool of human innovators. Machine intelligence could be much cheaper to produce, faster, and qualitatively smarter than human talent. A </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_cn4SiEmqWbu7K9em5\">feedback loop</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> from better machine intelligence technology, to more and better machine researchers, back to better machine intelligence technology could ensue.</span></p><p><a href=\"https://www.lesswrong.com/tag/robin-hanson\"><span class=\"by_cn4SiEmqWbu7K9em5\">Robin Hanson</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> has written much about the economics of whole brain emulation. In </span><a href=\"http://hanson.gmu.edu/uploads.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">his view</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, the unrestricted creation of additional uploads will cause a </span><a href=\"https://www.lesswrong.com/tag/malthusian-scenarios\"><span class=\"by_cn4SiEmqWbu7K9em5\">Malthusian scenario</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, where upload wages fall to subsistence levels. He sees the transition to whole brain emulation as a </span><a href=\"http://hanson.gmu.edu/longgrow.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">jump to a new \"growth mode\"</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> with higher exponential growth rates, similar to the transitions to agriculture and industry.</span></p><p><span class=\"by_woC2b5rav5sGrAo3E\">In </span><a href=\"http://www.nickbostrom.com/fut/evolution.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">\"The Future of Human Evolution\"</span></a><span class=\"by_woC2b5rav5sGrAo3E\">, </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_woC2b5rav5sGrAo3E\">Nick Bostrom</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> argues that in an emulated-brain society with individuals living at subsistence levels, entities that possess a large set of features we care about – which he calls flamboyant displays, or culture in general – will be outcompeted by more efficient ones that lack inefficient humans’ cultural aspects. This will lead to elimination of all forms of being that we care about. He proposes that only a </span><a href=\"https://www.lesswrong.com/tag/singleton\"><span class=\"by_woC2b5rav5sGrAo3E\">Singleton</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> could ensure strict control in order to prevent the elimination of culture through outcompetition.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">Others predict that growth will blow up even more suddenly (up to the point where physical limits become relevant), and that growth will be concentrated in a smaller and more coherent set of agents, so that instead of continued free market competition, we will see a </span><a href=\"https://www.lesswrong.com/tag/singleton\"><span class=\"by_cn4SiEmqWbu7K9em5\">singleton</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> emerge.</span></p><h2 id=\"Blog_posts\"><span class=\"by_cn4SiEmqWbu7K9em5\">Blog posts</span></h2><ul><li><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\"><span class=\"by_cn4SiEmqWbu7K9em5\">The Hanson-Yudkowsky Foom Debate</span></a></li><li><a href=\"http://www.overcomingbias.com/tag/future\"><span class=\"by_cn4SiEmqWbu7K9em5\">Overcoming Bias posts tagged \"Future\"</span></a></li><li><a href=\"http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html\"><span class=\"by_uMX6njxPsbMy8kaFo\">Is The City-ularity Near?</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_cn4SiEmqWbu7K9em5\">External links</span></h2><ul><li><a href=\"http://www.nickbostrom.com/fut/evolution.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">Bostrom's paper on elimination of culture</span></a></li><li><a href=\"http://hanson.gmu.edu/aigrow.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">Economic growth given machine intelligence</span></a></li><li><a href=\"http://intelligence.org/files/EconomicImplications.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">Economic implications of software minds</span></a></li><li><a href=\"http://hanson.gmu.edu/longgrow.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">Long-term growth as a sequence of exponential modes</span></a></li><li><a href=\"http://hanson.gmu.edu/fastgrow.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">Is a singularity just around the corner? What it takes to get explosive economic growth</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Technological_singularity\"><span class=\"by_cn4SiEmqWbu7K9em5\">Technological singularity</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hard_takeoff\"><span class=\"by_cn4SiEmqWbu7K9em5\">Hard takeoff</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Soft_takeoff\"><span class=\"by_cn4SiEmqWbu7K9em5\">Soft takeoff</span></a></li><li><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_cn4SiEmqWbu7K9em5\">Artificial general intelligence</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_cn4SiEmqWbu7K9em5\">Whole Brain Emulation</span></a></li><li><a href=\"https://www.lesswrong.com/tag/technological-forecasting\"><span class=\"by_cn4SiEmqWbu7K9em5\">Technological forecasting</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 18,
    "description": {
      "markdown": "The **economic consequences of** [**artificial general intelligence**](https://www.lesswrong.com/tag/artificial-general-intelligence) arise from their fundamentally new properties compared to the human brains currently driving the economy. Once such digital minds become generally intelligent enough to perform a wide range of economic functions, they are likely to bring radical changes, creating great wealth, but also displacing humans out of more and more types of job.\n\nAn important aspect of the question is that of economic growth. The invention of AGI or WBE could cause a sudden increase in growth by adding machine intelligence to the pool of human innovators. Machine intelligence could be much cheaper to produce, faster, and qualitatively smarter than human talent. A [feedback loop](https://www.lesswrong.com/tag/intelligence-explosion) from better machine intelligence technology, to more and better machine researchers, back to better machine intelligence technology could ensue.\n\n[Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) has written much about the economics of whole brain emulation. In [his view](http://hanson.gmu.edu/uploads.html), the unrestricted creation of additional uploads will cause a [Malthusian scenario](https://www.lesswrong.com/tag/malthusian-scenarios), where upload wages fall to subsistence levels. He sees the transition to whole brain emulation as a [jump to a new \"growth mode\"](http://hanson.gmu.edu/longgrow.pdf) with higher exponential growth rates, similar to the transitions to agriculture and industry.\n\nIn [\"The Future of Human Evolution\"](http://www.nickbostrom.com/fut/evolution.pdf), [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) argues that in an emulated-brain society with individuals living at subsistence levels, entities that possess a large set of features we care about – which he calls flamboyant displays, or culture in general – will be outcompeted by more efficient ones that lack inefficient humans’ cultural aspects. This will lead to elimination of all forms of being that we care about. He proposes that only a [Singleton](https://www.lesswrong.com/tag/singleton) could ensure strict control in order to prevent the elimination of culture through outcompetition.\n\nOthers predict that growth will blow up even more suddenly (up to the point where physical limits become relevant), and that growth will be concentrated in a smaller and more coherent set of agents, so that instead of continued free market competition, we will see a [singleton](https://www.lesswrong.com/tag/singleton) emerge.\n\nBlog posts\n----------\n\n*   [The Hanson-Yudkowsky Foom Debate](http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate)\n*   [Overcoming Bias posts tagged \"Future\"](http://www.overcomingbias.com/tag/future)\n*   [Is The City-ularity Near?](http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html)\n\nExternal links\n--------------\n\n*   [Bostrom's paper on elimination of culture](http://www.nickbostrom.com/fut/evolution.pdf)\n*   [Economic growth given machine intelligence](http://hanson.gmu.edu/aigrow.pdf)\n*   [Economic implications of software minds](http://intelligence.org/files/EconomicImplications.pdf)\n*   [Long-term growth as a sequence of exponential modes](http://hanson.gmu.edu/longgrow.pdf)\n*   [Is a singularity just around the corner? What it takes to get explosive economic growth](http://hanson.gmu.edu/fastgrow.html)\n\nSee also\n--------\n\n*   [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity)\n*   [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff), [Soft takeoff](https://wiki.lesswrong.com/wiki/Soft_takeoff)\n*   [Artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), [Whole Brain Emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\n*   [Technological forecasting](https://www.lesswrong.com/tag/technological-forecasting)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2aa",
    "name": "Infinities In Ethics",
    "core": null,
    "slug": "infinities-in-ethics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_cn4SiEmqWbu7K9em5\">Infinities in ethics</span></strong><span><span class=\"by_cn4SiEmqWbu7K9em5\"> pose some difficult problems. For example, if </span><span class=\"by_qxJ28GN72aiJu96iF\">the universe is infinite,</span><span class=\"by_cn4SiEmqWbu7K9em5\"> there are already infinite numbers of good and bad things. Adding or removing finitely many of them leaves infinitely many of both. This means aggregative consequentialist theories (those that maximize the sum of the values of individual structures) will be indifferent between any acts with merely finite effects. If you save the whales, there will be infinitely many whales, but if you don't save the whales, there will also be infinitely many whales.</span></span></p><p><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_cn4SiEmqWbu7K9em5\">Nick Bostrom</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> wrote a </span><a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">paper</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> discussing various possible solutions to this problem of \"infinitarian paralysis\" (as well as the \"fanaticism\" problem of theories that would sacrifice anything for a small chance of an infinite payoff). The solutions fall into three classes:</span></p><ul><li><span class=\"by_cn4SiEmqWbu7K9em5\">Modifications of the \"domain rule\" that determines what values are to be aggregated (e.g., discounting values far away in space and time)</span></li><li><span class=\"by_cn4SiEmqWbu7K9em5\">Modifications of the \"aggregation rule\" that determines </span><i><span class=\"by_cn4SiEmqWbu7K9em5\">how</span></i><span class=\"by_cn4SiEmqWbu7K9em5\"> these values are to be aggregated (e.g., representing total value as a hyperreal number)</span></li><li><span class=\"by_cn4SiEmqWbu7K9em5\">Modifications of the \"selection rule\" that uses the aggregation result to recommend an action (e.g., ignoring very small probabilities)</span></li></ul><p><span class=\"by_cn4SiEmqWbu7K9em5\">The best-known use of infinity in ethics is probably </span><a href=\"http://en.wikipedia.org/wiki/Pascal's_Wager\"><span class=\"by_cn4SiEmqWbu7K9em5\">Pascal's wager</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, which has a finite variant in </span><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\"><span class=\"by_cn4SiEmqWbu7K9em5\">Pascal's mugging</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_cn4SiEmqWbu7K9em5\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/\"><span class=\"by_cn4SiEmqWbu7K9em5\">The Pascal's Wager Fallacy Fallacy</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_cn4SiEmqWbu7K9em5\">Eliezer Yudkowsky</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_cn4SiEmqWbu7K9em5\">External links</span></h2><ul><li><a href=\"http://www.nickbostrom.com/ethics/infinite.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">Infinite Ethics</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, by Nick Bostrom</span></li><li><a href=\"http://philsci-archive.pitt.edu/1341/\"><span class=\"by_cn4SiEmqWbu7K9em5\">Philosophical implications of inflationary cosmology</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\"><span class=\"by_cn4SiEmqWbu7K9em5\">Pascal's mugging</span></a></li><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_cn4SiEmqWbu7K9em5\">Utilitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/quick-reference-guide-to-the-infinite\"><span class=\"by_cn4SiEmqWbu7K9em5\">Quick reference guide to the infinite</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 14,
    "description": {
      "markdown": "**Infinities in ethics** pose some difficult problems. For example, if the universe is infinite, there are already infinite numbers of good and bad things. Adding or removing finitely many of them leaves infinitely many of both. This means aggregative consequentialist theories (those that maximize the sum of the values of individual structures) will be indifferent between any acts with merely finite effects. If you save the whales, there will be infinitely many whales, but if you don't save the whales, there will also be infinitely many whales.\n\n[Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) wrote a [paper](http://www.nickbostrom.com/ethics/infinite.pdf) discussing various possible solutions to this problem of \"infinitarian paralysis\" (as well as the \"fanaticism\" problem of theories that would sacrifice anything for a small chance of an infinite payoff). The solutions fall into three classes:\n\n*   Modifications of the \"domain rule\" that determines what values are to be aggregated (e.g., discounting values far away in space and time)\n*   Modifications of the \"aggregation rule\" that determines *how* these values are to be aggregated (e.g., representing total value as a hyperreal number)\n*   Modifications of the \"selection rule\" that uses the aggregation result to recommend an action (e.g., ignoring very small probabilities)\n\nThe best-known use of infinity in ethics is probably [Pascal's wager](http://en.wikipedia.org/wiki/Pascal's_Wager), which has a finite variant in [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging).\n\nBlog posts\n----------\n\n*   [The Pascal's Wager Fallacy Fallacy](http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/), by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nExternal links\n--------------\n\n*   [Infinite Ethics](http://www.nickbostrom.com/ethics/infinite.pdf), by Nick Bostrom\n*   [Philosophical implications of inflationary cosmology](http://philsci-archive.pitt.edu/1341/)\n\nSee also\n--------\n\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Quick reference guide to the infinite](https://www.lesswrong.com/tag/quick-reference-guide-to-the-infinite)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb2a8",
    "name": "Astronomical Waste",
    "core": null,
    "slug": "astronomical-waste",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_cn4SiEmqWbu7K9em5\">Astronomical waste</span></strong><span class=\"by_cn4SiEmqWbu7K9em5\"> is a term introduced by </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_cn4SiEmqWbu7K9em5\">Nick Bostrom</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> for the opportunities we're losing out on by not colonizing the universe.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">The universe is vast. There are many galaxies, each containing many stars. In a future with space colonization, each star could support a large population of people leading worthwhile lives. During any given year, we irrecoverably lose an amount of energy that could have powered a civilization like ours for many billions of years.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">The prospect of advanced technology only makes the numbers more extreme. Such technology would make it possible to support far more, and better, lives with the same resources.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">Bostrom </span><a href=\"http://www.nickbostrom.com/astronomical/waste.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">notes</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> that in a wide range of moral theories — in particular, those based on linear aggregation of value — considerations of astronomical waste outweigh all others.</span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">If so much potential is lost every year, one could conclude that we should start colonization </span><i><span class=\"by_cn4SiEmqWbu7K9em5\">as soon as possible</span></i><span class=\"by_cn4SiEmqWbu7K9em5\">. But since the amount of resources available </span><i><span class=\"by_cn4SiEmqWbu7K9em5\">in total</span></i><span class=\"by_cn4SiEmqWbu7K9em5\"> is much larger still than the amount lost in a year, a better utilitarian prescription is to minimize the risk of losing out on space colonization entirely. Bostrom calls this prescription \"maxipok\", for the maximum probability of an OK outcome.</span></p><h2 id=\"External_links\"><span class=\"by_cn4SiEmqWbu7K9em5\">External links</span></h2><ul><li><a href=\"http://www.nickbostrom.com/astronomical/waste.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">Astronomical Waste: The Opportunity Cost of Delayed Technological Development</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_cn4SiEmqWbu7K9em5\">Utilitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_cn4SiEmqWbu7K9em5\">Existential risk</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 6,
    "description": {
      "markdown": "**Astronomical waste** is a term introduced by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) for the opportunities we're losing out on by not colonizing the universe.\n\nThe universe is vast. There are many galaxies, each containing many stars. In a future with space colonization, each star could support a large population of people leading worthwhile lives. During any given year, we irrecoverably lose an amount of energy that could have powered a civilization like ours for many billions of years.\n\nThe prospect of advanced technology only makes the numbers more extreme. Such technology would make it possible to support far more, and better, lives with the same resources.\n\nBostrom [notes](http://www.nickbostrom.com/astronomical/waste.html) that in a wide range of moral theories — in particular, those based on linear aggregation of value — considerations of astronomical waste outweigh all others.\n\nIf so much potential is lost every year, one could conclude that we should start colonization *as soon as possible*. But since the amount of resources available *in total* is much larger still than the amount lost in a year, a better utilitarian prescription is to minimize the risk of losing out on space colonization entirely. Bostrom calls this prescription \"maxipok\", for the maximum probability of an OK outcome.\n\nExternal links\n--------------\n\n*   [Astronomical Waste: The Opportunity Cost of Delayed Technological Development](http://www.nickbostrom.com/astronomical/waste.html)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb29d",
    "name": "Neuromorphic AI",
    "core": null,
    "slug": "neuromorphic-ai",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">A </span><strong><span><span class=\"by_Sp5wM4aRAhNERd4oY\">Neuromorphic</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> AI</span></span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> ('neuron-shaped') is a form of AI where most of the functionality has been copied from the human brain. This implies that its inner workings are not necessarily understood by the creators any further than is necessary to simulate them on a computer. It is considered </span><span class=\"by_Sp5wM4aRAhNERd4oY\">a more</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span></span><a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">unsafe</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> form of </span><span class=\"by_Sp5wM4aRAhNERd4oY\">AI than either </span></span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Whole Brain Emulation</span></a><span><span class=\"by_Sp5wM4aRAhNERd4oY\"> or de novo </span><span class=\"by_qgdGA4ZEyW7zNdK84\">AI</span><span class=\"by_Sp5wM4aRAhNERd4oY\"> because its lacks the high quality replication of human values of the former and the possibility of good theoretical guarantees that the latter may have due to cleaner design.</span></span></p><h2 id=\"External_Links\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">External Links</span></h2><ul><li><span class=\"by_5wu9jG4pm9q6xjZ9R\">Definition from </span><a href=\"http://www.theuncertainfuture.com/faq.html#3\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">The Uncertain Future</span></a></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 18,
    "description": {
      "markdown": "A **Neuromorphic AI** ('neuron-shaped') is a form of AI where most of the functionality has been copied from the human brain. This implies that its inner workings are not necessarily understood by the creators any further than is necessary to simulate them on a computer. It is considered a more [unsafe](https://wiki.lesswrong.com/wiki/Unfriendly_AI) form of AI than either [Whole Brain Emulation](https://www.lesswrong.com/tag/whole-brain-emulation) or de novo AI because its lacks the high quality replication of human values of the former and the possibility of good theoretical guarantees that the latter may have due to cleaner design.\n\nExternal Links\n--------------\n\n*   Definition from [The Uncertain Future](http://www.theuncertainfuture.com/faq.html#3)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb297",
    "name": "Superintelligence",
    "core": null,
    "slug": "superintelligence",
    "tableOfContents": {
      "html": "<p><span class=\"by_2YpRin5m5vBJu8Tg9\">A </span><strong><span class=\"by_NRg5Bw8H2DCYTpmHE\">Superintelligence</span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_2YpRin5m5vBJu8Tg9\">is a being with superhuman intelligence, and a focus of the </span></span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span class=\"by_LoykQRMTxJFxwwdPy\">Machine Intelligence Research Institute</span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\">'</span><span class=\"by_2YpRin5m5vBJu8Tg9\">s research. Specifically, Nick Bostrom (1997) defined it as</span></span></p><blockquote><p><span class=\"by_2YpRin5m5vBJu8Tg9\">\"An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\"</span></p></blockquote><p><span class=\"by_2YpRin5m5vBJu8Tg9\">The </span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">Machine Intelligence Research</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> Institute</span></span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> is dedicated</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> to </span><span class=\"by_2YpRin5m5vBJu8Tg9\">ensuring humanity's safety and prosperity by preparing for the development of an </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Artificial General Intelligence</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> with superintelligence. Given its intelligence, it is likely to be </span><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment\"><span><span class=\"by_2YpRin5m5vBJu8Tg9\">incapable of</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> being </span><span class=\"by_2YpRin5m5vBJu8Tg9\">controlled</span></span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by humanity. It is important to prepare early for the development of </span><a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\"><span><span class=\"by_2YpRin5m5vBJu8Tg9\">friendly artificial</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> intelligence</span></span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\">, as there may</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> be an </span></span><a href=\"https://www.lesswrong.com/tag/ai-arms-race\"><span class=\"by_2YpRin5m5vBJu8Tg9\">AI arms race</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\">. A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brain.</span></p><p><span class=\"by_2YpRin5m5vBJu8Tg9\">An </span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span><span class=\"by_NRg5Bw8H2DCYTpmHE\">Artificial </span><span class=\"by_2YpRin5m5vBJu8Tg9\">General </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">Intelligence</span></span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> will have a number of advantages aiding it in becoming a superintelligence. It can improve the hardware it runs </span><span class=\"by_qxJ28GN72aiJu96iF\">on and obtain better hardware.</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> It will be capable of directly editing its own </span><span class=\"by_qxJ28GN72aiJu96iF\">code. Depending on how easy</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> its </span><span class=\"by_qxJ28GN72aiJu96iF\">code is to modify, it might carry out software </span><span class=\"by_2YpRin5m5vBJu8Tg9\">improvements </span><span class=\"by_qxJ28GN72aiJu96iF\">that </span></span><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement\"><span class=\"by_qxJ28GN72aiJu96iF\">spark further improvements</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">.</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> Where a task</span><span class=\"by_qxJ28GN72aiJu96iF\"> can be</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> accomplished in a repetitive way, a module preforming the task far more efficiently </span><span class=\"by_qxJ28GN72aiJu96iF\">might</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> be developed. Its motivations </span><span class=\"by_qxJ28GN72aiJu96iF\">and preferences </span><span class=\"by_2YpRin5m5vBJu8Tg9\">can be </span><span class=\"by_qxJ28GN72aiJu96iF\">edited to be more consistent with each other.</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> It will have an indefinite life span, be capable of reproducing, and transfer knowledge, skills, and code among its copies as well as cooperating </span><span class=\"by_qxJ28GN72aiJu96iF\">and communicating </span><span class=\"by_2YpRin5m5vBJu8Tg9\">with them </span><span class=\"by_qxJ28GN72aiJu96iF\">better than humans do </span><span class=\"by_2YpRin5m5vBJu8Tg9\">with </span><span class=\"by_qxJ28GN72aiJu96iF\">each other.</span></span></p><p><span><span class=\"by_2YpRin5m5vBJu8Tg9\">The development of superintelligence from humans</span><span class=\"by_qxJ28GN72aiJu96iF\"> is</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> another possibility, sometimes termed a weak superintelligence. It may come in the form of </span></span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_2YpRin5m5vBJu8Tg9\">whole brain emulation</span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\">, where</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> a human </span><span class=\"by_2YpRin5m5vBJu8Tg9\">brain is scanned and simulated on a computer. Many</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">the advantages a AGI has in developing superintelligence apply here as well. The development of</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span></span><a href=\"https://www.lesswrong.com/tag/brain-computer-interfaces\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Brain-computer interfaces</span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> may also lead</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> to the </span><span class=\"by_2YpRin5m5vBJu8Tg9\">creation of superintelligence. Biological enhancements such as genetic engineering and the use of nootropics could lead</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> to </span><span class=\"by_2YpRin5m5vBJu8Tg9\">superintelligence as well.</span></span></p><h2 id=\"Blog_Posts\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Blog Posts</span></h2><ul><li><a href=\"http://www.acceleratingfuture.com/articles/superintelligencehowsoon.htm\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Superintelligence</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> by Michael Anissimov</span></li></ul><h2 id=\"External_Links\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">External Links</span></h2><ul><li><a href=\"http://www.nickbostrom.com/superintelligence.html\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">How long before Superintelligence?</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> by Nick Bostrom</span></li><li><a href=\"http://profhugodegaris.files.wordpress.com/2011/04/nocyborgsbghugo.pdf\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">A discussion between Hugo de Garis and Ben Goertzel on superintelligence</span></a></li><li><a href=\"http://www.xuenay.net/Papers/DigitalAdvantages.pdf\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Advantages of Artificial Intelligences, Uploads, And Digital Minds</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> by Kaj Sotala</span></li></ul><h2 id=\"See_Also\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/brain-computer-interfaces\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Brain-computer interfaces</span></a></li><li><a href=\"https://www.lesswrong.com/tag/singularity\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Singularity</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hard_takeoff\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Hard takeoff</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog Posts",
          "anchor": "Blog_Posts",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 20,
    "description": {
      "markdown": "A **Superintelligence** is a being with superhuman intelligence, and a focus of the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri)'s research. Specifically, Nick Bostrom (1997) defined it as\n\n> \"An intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.\"\n\nThe [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) is dedicated to ensuring humanity's safety and prosperity by preparing for the development of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) with superintelligence. Given its intelligence, it is likely to be [incapable of being controlled](https://www.lesswrong.com/tag/ai-boxing-containment) by humanity. It is important to prepare early for the development of [friendly artificial intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence), as there may be an [AI arms race](https://www.lesswrong.com/tag/ai-arms-race). A strong superintelligence is a term describing a superintelligence which is not designed with the same architecture as the human brain.\n\nAn [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) will have a number of advantages aiding it in becoming a superintelligence. It can improve the hardware it runs on and obtain better hardware. It will be capable of directly editing its own code. Depending on how easy its code is to modify, it might carry out software improvements that [spark further improvements](https://www.lesswrong.com/tag/recursive-self-improvement). Where a task can be accomplished in a repetitive way, a module preforming the task far more efficiently might be developed. Its motivations and preferences can be edited to be more consistent with each other. It will have an indefinite life span, be capable of reproducing, and transfer knowledge, skills, and code among its copies as well as cooperating and communicating with them better than humans do with each other.\n\nThe development of superintelligence from humans is another possibility, sometimes termed a weak superintelligence. It may come in the form of [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), where a human brain is scanned and simulated on a computer. Many of the advantages a AGI has in developing superintelligence apply here as well. The development of [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces) may also lead to the creation of superintelligence. Biological enhancements such as genetic engineering and the use of nootropics could lead to superintelligence as well.\n\nBlog Posts\n----------\n\n*   [Superintelligence](http://www.acceleratingfuture.com/articles/superintelligencehowsoon.htm) by Michael Anissimov\n\nExternal Links\n--------------\n\n*   [How long before Superintelligence?](http://www.nickbostrom.com/superintelligence.html) by Nick Bostrom\n*   [A discussion between Hugo de Garis and Ben Goertzel on superintelligence](http://profhugodegaris.files.wordpress.com/2011/04/nocyborgsbghugo.pdf)\n*   [Advantages of Artificial Intelligences, Uploads, And Digital Minds](http://www.xuenay.net/Papers/DigitalAdvantages.pdf) by Kaj Sotala\n\nSee Also\n--------\n\n*   [Brain-computer interfaces](https://www.lesswrong.com/tag/brain-computer-interfaces)\n*   [Singularity](https://www.lesswrong.com/tag/singularity)\n*   [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb294",
    "name": "Bayesian Decision Theory",
    "core": null,
    "slug": "bayesian-decision-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qQqgj5ScvgQsJk7Ti\">Bayesian decision theory</span></strong><span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> refers to</span><span class=\"by_LedhurJxi3baDAKDZ\"> a</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> </span></span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qQqgj5ScvgQsJk7Ti\">decision theory</span></a><span class=\"by_qQqgj5ScvgQsJk7Ti\"> which is informed by </span><a href=\"https://www.lesswrong.com/tag/bayesian-probability\"><span class=\"by_qQqgj5ScvgQsJk7Ti\">Bayesian probability</span></a><span><span class=\"by_qQqgj5ScvgQsJk7Ti\">.</span><span class=\"by_LedhurJxi3baDAKDZ\"> It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs.</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> An agent operating under such a decision theory uses the concepts of Bayesian statistics to estimate the </span></span><a href=\"https://www.lesswrong.com/tag/expected-value\"><span class=\"by_qQqgj5ScvgQsJk7Ti\">expected value</span></a><span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> of its actions, and update its expectations based on new information.</span><span class=\"by_LedhurJxi3baDAKDZ\"> These agents can and are usually referred to as estimators.</span></span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">From the perspective of Bayesian decision theory,</span><span class=\"by_LedhurJxi3baDAKDZ\"> any kind of probability distribution - such as the </span><span class=\"by_qxJ28GN72aiJu96iF\">distribution for tomorrow's </span><span class=\"by_LedhurJxi3baDAKDZ\">weather </span><span class=\"by_qxJ28GN72aiJu96iF\">-</span><span class=\"by_LedhurJxi3baDAKDZ\"> represents a </span></span><a href=\"https://www.lesswrong.com/tag/priors\"><span class=\"by_LedhurJxi3baDAKDZ\">prior</span></a><span><span class=\"by_LedhurJxi3baDAKDZ\"> distribution. That is, it represents how we </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">expect</span><span class=\"by_LedhurJxi3baDAKDZ\"> </span></span><i><span class=\"by_LedhurJxi3baDAKDZ\">today</span></i><span class=\"by_LedhurJxi3baDAKDZ\"> the weather is going to be </span><i><span class=\"by_LedhurJxi3baDAKDZ\">tomorrow.</span></i><span class=\"by_LedhurJxi3baDAKDZ\"> This contrasts with frequentist inference, the classical probability interpretation, where conclusions about an experiment are drawn from a set of repetitions of such experience, each producing statistically independent results. For a frequentist, a probability function would be a simple distribution function with no special meaning.</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">Suppose we </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">intend</span><span class=\"by_qxJ28GN72aiJu96iF\"> to meet a friend tomorrow, and </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">expect an 0.5</span><span class=\"by_qxJ28GN72aiJu96iF\"> chance </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">of</span><span class=\"by_qxJ28GN72aiJu96iF\"> raining. If we are choosing between various options </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">for</span><span class=\"by_qxJ28GN72aiJu96iF\"> the </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">meeting,</span><span class=\"by_qxJ28GN72aiJu96iF\"> with the pleasantness of some of the options (such as going to the park) being affected by the possibility of rain, we can </span></span><a href=\"http://lesswrong.com/lw/8uj/compressing_reality_to_math/\"><span><span class=\"by_qxJ28GN72aiJu96iF\">assign values to the different options with or </span><span class=\"by_ZzidgSpo5kEczWxGt\">without</span><span class=\"by_qxJ28GN72aiJu96iF\"> rain</span></span></a><span class=\"by_qxJ28GN72aiJu96iF\">. We can then pick the option whose expected value is the highest, given the probability of rain.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">One definition of </span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_qxJ28GN72aiJu96iF\">rationality</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">, used both on Less Wrong and in economics and psychology, is behavior which obeys the rules of</span><span class=\"by_LedhurJxi3baDAKDZ\"> Bayesian decision </span><span class=\"by_qxJ28GN72aiJu96iF\">theory. Due to computational constraints, this</span><span class=\"by_LedhurJxi3baDAKDZ\"> is </span><span class=\"by_qxJ28GN72aiJu96iF\">impossible</span><span class=\"by_LedhurJxi3baDAKDZ\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">do perfectly, but</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> naturally evolved </span><span class=\"by_qxJ28GN72aiJu96iF\">brains </span></span><a href=\"http://en.wikipedia.org/wiki/Bayesian_brain\"><span class=\"by_qxJ28GN72aiJu96iF\">do seem to mirror</span></a><span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> these probabilistic methods when they adapt to an uncertain environment. </span><span class=\"by_LedhurJxi3baDAKDZ\">Such models and distributions </span><span class=\"by_bdn8PRCzBLMmGqzhJ\">may be</span><span class=\"by_LedhurJxi3baDAKDZ\"> reconfigured according to </span><span class=\"by_qxJ28GN72aiJu96iF\">feedback from </span><span class=\"by_LedhurJxi3baDAKDZ\">the </span><span class=\"by_qxJ28GN72aiJu96iF\">environment.</span></span></p><h2 id=\"Further_Reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further Reading &amp; References</span></h2><ul><li><span class=\"by_LedhurJxi3baDAKDZ\">Berger, James O. (1985). Statistical decision theory and Bayesian Analysis (2nd ed.). New York: Springer-Verlag. ISBN 0-387-96098-8. MR 0804611</span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">Bernardo, José M.; Smith, Adrian F. M. (1994). Bayesian Theory. Wiley. ISBN 0-471-92416-4. MR 1274699</span></li></ul><h2 id=\"See_also\"><span class=\"by_qQqgj5ScvgQsJk7Ti\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/bayesian-probability\"><span class=\"by_qQqgj5ScvgQsJk7Ti\">Bayesian probability</span></a></li><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qQqgj5ScvgQsJk7Ti\">Decision theory</span></a></li></ul>",
      "sections": [
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 10,
    "description": {
      "markdown": "**Bayesian decision theory** refers to a [decision theory](https://www.lesswrong.com/tag/decision-theory) which is informed by [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability). It is a statistical system that tries to quantify the tradeoff between various decisions, making use of probabilities and costs. An agent operating under such a decision theory uses the concepts of Bayesian statistics to estimate the [expected value](https://www.lesswrong.com/tag/expected-value) of its actions, and update its expectations based on new information. These agents can and are usually referred to as estimators.\n\nFrom the perspective of Bayesian decision theory, any kind of probability distribution - such as the distribution for tomorrow's weather - represents a [prior](https://www.lesswrong.com/tag/priors) distribution. That is, it represents how we expect *today* the weather is going to be *tomorrow.* This contrasts with frequentist inference, the classical probability interpretation, where conclusions about an experiment are drawn from a set of repetitions of such experience, each producing statistically independent results. For a frequentist, a probability function would be a simple distribution function with no special meaning.\n\nSuppose we intend to meet a friend tomorrow, and expect an 0.5 chance of raining. If we are choosing between various options for the meeting, with the pleasantness of some of the options (such as going to the park) being affected by the possibility of rain, we can [assign values to the different options with or without rain](http://lesswrong.com/lw/8uj/compressing_reality_to_math/). We can then pick the option whose expected value is the highest, given the probability of rain.\n\nOne definition of [rationality](https://www.lesswrong.com/tag/rationality), used both on Less Wrong and in economics and psychology, is behavior which obeys the rules of Bayesian decision theory. Due to computational constraints, this is impossible to do perfectly, but naturally evolved brains [do seem to mirror](http://en.wikipedia.org/wiki/Bayesian_brain) these probabilistic methods when they adapt to an uncertain environment. Such models and distributions may be reconfigured according to feedback from the environment.\n\nFurther Reading & References\n----------------------------\n\n*   Berger, James O. (1985). Statistical decision theory and Bayesian Analysis (2nd ed.). New York: Springer-Verlag. ISBN 0-387-96098-8. MR 0804611\n*   Bernardo, José M.; Smith, Adrian F. M. (1994). Bayesian Theory. Wiley. ISBN 0-471-92416-4. MR 1274699\n\nSee also\n--------\n\n*   [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb292",
    "name": "Seed AI",
    "core": null,
    "slug": "seed-ai",
    "tableOfContents": {
      "html": "<p><span class=\"by_Ge36tWtFQERudEYhR\">A </span><strong><span class=\"by_NRg5Bw8H2DCYTpmHE\">Seed AI</span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_Ge36tWtFQERudEYhR\">(a</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> term coined by </span></span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Eliezer Yudkowsky</span></a><span><span class=\"by_Ge36tWtFQERudEYhR\">) is</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">an </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_Ge36tWtFQERudEYhR\">Artificial General Intelligence</span></a><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_Ge36tWtFQERudEYhR\">(AGI) which improves itself by</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span></span><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement\"><span><span class=\"by_NRg5Bw8H2DCYTpmHE\">recursively </span><span class=\"by_Ge36tWtFQERudEYhR\">rewriting</span></span></a><span><span class=\"by_Ge36tWtFQERudEYhR\"> its own source code without human intervention.</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> Initially this program </span><span class=\"by_Ge36tWtFQERudEYhR\">would likely</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> have a </span><span class=\"by_Ge36tWtFQERudEYhR\">minimal intelligence, but over the course of many iterations it would evolve to human-equivalent or even trans-</span><span class=\"by_NRg5Bw8H2DCYTpmHE\">human </span><span class=\"by_Ge36tWtFQERudEYhR\">reasoning.</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> The key for successful </span></span><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">AI takeoff</span></a><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> would lie in creating adequate starting </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">conditions.</span></span></p><h2 id=\"History\"><span class=\"by_Ge36tWtFQERudEYhR\">History</span></h2><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">The </span><span class=\"by_Ge36tWtFQERudEYhR\">notion</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_Ge36tWtFQERudEYhR\">machine learning without human intervention has been around nearly as long as the computers themselves. In 1959, </span></span><a href=\"http://en.wikipedia.org/wiki/Arthur_Samuel\"><span class=\"by_Ge36tWtFQERudEYhR\">Arthur Samuel</span></a><span class=\"by_Ge36tWtFQERudEYhR\"> stated that \"Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"</span><a href=\"#fn1\"><sup><span class=\"by_Ge36tWtFQERudEYhR\">1</span></sup></a><span><span class=\"by_Ge36tWtFQERudEYhR\"> Since that time, computers have been able to learn by </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">a</span><span class=\"by_Ge36tWtFQERudEYhR\"> variety of methods, including </span></span><a href=\"http://en.wikipedia.org/wiki/Artificial_neural_network\"><span class=\"by_Ge36tWtFQERudEYhR\">neural networks</span></a><span class=\"by_Ge36tWtFQERudEYhR\"> and </span><a href=\"http://en.wikipedia.org/wiki/Bayesian_inference\"><span class=\"by_Ge36tWtFQERudEYhR\">Bayesian inference</span></a><span class=\"by_Ge36tWtFQERudEYhR\">.</span></p><p><span class=\"by_Ge36tWtFQERudEYhR\">While these approaches have enabled machines to become better at various tasks</span><a href=\"#fn2\"><sup><span class=\"by_Ge36tWtFQERudEYhR\">2</span></sup></a><span class=\"by_Ge36tWtFQERudEYhR\"> </span><a href=\"#fn3\"><sup><span class=\"by_Ge36tWtFQERudEYhR\">3</span></sup></a><span class=\"by_Ge36tWtFQERudEYhR\">, it has not enabled them to overcome the limitations of these techniques, nor has it given them the ability to understand their own programming and make improvements. Hence, they are not able to adapt to new situations without human assistance.</span></p><h2 id=\"Properties\"><span class=\"by_Ge36tWtFQERudEYhR\">Properties</span></h2><p><span><span class=\"by_Ge36tWtFQERudEYhR\">A </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Seed AI </span><span class=\"by_Ge36tWtFQERudEYhR\">has abilities that previous approaches lack:</span></span></p><ul><li><i><span class=\"by_Ge36tWtFQERudEYhR\">Understanding its own source code</span></i><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">. </span><span class=\"by_Ge36tWtFQERudEYhR\">It must understand the purpose, syntax</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><span class=\"by_Ge36tWtFQERudEYhR\">architecture</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of its own programming. </span><span class=\"by_Ge36tWtFQERudEYhR\">This type of self-reflection enables the AGI to comprehend its utility and thus preserve it.</span></span></li><li><i><span class=\"by_Ge36tWtFQERudEYhR\">Rewriting its own source code</span></i><span><span class=\"by_Ge36tWtFQERudEYhR\">. The AGI must be able to overhaul the very code</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> it </span><span class=\"by_Ge36tWtFQERudEYhR\">uses</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to </span><span class=\"by_Ge36tWtFQERudEYhR\">fulfill its utility. A</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> critical consideration is that </span><span class=\"by_Ge36tWtFQERudEYhR\">it</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> must remain stable under </span><span class=\"by_Ge36tWtFQERudEYhR\">modifications, preserving its original goals.</span></span></li></ul><p><span><span class=\"by_Ge36tWtFQERudEYhR\">This combination of abilities would, in theory, allow an AGI</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to </span><span class=\"by_Ge36tWtFQERudEYhR\">recursively improve itself by becoming </span></span><i><span class=\"by_Ge36tWtFQERudEYhR\">smarter</span></i><span><span class=\"by_Ge36tWtFQERudEYhR\"> within</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> its </span><span class=\"by_Ge36tWtFQERudEYhR\">original purpose. A </span></span><a href=\"https://www.lesswrong.com/tag/gödel-machine\"><span class=\"by_Ge36tWtFQERudEYhR\">Gödel machine</span></a><span class=\"by_Ge36tWtFQERudEYhR\"> rigorously defines a specification for such an AGI.</span></p><h2 id=\"Development\"><span class=\"by_Ge36tWtFQERudEYhR\">Development</span></h2><p><span><span class=\"by_Ge36tWtFQERudEYhR\">Currently, there are no known Seed AIs in existence, but it is an active field of research. Several organizations continue to pursue this goal, such as</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span></span><a href=\"http://intelligence.org\"><span class=\"by_Ge36tWtFQERudEYhR\">Singularity Institute</span></a><span class=\"by_Ge36tWtFQERudEYhR\">, </span><a href=\"http://opencog.org/\"><span class=\"by_Ge36tWtFQERudEYhR\">OpenCog</span></a><span class=\"by_Ge36tWtFQERudEYhR\">, and </span><a href=\"http://adaptiveai.com/\"><span><span class=\"by_Ge36tWtFQERudEYhR\">Adaptive</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> AI</span></span></a><span class=\"by_Ge36tWtFQERudEYhR\">.</span></p><h2 id=\"See_Also\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/gödel-machine\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Gödel</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> machine</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">AI</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> takeoff</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Recursive self-improvement</span></a></li><li><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Intelligence explosion</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_Ge36tWtFQERudEYhR\">AGI</span></a></li></ul><h2 id=\"References\"><span class=\"by_Ge36tWtFQERudEYhR\">References</span></h2><ol><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Han, Zhimeng. </span><a href=\"http://researchcommons.waikato.ac.nz/bitstream/handle/10289/5701/thesis.pdf?sequence=3\"><i><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Smoothing in Probability Estimation Trees</span></u></i></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. The University of Waikato.</span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Seed_AI?_ga=2.93005205.966300592.1600626178-561901249.1600626178#cite_ref-eurisko_2-0\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Jump up↑</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> Lenat, Douglas. </span><a href=\"http://researchcommons.waikato.ac.nz/bitstream/handle/10289/5701/thesis.pdf?sequence=3\"><i><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Eurisko. A program that learns news heuristics and domain concepts.</span></u></i></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Artificial Intelligence.</span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Seed_AI?_ga=2.93005205.966300592.1600626178-561901249.1600626178#cite_ref-checkers_3-0\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Jump up↑</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> Chellapilla, Kumar; Fogel, David. </span><a href=\"http://www.cs.ru.ac.za/courses/Honours/ai/HybridSystems/P2.pdf\"><i><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Evolving an Expert Checkers Playing Program without Using Human Expertise</span></u></i></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Natural Selection, Inc.</span></li><li><span class=\"by_Ge36tWtFQERudEYhR\">Yudkowsky, Eliezer.</span><a href=\"http://intelligence.org/upload/LOGI/seedAI.html\"><span class=\"by_Ge36tWtFQERudEYhR\">Seed AI Levels of Organization in General Intelligence</span></a><span class=\"by_Ge36tWtFQERudEYhR\">. Singularity Institute.</span></li><li><a href=\"http://intelligence.org/files/GISAI.html#para_seedAI_advantage\"><span class=\"by_Ge36tWtFQERudEYhR\">General Intelligence and Seed AI</span></a><span class=\"by_Ge36tWtFQERudEYhR\">. Singularity Institute.</span></li><li><span class=\"by_Ge36tWtFQERudEYhR\">Schmidhuber, Jürgen. </span><a href=\"ftp://ftp.idsia.ch/pub/juergen/gm6.pdf\"><span class=\"by_Ge36tWtFQERudEYhR\">Gödel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements</span></a><span class=\"by_Ge36tWtFQERudEYhR\">. IDSIA</span></li></ol>",
      "sections": [
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "Properties",
          "anchor": "Properties",
          "level": 1
        },
        {
          "title": "Development",
          "anchor": "Development",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 5,
    "description": {
      "markdown": "A **Seed AI** (a term coined by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)) is an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) which improves itself by [recursively rewriting](https://www.lesswrong.com/tag/recursive-self-improvement) its own source code without human intervention. Initially this program would likely have a minimal intelligence, but over the course of many iterations it would evolve to human-equivalent or even trans-human reasoning. The key for successful [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff) would lie in creating adequate starting conditions.\n\nHistory\n-------\n\nThe notion of machine learning without human intervention has been around nearly as long as the computers themselves. In 1959, [Arthur Samuel](http://en.wikipedia.org/wiki/Arthur_Samuel) stated that \"Machine learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"[^1^](#fn1) Since that time, computers have been able to learn by a variety of methods, including [neural networks](http://en.wikipedia.org/wiki/Artificial_neural_network) and [Bayesian inference](http://en.wikipedia.org/wiki/Bayesian_inference).\n\nWhile these approaches have enabled machines to become better at various tasks[^2^](#fn2) [^3^](#fn3), it has not enabled them to overcome the limitations of these techniques, nor has it given them the ability to understand their own programming and make improvements. Hence, they are not able to adapt to new situations without human assistance.\n\nProperties\n----------\n\nA Seed AI has abilities that previous approaches lack:\n\n*   *Understanding its own source code*. It must understand the purpose, syntax and architecture of its own programming. This type of self-reflection enables the AGI to comprehend its utility and thus preserve it.\n*   *Rewriting its own source code*. The AGI must be able to overhaul the very code it uses to fulfill its utility. A critical consideration is that it must remain stable under modifications, preserving its original goals.\n\nThis combination of abilities would, in theory, allow an AGI to recursively improve itself by becoming *smarter* within its original purpose. A [Gödel machine](https://www.lesswrong.com/tag/gödel-machine) rigorously defines a specification for such an AGI.\n\nDevelopment\n-----------\n\nCurrently, there are no known Seed AIs in existence, but it is an active field of research. Several organizations continue to pursue this goal, such as the [Singularity Institute](http://intelligence.org), [OpenCog](http://opencog.org/), and [Adaptive AI](http://adaptiveai.com/).\n\nSee Also\n--------\n\n*   [Gödel machine](https://www.lesswrong.com/tag/gödel-machine)\n*   [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff)\n*   [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n*   [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)\n*   [AGI](https://wiki.lesswrong.com/wiki/AGI)\n\nReferences\n----------\n\n1.  Han, Zhimeng. [*Smoothing in Probability Estimation Trees*](http://researchcommons.waikato.ac.nz/bitstream/handle/10289/5701/thesis.pdf?sequence=3). The University of Waikato.\n2.  [Jump up↑](https://wiki.lesswrong.com/wiki/Seed_AI?_ga=2.93005205.966300592.1600626178-561901249.1600626178#cite_ref-eurisko_2-0) Lenat, Douglas. [*Eurisko. A program that learns news heuristics and domain concepts.*](http://researchcommons.waikato.ac.nz/bitstream/handle/10289/5701/thesis.pdf?sequence=3). Artificial Intelligence.\n3.  [Jump up↑](https://wiki.lesswrong.com/wiki/Seed_AI?_ga=2.93005205.966300592.1600626178-561901249.1600626178#cite_ref-checkers_3-0) Chellapilla, Kumar; Fogel, David. [*Evolving an Expert Checkers Playing Program without Using Human Expertise*](http://www.cs.ru.ac.za/courses/Honours/ai/HybridSystems/P2.pdf). Natural Selection, Inc.\n4.  Yudkowsky, Eliezer.[Seed AI Levels of Organization in General Intelligence](http://intelligence.org/upload/LOGI/seedAI.html). Singularity Institute.\n5.  [General Intelligence and Seed AI](http://intelligence.org/files/GISAI.html#para_seedAI_advantage). Singularity Institute.\n6.  Schmidhuber, Jürgen. [Gödel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements](ftp://ftp.idsia.ch/pub/juergen/gm6.pdf). IDSIA"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb28c",
    "name": "Brain-Computer Interfaces",
    "core": null,
    "slug": "brain-computer-interfaces",
    "tableOfContents": {
      "html": "<p><span class=\"by_NRg5Bw8H2DCYTpmHE\">A </span><strong><span class=\"by_LedhurJxi3baDAKDZ\">Brain Computer Interface (BCI)</span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> is </span><span class=\"by_LedhurJxi3baDAKDZ\">the generic term used to describe any kind of system that serves as </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">a </span><span class=\"by_LedhurJxi3baDAKDZ\">communication bridge</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> between </span><span class=\"by_LedhurJxi3baDAKDZ\">the brain (human or not) and an artificial module. It’s </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">a </span><span class=\"by_LedhurJxi3baDAKDZ\">field of research in which wide investment has been made since the 1970’s, especially in the clinical fields and ergonomics. Generally speaking, any kind of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">brain </span><span class=\"by_LedhurJxi3baDAKDZ\">activity that can be recorded can be used</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> as a means </span><span class=\"by_LedhurJxi3baDAKDZ\">of communicating with another system. Through the use of statistical classification techniques it’s possible to associate certain states or characteristics of the recorded signal – which the experiment subject learns to control - to any procedure, usually mediated </span><span class=\"by_2YpRin5m5vBJu8Tg9\">by </span><span class=\"by_LedhurJxi3baDAKDZ\">a computer.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">Many techniques have been developed to help us look and better understand the way the brain works. They range from imaging techniques (like MRI, fMRI, fNIRS or PET), to electrophysiological ones (like EEG, EcG or MEG). While the first category is usually used to obtain high resolution images of brain structures and the second one to register and analyze the electrical activity produced by the brain, with a high temporal resolution – </span><span class=\"by_2YpRin5m5vBJu8Tg9\">which </span><span class=\"by_LedhurJxi3baDAKDZ\">is why they are the ones mainly used in the field of BCI’s. In pair with such methods, although </span><span class=\"by_2YpRin5m5vBJu8Tg9\">a </span><span class=\"by_LedhurJxi3baDAKDZ\">different area in itself, includes brain implants capable of communicating directly with the neuronal tissue - </span></span><a href=\"http://en.wikipedia.org/wiki/Neuroprosthetics\"><span class=\"by_LedhurJxi3baDAKDZ\">neuroprosthetics</span></a><span class=\"by_LedhurJxi3baDAKDZ\">.</span></p><h2 id=\"EEG_BCIs\"><span class=\"by_LedhurJxi3baDAKDZ\">EEG BCIs</span></h2><p><span class=\"by_LedhurJxi3baDAKDZ\">Of all the different means avaliable, the registering of the </span><a href=\"http://en.wikipedia.org/wiki/Electroencephalography\"><span class=\"by_LedhurJxi3baDAKDZ\">electroencephalographic (EEG)</span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> </span><span class=\"by_LedhurJxi3baDAKDZ\">activity is the most developed and extensively researched of this fields. It allows us, in a non-invasive way, to peak the brain functioning with a high temporal resolution – furthermore, it is now well established that different brain states produce distinct observable activity. With the help of electrodes placed on the scalp, it is possible to feed this activity and their respective variations and patterns to any system capable of classifying and detecting them in real time and act accordingly (making this a field highly interconnected to that of machine learning).</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">The field</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> of BCIs </span><span class=\"by_LedhurJxi3baDAKDZ\">has followed closely the developments in signal processing and classification, along with the increasing computational power available. It was firstly researched as a communication means (for people unable to move, for instance) through the detection</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">ERPs – event related potentials, small variations of amplitude associated to the presentation of certain stimuli - as well as a way of automatically detecting epileptic seizures. Also, much owing to the first and major financers of</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> such </span><span class=\"by_LedhurJxi3baDAKDZ\">research,</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> the </span></span><a href=\"http://en.wikipedia.org/wiki/DARPA\"><span class=\"by_LedhurJxi3baDAKDZ\">DARPA</span></a><span><span class=\"by_2YpRin5m5vBJu8Tg9\">, </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">the </span><span class=\"by_LedhurJxi3baDAKDZ\">use</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> of BCIs </span><span class=\"by_LedhurJxi3baDAKDZ\">has been always closed associated to</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">military field. This has allowed insights regarding</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">detection</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">mental states of fatigue and attention variations, which has led</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> to</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> the development of </span><span class=\"by_LedhurJxi3baDAKDZ\">informatics systems capable of adapting to the mental state of the user.</span></span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">Currently we have available a considerable range of both research and commercial applications of EEG based BCI systems with a wide list of applications. It has shown to be a field due to receive increased attention in the next years, especially through the developing of increasingly efficient classification algorithms and computer power, and the fascination with the cognitive augmentation it might bring.</span></p><h2 id=\"Potential_applications\"><span class=\"by_LedhurJxi3baDAKDZ\">Potential applications</span></h2><p><span class=\"by_LedhurJxi3baDAKDZ\">Although the EEG has been the main technique used for the development of such systems, it has been shown to be possible to integrate electronic controllers directly in the functioning of single cells or even networks. The </span><a href=\"http://www.wired.com/wired/archive/10.09/vision.html\"><span class=\"by_LedhurJxi3baDAKDZ\">permanent implant of devices for interpretation</span></a><span class=\"by_LedhurJxi3baDAKDZ\"> and regulation of cortical activity has also been demonstrated.</span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">This has led</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> to a </span><span class=\"by_LedhurJxi3baDAKDZ\">renewed interest</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> in</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">field and the exploration</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">new hypothesis, like drug rehabilitation through the detection of relevant cues and</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> stimulation of</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">brain reward system, rehabilitation after strokes or lesion and even direct transmission of patterns of thought between subjects.</span></span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">Other attractive future application includes the </span><a href=\"http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">upload of the whole content of the brain</span></a><span><span class=\"by_LedhurJxi3baDAKDZ\">, and thus the mind, to a computer. Although still </span><span class=\"by_qxJ28GN72aiJu96iF\">speculative,</span><span class=\"by_LedhurJxi3baDAKDZ\"> it seems</span><span class=\"by_qxJ28GN72aiJu96iF\"> </span></span><a href=\"http://intelligence.org/files/CoalescingMinds.pdf\"><span><span class=\"by_qxJ28GN72aiJu96iF\">theoretically </span><span class=\"by_LedhurJxi3baDAKDZ\">possible</span></span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><h2 id=\"External_Links\"><span class=\"by_2YpRin5m5vBJu8Tg9\">External Links</span></h2><ul><li><a href=\"http://intelligence.org/files/CoalescingMinds.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">Brain content uploading</span></a></li><li><a href=\"http://intelligence.org/brain-computer-interfaces/\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Tech Summary: Brain-Computer Interfaces</span></a></li><li><a href=\"http://thinktechuk.wordpress.com/\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">ThinkTech</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> A blog dedicated to BCI developements</span></li><li><a href=\"http://www.emotiv.com\"><span class=\"by_LedhurJxi3baDAKDZ\">Commercial EEG BCI System example</span></a></li><li><a href=\"http://www.kurzweilai.net/people-with-paralysis-control-robotic-arms-using-brain-computer-interface\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Paralyzed patient controls robot arm using BCI</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> Article from KurzweilAI</span></li><li><a href=\"http://www.youtube.com/watch?v=ogBX18maUiM\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Demonstration of paralyzed patient using robot arm</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> from Nature Magazine YouTube</span></li><li><a href=\"http://www.youtube.com/watch?v=g0rRvBd7Dew&amp;feature=endscreen&amp;NR=1\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Demonstration of a blind patient with a Retinal Implant reading</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> from Discovery Magazine YouTube</span></li></ul><h2 id=\"Further_Reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further Reading &amp; References</span></h2><ul><li><span class=\"by_LedhurJxi3baDAKDZ\">Anderson, J. (1980). Neurocomputing. Cambridge: The MIT Press</span></li><li><span><span class=\"by_LedhurJxi3baDAKDZ\">Muller, D. (1995). Towards brain–</span><span class=\"by_NRg5Bw8H2DCYTpmHE\">computer </span><span class=\"by_LedhurJxi3baDAKDZ\">interfacing. MIT Press, Cambridge, MA, 409–422.</span></span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">Niedermeyer, E., &amp; Lopes da Silva, F. (2004). Electroencephalography: Basic Principles. Clinical Applications and Related Fields. London</span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">Vidal, J. (1977). Real-Time Detection of Brain Events in EEG. IEEE Proceedings, 65 (5), 633–641</span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">Parasuraman, R. (2003). Neuroergonomics: Research and practice. Theoretical Issues in Ergonomics Science, 4, 5–20.</span></li></ul><h2 id=\"See_Also\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Biological Cognitive Enhancement</span></a></li><li><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Whole brain emulation</span></a></li><li><a href=\"https://www.lesswrong.com/tag/wireheading\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Wireheading</span></a></li><li><a href=\"https://lesswrong.com/tag/neuralink\"><span class=\"by_HoGziwmhpMGqGeWZy\">Neuralink</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">&nbsp;</span></li></ul>",
      "sections": [
        {
          "title": "EEG BCIs",
          "anchor": "EEG_BCIs",
          "level": 1
        },
        {
          "title": "Potential applications",
          "anchor": "Potential_applications",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 15,
    "description": {
      "markdown": "A **Brain Computer Interface (BCI)** is the generic term used to describe any kind of system that serves as a communication bridge between the brain (human or not) and an artificial module. It’s a field of research in which wide investment has been made since the 1970’s, especially in the clinical fields and ergonomics. Generally speaking, any kind of brain activity that can be recorded can be used as a means of communicating with another system. Through the use of statistical classification techniques it’s possible to associate certain states or characteristics of the recorded signal – which the experiment subject learns to control - to any procedure, usually mediated by a computer.\n\nMany techniques have been developed to help us look and better understand the way the brain works. They range from imaging techniques (like MRI, fMRI, fNIRS or PET), to electrophysiological ones (like EEG, EcG or MEG). While the first category is usually used to obtain high resolution images of brain structures and the second one to register and analyze the electrical activity produced by the brain, with a high temporal resolution – which is why they are the ones mainly used in the field of BCI’s. In pair with such methods, although a different area in itself, includes brain implants capable of communicating directly with the neuronal tissue - [neuroprosthetics](http://en.wikipedia.org/wiki/Neuroprosthetics).\n\nEEG BCIs\n--------\n\nOf all the different means avaliable, the registering of the [electroencephalographic (EEG)](http://en.wikipedia.org/wiki/Electroencephalography) activity is the most developed and extensively researched of this fields. It allows us, in a non-invasive way, to peak the brain functioning with a high temporal resolution – furthermore, it is now well established that different brain states produce distinct observable activity. With the help of electrodes placed on the scalp, it is possible to feed this activity and their respective variations and patterns to any system capable of classifying and detecting them in real time and act accordingly (making this a field highly interconnected to that of machine learning).\n\nThe field of BCIs has followed closely the developments in signal processing and classification, along with the increasing computational power available. It was firstly researched as a communication means (for people unable to move, for instance) through the detection of ERPs – event related potentials, small variations of amplitude associated to the presentation of certain stimuli - as well as a way of automatically detecting epileptic seizures. Also, much owing to the first and major financers of such research, the [DARPA](http://en.wikipedia.org/wiki/DARPA), the use of BCIs has been always closed associated to the military field. This has allowed insights regarding the detection of mental states of fatigue and attention variations, which has led to the development of informatics systems capable of adapting to the mental state of the user.\n\nCurrently we have available a considerable range of both research and commercial applications of EEG based BCI systems with a wide list of applications. It has shown to be a field due to receive increased attention in the next years, especially through the developing of increasingly efficient classification algorithms and computer power, and the fascination with the cognitive augmentation it might bring.\n\nPotential applications\n----------------------\n\nAlthough the EEG has been the main technique used for the development of such systems, it has been shown to be possible to integrate electronic controllers directly in the functioning of single cells or even networks. The [permanent implant of devices for interpretation](http://www.wired.com/wired/archive/10.09/vision.html) and regulation of cortical activity has also been demonstrated.\n\nThis has led to a renewed interest in the field and the exploration of new hypothesis, like drug rehabilitation through the detection of relevant cues and stimulation of the brain reward system, rehabilitation after strokes or lesion and even direct transmission of patterns of thought between subjects.\n\nOther attractive future application includes the [upload of the whole content of the brain](http://www.sim.me.uk/neural/JournalArticles/Bamford2012IJMC.pdf), and thus the mind, to a computer. Although still speculative, it seems [theoretically possible](http://intelligence.org/files/CoalescingMinds.pdf).\n\nExternal Links\n--------------\n\n*   [Brain content uploading](http://intelligence.org/files/CoalescingMinds.pdf)\n*   [Tech Summary: Brain-Computer Interfaces](http://intelligence.org/brain-computer-interfaces/)\n*   [ThinkTech](http://thinktechuk.wordpress.com/) A blog dedicated to BCI developements\n*   [Commercial EEG BCI System example](http://www.emotiv.com)\n*   [Paralyzed patient controls robot arm using BCI](http://www.kurzweilai.net/people-with-paralysis-control-robotic-arms-using-brain-computer-interface) Article from KurzweilAI\n*   [Demonstration of paralyzed patient using robot arm](http://www.youtube.com/watch?v=ogBX18maUiM) from Nature Magazine YouTube\n*   [Demonstration of a blind patient with a Retinal Implant reading](http://www.youtube.com/watch?v=g0rRvBd7Dew&feature=endscreen&NR=1) from Discovery Magazine YouTube\n\nFurther Reading & References\n----------------------------\n\n*   Anderson, J. (1980). Neurocomputing. Cambridge: The MIT Press\n*   Muller, D. (1995). Towards brain–computer interfacing. MIT Press, Cambridge, MA, 409–422.\n*   Niedermeyer, E., & Lopes da Silva, F. (2004). Electroencephalography: Basic Principles. Clinical Applications and Related Fields. London\n*   Vidal, J. (1977). Real-Time Detection of Brain Events in EEG. IEEE Proceedings, 65 (5), 633–641\n*   Parasuraman, R. (2003). Neuroergonomics: Research and practice. Theoretical Issues in Ergonomics Science, 4, 5–20.\n\nSee Also\n--------\n\n*   [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement)\n*   [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\n*   [Wireheading](https://www.lesswrong.com/tag/wireheading)\n*   [Neuralink](https://lesswrong.com/tag/neuralink)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb285",
    "name": "Moore's Law",
    "core": null,
    "slug": "moores-law",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\">Moore's </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Law</span></span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> is a term attributed to Intel founder Gordon E. Moore who observed in 1965 that the number of transistors that could be purchased inexpensively and placed on an </span><span class=\"by_2YpRin5m5vBJu8Tg9\">integrated </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">circuit doubles every </span><span class=\"by_qxJ28GN72aiJu96iF\">year. In 1975,</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> he</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> revised </span><span class=\"by_qxJ28GN72aiJu96iF\">his estimate</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> to</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">every </span><span class=\"by_2YpRin5m5vBJu8Tg9\">two years. It</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> is often </span><span class=\"by_2YpRin5m5vBJu8Tg9\">discussed</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> as </span><span class=\"by_2YpRin5m5vBJu8Tg9\">a doubling every 18 months, but that is a separate claim by David House, Intel executive,</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> of overall chip </span><span class=\"by_2YpRin5m5vBJu8Tg9\">performance. Moore's law been approximately correct for four decades.</span></span></p><p><span><span class=\"by_2YpRin5m5vBJu8Tg9\">Though current CMOS technology is predicted to be nonviable below a certain size, many other technologies offer the potential for far greater miniaturization. This may delay Moore's law temporarily while the new technologies enter full-scale production. An end to Moore's law has often been </span><span class=\"by_qxJ28GN72aiJu96iF\">predicted,</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> but </span><span class=\"by_qxJ28GN72aiJu96iF\">has failed</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">materialize so far.</span></span></p><p><span><span class=\"by_NRg5Bw8H2DCYTpmHE\">Moore's law is </span><span class=\"by_2YpRin5m5vBJu8Tg9\">often cited as a reason to expect the creation</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> of </span><span class=\"by_2YpRin5m5vBJu8Tg9\">an </span></span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_2YpRin5m5vBJu8Tg9\">AGI</span></a><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> in the </span><span class=\"by_2YpRin5m5vBJu8Tg9\">future, and is crucial for the possibility</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> of </span></span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_2YpRin5m5vBJu8Tg9\">whole brain emulation</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\">.</span></p><h2 id=\"References\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">References</span></h2><ul><li><a href=\"http://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1965_Article.pdf\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Gordon Moore's orignal 1965 paper</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> Electronics, Volume 38, Number 8, April 19, 1965</span></li><li><a href=\"http://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1975_Speech.pdf\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Progress in Digital Integrated Circuits</span></a><span class=\"by_NRg5Bw8H2DCYTpmHE\"> Transcript of 1975 speech by Gordon Moore</span></li><li><a href=\"http://www.itrs.net/reports.html\"><span class=\"by_2YpRin5m5vBJu8Tg9\">International Technology Roadmap For Semiconductors</span></a></li><li><a href=\"http://www.slate.com/blogs/future_tense/2012/05/03/michio_kako_and_a_brief_history_of_warnings_about_the_end_of_moore_s_law_.html\"><span class=\"by_2YpRin5m5vBJu8Tg9\">A History of the End of Moore's Law</span></a><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_2YpRin5m5vBJu8Tg9\">Slate</span></span></li></ul>",
      "sections": [
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 15,
    "description": {
      "markdown": "**Moore's Law** is a term attributed to Intel founder Gordon E. Moore who observed in 1965 that the number of transistors that could be purchased inexpensively and placed on an integrated circuit doubles every year. In 1975, he revised his estimate to every two years. It is often discussed as a doubling every 18 months, but that is a separate claim by David House, Intel executive, of overall chip performance. Moore's law been approximately correct for four decades.\n\nThough current CMOS technology is predicted to be nonviable below a certain size, many other technologies offer the potential for far greater miniaturization. This may delay Moore's law temporarily while the new technologies enter full-scale production. An end to Moore's law has often been predicted, but has failed to materialize so far.\n\nMoore's law is often cited as a reason to expect the creation of an [AGI](https://wiki.lesswrong.com/wiki/AGI) in the future, and is crucial for the possibility of [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation).\n\nReferences\n----------\n\n*   [Gordon Moore's orignal 1965 paper](http://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1965_Article.pdf) Electronics, Volume 38, Number 8, April 19, 1965\n*   [Progress in Digital Integrated Circuits](http://download.intel.com/museum/Moores_Law/Articles-Press_Releases/Gordon_Moore_1975_Speech.pdf) Transcript of 1975 speech by Gordon Moore\n*   [International Technology Roadmap For Semiconductors](http://www.itrs.net/reports.html)\n*   [A History of the End of Moore's Law](http://www.slate.com/blogs/future_tense/2012/05/03/michio_kako_and_a_brief_history_of_warnings_about_the_end_of_moore_s_law_.html) Slate"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb26d",
    "name": "Oracle AI",
    "core": null,
    "slug": "oracle-ai",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">An </span><strong><span><span class=\"by_LedhurJxi3baDAKDZ\">Oracle</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> AI</span></span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is</span><span class=\"by_LedhurJxi3baDAKDZ\"> a regularly proposed solution to the problem of developing </span></span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_LedhurJxi3baDAKDZ\">Friendly AI</span></a><span><span class=\"by_LedhurJxi3baDAKDZ\">. It is conceptualized as</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> a super-intelligent system which is designed for </span><span class=\"by_LedhurJxi3baDAKDZ\">only answering questions, and has no ability to act in </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">the </span><span class=\"by_LedhurJxi3baDAKDZ\">world. The</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> name</span><span class=\"by_LedhurJxi3baDAKDZ\"> was first suggested</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by </span></span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Nick Bostrom</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\">.</span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/instrumental-convergence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Basic AI drives</span></a></li><li><a href=\"https://www.lesswrong.com/tag/tool-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Tool AI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/utility-indifference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility indifference</span></a></li><li><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI Boxing</span></a></li></ul><h1 id=\"Safety\"><span class=\"by_LedhurJxi3baDAKDZ\">Safety</span></h1><p><span class=\"by_qxJ28GN72aiJu96iF\">The question of whether Oracles – or just </span><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment\"><span class=\"by_qxJ28GN72aiJu96iF\">keeping an AGI forcibly confined</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> - are safer than fully free AGIs has been the subject of debate for a long time. </span><span class=\"by_LedhurJxi3baDAKDZ\">Armstrong, Sandberg and Bostrom discuss Oracle safety at length in their </span></span><a href=\"http://www.aleph.se/papers/oracleAI.pdf\"><span><span class=\"by_LedhurJxi3baDAKDZ\">Thinking inside the box: using and controlling</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> an </span><span class=\"by_LedhurJxi3baDAKDZ\">Oracle AI</span></span></a><span><span class=\"by_LedhurJxi3baDAKDZ\">. In the paper, the authors review various methods which might </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">be </span><span class=\"by_LedhurJxi3baDAKDZ\">used to measure </span><span class=\"by_qxJ28GN72aiJu96iF\">an</span><span class=\"by_LedhurJxi3baDAKDZ\"> Oracle's accuracy. They also try to shed some light on some weaknesses and dangers that can emerge on the human </span><span class=\"by_qxJ28GN72aiJu96iF\">side, such</span><span class=\"by_LedhurJxi3baDAKDZ\"> as psychological vulnerabilities</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> which </span><span class=\"by_LedhurJxi3baDAKDZ\">can be exploited</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by the </span><span class=\"by_LedhurJxi3baDAKDZ\">Oracle through social </span><span class=\"by_qxJ28GN72aiJu96iF\">engineering. The paper discusses</span><span class=\"by_LedhurJxi3baDAKDZ\"> ideas for physical security </span><span class=\"by_qxJ28GN72aiJu96iF\">(“boxing”),</span><span class=\"by_LedhurJxi3baDAKDZ\"> as well as </span><span class=\"by_qxJ28GN72aiJu96iF\">problems involved with trying</span><span class=\"by_LedhurJxi3baDAKDZ\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">program the AI to only answer questions. In the end, the paper reaches the cautious</span><span class=\"by_LedhurJxi3baDAKDZ\"> conclusion </span><span class=\"by_qxJ28GN72aiJu96iF\">of Oracle AIs probably being</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> safer than free </span><span class=\"by_qxJ28GN72aiJu96iF\">AGIs.</span></span></p><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">In</span><span class=\"by_LedhurJxi3baDAKDZ\"> a related work,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span></span><a href=\"http://lesswrong.com/lw/tj/dreams_of_friendliness/\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Dreams of Friendliness</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\">, </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Eliezer Yudkowsky</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> gives an informal argument </span><span class=\"by_LedhurJxi3baDAKDZ\">stating </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">that all oracles will be agent-</span><span class=\"by_LedhurJxi3baDAKDZ\">like, that is, driven by its own goals. He</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> rests on the </span><span class=\"by_LedhurJxi3baDAKDZ\">idea</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> that anything considered \"intelligent\" must </span><span class=\"by_LedhurJxi3baDAKDZ\">choose the correct course of action among all actions </span><span class=\"by_ye3Gk7544RaxnNaf7\">available.</span><span class=\"by_LedhurJxi3baDAKDZ\"> That means that the Oracle will have</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> many possible things to </span><span class=\"by_LedhurJxi3baDAKDZ\">believe, although</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> very few </span><span class=\"by_LedhurJxi3baDAKDZ\">of them are correct.</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> Therefore believing the correct thing means some method was used to select the correct belief from the many incorrect beliefs. By definition, this is an </span></span><a href=\"https://www.lesswrong.com/tag/optimization\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">optimization process</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> which has a goal of selecting correct beliefs.</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">One can then imagine all</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">things that might be useful in achieving the goal</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_qxJ28GN72aiJu96iF\">\"have correct beliefs\". For</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> instance, </span></span><a href=\"https://www.lesswrong.com/tag/instrumental-convergence\"><span><span class=\"by_qxJ28GN72aiJu96iF\">acquiring</span><span class=\"by_LedhurJxi3baDAKDZ\"> more computing power and resources</span></span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> could help this goal.</span><span class=\"by_LedhurJxi3baDAKDZ\"> As such, an Oracle could determine that it might</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> answer more accurately and easily </span><span class=\"by_LedhurJxi3baDAKDZ\">to a certain question </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">if it </span><span class=\"by_LedhurJxi3baDAKDZ\">turned</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> all matter outside </span><span class=\"by_LedhurJxi3baDAKDZ\">the</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> box </span><span class=\"by_qxJ28GN72aiJu96iF\">to</span><span class=\"by_LedhurJxi3baDAKDZ\"> </span></span><a href=\"https://www.lesswrong.com/tag/computronium\"><span class=\"by_LedhurJxi3baDAKDZ\">computronium</span></a><span class=\"by_LedhurJxi3baDAKDZ\">, therefore killing all the existing life.</span></p><h1 id=\"Taxonomy\"><span class=\"by_LedhurJxi3baDAKDZ\">Taxonomy</span></h1><p><span class=\"by_LedhurJxi3baDAKDZ\">Based on an old draft by Daniel Dewey, Luke Muehlhauser has </span><a href=\"http://lesswrong.com/lw/any/a_taxonomy_of_oracle_ais/\"><span class=\"by_LedhurJxi3baDAKDZ\">published</span></a><span class=\"by_LedhurJxi3baDAKDZ\"> a possible taxonomy of Oracle AIs, broadly divided between True Oracular AIs and Oracular non-AIs.</span></p><h2 id=\"True_Oracular_AIs\"><span class=\"by_LedhurJxi3baDAKDZ\">True Oracular AIs</span></h2><p><span><span class=\"by_LedhurJxi3baDAKDZ\">Given that true AIs are goal-oriented agents, it follows that a True Oracular AI has some kind of oracular goals. These act as the motivation system </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">for the </span><span class=\"by_LedhurJxi3baDAKDZ\">Oracle to give us</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">information we ask and nothing else.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">It is first noted that such a True AI is</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> not </span><span class=\"by_LedhurJxi3baDAKDZ\">actually nor causally isolated from </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">the </span><span class=\"by_LedhurJxi3baDAKDZ\">world, as it has at least an input (questions and information) and an output (answers) channel. Since we expect such an intelligent agent to be able to have a deep impact</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> on the </span><span class=\"by_LedhurJxi3baDAKDZ\">world even through these limited channels, it can only be safe if its goals are fully compatible with human goals.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">This means that a True Oracular AI has to have a full specification</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">human values, thus making it a </span></span><a href=\"https://www.lesswrong.com/tag/fai-complete\"><span class=\"by_LedhurJxi3baDAKDZ\">FAI-complete</span></a><span><span class=\"by_LedhurJxi3baDAKDZ\"> problem – if we could achieve such skill and knowledge we could just build a Friendly AI and bypass </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">the </span><span class=\"by_LedhurJxi3baDAKDZ\">Oracle AI concept.</span></span></p><h2 id=\"Oracular_non_AIs\"><span class=\"by_LedhurJxi3baDAKDZ\">Oracular non-AIs</span></h2><p><span class=\"by_LedhurJxi3baDAKDZ\">Any system that acts only as an informative machine, only answering questions and has no goals is by definition not an AI at all. That means that a non-AI Oracular is but a calculator of outputs based on inputs. Since the term in itself is heterogeneous, the proposals made for a sub-division are merely informal.</span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">An </span><i><span class=\"by_LedhurJxi3baDAKDZ\">Advisor</span></i><span class=\"by_LedhurJxi3baDAKDZ\"> can be seen as a system that gathers data from the real world and computes the answer to an informal “what we ought to do?” question. They also represent a FAI-complete problem.</span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">A </span><i><span class=\"by_LedhurJxi3baDAKDZ\">Question-Answerer</span></i><span><span class=\"by_LedhurJxi3baDAKDZ\"> is a similar system that gathers data from the real world but coupled with a question. It then somehow computes the answer. The difficulty can lay on distinguishing</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> it </span><span class=\"by_LedhurJxi3baDAKDZ\">from an Advisor</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and controlling </span><span class=\"by_LedhurJxi3baDAKDZ\">the safety of its answers.</span></span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">Finally, a </span><i><span class=\"by_LedhurJxi3baDAKDZ\">Predictor</span></i><span><span class=\"by_LedhurJxi3baDAKDZ\"> is seen</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> as a </span><span class=\"by_LedhurJxi3baDAKDZ\">system that takes a corpus of data</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><span class=\"by_LedhurJxi3baDAKDZ\">produces a probability distribution over future possible data. There</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> are </span><span class=\"by_LedhurJxi3baDAKDZ\">some proposed</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> dangers with </span><span class=\"by_LedhurJxi3baDAKDZ\">predictors, namely exhibiting goal-seeking behavior which does not converge with humanity goals and</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">ability to influence us through</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> the predictions.</span></span></p><h2 id=\"Further_reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further reading &amp; References</span></h2><ul><li><a href=\"http://lesswrong.com/lw/tj/dreams_of_friendliness/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Dreams of Friendliness</span></a></li><li><a href=\"http://www.aleph.se/papers/oracleAI.pdf\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Thinking inside the box: using and controlling an Oracle AI</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> by Armstrong, Sandberg and </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Bostrom</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 2
        },
        {
          "title": "Safety",
          "anchor": "Safety",
          "level": 1
        },
        {
          "title": "Taxonomy",
          "anchor": "Taxonomy",
          "level": 1
        },
        {
          "title": "True Oracular AIs",
          "anchor": "True_Oracular_AIs",
          "level": 2
        },
        {
          "title": "Oracular non-AIs",
          "anchor": "Oracular_non_AIs",
          "level": 2
        },
        {
          "title": "Further reading & References",
          "anchor": "Further_reading___References",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 67,
    "description": {
      "markdown": "An **Oracle AI** is a regularly proposed solution to the problem of developing [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). It is conceptualized as a super-intelligent system which is designed for only answering questions, and has no ability to act in the world. The name was first suggested by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom).\n\nSee also\n--------\n\n*   [Basic AI drives](https://www.lesswrong.com/tag/instrumental-convergence)\n*   [Tool AI](https://www.lesswrong.com/tag/tool-ai)\n*   [Utility indifference](https://www.lesswrong.com/tag/utility-indifference)\n*   [AI Boxing](https://www.lesswrong.com/tag/ai-boxing-containment)\n\nSafety\n======\n\nThe question of whether Oracles – or just [keeping an AGI forcibly confined](https://www.lesswrong.com/tag/ai-boxing-containment) \\- are safer than fully free AGIs has been the subject of debate for a long time. Armstrong, Sandberg and Bostrom discuss Oracle safety at length in their [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf). In the paper, the authors review various methods which might be used to measure an Oracle's accuracy. They also try to shed some light on some weaknesses and dangers that can emerge on the human side, such as psychological vulnerabilities which can be exploited by the Oracle through social engineering. The paper discusses ideas for physical security (“boxing”), as well as problems involved with trying to program the AI to only answer questions. In the end, the paper reaches the cautious conclusion of Oracle AIs probably being safer than free AGIs.\n\nIn a related work, [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/), [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) gives an informal argument stating that all oracles will be agent-like, that is, driven by its own goals. He rests on the idea that anything considered \"intelligent\" must choose the correct course of action among all actions available. That means that the Oracle will have many possible things to believe, although very few of them are correct. Therefore believing the correct thing means some method was used to select the correct belief from the many incorrect beliefs. By definition, this is an [optimization process](https://www.lesswrong.com/tag/optimization) which has a goal of selecting correct beliefs.\n\nOne can then imagine all the things that might be useful in achieving the goal of \"have correct beliefs\". For instance, [acquiring more computing power and resources](https://www.lesswrong.com/tag/instrumental-convergence) could help this goal. As such, an Oracle could determine that it might answer more accurately and easily to a certain question if it turned all matter outside the box to [computronium](https://www.lesswrong.com/tag/computronium), therefore killing all the existing life.\n\nTaxonomy\n========\n\nBased on an old draft by Daniel Dewey, Luke Muehlhauser has [published](http://lesswrong.com/lw/any/a_taxonomy_of_oracle_ais/) a possible taxonomy of Oracle AIs, broadly divided between True Oracular AIs and Oracular non-AIs.\n\nTrue Oracular AIs\n-----------------\n\nGiven that true AIs are goal-oriented agents, it follows that a True Oracular AI has some kind of oracular goals. These act as the motivation system for the Oracle to give us the information we ask and nothing else.\n\nIt is first noted that such a True AI is not actually nor causally isolated from the world, as it has at least an input (questions and information) and an output (answers) channel. Since we expect such an intelligent agent to be able to have a deep impact on the world even through these limited channels, it can only be safe if its goals are fully compatible with human goals.\n\nThis means that a True Oracular AI has to have a full specification of human values, thus making it a [FAI-complete](https://www.lesswrong.com/tag/fai-complete) problem – if we could achieve such skill and knowledge we could just build a Friendly AI and bypass the Oracle AI concept.\n\nOracular non-AIs\n----------------\n\nAny system that acts only as an informative machine, only answering questions and has no goals is by definition not an AI at all. That means that a non-AI Oracular is but a calculator of outputs based on inputs. Since the term in itself is heterogeneous, the proposals made for a sub-division are merely informal.\n\nAn *Advisor* can be seen as a system that gathers data from the real world and computes the answer to an informal “what we ought to do?” question. They also represent a FAI-complete problem.\n\nA *Question-Answerer* is a similar system that gathers data from the real world but coupled with a question. It then somehow computes the answer. The difficulty can lay on distinguishing it from an Advisor and controlling the safety of its answers.\n\nFinally, a *Predictor* is seen as a system that takes a corpus of data and produces a probability distribution over future possible data. There are some proposed dangers with predictors, namely exhibiting goal-seeking behavior which does not converge with humanity goals and the ability to influence us through the predictions.\n\nFurther reading & References\n----------------------------\n\n*   [Dreams of Friendliness](http://lesswrong.com/lw/tj/dreams_of_friendliness/)\n*   [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf) by Armstrong, Sandberg and [Bostrom](https://www.lesswrong.com/tag/nick-bostrom)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb26b",
    "name": "Applause Light",
    "core": null,
    "slug": "applause-light",
    "tableOfContents": {
      "html": "<p><span class=\"by_7r4pRYHgRRMuxw9fL\">An </span><strong><span class=\"by_7r4pRYHgRRMuxw9fL\">applause light</span></strong><span class=\"by_7r4pRYHgRRMuxw9fL\"> is an empty statement which evokes positive affect without providing new information.</span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">It [was] not so much a </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">propositional</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> statement, as the equivalent of the \"Applause\" light that tells a studio audience when to clap.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—</span><a href=\"http://lesswrong.com/lw/jb/applause_lights/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Applause Lights</span></u></a></p></blockquote><h2 id=\"See_also\"><span class=\"by_iRXwaeHoALf2AtqDZ\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Guessing_the_teacher's_password\"><span class=\"by_qf77EiaoMw7tH3GSr\">Guessing the teacher's password</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">&nbsp;</span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Belief_as_attire\"><span class=\"by_qf77EiaoMw7tH3GSr\">Belief as attire</span></a></li><li><a href=\"https://www.lesswrong.com/tag/cached-thoughts\"><span class=\"by_HoGziwmhpMGqGeWZy\">Cached Thoughts</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 4,
    "description": {
      "markdown": "An **applause light** is an empty statement which evokes positive affect without providing new information.\n\n> It \\[was\\] not so much a *propositional* statement, as the equivalent of the \"Applause\" light that tells a studio audience when to clap.\n> \n> —[Applause Lights](http://lesswrong.com/lw/jb/applause_lights/)\n\nSee also\n--------\n\n*   [Guessing the teacher's password](https://wiki.lesswrong.com/wiki/Guessing_the_teacher's_password) \n*   [Belief as attire](https://wiki.lesswrong.com/wiki/Belief_as_attire)\n*   [Cached Thoughts](https://www.lesswrong.com/tag/cached-thoughts)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb25c",
    "name": "Kolmogorov Complexity",
    "core": null,
    "slug": "kolmogorov-complexity",
    "tableOfContents": {
      "html": "<p><span class=\"by_woC2b5rav5sGrAo3E\">The </span><strong><span class=\"by_woC2b5rav5sGrAo3E\">Kolmogorov Complexity</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> (sometimes called Algorithmic Complexity)</span><span class=\"by_woC2b5rav5sGrAo3E\"> of a set of data is the size of the shortest possible description of </span><span class=\"by_qxJ28GN72aiJu96iF\">the</span><span class=\"by_woC2b5rav5sGrAo3E\"> data.</span></span><br><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff Induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/aixi\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI</span></a></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">Algorithmic complexity is</span><span class=\"by_woC2b5rav5sGrAo3E\"> an inverse measure of compressibility. </span><span class=\"by_qxJ28GN72aiJu96iF\">If the</span><span class=\"by_woC2b5rav5sGrAo3E\"> data is </span><span class=\"by_qxJ28GN72aiJu96iF\">complex and random,</span><span class=\"by_woC2b5rav5sGrAo3E\"> the shortest </span><span class=\"by_qxJ28GN72aiJu96iF\">possible description of it becomes longer.</span><span class=\"by_woC2b5rav5sGrAo3E\"> This is also one of the best </span><span class=\"by_qxJ28GN72aiJu96iF\">definitions</span><span class=\"by_woC2b5rav5sGrAo3E\"> of randomness so far</span></span><a href=\"#fn1\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">1</span></sup></a><span><span class=\"by_woC2b5rav5sGrAo3E\">. </span><span class=\"by_qxJ28GN72aiJu96iF\">If</span><span class=\"by_woC2b5rav5sGrAo3E\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">data has few regular patterns,</span><span class=\"by_woC2b5rav5sGrAo3E\"> it </span><span class=\"by_qxJ28GN72aiJu96iF\">is difficult</span><span class=\"by_woC2b5rav5sGrAo3E\"> to compress it or describe it shortly, </span><span class=\"by_qxJ28GN72aiJu96iF\">giving it a high </span><span class=\"by_Pt8FYFHnhJzqTsJRq\">Kolmogorov</span><span class=\"by_woC2b5rav5sGrAo3E\"> complexity and randomness. </span><span class=\"by_qxJ28GN72aiJu96iF\">If</span><span class=\"by_woC2b5rav5sGrAo3E\"> there </span><span class=\"by_qxJ28GN72aiJu96iF\">isn'</span><span class=\"by_woC2b5rav5sGrAo3E\">t </span><span class=\"by_qxJ28GN72aiJu96iF\">any</span><span class=\"by_woC2b5rav5sGrAo3E\"> way to describe the data </span><span class=\"by_qxJ28GN72aiJu96iF\">so that the description is shorter</span><span class=\"by_woC2b5rav5sGrAo3E\"> than the data itself, the data is incompressible. </span></span><a href=\"#fn2\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">2</span></sup></a></p><p><span class=\"by_woC2b5rav5sGrAo3E\">More formally, the Kolmogorov complexity C(x) of a set x, is the size in bits of the shortest binary program (in a fixed programming language) that prints the set x as its only output. If C(x) is equal or greater than the size of x in bits, x is incompressible. </span><a href=\"#fn3\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">3</span></sup></a></p><p><span><span class=\"by_woC2b5rav5sGrAo3E\">This notion can be used to state many important results in computational theory. </span><span class=\"by_qxJ28GN72aiJu96iF\">Possibly the</span><span class=\"by_woC2b5rav5sGrAo3E\"> most </span><span class=\"by_qxJ28GN72aiJu96iF\">famous</span><span class=\"by_woC2b5rav5sGrAo3E\"> is </span></span><a href=\"http://en.wikipedia.org/wiki/Kolmogorov_complexity#Chaitin.27s_incompleteness_theorem\"><span class=\"by_woC2b5rav5sGrAo3E\">Chaitin's incompleteness theorem</span></a><span class=\"by_woC2b5rav5sGrAo3E\">, a version of Gödel’s incompleteness theorem.</span></p><h2 id=\"References\"><span class=\"by_woC2b5rav5sGrAo3E\">References</span></h2><ol><li><span class=\"by_woC2b5rav5sGrAo3E\">SIPSER, M. (1983) \"A complexity theoretic approach to randomness\". In Proceedings of the 15th ACM Symposium on the Theory of Computing, pages 330{335. ACM, New York.</span><a href=\"#fnref1\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">FORTNOW, Lance. \"Kolmogorov Complexity\" Available at: </span><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.4949&amp;rep=rep1&amp;type=pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.4949&amp;rep=rep1&amp;type=pdf</span></a><a href=\"#fnref2\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">LI, Ming. &amp; VITANY, Paul. “Algorithmic Complexity”. Available at: </span><a href=\"http://homepages.cwi.nl/~paulv/papers/020608isb.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">http://homepages.cwi.nl/~paulv/papers/020608isb.pdf</span></a><a href=\"#fnref3\"><span class=\"by_woC2b5rav5sGrAo3E\">↩</span></a></li></ol>",
      "sections": [
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 27,
    "description": {
      "markdown": "The **Kolmogorov Complexity** (sometimes called Algorithmic Complexity) of a set of data is the size of the shortest possible description of the data.  \n  \n*See also*: [Solomonoff Induction](https://www.lesswrong.com/tag/solomonoff-induction), [AIXI](https://www.lesswrong.com/tag/aixi)\n\nAlgorithmic complexity is an inverse measure of compressibility. If the data is complex and random, the shortest possible description of it becomes longer. This is also one of the best definitions of randomness so far[^1^](#fn1). If the data has few regular patterns, it is difficult to compress it or describe it shortly, giving it a high Kolmogorov complexity and randomness. If there isn't any way to describe the data so that the description is shorter than the data itself, the data is incompressible. [^2^](#fn2)\n\nMore formally, the Kolmogorov complexity C(x) of a set x, is the size in bits of the shortest binary program (in a fixed programming language) that prints the set x as its only output. If C(x) is equal or greater than the size of x in bits, x is incompressible. [^3^](#fn3)\n\nThis notion can be used to state many important results in computational theory. Possibly the most famous is [Chaitin's incompleteness theorem](http://en.wikipedia.org/wiki/Kolmogorov_complexity#Chaitin.27s_incompleteness_theorem), a version of Gödel’s incompleteness theorem.\n\nReferences\n----------\n\n1.  SIPSER, M. (1983) \"A complexity theoretic approach to randomness\". In Proceedings of the 15th ACM Symposium on the Theory of Computing, pages 330{335. ACM, New York.[↩](#fnref1)\n2.  FORTNOW, Lance. \"Kolmogorov Complexity\" Available at: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.4949&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.4949&rep=rep1&type=pdf)[↩](#fnref2)\n3.  LI, Ming. & VITANY, Paul. “Algorithmic Complexity”. Available at: [http://homepages.cwi.nl/~paulv/papers/020608isb.pdf](http://homepages.cwi.nl/~paulv/papers/020608isb.pdf)[↩](#fnref3)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb253",
    "name": "Luminosity",
    "core": null,
    "slug": "luminosity",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Luminosity </span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\">was Alicorn's term for</span><span class=\"by_qf77EiaoMw7tH3GSr\"> reflective awareness. </span><span class=\"by_r38pkCm7wF4M44MDQ\">More recent posts tend to use the term </span></span><a href=\"https://www.lesswrong.com/tag/introspection\"><span class=\"by_r38pkCm7wF4M44MDQ\">introspection</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">. See also </span><a href=\"https://www.lesswrong.com/tag/noticing\"><span class=\"by_r38pkCm7wF4M44MDQ\">noticing</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">.</span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">A luminous mental state is one that you have and know that you have. It could be an </span><a href=\"https://www.lesswrong.com/tag/emotions\"><span class=\"by_qf77EiaoMw7tH3GSr\">emotion</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, a </span><a href=\"https://www.lesswrong.com/tag/belief\"><span class=\"by_qf77EiaoMw7tH3GSr\">belief</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> or </span><a href=\"https://www.lesswrong.com/tag/alief\"><span class=\"by_qf77EiaoMw7tH3GSr\">alief</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, a disposition, a quale, a memory - anything that might happen or be stored in your brain. What's going on in your head?</span></p><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/s/ynMFrq9K5iNMfSZNg\"><span class=\"by_qf77EiaoMw7tH3GSr\">Living Luminously (sequence)</span></a></li><li><a href=\"https://www.lesswrong.com/tag/luminosity-fanfiction\"><span class=\"by_qf77EiaoMw7tH3GSr\">Luminosity (fanfiction)</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 6,
    "description": {
      "markdown": "**Luminosity** was Alicorn's term for reflective awareness. More recent posts tend to use the term [introspection](https://www.lesswrong.com/tag/introspection). See also [noticing](https://www.lesswrong.com/tag/noticing).\n\nA luminous mental state is one that you have and know that you have. It could be an [emotion](https://www.lesswrong.com/tag/emotions), a [belief](https://www.lesswrong.com/tag/belief) or [alief](https://www.lesswrong.com/tag/alief), a disposition, a quale, a memory - anything that might happen or be stored in your brain. What's going on in your head?\n\nSee also\n--------\n\n*   [Living Luminously (sequence)](https://www.lesswrong.com/s/ynMFrq9K5iNMfSZNg)\n*   [Luminosity (fanfiction)](https://www.lesswrong.com/tag/luminosity-fanfiction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb23e",
    "name": "Trivial Inconvenience",
    "core": null,
    "slug": "trivial-inconvenience",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_fmTiLqp6mmXeLjwfN\">Trivial inconveniences</span></strong><span><span class=\"by_fmTiLqp6mmXeLjwfN\"> are inconveniences that take </span><span class=\"by_LoykQRMTxJFxwwdPy\">few</span><span class=\"by_fmTiLqp6mmXeLjwfN\"> resources</span><span class=\"by_LoykQRMTxJFxwwdPy\"> to counteract</span><span class=\"by_fmTiLqp6mmXeLjwfN\"> but have a disproportionate impact on people deciding whether to take a course of action.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/akrasia\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Akrasia</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/aversion-ugh-fields\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ugh field</span></a></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/f1/beware_trivial_inconveniences/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Beware Trivial Inconveniences</span></a></li><li><a href=\"http://lesswrong.com/lw/13z/celebrate_trivial_impetuses/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Celebrate Trivial Impetuses</span></a></li></ul>",
      "sections": [
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 6,
    "description": {
      "markdown": "**Trivial inconveniences** are inconveniences that take few resources to counteract but have a disproportionate impact on people deciding whether to take a course of action.\n\n*See also*: [Akrasia](https://www.lesswrong.com/tag/akrasia), [Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)\n\nNotable Posts\n-------------\n\n*   [Beware Trivial Inconveniences](http://lesswrong.com/lw/f1/beware_trivial_inconveniences/)\n*   [Celebrate Trivial Impetuses](http://lesswrong.com/lw/13z/celebrate_trivial_impetuses/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb23b",
    "name": "Epistemic Luck",
    "core": null,
    "slug": "epistemic-luck",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_LoykQRMTxJFxwwdPy\">You would have different beliefs if certain events in your life were </span><span class=\"by_qgdGA4ZEyW7zNdK84\">different and since you don't choose many of the events in your life, this implies that your beliefs are in large part due to </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic Luck</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><span class=\"by_LoykQRMTxJFxwwdPy\"> How should you react to this fact?</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">It has been noted that one's feelings about </span><a href=\"/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> correlate a great deal with whether one lives in Berkeley (MIRI-centric) or Oxford (FHI-centric).</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/1r1/epistemic_luck/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Epistemic Luck</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Alicorn\"><span class=\"by_qf77EiaoMw7tH3GSr\">Alicorn</span></a></li><li><a href=\"http://lesswrong.com/lw/m2/the_litany_against_gurus/\"><span class=\"by_LoykQRMTxJFxwwdPy\">The Litany Against Gurus</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/information-cascades\"><span class=\"by_qf77EiaoMw7tH3GSr\">Information cascade</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Availability_bias\"><span class=\"by_qf77EiaoMw7tH3GSr\">Availability bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/privileging-the-hypothesis\"><span class=\"by_qf77EiaoMw7tH3GSr\">Privileging the hypothesis</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://www.iep.utm.edu/epi-luck/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Epistemic Luck</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, at the Internet Encyclopedia of Philosophy</span></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 2,
    "description": {
      "markdown": "You would have different beliefs if certain events in your life were different and since you don't choose many of the events in your life, this implies that your beliefs are in large part due to **Epistemic Luck**. How should you react to this fact?\n\nIt has been noted that one's feelings about [Decision Theory](/tag/decision-theory) correlate a great deal with whether one lives in Berkeley (MIRI-centric) or Oxford (FHI-centric).\n\nBlog posts\n----------\n\n*   [Epistemic Luck](http://lesswrong.com/lw/1r1/epistemic_luck/) by [Alicorn](https://wiki.lesswrong.com/wiki/Alicorn)\n*   [The Litany Against Gurus](http://lesswrong.com/lw/m2/the_litany_against_gurus/)\n\nSee also\n--------\n\n*   [Information cascade](https://www.lesswrong.com/tag/information-cascades)\n*   [Availability bias](https://wiki.lesswrong.com/wiki/Availability_bias)\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis)\n\nExternal links\n--------------\n\n*   [Epistemic Luck](http://www.iep.utm.edu/epi-luck/), at the Internet Encyclopedia of Philosophy"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb226",
    "name": "Robot",
    "core": null,
    "slug": "robot",
    "tableOfContents": {
      "html": "<p><span class=\"by_yHHBZZDtuWZw92SAf\">An </span><strong><span><span class=\"by_yHHBZZDtuWZw92SAf\">online</span><span class=\"by_9c2mQkLQq6gQSksMs\"> debate tool</span></span></strong><span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">facilitates</span><span class=\"by_yHHBZZDtuWZw92SAf\"> the act</span><span class=\"by_9c2mQkLQq6gQSksMs\"> of </span><span class=\"by_yHHBZZDtuWZw92SAf\">debating by helping </span><span class=\"by_qf77EiaoMw7tH3GSr\">to </span><span class=\"by_yHHBZZDtuWZw92SAf\">manage the structure of argumentation. This </span><span class=\"by_qf77EiaoMw7tH3GSr\">distinguishes</span><span class=\"by_yHHBZZDtuWZw92SAf\"> it from general purpose communication tools such as wikis and forums. Some online debate tools provide graphical representations of arguments, but this is not</span><span class=\"by_9c2mQkLQq6gQSksMs\"> a </span><span class=\"by_yHHBZZDtuWZw92SAf\">requirement.</span></span></p><p><span><span class=\"by_9c2mQkLQq6gQSksMs\">This wiki page </span><span class=\"by_wTfwLW2tgbC54JGex\">and tag </span><span class=\"by_qf77EiaoMw7tH3GSr\">gives</span><span class=\"by_9c2mQkLQq6gQSksMs\"> a list </span><span class=\"by_qf77EiaoMw7tH3GSr\">and characterization </span><span class=\"by_9c2mQkLQq6gQSksMs\">of </span><span class=\"by_qf77EiaoMw7tH3GSr\">debate tools. Debate tools were previously </span></span><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">discussed </span><span class=\"by_wTfwLW2tgbC54JGex\">here</span></span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><h1 id=\"Literature\"><span class=\"by_wTfwLW2tgbC54JGex\">Literature</span></h1><p><span><span class=\"by_wTfwLW2tgbC54JGex\">There exists an academic literature</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">on</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span><span class=\"by_wTfwLW2tgbC54JGex\">argument mapping and other tools (computer aided or not) for assisting debate. The most recent survey seems to be \"</span></span><a href=\"http://www.springerlink.com/content/j3p581601n3x1200/\"><span><span class=\"by_wTfwLW2tgbC54JGex\">Computer-supported argumentation: A review of </span><span class=\"by_9c2mQkLQq6gQSksMs\">the </span><span class=\"by_wTfwLW2tgbC54JGex\">state of the art</span></span></a><span><span class=\"by_wTfwLW2tgbC54JGex\">\" written in June 2009, which lists 50 tools (starting on</span><span class=\"by_9c2mQkLQq6gQSksMs\"> page</span><span class=\"by_wTfwLW2tgbC54JGex\"> 94).</span></span></p><p><a href=\"https://en.wikipedia.org/wiki/Online_deliberation\"><span class=\"by_wTfwLW2tgbC54JGex\">Online Deliberation</span></a><span><span class=\"by_wTfwLW2tgbC54JGex\"> is a related discipline that asks what are</span><span class=\"by_9c2mQkLQq6gQSksMs\"> the </span><span class=\"by_wTfwLW2tgbC54JGex\">effects</span><span class=\"by_9c2mQkLQq6gQSksMs\"> of </span><span class=\"by_wTfwLW2tgbC54JGex\">online discussions, when are they effective, and how to design better systems.&nbsp;</span></span></p><h1 id=\"List_of_debate_tools\"><span class=\"by_qf77EiaoMw7tH3GSr\">List of debate tools</span></h1><h2 id=\"Debate_Map\"><a href=\"https://debatemap.app/\"><span class=\"by_6YASRE8fcSDwJw3Lg\">Debate Map</span></a></h2><p><span class=\"by_6YASRE8fcSDwJw3Lg\">Summary: Tree-based mapping of beliefs, arguments, and evidence.</span></p><ul><li><span class=\"by_6YASRE8fcSDwJw3Lg\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/dw0i\"><span class=\"by_6YASRE8fcSDwJw3Lg\">a comment by Venryx</span></a></li></ul></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">pros:</span><ul><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Collaborative creation, editing, and evaluation of debate/argument maps.</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Open source. (under the MIT license)</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Developed using modern web technologies. (react-js, mobx, firestore)</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Ability to enter both formal arguments (premises and conclusion), or less structured \"single-premise arguments\".</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Rating system for the truth/probability of claims, as well as the relevance/validity of arguments.</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Tree-based structure which can extend very deep without loss of clarity or usability.</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Integrated term/definition system. Terms can be defined once, then used anywhere, with hover-based definition display.</span></li></ul></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">cons:</span><ul><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Has a learning curve for casual users, as content must conform to the argument&lt;-premise structure at each level.</span></li><li><span class=\"by_6YASRE8fcSDwJw3Lg\">Not yet made usable on mobile devices.</span></li></ul></li></ul><h2 id=\"DebateArt\"><a href=\"https://www.debateart.com/\"><span class=\"by_RR5TWzKNzE6Zntc6Z\">DebateArt</span></a></h2><p><span class=\"by_RR5TWzKNzE6Zntc6Z\">Summary: Debating platform with rich one-on-one debates functionality and advanced discussions forum.</span></p><ul><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">pros:</span><ul><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Clean and convenient design.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Advanced and flexible one-on-one debating system.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Separate forum for casual discussions.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Private messaging system.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Great performance.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Friendly community.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Active and thorough moderation.</span></li></ul></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">cons:</span><ul><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">Has a learning curve for casual users.</span></li><li><span class=\"by_RR5TWzKNzE6Zntc6Z\">No dedicated mobile version.</span></li></ul></li></ul><h2 id=\"debategraph_org\"><a href=\"http://debategraph.org/\"><span class=\"by_9c2mQkLQq6gQSksMs\">debategraph.org</span></a></h2><p><span class=\"by_9c2mQkLQq6gQSksMs\">Summary: This... is pretty much exactly what we were looking for, isn't it? Though it doesn't do anything with probabilities.</span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/20v5\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by Peer Infinity</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">collaboratively edit argument maps</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">it doesn't do anything with probabilities.</span></li><li><i><span class=\"by_wTfwLW2tgbC54JGex\">Do not zoom out too much!</span></i></li></ul></li></ul><h2 id=\"Argunet\"><a href=\"http://www.argunet.org/working-with-argunet/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Argunet</span></a></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Summary: </span><span class=\"by_9c2mQkLQq6gQSksMs\">Argunet enables you to create argument maps of complex debates online or offline, on your own or in a team.</span></span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/\"><span class=\"by_9c2mQkLQq6gQSksMs\">the original article</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">collaboratively edit argument maps</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">not entirely straightforward to use, Morendil had trouble figuring out how to move boxes around.</span></li></ul></li></ul><h2 id=\"bCisive_Online\"><a href=\"http://bcisiveonline.com/\"><span class=\"by_9c2mQkLQq6gQSksMs\">bCisive Online</span></a></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Summary: </span><span class=\"by_9c2mQkLQq6gQSksMs\">a simple canvas for creating a tree diagram of a </span><span class=\"by_qf77EiaoMw7tH3GSr\">debate.</span></span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/\"><span class=\"by_9c2mQkLQq6gQSksMs\">the original article</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">easy to use</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">all it does is let you make the tree diagram, it doesn't do anything else with the data</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">examples:</span><ul><li><a href=\"http://morendil.bcisiveonline.com/spaces/989ae551bc100d0365c96a7bcc20f188d95fb58d/\"><span class=\"by_9c2mQkLQq6gQSksMs\">a map of Morendil's current thinking on cryonics</span></a></li></ul></li></ul><h2 id=\"Flow\"><a href=\"http://en.wikipedia.org/wiki/Flow_%28policy_debate%29\"><span class=\"by_9c2mQkLQq6gQSksMs\">Flow</span></a></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Summary: </span><span class=\"by_9c2mQkLQq6gQSksMs\">a specialized form of note taking called \"flowing\" within the policy/CEDA/NDT debate community.</span></span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kx1\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by JenniferRM</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">lots of people have used this technique, and it has been proven to work well</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">it requires a very specific format for the debate</span></li></ul></li></ul><h2 id=\"PyMC\"><a href=\"http://code.google.com/p/pymc/\"><span class=\"by_9c2mQkLQq6gQSksMs\">PyMC</span></a></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Summary: </span><span class=\"by_9c2mQkLQq6gQSksMs\">a DSL in python for (non-recursive) Bayesian models and Bayesian probability computations.</span></span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kx3\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by Steve_Rayhawk</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">it does Bayesian calculations</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">requires literacy in python and bayesian statistics</span></li></ul></li></ul><p><a href=\"http://www.demoscience.org/\"><span class=\"by_9c2mQkLQq6gQSksMs\">MACOSPOL</span></a></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kt6\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by Morendil</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">examples:</span><ul><li><a href=\"http://medialab.sciences-po.fr/controversies/\"><span class=\"by_9c2mQkLQq6gQSksMs\">mapped controversies</span></a></li></ul></li></ul><h2 id=\"Scott_Aaronson_s_worldview_manager\"><a href=\"http://projects.csail.mit.edu/worldview/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Scott Aaronson's worldview manager</span></a></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Summary: this</span><span class=\"by_9c2mQkLQq6gQSksMs\"> is designed to point out hidden contradictions (or at least tensions) between one's beliefs, by using programmed in implications to exhibit (possibly long) inferential chains that demonstrate a contradiction.</span></span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1ku8\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by wnoise</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">it does lots of stuff</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">it's kinda complicated</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">examples:</span><ul><li><a href=\"http://www.gitorious.org/worldview/worldview/blobs/master/topics/axiom_of_choice.wvm\"><span class=\"by_9c2mQkLQq6gQSksMs\">sample worldview</span></a></li><li><a href=\"http://www.gitorious.org/worldview/worldview/blobs/master/topics/libertarianism.wvm\"><span class=\"by_9c2mQkLQq6gQSksMs\">The model of libertarian ideas</span></a></li></ul></li></ul><h2 id=\"Canonizer_com\"><a href=\"http://canonizer.com/\"><span class=\"by_GSPAs9ktLH4RovB9T\">Canonizer.com</span></a></h2><p><span class=\"by_GSPAs9ktLH4RovB9T\">Summary: Canonizer.com is a wiki system with added camp and survey capabilities. The system provides a rigorous way to measure scientific / moral expert consensus. It is designed for collaborative development of concise descriptions of various competing scientific or moral theories, and the best arguments for such. People can join the camps representing such, giving a quantitative survey or measure of consensus compared to all others. Proposed changes to supported camps go into a review mode for one week. Any supporters of a camp can object to any such proposed change during this time. If it survives a week with no objection, it goes live, guaranteeing unanimous agreement to such changes to the petition by all current signers. If anyone does object, the camp can be forked (taking all supporters of the 'improvement'), or the info can be included in a sporting sub camp.</span></p><p><span class=\"by_GSPAs9ktLH4RovB9T\">The karma or 'canonization' system enables the readers to select any algorithm they wish on the side bar to 'find the good stuff'. For example, you can compare the </span><a href=\"http://canonizer.com/topic.asp/53/11\"><span class=\"by_GSPAs9ktLH4RovB9T\">mind expert</span></a><span class=\"by_GSPAs9ktLH4RovB9T\"> scientific consensus with the default general population consensus. Each camp has a forum to discuss and debate further improvements for camps. The general idea is to debate things in the forums, or elsewhere, and summarize everyone's final / current / state of the art view in the camp statements. A history of everything is maintained, providing a dynamic quantitative measure of how well accepted any theory is, as ever more theory falsifying (when experts abandon a falsified camp) scientific data / new arguments... come in.</span></p><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/2j4l?c=1\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by PeerInfinity</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">the whole canonization thing</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">it's kinda complicated</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">examples:</span><ul><li><a href=\"http://canonizer.com\"><span class=\"by_9c2mQkLQq6gQSksMs\">the main list of canonized camps</span></a></li></ul></li></ul><h2 id=\"Explore_Ideas_com\"><a href=\"http://www.explore-ideas.com/\"><span class=\"by_94RpdHBckTPmskbeR\">Explore-Ideas.com</span></a></h2><p><span class=\"by_94RpdHBckTPmskbeR\">Summary: explore-ideas.com is a graph structure forum where users start with a topic and follow arguments they agree with, creating a personalized 'story'.</span></p><ul><li><span class=\"by_94RpdHBckTPmskbeR\">pros: users can link any two comments and merge ideas from different discussions into a single logical argument, as well as make loops in reasoning. Each user comes to his personal 'win' ending based on arguments (s)he agrees with. There is no global 'win' or 'lose' argument.</span></li></ul><p><span class=\"by_94RpdHBckTPmskbeR\">This forum encourages dynamic debate that goes beyond pro/con binary approach, similar to that employed in </span><a href=\"http://en.wikipedia.org/wiki/Proofs_and_Refutations\"><span class=\"by_94RpdHBckTPmskbeR\">Proofs and Refutations</span></a><span class=\"by_94RpdHBckTPmskbeR\">.</span></p><ul><li><span class=\"by_94RpdHBckTPmskbeR\">cons: UI is rudimentary.</span></li></ul><h2 id=\"Debate_fm\"><a href=\"http://www.debate.fm/\"><span class=\"by_C5umY4rLHzRwi3H9k\">Debate.fm</span></a></h2><p><span class=\"by_C5umY4rLHzRwi3H9k\">Summary: A simple platform to start a debate on any topic. Mainly focused at general user participation.</span></p><ul><li><span class=\"by_C5umY4rLHzRwi3H9k\">pros: Simple structured content - side by side. Mainly focused at pro/con binary approach</span></li><li><span class=\"by_C5umY4rLHzRwi3H9k\">cons: Till now not ready for academic use.</span></li></ul><h2 id=\"cartargrapher\"><a href=\"http://cartargrapher.appspot.com/\"><span class=\"by_mcKSiwq2TBrTMZS6X\">cartargrapher</span></a></h2><p><span class=\"by_mcKSiwq2TBrTMZS6X\">\"a simple argument mapping app, made using Google’s visualization API, jquery, and python, and running on Google’s AppEngine. Note: at this point, I don’t guarantee the persistence of saved argument maps!\" - </span><a href=\"http://johnmacfarlane.net/tools.html\"><span class=\"by_mcKSiwq2TBrTMZS6X\">John MacFarlane</span></a></p><h2 id=\"Consider_it\"><a href=\"https://consider.it\"><span class=\"by_wTfwLW2tgbC54JGex\">Consider.it</span></a></h2><p><span><span class=\"by_wTfwLW2tgbC54JGex\">Graphically represents people's agreement with a statement</span><span class=\"by_8HmuRoMkPMR2D5jZT\"> and </span><span class=\"by_wTfwLW2tgbC54JGex\">which arguments were most used.</span><span class=\"by_8HmuRoMkPMR2D5jZT\"> The </span><span class=\"by_wTfwLW2tgbC54JGex\">arguments themselves are not subjects</span><span class=\"by_8HmuRoMkPMR2D5jZT\"> of </span><span class=\"by_wTfwLW2tgbC54JGex\">further investigation though. Check out </span></span><a href=\"https://hala.consider.it/?tab=Feedback%20on%20key%20principles\"><span class=\"by_wTfwLW2tgbC54JGex\">this use-case</span></a><span><span class=\"by_8HmuRoMkPMR2D5jZT\"> of </span><span class=\"by_wTfwLW2tgbC54JGex\">public decisions</span><span class=\"by_8HmuRoMkPMR2D5jZT\"> in </span><span class=\"by_wTfwLW2tgbC54JGex\">Seattle.</span></span></p><h1 id=\"Ideas_for_new_tools\"><span class=\"by_qf77EiaoMw7tH3GSr\">Ideas for new tools</span></h1><ul><li><span><span class=\"by_qf77EiaoMw7tH3GSr\">Based</span><span class=\"by_9c2mQkLQq6gQSksMs\"> on MediaWiki, PHP, </span><span class=\"by_qf77EiaoMw7tH3GSr\">GraphViz,</span><span class=\"by_9c2mQkLQq6gQSksMs\"> and maybe XML</span></span></li><li><span><span class=\"by_qf77EiaoMw7tH3GSr\">Summary: </span><span class=\"by_9c2mQkLQq6gQSksMs\">a tool that we make ourselves, so that it works the way we want it to work</span></span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">first mentioned:</span><ul><li><a href=\"http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1l1w\"><span class=\"by_9c2mQkLQq6gQSksMs\">a comment by PeerInfinity</span></a></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">pros:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">we're writing it, so we can make it work how we want</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">cons:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">we would need to write it from scratch</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">examples:</span><ul><li><a href=\"http://transhumanistwiki.com/wiki/Peer_Infinity/Chat_With_Fael_About_AI\"><span class=\"by_9c2mQkLQq6gQSksMs\">a conversation about AI</span></a></li></ul></li></ul><h2 id=\"Brass_Tacks\"><span class=\"by_28YXNgAMqSHizndKA\">Brass Tacks</span></h2><ul><li><a href=\"http://issuepedia.org/Issuepedia:Structured_Debate\"><span class=\"by_28YXNgAMqSHizndKA\">Structured debate</span></a><span class=\"by_28YXNgAMqSHizndKA\">: a set of rules which debate software could help enforce. Some data design is on paper, not yet transcribed. The plan is to write this first as a MediaWiki extension. </span><a href=\"http://issuepedia.org/Category:Debates\"><span class=\"by_28YXNgAMqSHizndKA\">some mockups</span></a></li></ul><h1 id=\"Other_links\"><span class=\"by_qf77EiaoMw7tH3GSr\">Other links</span></h1><ul><li><a href=\"http://www.visualizingargumentation.info/\"><span class=\"by_9c2mQkLQq6gQSksMs\">http://www.visualizingargumentation.info/</span></a></li><li><a href=\"http://www.tandf.co.uk/journals/tarc\"><span class=\"by_9c2mQkLQq6gQSksMs\">http://www.tandf.co.uk/journals/tarc</span></a></li><li><a href=\"http://issuepedia.org/Structured_debate\"><span class=\"by_9c2mQkLQq6gQSksMs\">structured debate</span></a></li><li><a href=\"http://issuepedia.org/Issuepedia:Dispute_Resolution_Technology\"><span class=\"by_9c2mQkLQq6gQSksMs\">dispute resolution technology</span></a></li></ul><h1 id=\"Features_that_a_debate_tool_should_have\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">Features that </span><span class=\"by_qf77EiaoMw7tH3GSr\">a</span><span class=\"by_9c2mQkLQq6gQSksMs\"> debate tool should </span><span class=\"by_qf77EiaoMw7tH3GSr\">have</span></span></h1><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">from almost everyone:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">an easy to use interface</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">from </span><a href=\"http://lesswrong.com/user/Morendil/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Morendil</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">a conclusion or a decision, which is to be \"tested\" by the use of the tool</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">various hypotheses, which are offered in support or in opposition to the conclusion, with degrees of plausibility</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">logical structure, such as \"X follows from Y\"</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">challenges to logical structure, such as \"X may not necessarily follow from Y, if you grant Z\"</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">elements of evidence, which make hypotheses more or less probable</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">recursive relations between these elements</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">from </span><a href=\"http://lesswrong.com/user/PhilGoetz/\"><span class=\"by_9c2mQkLQq6gQSksMs\">PhilGoetz</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">an XML-based representation of the data</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">from </span><a href=\"http://lesswrong.com/user/PeerInfinity/\"><span class=\"by_9c2mQkLQq6gQSksMs\">PeerInfinity</span></a><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">generates its results from an annotated log of a debate</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">collaboratively editable, possibly using MediaWiki</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">multiple outfut formats: graphs, tables, the raw data</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">from </span><a href=\"http://lesswrong.com/user/Johnicholas/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Johnicholas</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">Compose in ordinary ASCII or UTF-8</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">Compose primarily a running-text argument, indicating the formal structure with annotations</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">Export as a prettified document, still mostly running text (html and LaTeX)</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">Export as a diagram (automatically layed out, perhaps by graphviz)</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">Export as a bayes net (in possibly several bayes net formats)</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">Export as a machine-checkable proof (in possibly several formats)</span></li></ul></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">from </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_9c2mQkLQq6gQSksMs\">Eliezer Yudkowsky</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">:</span><ul><li><span class=\"by_9c2mQkLQq6gQSksMs\">prevents online arguments from retracing the same points over and over.</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">not just graphical with boxes, because that makes poor use of screen real estate.</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">not have lots of fancy argument types and patterns, because no one really uses that stuff</span></li><li><span class=\"by_9c2mQkLQq6gQSksMs\">a karma system, because otherwise there's no way to find the good stuff.</span></li></ul></li></ul><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">(</span><span class=\"by_9c2mQkLQq6gQSksMs\">So, now that everything's all neatly arranged in a list, the next step is to decide whether we want to start using any of these tools, or if we want to create our own.</span><span class=\"by_qf77EiaoMw7tH3GSr\">)</span></span></p><h1 id=\"LWers_interested_in_developing_debate_tools\"><span class=\"by_83AFpCu3r6PQP6qxE\">LWers interested in developing debate tools</span></h1><ul><li><a href=\"http://lesswrong.com/lw/le5/welcome_to_less_wrong_7th_thread_december_2014/c64f\"><span class=\"by_83AFpCu3r6PQP6qxE\">Curtis SerVaas</span></a></li><li><a href=\"http://lesswrong.com/lw/jfn/introducing_impact/\"><span class=\"by_83AFpCu3r6PQP6qxE\">.impact has discussed making argument mapping software</span></a></li><li><span class=\"by_83AFpCu3r6PQP6qxE\">The \"Collaborative Argumentation Analysis\" Facebook Group has a lot of links/discussion/people.</span></li></ul><h1 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h1><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Rationality_power_tools\"><span class=\"by_qf77EiaoMw7tH3GSr\">Rationality power tools</span></a></li><li><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_qf77EiaoMw7tH3GSr\">Prediction market</span></a></li><li><a href=\"https://www.lesswrong.com/tag/predictionbook\"><span class=\"by_qf77EiaoMw7tH3GSr\">PredictionBook</span></a></li></ul>",
      "sections": [
        {
          "title": "Literature",
          "anchor": "Literature",
          "level": 1
        },
        {
          "title": "List of debate tools",
          "anchor": "List_of_debate_tools",
          "level": 1
        },
        {
          "title": "Debate Map",
          "anchor": "Debate_Map",
          "level": 2
        },
        {
          "title": "DebateArt",
          "anchor": "DebateArt",
          "level": 2
        },
        {
          "title": "debategraph.org",
          "anchor": "debategraph_org",
          "level": 2
        },
        {
          "title": "Argunet",
          "anchor": "Argunet",
          "level": 2
        },
        {
          "title": "bCisive Online",
          "anchor": "bCisive_Online",
          "level": 2
        },
        {
          "title": "Flow",
          "anchor": "Flow",
          "level": 2
        },
        {
          "title": "PyMC",
          "anchor": "PyMC",
          "level": 2
        },
        {
          "title": "Scott Aaronson's worldview manager",
          "anchor": "Scott_Aaronson_s_worldview_manager",
          "level": 2
        },
        {
          "title": "Canonizer.com",
          "anchor": "Canonizer_com",
          "level": 2
        },
        {
          "title": "Explore-Ideas.com",
          "anchor": "Explore_Ideas_com",
          "level": 2
        },
        {
          "title": "Debate.fm",
          "anchor": "Debate_fm",
          "level": 2
        },
        {
          "title": "cartargrapher",
          "anchor": "cartargrapher",
          "level": 2
        },
        {
          "title": "Consider.it",
          "anchor": "Consider_it",
          "level": 2
        },
        {
          "title": "Ideas for new tools",
          "anchor": "Ideas_for_new_tools",
          "level": 1
        },
        {
          "title": "Brass Tacks",
          "anchor": "Brass_Tacks",
          "level": 2
        },
        {
          "title": "Other links",
          "anchor": "Other_links",
          "level": 1
        },
        {
          "title": "Features that a debate tool should have",
          "anchor": "Features_that_a_debate_tool_should_have",
          "level": 1
        },
        {
          "title": "LWers interested in developing debate tools",
          "anchor": "LWers_interested_in_developing_debate_tools",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 22
    },
    "postCount": 6,
    "description": {
      "markdown": "An **online debate tool** facilitates the act of debating by helping to manage the structure of argumentation. This distinguishes it from general purpose communication tools such as wikis and forums. Some online debate tools provide graphical representations of arguments, but this is not a requirement.\n\nThis wiki page and tag gives a list and characterization of debate tools. Debate tools were previously [discussed here](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/).\n\nLiterature\n==========\n\nThere exists an academic literature on argument mapping and other tools (computer aided or not) for assisting debate. The most recent survey seems to be \"[Computer-supported argumentation: A review of the state of the art](http://www.springerlink.com/content/j3p581601n3x1200/)\" written in June 2009, which lists 50 tools (starting on page 94).\n\n[Online Deliberation](https://en.wikipedia.org/wiki/Online_deliberation) is a related discipline that asks what are the effects of online discussions, when are they effective, and how to design better systems. \n\nList of debate tools\n====================\n\n[Debate Map](https://debatemap.app/)\n------------------------------------\n\nSummary: Tree-based mapping of beliefs, arguments, and evidence.\n\n*   first mentioned:\n    *   [a comment by Venryx](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/dw0i)\n*   pros:\n    *   Collaborative creation, editing, and evaluation of debate/argument maps.\n    *   Open source. (under the MIT license)\n    *   Developed using modern web technologies. (react-js, mobx, firestore)\n    *   Ability to enter both formal arguments (premises and conclusion), or less structured \"single-premise arguments\".\n    *   Rating system for the truth/probability of claims, as well as the relevance/validity of arguments.\n    *   Tree-based structure which can extend very deep without loss of clarity or usability.\n    *   Integrated term/definition system. Terms can be defined once, then used anywhere, with hover-based definition display.\n*   cons:\n    *   Has a learning curve for casual users, as content must conform to the argument<-premise structure at each level.\n    *   Not yet made usable on mobile devices.\n\n[DebateArt](https://www.debateart.com/)\n---------------------------------------\n\nSummary: Debating platform with rich one-on-one debates functionality and advanced discussions forum.\n\n*   pros:\n    *   Clean and convenient design.\n    *   Advanced and flexible one-on-one debating system.\n    *   Separate forum for casual discussions.\n    *   Private messaging system.\n    *   Great performance.\n    *   Friendly community.\n    *   Active and thorough moderation.\n*   cons:\n    *   Has a learning curve for casual users.\n    *   No dedicated mobile version.\n\n[debategraph.org](http://debategraph.org/)\n------------------------------------------\n\nSummary: This... is pretty much exactly what we were looking for, isn't it? Though it doesn't do anything with probabilities.\n\n*   first mentioned:\n    *   [a comment by Peer Infinity](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/20v5)\n*   pros:\n    *   collaboratively edit argument maps\n*   cons:\n    *   it doesn't do anything with probabilities.\n    *   *Do not zoom out too much!*\n\n[Argunet](http://www.argunet.org/working-with-argunet/)\n-------------------------------------------------------\n\nSummary: Argunet enables you to create argument maps of complex debates online or offline, on your own or in a team.\n\n*   first mentioned:\n    *   [the original article](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/)\n*   pros:\n    *   collaboratively edit argument maps\n*   cons:\n    *   not entirely straightforward to use, Morendil had trouble figuring out how to move boxes around.\n\n[bCisive Online](http://bcisiveonline.com/)\n-------------------------------------------\n\nSummary: a simple canvas for creating a tree diagram of a debate.\n\n*   first mentioned:\n    *   [the original article](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/)\n*   pros:\n    *   easy to use\n*   cons:\n    *   all it does is let you make the tree diagram, it doesn't do anything else with the data\n*   examples:\n    *   [a map of Morendil's current thinking on cryonics](http://morendil.bcisiveonline.com/spaces/989ae551bc100d0365c96a7bcc20f188d95fb58d/)\n\n[Flow](http://en.wikipedia.org/wiki/Flow_%28policy_debate%29)\n-------------------------------------------------------------\n\nSummary: a specialized form of note taking called \"flowing\" within the policy/CEDA/NDT debate community.\n\n*   first mentioned:\n    *   [a comment by JenniferRM](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kx1)\n*   pros:\n    *   lots of people have used this technique, and it has been proven to work well\n*   cons:\n    *   it requires a very specific format for the debate\n\n[PyMC](http://code.google.com/p/pymc/)\n--------------------------------------\n\nSummary: a DSL in python for (non-recursive) Bayesian models and Bayesian probability computations.\n\n*   first mentioned:\n    *   [a comment by Steve_Rayhawk](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kx3)\n*   pros:\n    *   it does Bayesian calculations\n*   cons:\n    *   requires literacy in python and bayesian statistics\n\n[MACOSPOL](http://www.demoscience.org/)\n\n*   first mentioned:\n    *   [a comment by Morendil](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1kt6)\n*   examples:\n    *   [mapped controversies](http://medialab.sciences-po.fr/controversies/)\n\n[Scott Aaronson's worldview manager](http://projects.csail.mit.edu/worldview/)\n------------------------------------------------------------------------------\n\nSummary: this is designed to point out hidden contradictions (or at least tensions) between one's beliefs, by using programmed in implications to exhibit (possibly long) inferential chains that demonstrate a contradiction.\n\n*   first mentioned:\n    *   [a comment by wnoise](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1ku8)\n*   pros:\n    *   it does lots of stuff\n*   cons:\n    *   it's kinda complicated\n*   examples:\n    *   [sample worldview](http://www.gitorious.org/worldview/worldview/blobs/master/topics/axiom_of_choice.wvm)\n    *   [The model of libertarian ideas](http://www.gitorious.org/worldview/worldview/blobs/master/topics/libertarianism.wvm)\n\n[Canonizer.com](http://canonizer.com/)\n--------------------------------------\n\nSummary: Canonizer.com is a wiki system with added camp and survey capabilities. The system provides a rigorous way to measure scientific / moral expert consensus. It is designed for collaborative development of concise descriptions of various competing scientific or moral theories, and the best arguments for such. People can join the camps representing such, giving a quantitative survey or measure of consensus compared to all others. Proposed changes to supported camps go into a review mode for one week. Any supporters of a camp can object to any such proposed change during this time. If it survives a week with no objection, it goes live, guaranteeing unanimous agreement to such changes to the petition by all current signers. If anyone does object, the camp can be forked (taking all supporters of the 'improvement'), or the info can be included in a sporting sub camp.\n\nThe karma or 'canonization' system enables the readers to select any algorithm they wish on the side bar to 'find the good stuff'. For example, you can compare the [mind expert](http://canonizer.com/topic.asp/53/11) scientific consensus with the default general population consensus. Each camp has a forum to discuss and debate further improvements for camps. The general idea is to debate things in the forums, or elsewhere, and summarize everyone's final / current / state of the art view in the camp statements. A history of everything is maintained, providing a dynamic quantitative measure of how well accepted any theory is, as ever more theory falsifying (when experts abandon a falsified camp) scientific data / new arguments... come in.\n\n*   first mentioned:\n    *   [a comment by PeerInfinity](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/2j4l?c=1)\n*   pros:\n    *   the whole canonization thing\n*   cons:\n    *   it's kinda complicated\n*   examples:\n    *   [the main list of canonized camps](http://canonizer.com)\n\n[Explore-Ideas.com](http://www.explore-ideas.com/)\n--------------------------------------------------\n\nSummary: explore-ideas.com is a graph structure forum where users start with a topic and follow arguments they agree with, creating a personalized 'story'.\n\n*   pros: users can link any two comments and merge ideas from different discussions into a single logical argument, as well as make loops in reasoning. Each user comes to his personal 'win' ending based on arguments (s)he agrees with. There is no global 'win' or 'lose' argument.\n\nThis forum encourages dynamic debate that goes beyond pro/con binary approach, similar to that employed in [Proofs and Refutations](http://en.wikipedia.org/wiki/Proofs_and_Refutations).\n\n*   cons: UI is rudimentary.\n\n[Debate.fm](http://www.debate.fm/)\n----------------------------------\n\nSummary: A simple platform to start a debate on any topic. Mainly focused at general user participation.\n\n*   pros: Simple structured content - side by side. Mainly focused at pro/con binary approach\n*   cons: Till now not ready for academic use.\n\n[cartargrapher](http://cartargrapher.appspot.com/)\n--------------------------------------------------\n\n\"a simple argument mapping app, made using Google’s visualization API, jquery, and python, and running on Google’s AppEngine. Note: at this point, I don’t guarantee the persistence of saved argument maps!\" - [John MacFarlane](http://johnmacfarlane.net/tools.html)\n\n[Consider.it](https://consider.it)\n----------------------------------\n\nGraphically represents people's agreement with a statement and which arguments were most used. The arguments themselves are not subjects of further investigation though. Check out [this use-case](https://hala.consider.it/?tab=Feedback%20on%20key%20principles) of public decisions in Seattle.\n\nIdeas for new tools\n===================\n\n*   Based on MediaWiki, PHP, GraphViz, and maybe XML\n*   Summary: a tool that we make ourselves, so that it works the way we want it to work\n*   first mentioned:\n    *   [a comment by PeerInfinity](http://lesswrong.com/lw/1qq/debate_tools_an_experience_report/1l1w)\n*   pros:\n    *   we're writing it, so we can make it work how we want\n*   cons:\n    *   we would need to write it from scratch\n*   examples:\n    *   [a conversation about AI](http://transhumanistwiki.com/wiki/Peer_Infinity/Chat_With_Fael_About_AI)\n\nBrass Tacks\n-----------\n\n*   [Structured debate](http://issuepedia.org/Issuepedia:Structured_Debate): a set of rules which debate software could help enforce. Some data design is on paper, not yet transcribed. The plan is to write this first as a MediaWiki extension. [some mockups](http://issuepedia.org/Category:Debates)\n\nOther links\n===========\n\n*   [http://www.visualizingargumentation.info/](http://www.visualizingargumentation.info/)\n*   [http://www.tandf.co.uk/journals/tarc](http://www.tandf.co.uk/journals/tarc)\n*   [structured debate](http://issuepedia.org/Structured_debate)\n*   [dispute resolution technology](http://issuepedia.org/Issuepedia:Dispute_Resolution_Technology)\n\nFeatures that a debate tool should have\n=======================================\n\n*   from almost everyone:\n    *   an easy to use interface\n*   from [Morendil](http://lesswrong.com/user/Morendil/):\n    *   a conclusion or a decision, which is to be \"tested\" by the use of the tool\n    *   various hypotheses, which are offered in support or in opposition to the conclusion, with degrees of plausibility\n    *   logical structure, such as \"X follows from Y\"\n    *   challenges to logical structure, such as \"X may not necessarily follow from Y, if you grant Z\"\n    *   elements of evidence, which make hypotheses more or less probable\n    *   recursive relations between these elements\n*   from [PhilGoetz](http://lesswrong.com/user/PhilGoetz/):\n    *   an XML-based representation of the data\n*   from [PeerInfinity](http://lesswrong.com/user/PeerInfinity/)\n    *   generates its results from an annotated log of a debate\n    *   collaboratively editable, possibly using MediaWiki\n    *   multiple outfut formats: graphs, tables, the raw data\n*   from [Johnicholas](http://lesswrong.com/user/Johnicholas/):\n    *   Compose in ordinary ASCII or UTF-8\n    *   Compose primarily a running-text argument, indicating the formal structure with annotations\n    *   Export as a prettified document, still mostly running text (html and LaTeX)\n    *   Export as a diagram (automatically layed out, perhaps by graphviz)\n    *   Export as a bayes net (in possibly several bayes net formats)\n    *   Export as a machine-checkable proof (in possibly several formats)\n*   from [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky):\n    *   prevents online arguments from retracing the same points over and over.\n    *   not just graphical with boxes, because that makes poor use of screen real estate.\n    *   not have lots of fancy argument types and patterns, because no one really uses that stuff\n    *   a karma system, because otherwise there's no way to find the good stuff.\n\n(So, now that everything's all neatly arranged in a list, the next step is to decide whether we want to start using any of these tools, or if we want to create our own.)\n\nLWers interested in developing debate tools\n===========================================\n\n*   [Curtis SerVaas](http://lesswrong.com/lw/le5/welcome_to_less_wrong_7th_thread_december_2014/c64f)\n*   [.impact has discussed making argument mapping software](http://lesswrong.com/lw/jfn/introducing_impact/)\n*   The \"Collaborative Argumentation Analysis\" Facebook Group has a lot of links/discussion/people.\n\nSee also\n========\n\n*   [Rationality power tools](https://wiki.lesswrong.com/wiki/Rationality_power_tools)\n*   [Prediction market](https://www.lesswrong.com/tag/prediction-markets)\n*   [PredictionBook](https://www.lesswrong.com/tag/predictionbook)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb210",
    "name": "Reversed Stupidity Is Not Intelligence",
    "core": null,
    "slug": "reversed-stupidity-is-not-intelligence",
    "tableOfContents": {
      "html": "<p><span class=\"by_mPipmBTniuABY5PQy\">It takes a lot of evidence and rationality just to </span><a href=\"https://www.lesswrong.com/tag/locate-the-hypothesis\"><i><span class=\"by_mPipmBTniuABY5PQy\">locate</span></i><span class=\"by_mPipmBTniuABY5PQy\"> a good hypothesis</span></a><span class=\"by_mPipmBTniuABY5PQy\"> in the search space. Getting it right is a precise Art, whereas there are a million ways to get it wrong. A stopped clock is right twice a day; answering true-or-false questions at entirely at random will still yield a fifty percent success rate. To </span><a href=\"https://www.lesswrong.com/tag/tsuyoku-naritai\"><span class=\"by_mPipmBTniuABY5PQy\">do better</span></a><span class=\"by_mPipmBTniuABY5PQy\">, you can't just perform a reversal of an obvious fallacy. </span><strong><span class=\"by_mPipmBTniuABY5PQy\">Reversed stupidity is not intelligence</span></strong><span class=\"by_mPipmBTniuABY5PQy\">; faulty reasoning does not produce \"antitrue\" conclusions that can just be negated; the true theory should make sense on its own terms; not just as a refutation of some nonsense.</span></p><h2 id=\"Blog_posts\"><span class=\"by_mPipmBTniuABY5PQy\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/\"><span class=\"by_mPipmBTniuABY5PQy\">Reversed Stupidity is Not Intelligence</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 3,
    "description": {
      "markdown": "It takes a lot of evidence and rationality just to [*locate* a good hypothesis](https://www.lesswrong.com/tag/locate-the-hypothesis) in the search space. Getting it right is a precise Art, whereas there are a million ways to get it wrong. A stopped clock is right twice a day; answering true-or-false questions at entirely at random will still yield a fifty percent success rate. To [do better](https://www.lesswrong.com/tag/tsuyoku-naritai), you can't just perform a reversal of an obvious fallacy. **Reversed stupidity is not intelligence**; faulty reasoning does not produce \"antitrue\" conclusions that can just be negated; the true theory should make sense on its own terms; not just as a refutation of some nonsense.\n\nBlog posts\n----------\n\n*   [Reversed Stupidity is Not Intelligence](http://lesswrong.com/lw/lw/reversed_stupidity_is_not_intelligence/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb20f",
    "name": "Something To Protect",
    "core": null,
    "slug": "something-to-protect",
    "tableOfContents": {
      "html": "<h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"/tag/heroic-responsibility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Heroic Responsibility</span></a></li></ul><h2 id=\"Blog_posts\"><span class=\"by_mPipmBTniuABY5PQy\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/nb/something_to_protect/\"><span class=\"by_mPipmBTniuABY5PQy\">Something to Protect</span></a></li><li><a href=\"http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/\"><span class=\"by_mPipmBTniuABY5PQy\">Newcomb's Problem and Regret of Rationality</span></a></li><li><a href=\"http://lesswrong.com/lw/7i/rationality_is_systematized_winning/\"><span class=\"by_mPipmBTniuABY5PQy\">Rationality is Systematized Winning</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 9,
    "description": {
      "markdown": "See Also\n--------\n\n*   [Heroic Responsibility](/tag/heroic-responsibility)\n\nBlog posts\n----------\n\n*   [Something to Protect](http://lesswrong.com/lw/nb/something_to_protect/)\n*   [Newcomb's Problem and Regret of Rationality](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/)\n*   [Rationality is Systematized Winning](http://lesswrong.com/lw/7i/rationality_is_systematized_winning/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1f2",
    "name": "Valley of Bad Rationality",
    "core": null,
    "slug": "valley-of-bad-rationality",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Then I finally reply that my experience so far - even in this realm of merely human possibility - does seem to indicate that, once you sort yourself out a bit and you aren't doing quite so many other things wrong, striving for more rationality actually will make you better off. The long road leads out of the valley and higher than before, even in the human lands.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—</span><a href=\"http://lesswrong.com/lw/7k/incremental_progress_and_the_valley/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Incremental Progress and the Valley</span></u></a></p></blockquote><p><span><span class=\"by_9c2mQkLQq6gQSksMs\">It has been observed that when someone is just starting to learn rationality, they </span><span class=\"by_nmk3nLpQE89dMRzzN\">sometimes </span><span class=\"by_9c2mQkLQq6gQSksMs\">appear to be worse off than they were before. </span><span class=\"by_nmk3nLpQE89dMRzzN\">Someone else may then allege</span><span class=\"by_9c2mQkLQq6gQSksMs\"> that after </span><span class=\"by_nmk3nLpQE89dMRzzN\">this person learns even</span><span class=\"by_9c2mQkLQq6gQSksMs\"> more about rationality, </span><span class=\"by_nmk3nLpQE89dMRzzN\">they</span><span class=\"by_9c2mQkLQq6gQSksMs\"> will</span><span class=\"by_nmk3nLpQE89dMRzzN\"> finally</span><span class=\"by_9c2mQkLQq6gQSksMs\"> be better off than </span><span class=\"by_nmk3nLpQE89dMRzzN\">they</span><span class=\"by_9c2mQkLQq6gQSksMs\"> were before </span><span class=\"by_nmk3nLpQE89dMRzzN\">they</span><span class=\"by_9c2mQkLQq6gQSksMs\"> started. The period before this improvement is known as \"the valley of bad rationality\".</span></span></p><h2 id=\"Non_post_example_\"><span class=\"by_HoGziwmhpMGqGeWZy\">Non-post example:</span></h2><ul><li><a href=\"http://lesswrong.com/lw/5f/bayesians_vs_barbarians/7hr\"><span class=\"by_h48TMtPzfimsEobTm\">This comment</span></a><span><span class=\"by_h48TMtPzfimsEobTm\"> by </span><span class=\"by_HoGziwmhpMGqGeWZy\">Scott Alexander</span><span class=\"by_h48TMtPzfimsEobTm\"> on </span></span><a href=\"http://lesswrong.com/lw/5f/bayesians_vs_barbarians/\"><span class=\"by_h48TMtPzfimsEobTm\">Bayesians vs. Barbarians</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/value-of-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Value of Rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/costs-of-rationality\"><span class=\"by_9c2mQkLQq6gQSksMs\">Costs of rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/dangerous-knowledge\"><span class=\"by_9c2mQkLQq6gQSksMs\">Dangerous knowledge</span></a></li><li><a href=\"https://www.lesswrong.com/tag/debiasing\"><span class=\"by_qf77EiaoMw7tH3GSr\">Debiasing</span></a></li><li><a href=\"https://www.lesswrong.com/tag/information-hazards\"><span class=\"by_nLbwLhBaQeG6tCNDN\">Information Hazards</span></a></li></ul>",
      "sections": [
        {
          "title": "Non-post example:",
          "anchor": "Non_post_example_",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 15,
    "description": {
      "markdown": "> Then I finally reply that my experience so far - even in this realm of merely human possibility - does seem to indicate that, once you sort yourself out a bit and you aren't doing quite so many other things wrong, striving for more rationality actually will make you better off. The long road leads out of the valley and higher than before, even in the human lands.\n> \n> —[Incremental Progress and the Valley](http://lesswrong.com/lw/7k/incremental_progress_and_the_valley/)\n\nIt has been observed that when someone is just starting to learn rationality, they sometimes appear to be worse off than they were before. Someone else may then allege that after this person learns even more about rationality, they will finally be better off than they were before they started. The period before this improvement is known as \"the valley of bad rationality\".\n\nNon-post example:\n-----------------\n\n*   [This comment](http://lesswrong.com/lw/5f/bayesians_vs_barbarians/7hr) by Scott Alexander on [Bayesians vs. Barbarians](http://lesswrong.com/lw/5f/bayesians_vs_barbarians/)\n\nSee also\n--------\n\n*   [Value of Rationality](https://www.lesswrong.com/tag/value-of-rationality)\n*   [Costs of rationality](https://www.lesswrong.com/tag/costs-of-rationality)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)\n*   [Debiasing](https://www.lesswrong.com/tag/debiasing)\n*   [Information Hazards](https://www.lesswrong.com/tag/information-hazards)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1ec",
    "name": "Illusion of Transparency",
    "core": null,
    "slug": "illusion-of-transparency",
    "tableOfContents": {
      "html": "<p><span class=\"by_mPipmBTniuABY5PQy\">The </span><strong><span class=\"by_mPipmBTniuABY5PQy\">illusion of transparency</span></strong><span class=\"by_mPipmBTniuABY5PQy\"> is the misleading impression that your words convey more to others than they really do. Words are a means of communication, but they don't in themselves </span><em><span class=\"by_mPipmBTniuABY5PQy\">contain</span></em><span class=\"by_mPipmBTniuABY5PQy\"> meaning. The word </span><em><span class=\"by_mPipmBTniuABY5PQy\">apple</span></em><span class=\"by_mPipmBTniuABY5PQy\"> is just five letters, two syllables. I use it to </span><em><span class=\"by_mPipmBTniuABY5PQy\">refer</span></em><span class=\"by_mPipmBTniuABY5PQy\"> to a concept and its associations in my mind, under the reasonable assumption that it refers to a similar concept and group of associations in </span><em><span class=\"by_mPipmBTniuABY5PQy\">your</span></em><span class=\"by_mPipmBTniuABY5PQy\"> mind; this is the only power words have, great though it may be. Unfortunately, it's easy to lose track of this fact, think as if your words have meanings inherently encoded in them, leading to a tendency to systematically overestimate the effectiveness of communication.</span></p><span class=\"by_mPipmBTniuABY5PQy\">\n</span><h2 id=\"Related_Pages\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Related </span><span class=\"by_qf77EiaoMw7tH3GSr\">Pages</span></span></h2><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><ul><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><li><a href=\"https://www.lesswrong.com/tag/inferential-distance\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inferential distance</span></a></li><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><li><a href=\"https://www.lesswrong.com/tag/detached-lever-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Detached lever fallacy</span></a></li><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><li><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Absurdity heuristic</span></a></li><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><li><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind projection fallacy</span></a></li><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span></ul><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><h2 id=\"External_Links\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">External </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Links</span></span></h2><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><ul><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><li><a href=\"http://www.cs.tut.fi/~jkorpela/wiio.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">How all human communication fails, except by accident, or a commentary of Wiio's laws</span></a></li><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span></ul><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span>",
      "sections": [
        {
          "title": "Related Pages",
          "anchor": "Related_Pages",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 8,
    "description": {
      "markdown": "The **illusion of transparency** is the misleading impression that your words convey more to others than they really do. Words are a means of communication, but they don't in themselves *contain* meaning. The word *apple* is just five letters, two syllables. I use it to *refer* to a concept and its associations in my mind, under the reasonable assumption that it refers to a similar concept and group of associations in *your* mind; this is the only power words have, great though it may be. Unfortunately, it's easy to lose track of this fact, think as if your words have meanings inherently encoded in them, leading to a tendency to systematically overestimate the effectiveness of communication.\n\nRelated Pages\n--------------\n\n*   [Inferential distance](https://www.lesswrong.com/tag/inferential-distance)\n*   [Detached lever fallacy](https://www.lesswrong.com/tag/detached-lever-fallacy)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n\nExternal Links\n--------------\n\n*   [How all human communication fails, except by accident, or a commentary of Wiio's laws](http://www.cs.tut.fi/~jkorpela/wiio.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1dc",
    "name": "Updateless Decision Theory",
    "core": null,
    "slug": "updateless-decision-theory",
    "tableOfContents": {
      "html": "<h2 id=\"Motivation\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Motivation</span></h2><p><strong><span class=\"by_5yNJS8bxEYhgFD9XJ\">Updateless Decision Theory</span></strong><span class=\"by_5yNJS8bxEYhgFD9XJ\"> (UDT) is a decision theory meant to deal with a fundamental problem in the existing decision theories: the need to treat the agent as a part of the world in which it makes its decisions. In contrast, in the most common decision theory today, </span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Causal Decision Theory</span></a><span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> (CDT), the deciding agent is not part of the world model--its decision is the output of the CDT, but the agent's decision in the world context is \"magic\": in the </span><span class=\"by_9zJ7ffPXRTMyAqfPh\">moment of deciding, no causal links feed into its chosen action. It acts</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> as </span><span class=\"by_9zJ7ffPXRTMyAqfPh\">though its</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> decision </span><span class=\"by_9zJ7ffPXRTMyAqfPh\">was causeless,</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> as in some dualist free-will theories.</span></span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Getting this issue right is critical in building a self-improving artificial general intelligence, as such an AI must analyze its own behavior and that of a next generation that it may build.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">Updateless Decision Theory was invented by </span><a href=\"https://www.lesswrong.com/users/wei_dai\"><span class=\"by_XLwKyCK7JmC292ZCC\">Wei Dai</span></a><span><span class=\"by_XLwKyCK7JmC292ZCC\"> and first </span><span class=\"by_qgdGA4ZEyW7zNdK84\">described in</span><span class=\"by_XLwKyCK7JmC292ZCC\"> </span></span><a href=\"https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory\"><span class=\"by_XLwKyCK7JmC292ZCC\">Towards a New Decision Theory</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">.</span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/timeless-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Timeless decision theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/ambient-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ambient decision theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/counterfactual-mugging\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Counterfactual mugging</span></a></li><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory</span></a></li><li><a href=\"https://www.lesswrong.com/posts/AGAGgoWymRhJ5Rqyv/functional-decision-theory-a-new-theory-of-instrumental\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Functional Decision Theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/embedded-agency\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Embedded Agency</span></a></li></ul><h2 id=\"Content\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Content</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">UDT specifies that the optimal agent is the one with the best algorithm--the best mapping from observations to actions--across a probability distribution of all world-histories. (\"Best\" here, as in other decision theories, means one that maximizes a utility/reward function.)</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">This definition may seem trivial, but in contrast, CDT says that an agent should choose the best *option* at any given moment, based on the effects of that action. As in </span><a href=\"http://lesswrong.com/lw/emc/causality_a_chapter_by_chapter_review/\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Judea Pearl's definition of causality</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">, CDT ignores any causal links inbound to the decider, treating this agent as an uncaused cause. The agent is unconcerned about what evidence its decision may provide about the agent's own mental makeup--evidence which may suggest that the agent will make suboptimal decisions in other cases.</span></p><p><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Evidential Decision Theory is the other leading decision theory today. It says that the agent should make the choice for which the expected utility, as calculated with Bayes' Rule, is the highest. EDT avoids CDT's pitfalls, but has its own flaw: It ignores</span><span class=\"by_KTefAYDBNT64CLRau\"> the</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> distinction between causation and correlation. In CDT, the agent is an uncaused cause, and in EDT, the converse: It is caused, but not a cause.</span></span></p><p><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">One valuable insight from EDT is reflected in \"UDT 1.1\" (see article by McAllister in references), a variant of UDT in which the agent takes into account that some of its algorithm (mapping from observations to actions) may be prespecified and not entirely in its control, so that it has to gather evidence and draw conclusions about part of its own mental makeup.</span><span class=\"by_my98h9K2ygm9SeEEK\"> The difference between UDT 1.0 and 1.1 is that UDT 1.1 iterates over policies, whereas UDT 1.0 iterates over actions.&nbsp;</span></span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">Both UDT and </span><a href=\"https://www.lesswrong.com/tag/timeless-decision-theory\"><span class=\"by_XLwKyCK7JmC292ZCC\">Timeless Decision Theory</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> (TDT) make decisions on the basis of what you would have pre-committed to. The difference is that UDT asks what you would have pre-committed to without the benefit of any observations you have made about the universe, while TDT asks what you would have pre-committed to give all information you've observed so far. This means that UDT pays in </span><a href=\"https://www.lesswrong.com/tag/counterfactual-mugging\"><span class=\"by_XLwKyCK7JmC292ZCC\">Counterfactual Mugging</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, while TDT does not.</span></p><p><span><span class=\"by_XLwKyCK7JmC292ZCC\">UDT is very similar to Functional Decision Theory (FDT), but there are </span><span class=\"by_my98h9K2ygm9SeEEK\">differences.</span><span class=\"by_XLwKyCK7JmC292ZCC\"> FDT doesn't include the UDT1.1 fix and Nate </span><span class=\"by_my98h9K2ygm9SeEEK\">Soares</span><span class=\"by_XLwKyCK7JmC292ZCC\"> </span></span><a href=\"https://www.lesswrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory?commentId=LzPH8utKGSf97NihW\"><span class=\"by_XLwKyCK7JmC292ZCC\">states</span></a><span><span class=\"by_XLwKyCK7JmC292ZCC\">: \"Wei Dai doesn't endorse FDT's focus on causal-graph-style counterpossible reasoning; IIRC he's holding out for an approach to counterpossible reasoning that falls out of evidential-style conditioning on a logically uncertain distribution\". Rob </span><span class=\"by_my98h9K2ygm9SeEEK\">Bensinger</span><span class=\"by_XLwKyCK7JmC292ZCC\"> says that he's heard UDT described as \"FDT + a theory of anthropics\".</span></span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">Since it is formalised using input-output maps instead of in terms of situations, it allows us to make predictions about what an agent would do given </span><a href=\"https://www.lesswrong.com/posts/EXtzy3v4soZcoZjuH/a-short-note-on-udt\"><span class=\"by_XLwKyCK7JmC292ZCC\">input representing an inconsistent situation</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, which can be important when dealing with perfect predictors.</span></p><h2 id=\"Logical_Uncertainty\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Logical Uncertainty</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">A robust theory of </span><a href=\"https://www.lesswrong.com/tag/logical-uncertainty\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">logical uncertainty</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> is essential to a full formalization of UDT. &nbsp;A UDT agent must calculate probabilities and expected values on the outcome of its possible actions in all possible worlds--sequences of observations and its own actions. However, it does not know its own actions in all possible worlds. (The whole point is to derive its actions.) On the other hand, it does have some knowledge about its actions, just as you know that you are unlikely to walk straight into a wall the next chance you get. So, the UDT agent models itself as an algorithm, and its probability distribution about what it itself will do is an important input into its maximization calculation.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Logical uncertainty is an area which has not yet been properly formalized, and much UDT research is focused on this area.</span></p><h2 id=\"Blog_posts\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">Blog</span><span class=\"by_qf77EiaoMw7tH3GSr\"> posts</span></span></h2><ul><li><a href=\"http://lesswrong.com/lw/102/indexical_uncertainty_and_the_axiom_of/\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Indexical</span><span class=\"by_4SHky5j2PNcRwBiZt\"> uncertainty and the Axiom of Independence</span></span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/15m/towards_a_new_decision_theory/\"><span><span class=\"by_nmk3nLpQE89dMRzzN\">Towards a </span><span class=\"by_9c2mQkLQq6gQSksMs\">New Decision Theory</span></span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> by </span><a href=\"http://weidai.com/\"><span><span class=\"by_nmk3nLpQE89dMRzzN\">Wei </span><span class=\"by_qf77EiaoMw7tH3GSr\">Dai</span></span></a></li><li><a href=\"http://lesswrong.com/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/\"><span class=\"by_4SHky5j2PNcRwBiZt\">Anthropic Reasoning in UDT</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/182/the_absentminded_driver/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Absent-Minded Driver</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/1fu/why_and_why_not_bayesian_updating/\"><span class=\"by_4SHky5j2PNcRwBiZt\">Why (and why not) Bayesian Updating?</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/1iy/what_are_probabilities_anyway/\"><span class=\"by_4SHky5j2PNcRwBiZt\">What Are Probabilities, Anyway?</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/cs9/list_of_problems_that_motivated_udt/\"><span class=\"by_4SHky5j2PNcRwBiZt\">List of Problems That Motivated UDT</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by Wei Dai</span></li><li><a href=\"http://lesswrong.com/lw/334/another_attempt_to_explain_udt/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Another attempt to explain UDT</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by cousin_it</span></li><li><a href=\"http://lesswrong.com/lw/294/what_is_wei_dais_updateless_decision_theory/\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">What is Wei Dai's Updateless Decision Theory?</span></a><span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> by </span><span class=\"by_HSANMQBsHiGrZzwTB\">AlephNeil</span></span></li><li><a href=\"https://www.lesswrong.com/posts/QPhY8Nb7gtT5wvoPH/comparison-of-decision-theories-with-a-focus-on-logical\"><span class=\"by_my98h9K2ygm9SeEEK\">Comparison of decision theories (with a focus on logical-counterfactual decision theories)</span></a><span class=\"by_my98h9K2ygm9SeEEK\">, by riceissa. (Gives a good description of UDT 1.0 vs 1.1)</span></li><li><a href=\"http://lesswrong.com/tag/udt/\"><span class=\"by_4SHky5j2PNcRwBiZt\">All posts tagged \"UDT\"</span></a></li></ul><h2 id=\"Relevant_Comments\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Relevant Comments</span></h2><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In addition to whole posts on UDT, there are also a number of comments which contain important information, often on less relevant posts.</span></p><ul><li><a href=\"http://lesswrong.com/lw/jhj/functional_side_effects/adhy\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">A comment about UDT2</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qf77EiaoMw7tH3GSr\">External links</span></h2><ul><li><a href=\"https://drive.google.com/file/d/0BzUiCL-Kpxc1NGxab3ZfZGZkVUE/view?usp=sharing&amp;resourcekey=0-EuvTP8RRdpKivUtHwfqgSQ\"><span class=\"by_qf77EiaoMw7tH3GSr\">Formal description of UDT</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by Tyrrell McAllister</span></li><li><a href=\"http://intelligence.org/2014/10/30/new-report-udt-known-search-order/\"><span class=\"by_dFuMBaDDrKcKtdCEK\">UDT with known search order</span></a><span class=\"by_dFuMBaDDrKcKtdCEK\"> by Tsvi Benson-Tilsen</span></li><li><a href=\"http://intelligence.org/files/ProblemClassDominance.pdf\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Problem Class Dominance in Predictive Dilemmas</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">, section 3.4. (The best summary to date.)</span></li><li><a href=\"https://formalisedthinking.wordpress.com/2010/08/18/an-introduction-to-decision-theory/\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">An introduction to decision theory</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> (series of posts)</span></li><li><a href=\"https://arbital.com/p/updateless_dt/\"><span class=\"by_fbEg8jfgqQYPeSX43\">Arbital page on updateless decision theories</span></a><span class=\"by_fbEg8jfgqQYPeSX43\"> by Eliezer Yudkowsky</span></li></ul>",
      "sections": [
        {
          "title": "Motivation",
          "anchor": "Motivation",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "Content",
          "anchor": "Content",
          "level": 1
        },
        {
          "title": "Logical Uncertainty",
          "anchor": "Logical_Uncertainty",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "Relevant Comments",
          "anchor": "Relevant_Comments",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 8
    },
    "postCount": 17,
    "description": {
      "markdown": "Motivation\n----------\n\n**Updateless Decision Theory** (UDT) is a decision theory meant to deal with a fundamental problem in the existing decision theories: the need to treat the agent as a part of the world in which it makes its decisions. In contrast, in the most common decision theory today, [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) (CDT), the deciding agent is not part of the world model--its decision is the output of the CDT, but the agent's decision in the world context is \"magic\": in the moment of deciding, no causal links feed into its chosen action. It acts as though its decision was causeless, as in some dualist free-will theories.\n\nGetting this issue right is critical in building a self-improving artificial general intelligence, as such an AI must analyze its own behavior and that of a next generation that it may build.\n\nUpdateless Decision Theory was invented by [Wei Dai](https://www.lesswrong.com/users/wei_dai) and first described in [Towards a New Decision Theory](https://www.lesswrong.com/posts/de3xjFaACCAk6imzv/towards-a-new-decision-theory).\n\nSee also\n--------\n\n*   [Timeless decision theory](https://www.lesswrong.com/tag/timeless-decision-theory)\n*   [Ambient decision theory](https://www.lesswrong.com/tag/ambient-decision-theory)\n*   [Counterfactual mugging](https://www.lesswrong.com/tag/counterfactual-mugging)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Functional Decision Theory](https://www.lesswrong.com/posts/AGAGgoWymRhJ5Rqyv/functional-decision-theory-a-new-theory-of-instrumental)\n*   [Embedded Agency](https://www.lesswrong.com/tag/embedded-agency)\n\nContent\n-------\n\nUDT specifies that the optimal agent is the one with the best algorithm--the best mapping from observations to actions--across a probability distribution of all world-histories. (\"Best\" here, as in other decision theories, means one that maximizes a utility/reward function.)\n\nThis definition may seem trivial, but in contrast, CDT says that an agent should choose the best \\*option\\* at any given moment, based on the effects of that action. As in [Judea Pearl's definition of causality](http://lesswrong.com/lw/emc/causality_a_chapter_by_chapter_review/), CDT ignores any causal links inbound to the decider, treating this agent as an uncaused cause. The agent is unconcerned about what evidence its decision may provide about the agent's own mental makeup--evidence which may suggest that the agent will make suboptimal decisions in other cases.\n\nEvidential Decision Theory is the other leading decision theory today. It says that the agent should make the choice for which the expected utility, as calculated with Bayes' Rule, is the highest. EDT avoids CDT's pitfalls, but has its own flaw: It ignores the distinction between causation and correlation. In CDT, the agent is an uncaused cause, and in EDT, the converse: It is caused, but not a cause.\n\nOne valuable insight from EDT is reflected in \"UDT 1.1\" (see article by McAllister in references), a variant of UDT in which the agent takes into account that some of its algorithm (mapping from observations to actions) may be prespecified and not entirely in its control, so that it has to gather evidence and draw conclusions about part of its own mental makeup. The difference between UDT 1.0 and 1.1 is that UDT 1.1 iterates over policies, whereas UDT 1.0 iterates over actions. \n\nBoth UDT and [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory) (TDT) make decisions on the basis of what you would have pre-committed to. The difference is that UDT asks what you would have pre-committed to without the benefit of any observations you have made about the universe, while TDT asks what you would have pre-committed to give all information you've observed so far. This means that UDT pays in [Counterfactual Mugging](https://www.lesswrong.com/tag/counterfactual-mugging), while TDT does not.\n\nUDT is very similar to Functional Decision Theory (FDT), but there are differences. FDT doesn't include the UDT1.1 fix and Nate Soares [states](https://www.lesswrong.com/posts/2THFt7BChfCgwYDeA/let-s-discuss-functional-decision-theory?commentId=LzPH8utKGSf97NihW): \"Wei Dai doesn't endorse FDT's focus on causal-graph-style counterpossible reasoning; IIRC he's holding out for an approach to counterpossible reasoning that falls out of evidential-style conditioning on a logically uncertain distribution\". Rob Bensinger says that he's heard UDT described as \"FDT + a theory of anthropics\".\n\nSince it is formalised using input-output maps instead of in terms of situations, it allows us to make predictions about what an agent would do given [input representing an inconsistent situation](https://www.lesswrong.com/posts/EXtzy3v4soZcoZjuH/a-short-note-on-udt), which can be important when dealing with perfect predictors.\n\nLogical Uncertainty\n-------------------\n\nA robust theory of [logical uncertainty](https://www.lesswrong.com/tag/logical-uncertainty) is essential to a full formalization of UDT.  A UDT agent must calculate probabilities and expected values on the outcome of its possible actions in all possible worlds--sequences of observations and its own actions. However, it does not know its own actions in all possible worlds. (The whole point is to derive its actions.) On the other hand, it does have some knowledge about its actions, just as you know that you are unlikely to walk straight into a wall the next chance you get. So, the UDT agent models itself as an algorithm, and its probability distribution about what it itself will do is an important input into its maximization calculation.\n\nLogical uncertainty is an area which has not yet been properly formalized, and much UDT research is focused on this area.\n\nBlog posts\n----------\n\n*   [Indexical uncertainty and the Axiom of Independence](http://lesswrong.com/lw/102/indexical_uncertainty_and_the_axiom_of/) by Wei Dai\n*   [Towards a New Decision Theory](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) by [Wei Dai](http://weidai.com/)\n*   [Anthropic Reasoning in UDT](http://lesswrong.com/lw/175/torture_vs_dust_vs_the_presumptuous_philosopher/) by Wei Dai\n*   [The Absent-Minded Driver](http://lesswrong.com/lw/182/the_absentminded_driver/) by Wei Dai\n*   [Why (and why not) Bayesian Updating?](http://lesswrong.com/lw/1fu/why_and_why_not_bayesian_updating/) by Wei Dai\n*   [What Are Probabilities, Anyway?](http://lesswrong.com/lw/1iy/what_are_probabilities_anyway/) by Wei Dai\n*   [Explicit Optimization of Global Strategy (Fixing a Bug in UDT1)](http://lesswrong.com/lw/1s5/explicit_optimization_of_global_strategy_fixing_a/) by Wei Dai\n*   [List of Problems That Motivated UDT](http://lesswrong.com/lw/cs9/list_of_problems_that_motivated_udt/) by Wei Dai\n*   [Another attempt to explain UDT](http://lesswrong.com/lw/334/another_attempt_to_explain_udt/) by cousin_it\n*   [What is Wei Dai's Updateless Decision Theory?](http://lesswrong.com/lw/294/what_is_wei_dais_updateless_decision_theory/) by AlephNeil\n*   [Comparison of decision theories (with a focus on logical-counterfactual decision theories)](https://www.lesswrong.com/posts/QPhY8Nb7gtT5wvoPH/comparison-of-decision-theories-with-a-focus-on-logical), by riceissa. (Gives a good description of UDT 1.0 vs 1.1)\n*   [All posts tagged \"UDT\"](http://lesswrong.com/tag/udt/)\n\nRelevant Comments\n-----------------\n\nIn addition to whole posts on UDT, there are also a number of comments which contain important information, often on less relevant posts.\n\n*   [A comment about UDT2](http://lesswrong.com/lw/jhj/functional_side_effects/adhy)\n\nExternal links\n--------------\n\n*   [Formal description of UDT](https://drive.google.com/file/d/0BzUiCL-Kpxc1NGxab3ZfZGZkVUE/view?usp=sharing&resourcekey=0-EuvTP8RRdpKivUtHwfqgSQ) by Tyrrell McAllister\n*   [UDT with known search order](http://intelligence.org/2014/10/30/new-report-udt-known-search-order/) by Tsvi Benson-Tilsen\n*   [Problem Class Dominance in Predictive Dilemmas](http://intelligence.org/files/ProblemClassDominance.pdf), section 3.4. (The best summary to date.)\n*   [An introduction to decision theory](https://formalisedthinking.wordpress.com/2010/08/18/an-introduction-to-decision-theory/) (series of posts)\n*   [Arbital page on updateless decision theories](https://arbital.com/p/updateless_dt/) by Eliezer Yudkowsky"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1db",
    "name": "Timeless Decision Theory",
    "core": null,
    "slug": "timeless-decision-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_JjaJBbvGJE6D6FYvF\">Timeless decision theory</span></strong><span class=\"by_JjaJBbvGJE6D6FYvF\"> (TDT) is a </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span><span class=\"by_JjaJBbvGJE6D6FYvF\">decision </span><span class=\"by_qxJ28GN72aiJu96iF\">theory</span></span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">,</span><span class=\"by_JjaJBbvGJE6D6FYvF\"> </span></span><a href=\"http://intelligence.org/files/TDT.pdf\"><span class=\"by_JjaJBbvGJE6D6FYvF\">developed by Eliezer Yudkowsky</span></a><span class=\"by_JjaJBbvGJE6D6FYvF\"> which, in slogan form, says that agents should decide as if they are determining the output of the abstract computation that they implement. This theory was developed in response to the view that rationality should be about winning (that is, about agents achieving their desired ends) rather than about behaving in a manner that we would intuitively label as rational. Prominent existing decision theories (including </span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span><span class=\"by_JjaJBbvGJE6D6FYvF\">causal decision </span><span class=\"by_qxJ28GN72aiJu96iF\">theory</span></span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">,</span><span class=\"by_JjaJBbvGJE6D6FYvF\"> or CDT) fail to choose the winning decision in some scenarios and so there is a need to develop a more successful theory.</span></span></p><h2 id=\"Timeless_Decision_Theory_has_been_replaced_by_Functional_Decision_Theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Timeless Decision Theory has been replaced by </span><a href=\"https://www.lesswrong.com/tag/functional-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Functional Decision Theory</span></a></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&lt;more needed&gt;</span></p><h2 id=\"TDT_and_Newcomb_s_problem\"><span class=\"by_JjaJBbvGJE6D6FYvF\">TDT and Newcomb's problem</span></h2><p><span><span class=\"by_LoykQRMTxJFxwwdPy\">A </span><span class=\"by_JjaJBbvGJE6D6FYvF\">better sense of the motivations behind, and form of, TDT can be gained by considering a particular decision scenario: </span></span><a href=\"http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/\"><span class=\"by_JjaJBbvGJE6D6FYvF\">Newcomb's problem</span></a><span class=\"by_JjaJBbvGJE6D6FYvF\">. In Newcomb's problem, a superintelligent artificial intelligence, Omega, presents you with a transparent box and an opaque box. The transparent box contains $1000 while the opaque box contains either $1,000,000 or nothing. You are given the choice to either take both boxes (called two-boxing) or just the opaque box (one-boxing). However, things are complicated by the fact that Omega is an almost perfect predictor of human behavior and has filled the opaque box as follows: if Omega predicted that you would one-box, it filled the box with $1,000,000 whereas if Omega predicted that you would two-box it filled it with nothing.</span></p><p><span class=\"by_JjaJBbvGJE6D6FYvF\">Many people find it intuitive that it is rational to two-box in this case. As the opaque box is already filled, you cannot influence its contents with your decision so you may as well take both boxes and gain the extra $1000 from the transparent box. CDT formalizes this style of reasoning. However, one-boxers win in this scenario. After all, if you one-box then Omega (almost certainly) predicted that you would do so and hence filled the opaque box with $1,000,000. So you will almost certainly end up with $1,000,000 if you one-box. On the other hand, if you two-box, Omega (almost certainly) predicted this and so left the opaque box empty . So you will almost certainly end up with $1000 (from the transparent box) if you two-box. Consequently, if rationality is about winning then it's rational to one-box in Newcomb's problem (and hence CDT fails to be an adequate decision theory).</span></p><p><span class=\"by_JjaJBbvGJE6D6FYvF\">TDT will endorse one-boxing in this scenario and hence endorses the winning decision. When Omega predicts your behavior, it carries out the same abstract computation as you do when you decide whether to one-box or two-box. To make this point clear, we can imagine that Omega makes this prediction by creating a simulation of you and observing its behavior in Newcomb's problem. This simulation will clearly decide according to the same abstract computation as you do as both you and it decide in the same manner. Now given that TDT says to act as if deciding the output of this computation, it tells you to act as if your decision to one-box can determine the behavior of the simulation (or, more generally, Omega's prediction) and hence the filling of the boxes. So TDT correctly endorses one-boxing in Newcomb's problem as it tells the agent to act as if doing so will lead them to get $1,000,000 instead of $1,000.</span></p><h2 id=\"TDT_and_other_decision_scenarios\"><span class=\"by_JjaJBbvGJE6D6FYvF\">TDT and other decision scenarios</span></h2><p><span class=\"by_JjaJBbvGJE6D6FYvF\">TDT also wins in a range of other cases including </span><a href=\"https://www.lesswrong.com/tag/smoking-lesion\"><span class=\"by_JjaJBbvGJE6D6FYvF\">medical Newcomb's problems</span></a><span class=\"by_JjaJBbvGJE6D6FYvF\">, </span><a href=\"http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/\"><span class=\"by_JjaJBbvGJE6D6FYvF\">Parfit's hitchhiker</span></a><span class=\"by_JjaJBbvGJE6D6FYvF\">, and </span><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\"><span class=\"by_JjaJBbvGJE6D6FYvF\">the one-shot prisoners' dilemma</span></a><span class=\"by_JjaJBbvGJE6D6FYvF\">. However, there are </span><a href=\"http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/\"><span class=\"by_JjaJBbvGJE6D6FYvF\">other scenarios</span></a><span class=\"by_JjaJBbvGJE6D6FYvF\"> where TDT does not win, including </span><a href=\"http://lesswrong.com/lw/3l/counterfactual_mugging/\"><span class=\"by_JjaJBbvGJE6D6FYvF\">counterfactual mugging</span></a><span><span class=\"by_JjaJBbvGJE6D6FYvF\">. This suggests that TDT still requires further development if it is to become a fully adequate decision theory. Given this, there is some motivation to also consider</span><span class=\"by_LoykQRMTxJFxwwdPy\"> alternative </span><span class=\"by_JjaJBbvGJE6D6FYvF\">decision theories alongside TDT, like</span><span class=\"by_LoykQRMTxJFxwwdPy\"> </span></span><a href=\"http://lesswrong.com/lw/15m/towards_a_new_decision_theory/\"><span><span class=\"by_JjaJBbvGJE6D6FYvF\">updateless</span><span class=\"by_LoykQRMTxJFxwwdPy\"> decision theory</span></span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_JjaJBbvGJE6D6FYvF\">(UDT), which also wins in a range of scenarios but has its own problem cases. It seems likely that both of these theories draw on insights which are crucial to progressing our understanding of decision theory. So while TDT requires further development to be entirely adequate, it nevertheless represents a substantial step toward developing a</span><span class=\"by_LoykQRMTxJFxwwdPy\"> decision theory</span><span class=\"by_JjaJBbvGJE6D6FYvF\"> that always endorses the winning decision</span></span></p><h2 id=\"Formalization_of_TDT\"><span class=\"by_JjaJBbvGJE6D6FYvF\">Formalization of TDT</span></h2><p><span class=\"by_JjaJBbvGJE6D6FYvF\">Coming to fully grasp TDT requires an understanding of how the theory is formalized. Very briefly, TDT is formalized by supplementing causal Bayesian networks, which can be thought of as graphs representing causal relations, in two ways. First, these graphs should be supplemented with nodes representing abstract computations and an agent's uncertainty about the result of these computations. Such a node might represent an agent's uncertainty about the result of a mathematical sum. Second, TDT treats decisions as the abstract computation that underlies the agent's decision process. These two features transform causal Bayesian networks into timeless decision diagrams. Using these supplemented diagrams, TDT is able to determine the winning decision in a whole range of a decision scenarios. For a more detailed description of the formalization of TDT, see Eliezer Yudkowsky's </span><a href=\"http://intelligence.org/files/TDT.pdf\"><span class=\"by_JjaJBbvGJE6D6FYvF\">timeless decision theory paper</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">.</span></p><h2 id=\"Further_Reading\"><span><span class=\"by_9dhw3PngyAWKqTymS\">Further </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Reading</span></span></h2><ul><li><a href=\"https://intelligence.org/files/Comparison.pdf\"><span class=\"by_9dhw3PngyAWKqTymS\">A Comparison of Decision Algorithms on Newcomblike Problems</span></a><span class=\"by_9dhw3PngyAWKqTymS\">, by Alex Altair</span></li><li><a href=\"https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf\"><span class=\"by_2fHm6t2WFDMPShg5b\">Problem Class Dominance in Predictive Dilemmas</span></a><span class=\"by_2fHm6t2WFDMPShg5b\">, by Danny Hintze</span></li></ul><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/15z/ingredients_of_timeless_decision_theory/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Ingredients of Timeless Decision Theory</span></a></li><li><a href=\"http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Timeless Decision Theory: Problems I Can't Solve</span></a></li><li><a href=\"http://lesswrong.com/lw/164/timeless_decision_theory_and_metacircular/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Timeless Decision Theory and Meta-Circular Decision Theory</span></a></li><li><a href=\"http://lesswrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Decision theory: Why Pearl helps reduce </span><span class=\"by_9c2mQkLQq6gQSksMs\">\"could\"</span><span class=\"by_qf77EiaoMw7tH3GSr\"> and </span><span class=\"by_9c2mQkLQq6gQSksMs\">\"would\"</span><span class=\"by_qf77EiaoMw7tH3GSr\">, but still leaves us with at least three alternatives</span></span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://www.lesswrong.com/tag/anna-salamon\"><span class=\"by_qf77EiaoMw7tH3GSr\">Anna Salamon</span></a></li></ul><h2 id=\"External_Links\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">External </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Links</span></span></h2><ul><li><a href=\"http://intelligence.org/files/TDT.pdf\"><span class=\"by_LoykQRMTxJFxwwdPy\">Timeless Decision Theory</span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\"> </span><span class=\"by_SdZmP36R37riQrHAw\">(2010)</span><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span></span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_LoykQRMTxJFxwwdPy\">Eliezer Yudkowsky</span></a></li><li><a href=\"http://formalisedthinking.wordpress.com/2010/08/19/an-introduction-to-timeless-decision-theory/\"><span class=\"by_LoykQRMTxJFxwwdPy\">An Introduction to Timeless Decision Theory</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> at Formalised Thinking</span></li></ul><h2 id=\"See_Also\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_LoykQRMTxJFxwwdPy\">Decision theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_qf77EiaoMw7tH3GSr\">Newcomb's problem</span></a></li><li><a href=\"https://www.lesswrong.com/tag/causality\"><span class=\"by_qf77EiaoMw7tH3GSr\">Causality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_qf77EiaoMw7tH3GSr\">Updateless decision theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationality-is-systematized-winning\"><span class=\"by_LoykQRMTxJFxwwdPy\">Rationality is systematized winning</span></a></li></ul>",
      "sections": [
        {
          "title": "Timeless Decision Theory has been replaced by Functional Decision Theory",
          "anchor": "Timeless_Decision_Theory_has_been_replaced_by_Functional_Decision_Theory",
          "level": 1
        },
        {
          "title": "TDT and Newcomb's problem",
          "anchor": "TDT_and_Newcomb_s_problem",
          "level": 1
        },
        {
          "title": "TDT and other decision scenarios",
          "anchor": "TDT_and_other_decision_scenarios",
          "level": 1
        },
        {
          "title": "Formalization of TDT",
          "anchor": "Formalization_of_TDT",
          "level": 1
        },
        {
          "title": "Further Reading",
          "anchor": "Further_Reading",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 9
    },
    "postCount": 15,
    "description": {
      "markdown": "**Timeless decision theory** (TDT) is a [decision theory](https://www.lesswrong.com/tag/decision-theory), [developed by Eliezer Yudkowsky](http://intelligence.org/files/TDT.pdf) which, in slogan form, says that agents should decide as if they are determining the output of the abstract computation that they implement. This theory was developed in response to the view that rationality should be about winning (that is, about agents achieving their desired ends) rather than about behaving in a manner that we would intuitively label as rational. Prominent existing decision theories (including [causal decision theory](https://www.lesswrong.com/tag/causal-decision-theory), or CDT) fail to choose the winning decision in some scenarios and so there is a need to develop a more successful theory.\n\nTimeless Decision Theory has been replaced by [Functional Decision Theory](https://www.lesswrong.com/tag/functional-decision-theory)\n------------------------------------------------------------------------------------------------------------------------------------\n\n<more needed>\n\nTDT and Newcomb's problem\n-------------------------\n\nA better sense of the motivations behind, and form of, TDT can be gained by considering a particular decision scenario: [Newcomb's problem](http://lesswrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/). In Newcomb's problem, a superintelligent artificial intelligence, Omega, presents you with a transparent box and an opaque box. The transparent box contains $1000 while the opaque box contains either $1,000,000 or nothing. You are given the choice to either take both boxes (called two-boxing) or just the opaque box (one-boxing). However, things are complicated by the fact that Omega is an almost perfect predictor of human behavior and has filled the opaque box as follows: if Omega predicted that you would one-box, it filled the box with $1,000,000 whereas if Omega predicted that you would two-box it filled it with nothing.\n\nMany people find it intuitive that it is rational to two-box in this case. As the opaque box is already filled, you cannot influence its contents with your decision so you may as well take both boxes and gain the extra $1000 from the transparent box. CDT formalizes this style of reasoning. However, one-boxers win in this scenario. After all, if you one-box then Omega (almost certainly) predicted that you would do so and hence filled the opaque box with $1,000,000. So you will almost certainly end up with $1,000,000 if you one-box. On the other hand, if you two-box, Omega (almost certainly) predicted this and so left the opaque box empty . So you will almost certainly end up with $1000 (from the transparent box) if you two-box. Consequently, if rationality is about winning then it's rational to one-box in Newcomb's problem (and hence CDT fails to be an adequate decision theory).\n\nTDT will endorse one-boxing in this scenario and hence endorses the winning decision. When Omega predicts your behavior, it carries out the same abstract computation as you do when you decide whether to one-box or two-box. To make this point clear, we can imagine that Omega makes this prediction by creating a simulation of you and observing its behavior in Newcomb's problem. This simulation will clearly decide according to the same abstract computation as you do as both you and it decide in the same manner. Now given that TDT says to act as if deciding the output of this computation, it tells you to act as if your decision to one-box can determine the behavior of the simulation (or, more generally, Omega's prediction) and hence the filling of the boxes. So TDT correctly endorses one-boxing in Newcomb's problem as it tells the agent to act as if doing so will lead them to get $1,000,000 instead of $1,000.\n\nTDT and other decision scenarios\n--------------------------------\n\nTDT also wins in a range of other cases including [medical Newcomb's problems](https://www.lesswrong.com/tag/smoking-lesion), [Parfit's hitchhiker](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/), and [the one-shot prisoners' dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma). However, there are [other scenarios](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/) where TDT does not win, including [counterfactual mugging](http://lesswrong.com/lw/3l/counterfactual_mugging/). This suggests that TDT still requires further development if it is to become a fully adequate decision theory. Given this, there is some motivation to also consider alternative decision theories alongside TDT, like [updateless decision theory](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) (UDT), which also wins in a range of scenarios but has its own problem cases. It seems likely that both of these theories draw on insights which are crucial to progressing our understanding of decision theory. So while TDT requires further development to be entirely adequate, it nevertheless represents a substantial step toward developing a decision theory that always endorses the winning decision\n\nFormalization of TDT\n--------------------\n\nComing to fully grasp TDT requires an understanding of how the theory is formalized. Very briefly, TDT is formalized by supplementing causal Bayesian networks, which can be thought of as graphs representing causal relations, in two ways. First, these graphs should be supplemented with nodes representing abstract computations and an agent's uncertainty about the result of these computations. Such a node might represent an agent's uncertainty about the result of a mathematical sum. Second, TDT treats decisions as the abstract computation that underlies the agent's decision process. These two features transform causal Bayesian networks into timeless decision diagrams. Using these supplemented diagrams, TDT is able to determine the winning decision in a whole range of a decision scenarios. For a more detailed description of the formalization of TDT, see Eliezer Yudkowsky's [timeless decision theory paper](http://intelligence.org/files/TDT.pdf).\n\nFurther Reading\n---------------\n\n*   [A Comparison of Decision Algorithms on Newcomblike Problems](https://intelligence.org/files/Comparison.pdf), by Alex Altair\n*   [Problem Class Dominance in Predictive Dilemmas](https://intelligence.org/wp-content/uploads/2014/10/Hintze-Problem-Class-Dominance-In-Predictive-Dilemmas.pdf), by Danny Hintze\n\nNotable Posts\n-------------\n\n*   [Ingredients of Timeless Decision Theory](http://lesswrong.com/lw/15z/ingredients_of_timeless_decision_theory/)\n*   [Timeless Decision Theory: Problems I Can't Solve](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/)\n*   [Timeless Decision Theory and Meta-Circular Decision Theory](http://lesswrong.com/lw/164/timeless_decision_theory_and_metacircular/)\n*   [Decision theory: Why Pearl helps reduce \"could\" and \"would\", but still leaves us with at least three alternatives](http://lesswrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/) by [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon)\n\nExternal Links\n--------------\n\n*   [Timeless Decision Theory](http://intelligence.org/files/TDT.pdf) (2010) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [An Introduction to Timeless Decision Theory](http://formalisedthinking.wordpress.com/2010/08/19/an-introduction-to-timeless-decision-theory/) at Formalised Thinking\n\nSee Also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Causality](https://www.lesswrong.com/tag/causality)\n*   [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)\n*   [Rationality is systematized winning](https://www.lesswrong.com/tag/rationality-is-systematized-winning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1c9",
    "name": "Many-Worlds Interpretation",
    "core": null,
    "slug": "many-worlds-interpretation",
    "tableOfContents": {
      "html": "<p><span class=\"by_cn4SiEmqWbu7K9em5\">Standard </span><a href=\"https://www.lesswrong.com/tag/quantum-physics\"><span class=\"by_cn4SiEmqWbu7K9em5\">quantum mechanics</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> is made of two parts: a part that describes the unitary and deterministic evolution of a state vector, and a part that describes how a state vector randomly collapses when subjected to \"measurement\".</span><br><span class=\"by_cn4SiEmqWbu7K9em5\">The </span><strong><span class=\"by_cn4SiEmqWbu7K9em5\">many-worlds interpretation</span></strong><span><span class=\"by_JtChJYGsjzgAh5Ag8\"> (MWI)</span><span class=\"by_cn4SiEmqWbu7K9em5\"> </span></span><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><span class=\"by_cn4SiEmqWbu7K9em5\">cuts away</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> the latter part. It uses </span><a href=\"https://www.lesswrong.com/tag/decoherence\"><span class=\"by_cn4SiEmqWbu7K9em5\">decoherence</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> to explain how the universe splits into many separate branches, each of which looks like it came out of a random collapse.</span></p><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/quantum-physics\"><span class=\"by_cn4SiEmqWbu7K9em5\">Quantum mechanics</span></a></li><li><a href=\"https://www.lesswrong.com/tag/decoherence\"><span class=\"by_cn4SiEmqWbu7K9em5\">Decoherence</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_cn4SiEmqWbu7K9em5\">External links</span></h2><ul><li><a href=\"http://users.ox.ac.uk/~mert0130/papers/proc_dec.pdf\"><span class=\"by_cn4SiEmqWbu7K9em5\">Decoherence and Ontology</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> (David Wallace)</span></li><li><a href=\"http://www.hedweb.com/manworld.htm\"><span class=\"by_cn4SiEmqWbu7K9em5\">The Everett FAQ</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> (Michael Clive Price)</span></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 26,
    "description": {
      "markdown": "Standard [quantum mechanics](https://www.lesswrong.com/tag/quantum-physics) is made of two parts: a part that describes the unitary and deterministic evolution of a state vector, and a part that describes how a state vector randomly collapses when subjected to \"measurement\".  \nThe **many-worlds interpretation** (MWI) [cuts away](https://www.lesswrong.com/tag/occam-s-razor) the latter part. It uses [decoherence](https://www.lesswrong.com/tag/decoherence) to explain how the universe splits into many separate branches, each of which looks like it came out of a random collapse.\n\nSee also\n--------\n\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-physics)\n*   [Decoherence](https://www.lesswrong.com/tag/decoherence)\n\nExternal links\n--------------\n\n*   [Decoherence and Ontology](http://users.ox.ac.uk/~mert0130/papers/proc_dec.pdf) (David Wallace)\n*   [The Everett FAQ](http://www.hedweb.com/manworld.htm) (Michael Clive Price)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1be",
    "name": "Intelligence Explosion",
    "core": null,
    "slug": "intelligence-explosion",
    "tableOfContents": {
      "html": "<p><span class=\"by_sQdyMYniENgxfQsf2\">An </span><strong><span><span class=\"by_sQdyMYniENgxfQsf2\">intelligence</span><span class=\"by_mPipmBTniuABY5PQy\"> explosion</span></span></strong><span><span class=\"by_mPipmBTniuABY5PQy\"> is </span><span class=\"by_sQdyMYniENgxfQsf2\">theoretical scenario in which an intelligent agent analyzes </span><span class=\"by_qf77EiaoMw7tH3GSr\">the </span><span class=\"by_sQdyMYniENgxfQsf2\">processes that produce its intelligence, improves upon them, and creates a successor which does the same. This process repeats in</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_mPipmBTniuABY5PQy\">a positive feedback </span><span class=\"by_sQdyMYniENgxfQsf2\">loop– each successive agent more intelligent than the last and thus more able to increase the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> intelligence </span><span class=\"by_sQdyMYniENgxfQsf2\">of its successor – until some limit </span><span class=\"by_qf77EiaoMw7tH3GSr\">is </span><span class=\"by_sQdyMYniENgxfQsf2\">reached. This limit is conjectured to be much, much higher than human intelligence.</span></span></p><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">A strong version of this idea suggests that once the positive feedback starts to play </span><span class=\"by_mPipmBTniuABY5PQy\">a</span><span class=\"by_qf77EiaoMw7tH3GSr\"> role, it will lead to a </span><span class=\"by_sQdyMYniENgxfQsf2\">very </span><span class=\"by_qf77EiaoMw7tH3GSr\">dramatic leap in capability very </span><span class=\"by_sQdyMYniENgxfQsf2\">quickly. This is known as</span><span class=\"by_woC2b5rav5sGrAo3E\"> a </span><span class=\"by_sQdyMYniENgxfQsf2\">“hard takeoff.” In this scenario,</span><span class=\"by_qf77EiaoMw7tH3GSr\"> technological progress drops into the characteristic timescale of transistors rather than human neurons, and the ascent rapidly surges upward and creates superintelligence </span><span class=\"by_sQdyMYniENgxfQsf2\">(a mind</span><span class=\"by_qf77EiaoMw7tH3GSr\"> orders of magnitude more powerful than </span><span class=\"by_sQdyMYniENgxfQsf2\">a human's)</span><span class=\"by_qf77EiaoMw7tH3GSr\"> before it hits physical limits.</span><span class=\"by_sQdyMYniENgxfQsf2\"> A hard takeoff is distinguished from a \"soft takeoff\" only by the speed with which said limits are reached.</span></span></p><h2 id=\"Published_arguments\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Published arguments</span></h2><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">Philosopher David Chalmers published a </span><a href=\"http://consc.net/papers/singularity.pdf\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">significant analysis of the </span><span class=\"by_woC2b5rav5sGrAo3E\">Singularity</span></span></a><span><span class=\"by_woC2b5rav5sGrAo3E\">,</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> focusing on intelligence explosions, in </span></span><i><span class=\"by_5wu9jG4pm9q6xjZ9R\">Journal of Consciousness Studies</span></i><span class=\"by_5wu9jG4pm9q6xjZ9R\">. </span><a href=\"https://wiki.lesswrong.com/wiki/Singularity#Chalmers.27_analysis\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">His analysis</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> of how they could occur defends the likelihood of an intelligence explosion. </span><span class=\"by_woC2b5rav5sGrAo3E\">He performed a very careful analysis of the main premises and arguments for the existence of the a singularity from an intelligence explosion. According to him, the main argument is:\"</span></span></p><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">1. There will be AI (before long, absent defeaters).</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">2. If there is AI, there will be AI+ (soon after, absent defeaters).</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">3. If there is AI+, there will be AI++ (soon after, absent defeaters).</span></li></ul><p><span class=\"by_woC2b5rav5sGrAo3E\">—————-</span></p><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">4. There will be AI++ (before too long, absent defeaters). \"</span></li></ul><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">He also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.</span></p><p><a href=\"http://lesswrong.com/user/lukeprog\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Luke Muehlhauser</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><a href=\"http://lesswrong.com/user/AnnaSalamon\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Anna Salamon</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> argue in </span><a href=\"http://intelligence.org/files/IE-EI.pdf\"><i><span class=\"by_5wu9jG4pm9q6xjZ9R\">Intelligence Explosion: Evidence and Import</span></i></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> in detail that </span><span class=\"by_woC2b5rav5sGrAo3E\">there is a substantial chance of </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">an intelligence explosion within 100 years, and extremely critical in determining the future. They trace the implications of many types of upcoming technologies, and point out the feedback loops present in them. This leads them to deduce that an above-human level AI will almost certainly lead to an intelligence explosion. They conclude with recommendations</span><span class=\"by_qf77EiaoMw7tH3GSr\"> for </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">bringing about a safe intelligence explosion.</span></span></p><h2 id=\"Hypothetical_path\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Hypothetical path</span></h2><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">The following is a common example of a possible path for</span><span class=\"by_qf77EiaoMw7tH3GSr\"> an</span><span class=\"by_mPipmBTniuABY5PQy\"> AI </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">to bring about an intelligence explosion. First, the AI is smart enough to conclude that inventing</span><span class=\"by_mPipmBTniuABY5PQy\"> molecular </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">nanotechnology will be of greatest benefit to it. Its first act of recursive self-improvement is</span><span class=\"by_mPipmBTniuABY5PQy\"> to gain </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">access to other computers over the internet. This extra computational ability increases the depth and breadth of its search processes. It then uses gained knowledge of material physics and a distributed computing program to invent the first general assembler nanomachine. Then it uses some manufacturing technology, accessible from the internet, to build and</span><span class=\"by_mPipmBTniuABY5PQy\"> deploy the </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">nanotech. It programs the nanotech</span><span class=\"by_qf77EiaoMw7tH3GSr\"> to </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">turn a large section of bedrock into a supercomputer. This is</span><span class=\"by_qf77EiaoMw7tH3GSr\"> its </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">second act</span><span class=\"by_mPipmBTniuABY5PQy\"> of recursive self-</span><span class=\"by_5wu9jG4pm9q6xjZ9R\">improvement, only possible because of the first. Then it could use this enormous computing power to consider hundreds of alternative decision algorithms, better computing </span><span class=\"by_woC2b5rav5sGrAo3E\">structures and so on. After this, this AI would go from a near to human level intelligence to a superintelligence, providing a dramatic and abruptly increase in capability.</span></span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/w5/cascades_cycles_insight/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Cascades, Cycles, Insight...</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"http://lesswrong.com/lw/w6/recursion_magic/\"><span class=\"by_qf77EiaoMw7tH3GSr\">...Recursion, Magic</span></a></li><li><a href=\"http://lesswrong.com/lw/we/recursive_selfimprovement/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Recursive Self-Improvement</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"http://lesswrong.com/lw/wf/hard_takeoff/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Hard Takeoff</span></a><span class=\"by_mPipmBTniuABY5PQy\">, </span><a href=\"http://lesswrong.com/lw/wg/permitted_possibilities_locality/\"><span class=\"by_mPipmBTniuABY5PQy\">Permitted Possibilities, &amp; Locality</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Technological_singularity\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Technological singularity</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Hard_takeoff\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Hard takeoff</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Existential risk</span></a></li><li><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Artificial General Intelligence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/lawful-intelligence\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Lawful intelligence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">The Hanson-Yudkowsky AI-Foom Debate</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://intelligenceexplosion.com/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Intelligence Explosion website</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, a landing page for introducing the concept</span></li><li><a href=\"http://yudkowsky.net/singularity/schools\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Three Major Singularity Schools</span></a></li></ul><h2 id=\"References\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">References</span></h2><ul><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Good, Irving John (1965). Franz L. Alt and Morris Rubinoff. ed. \"</span><a href=\"http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Speculations concerning the first ultraintelligent machine</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">.\" </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Advances in computers</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (New York: Academic Press) </span><strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">6</span></strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">: 31-88. </span><a href=\"https://wiki.lesswrong.com/index.php?title=Digital_object_identifier&amp;action=edit&amp;redlink=1\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">doi</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">:</span><a href=\"http://dx.doi.org/10.1016%2FS0065-2458%2808%2960418-0\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">10.1016/S0065-2458(08)60418-0</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">David Chalmers (2010). \"</span><a href=\"http://consc.net/papers/singularity.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Singularity: A Philosophical Analysis</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">.\" </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Journal of Consciousness Studies</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> </span><strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">17</span></strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">: 7-65.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Muehlhauser, Luke; Salamon, Anna (2012). </span><a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"Intelligence Explosion: Evidence and Import\"</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. in Eden, Amnon; Søraker, Johnny; Moor, James H. et al. </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">The singularity hypothesis: A scientific and philosophical assessment</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Berlin: Springer.</span></li></ul>",
      "sections": [
        {
          "title": "Published arguments",
          "anchor": "Published_arguments",
          "level": 1
        },
        {
          "title": "Hypothetical path",
          "anchor": "Hypothetical_path",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 19,
    "description": {
      "markdown": "An **intelligence explosion** is theoretical scenario in which an intelligent agent analyzes the processes that produce its intelligence, improves upon them, and creates a successor which does the same. This process repeats in a positive feedback loop– each successive agent more intelligent than the last and thus more able to increase the intelligence of its successor – until some limit is reached. This limit is conjectured to be much, much higher than human intelligence.\n\nA strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a very dramatic leap in capability very quickly. This is known as a “hard takeoff.” In this scenario, technological progress drops into the characteristic timescale of transistors rather than human neurons, and the ascent rapidly surges upward and creates superintelligence (a mind orders of magnitude more powerful than a human's) before it hits physical limits. A hard takeoff is distinguished from a \"soft takeoff\" only by the speed with which said limits are reached.\n\nPublished arguments\n-------------------\n\nPhilosopher David Chalmers published a [significant analysis of the Singularity](http://consc.net/papers/singularity.pdf), focusing on intelligence explosions, in *Journal of Consciousness Studies*. [His analysis](https://wiki.lesswrong.com/wiki/Singularity#Chalmers.27_analysis) of how they could occur defends the likelihood of an intelligence explosion. He performed a very careful analysis of the main premises and arguments for the existence of the a singularity from an intelligence explosion. According to him, the main argument is:\"\n\n*   1\\. There will be AI (before long, absent defeaters).\n*   2\\. If there is AI, there will be AI+ (soon after, absent defeaters).\n*   3\\. If there is AI+, there will be AI++ (soon after, absent defeaters).\n\n—————-\n\n*   4\\. There will be AI++ (before too long, absent defeaters). \"\n\nHe also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.\n\n[Luke Muehlhauser](http://lesswrong.com/user/lukeprog) and [Anna Salamon](http://lesswrong.com/user/AnnaSalamon) argue in [*Intelligence Explosion: Evidence and Import*](http://intelligence.org/files/IE-EI.pdf) in detail that there is a substantial chance of an intelligence explosion within 100 years, and extremely critical in determining the future. They trace the implications of many types of upcoming technologies, and point out the feedback loops present in them. This leads them to deduce that an above-human level AI will almost certainly lead to an intelligence explosion. They conclude with recommendations for bringing about a safe intelligence explosion.\n\nHypothetical path\n-----------------\n\nThe following is a common example of a possible path for an AI to bring about an intelligence explosion. First, the AI is smart enough to conclude that inventing molecular nanotechnology will be of greatest benefit to it. Its first act of recursive self-improvement is to gain access to other computers over the internet. This extra computational ability increases the depth and breadth of its search processes. It then uses gained knowledge of material physics and a distributed computing program to invent the first general assembler nanomachine. Then it uses some manufacturing technology, accessible from the internet, to build and deploy the nanotech. It programs the nanotech to turn a large section of bedrock into a supercomputer. This is its second act of recursive self-improvement, only possible because of the first. Then it could use this enormous computing power to consider hundreds of alternative decision algorithms, better computing structures and so on. After this, this AI would go from a near to human level intelligence to a superintelligence, providing a dramatic and abruptly increase in capability.\n\nBlog posts\n----------\n\n*   [Cascades, Cycles, Insight...](http://lesswrong.com/lw/w5/cascades_cycles_insight/), [...Recursion, Magic](http://lesswrong.com/lw/w6/recursion_magic/)\n*   [Recursive Self-Improvement](http://lesswrong.com/lw/we/recursive_selfimprovement/), [Hard Takeoff](http://lesswrong.com/lw/wf/hard_takeoff/), [Permitted Possibilities, & Locality](http://lesswrong.com/lw/wg/permitted_possibilities_locality/)\n\nSee also\n--------\n\n*   [Technological singularity](https://wiki.lesswrong.com/wiki/Technological_singularity), [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence)\n*   [Lawful intelligence](https://www.lesswrong.com/tag/lawful-intelligence)\n*   [The Hanson-Yudkowsky AI-Foom Debate](https://www.lesswrong.com/tag/the-hanson-yudkowsky-ai-foom-debate)\n\nExternal links\n--------------\n\n*   [Intelligence Explosion website](http://intelligenceexplosion.com/), a landing page for introducing the concept\n*   [Three Major Singularity Schools](http://yudkowsky.net/singularity/schools)\n\nReferences\n----------\n\n*   Good, Irving John (1965). Franz L. Alt and Morris Rubinoff. ed. \"[Speculations concerning the first ultraintelligent machine](http://commonsenseatheism.com/wp-content/uploads/2011/02/Good-Speculations-Concerning-the-First-Ultraintelligent-Machine.pdf).\" *Advances in computers* (New York: Academic Press) **6**: 31-88. [doi](https://wiki.lesswrong.com/index.php?title=Digital_object_identifier&action=edit&redlink=1):[10.1016/S0065-2458(08)60418-0](http://dx.doi.org/10.1016%2FS0065-2458%2808%2960418-0).\n*   David Chalmers (2010). \"[The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf).\" *Journal of Consciousness Studies* **17**: 7-65.\n*   Muehlhauser, Luke; Salamon, Anna (2012). [\"Intelligence Explosion: Evidence and Import\"](http://commonsenseatheism.com/wp-content/uploads/2012/02/Muehlhauser-Salamon-Intelligence-Explosion-Evidence-and-Import.pdf). in Eden, Amnon; Søraker, Johnny; Moor, James H. et al. *The singularity hypothesis: A scientific and philosophical assessment*. Berlin: Springer."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b8",
    "name": "Free Will",
    "core": null,
    "slug": "free-will",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Free will</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is one</span><span class=\"by_nmk3nLpQE89dMRzzN\"> of the </span></span><i><span class=\"by_nmk3nLpQE89dMRzzN\">easiest</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> hard questions, as millennia-old philosophical dilemmas go. Though this </span><a href=\"http://lesswrong.com/lw/og/wrong_questions/\"><span class=\"by_nmk3nLpQE89dMRzzN\">impossible question</span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\"> is</span><span class=\"by_XLwKyCK7JmC292ZCC\"> generally considered</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span></span><a href=\"http://lesswrong.com/lw/of/dissolving_the_question/\"><span class=\"by_nmk3nLpQE89dMRzzN\">fully and completely dissolved</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> on Less Wrong, aspiring reductionists should </span><strong><span class=\"by_nmk3nLpQE89dMRzzN\">try to solve it on their own</span></strong><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><h2 id=\"Non_spoiler_posts\"><span class=\"by_nmk3nLpQE89dMRzzN\">Non-spoiler posts</span></h2><p><span class=\"by_nmk3nLpQE89dMRzzN\">The following posts can be read to set up the problem of \"free will\" and what constitutes a good solution from a reductionist perspective, without entirely giving away the solution. (When Yudkowsky wrote these posts, he thought he could get away with just leaving it as a practice problem, and some of the posts state that the problem will be left open. However Yudkowsky did eventually find that he needed to write out the whole solution.)</span></p><ul><li><a href=\"http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/\"><span class=\"by_nmk3nLpQE89dMRzzN\">How An Algorithm Feels From Inside</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> (see also the </span><a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\"><span class=\"by_nmk3nLpQE89dMRzzN\">wiki page</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">)</span></li><li><a href=\"http://lesswrong.com/lw/of/dissolving_the_question/\"><strong><span class=\"by_nmk3nLpQE89dMRzzN\">Dissolving the Question</span></strong></a><span class=\"by_nmk3nLpQE89dMRzzN\"> - this is where the \"free will\" puzzle is explicitly posed, along with criteria for what does and does not constitute a satisfying answer.</span></li><li><a href=\"http://lesswrong.com/lw/og/wrong_questions/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Wrong Questions</span></a></li><li><a href=\"http://lesswrong.com/lw/oh/righting_a_wrong_question/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Righting a Wrong Question</span></a></li></ul><p><span class=\"by_nmk3nLpQE89dMRzzN\">For spoiler posts see </span><a href=\"https://www.lesswrong.com/tag/free-will-solution\"><span class=\"by_nmk3nLpQE89dMRzzN\">free will (solution)</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">How an algorithm feels</span><span class=\"by_XLwKyCK7JmC292ZCC\"> from the inside</span></span></a></li></ul>",
      "sections": [
        {
          "title": "Non-spoiler posts",
          "anchor": "Non_spoiler_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 27,
    "description": {
      "markdown": "**Free will** is one of the *easiest* hard questions, as millennia-old philosophical dilemmas go. Though this [impossible question](http://lesswrong.com/lw/og/wrong_questions/) is generally considered [fully and completely dissolved](http://lesswrong.com/lw/of/dissolving_the_question/) on Less Wrong, aspiring reductionists should **try to solve it on their own**.\n\nNon-spoiler posts\n-----------------\n\nThe following posts can be read to set up the problem of \"free will\" and what constitutes a good solution from a reductionist perspective, without entirely giving away the solution. (When Yudkowsky wrote these posts, he thought he could get away with just leaving it as a practice problem, and some of the posts state that the problem will be left open. However Yudkowsky did eventually find that he needed to write out the whole solution.)\n\n*   [How An Algorithm Feels From Inside](http://lesswrong.com/lw/no/how_an_algorithm_feels_from_inside/) (see also the [wiki page](https://www.lesswrong.com/tag/how-an-algorithm-feels))\n*   [**Dissolving the Question**](http://lesswrong.com/lw/of/dissolving_the_question/) \\- this is where the \"free will\" puzzle is explicitly posed, along with criteria for what does and does not constitute a satisfying answer.\n*   [Wrong Questions](http://lesswrong.com/lw/og/wrong_questions/)\n*   [Righting a Wrong Question](http://lesswrong.com/lw/oh/righting_a_wrong_question/)\n\nFor spoiler posts see [free will (solution)](https://www.lesswrong.com/tag/free-will-solution).\n\nSee also\n--------\n\n*   [How an algorithm feels from the inside](https://www.lesswrong.com/tag/how-an-algorithm-feels)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b6",
    "name": "Counterfactual Mugging",
    "core": null,
    "slug": "counterfactual-mugging",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_4SHky5j2PNcRwBiZt\">Counterfactual </span><span class=\"by_qf77EiaoMw7tH3GSr\">mugging</span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\"> is a thought experiment for testing</span><span class=\"by_4SHky5j2PNcRwBiZt\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">and differentiating </span></span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qf77EiaoMw7tH3GSr\">decision theories</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\">,</span><span class=\"by_4SHky5j2PNcRwBiZt\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">stated</span><span class=\"by_4SHky5j2PNcRwBiZt\"> as </span><span class=\"by_qf77EiaoMw7tH3GSr\">follows:</span></span></p><blockquote><p><span><span class=\"by_XLwKyCK7JmC292ZCC\">Omega, a perfect predictor, flips a coin. If </span><span class=\"by_8YfqEA4wXXDHC83t2\">it</span><span class=\"by_XLwKyCK7JmC292ZCC\"> comes up tails Omega asks you for $100. If it comes up heads, Omega pays you $10,000 if it predicts that you would have paid if it had come up tails.</span></span></p></blockquote><p><span class=\"by_woC2b5rav5sGrAo3E\">Depending on how the problem in phrased, intuition calls for different answers. For example, </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_woC2b5rav5sGrAo3E\">Eliezer Yudkowsky</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> has argued that framing the problem in a way Omega is a regular aspect of the environment which regularly asks such types of questions makes most people answer 'Yes'</span><span class=\"by_XLwKyCK7JmC292ZCC\">. However, Vladimir Nesov points out that </span></span><a href=\"https://www.lesswrong.com/posts/4ARtkT3EYox3THYjF/rationality-is-systematized-winning\"><span class=\"by_XLwKyCK7JmC292ZCC\">Rationalists Should Win</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> could be interpreted as suggesting that we should not pay. After all, even though paying in the tails case would cause you to do worse in the counterfactual where the coin came up heads, you already know the counterfactual didn't happen, so it's not obvious that you should pay. This issue has been discussed in </span><a href=\"https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay#h9Lc5j42HPao4aaep\"><span class=\"by_XLwKyCK7JmC292ZCC\">this question</span></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></p><p><span class=\"by_woC2b5rav5sGrAo3E\">Formal decision theories also diverge. For </span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span class=\"by_woC2b5rav5sGrAo3E\">Causal Decision Theory</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\">, you can only affect those probabilities that you are </span><span class=\"by_my98h9K2ygm9SeEEK\">causally</span><span class=\"by_woC2b5rav5sGrAo3E\"> linked to. Hence, the answer should be 'No'. In </span></span><a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\"><span class=\"by_woC2b5rav5sGrAo3E\">Evidential Decision Theory</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> any kind of connection is accounted, then the answer should be 'No'. </span><a href=\"https://www.lesswrong.com/tag/timeless-decision-theory\"><span class=\"by_woC2b5rav5sGrAo3E\">Timeless Decision Theory</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> answer seems undefined, however Yudkowsky has argued that if the problem is </span><span class=\"by_nHoZkgQWN55r33tcM\">recurrently</span><span class=\"by_woC2b5rav5sGrAo3E\"> presented, one should answer 'Yes' on the basis of enhancing its probability of gaining $10000 in the next round. This seems to be Causal Decision Theory prescription as well. </span></span><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_woC2b5rav5sGrAo3E\">Updateless decision theory</span></a><a href=\"http://lesswrong.com/lw/15m/towards_a_new_decision_theory/\"><span class=\"by_woC2b5rav5sGrAo3E\">1</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> prescribes giving the $100, on the basis your decision can influence both the 'heads branch' and 'tails branch' of the universe.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">Regardless of the particular decision theory, it is generally agreed that if you can pre-commit in advance that you should do so. The dispute is purely over what you should do if you didn't pre-commit.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">Eliezer listed this in his 2009 post </span><a href=\"https://www.lesswrong.com/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve\"><span class=\"by_XLwKyCK7JmC292ZCC\">Timeless Decision Theory Problems I can't Solve</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, although that was written before </span><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_XLwKyCK7JmC292ZCC\">Updateless Decision Theory</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">.</span></p><h2 id=\"Variants\"><span class=\"by_XLwKyCK7JmC292ZCC\">Variants</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">The </span><a href=\"https://www.lesswrong.com/posts/sY2rHNcWdg94RiSSR/the-counterfactual-prisoner-s-dilemma\"><span class=\"by_XLwKyCK7JmC292ZCC\">Counterfactual Prisoner's Dilemma</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> is a symmetric variant of he original independently suggested by Chris Leong and Cousin_it:</span></p><blockquote><p><span class=\"by_XLwKyCK7JmC292ZCC\">Omega, a perfect predictor, flips a coin. If if comes up heads, Omega asks you for $100, then pays you $10,000 if it predict you would have paid if it had come up tails and you were told it was tails. If it comes up tails, Omega asks you for $100, then pays you $10,000 if it predicts you would have paid if it had come up heads and you were told it was heads</span></p></blockquote><p><span class=\"by_XLwKyCK7JmC292ZCC\">In this scenario, an updateless agent receives $9900 and an updateful agent receives nothing regardless of the coin flip, while in the original scenario the upateless agent only comes out ahead if the coin shows tails. This is claimed as a demonstration of the principle that when evaluating decisions we should consider the counterfactual and not just our particular branch of possibility space.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">In </span><a href=\"https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty\"><span class=\"by_XLwKyCK7JmC292ZCC\">Logical Counterfactual Mugging</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> instead of flipping a coin, Omega tells you the 10,000th digit of pi, which we assume you don't know off the top of your head. If it is odd, we treat it like heads in the original problem and if it is even treat it like tails.&nbsp;Logical inductors have been proposed as a solution to this problem. Applying this to </span><a href=\"https://www.lesswrong.com/posts/XzvR3QKkt9EPbAYyT/applying-the-counterfactual-prisoner-s-dilemma-to-logical\"><span class=\"by_XLwKyCK7JmC292ZCC\">Logical Counterfactual Mugging</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">.</span></p><p><span class=\"by_XLwKyCK7JmC292ZCC\">The </span><a href=\"https://www.lesswrong.com/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game\"><span class=\"by_XLwKyCK7JmC292ZCC\">Counterfactual Mugging Poker Game</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> is a somewhat complicated variant by Scott Garrabrant. Player A receives a single card that is either high or low, which they can then reveal if they so desire. Player B then shares their true probability estimate that player A has a high card. Player B is essentially perfect at predicting your behaviour, but doesn't get to see you after you've drawn the card. Additionally, player A loses&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"p^2\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-msubsup\"><span class=\"mjx-base\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.446em;\"><span class=\"by_XLwKyCK7JmC292ZCC\">p</span></span></span></span><span class=\"mjx-sup\" style=\"font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;\"><span class=\"mjx-mn\" style=\"\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.372em; padding-bottom: 0.372em;\"><span class=\"by_XLwKyCK7JmC292ZCC\">2</span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><span class=\"by_XLwKyCK7JmC292ZCC\">&nbsp;dollars. If you show the card if it is low, then you lose 0. However, since B can predict your behaviour, this means that if the card had been high then player B would be able to guess that you had a high card even if you hadn't revealed it. This would lose you a whole dollar and on average you'd be better if you always showed it. Garrabrant states that he prefers this scenario because Counterfactual Mugging feels like it is trying to trick you, while in this scenario you are the one creating the Counterfactual Mugging like situation to withhold information.</span></p><h2 id=\"Comparison_to_Other_Problem\"><span class=\"by_XLwKyCK7JmC292ZCC\">Comparison to Other Problem</span></h2><p><span class=\"by_XLwKyCK7JmC292ZCC\">In </span><a href=\"https://www.lesswrong.com/posts/pneKTZG9KqnSe2RdQ/two-types-of-updatelessness\"><span class=\"by_XLwKyCK7JmC292ZCC\">Two Types of Updatelessness</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, makes a distinction between all-upside updatelessness and mixed-upside updatelessness. In all-upside case, utilising an updateless decision theory provides a better result in the current situation, while in a mixed-upside case the benefits go to other possible selves. Unlike </span><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_XLwKyCK7JmC292ZCC\">Newcomb's Problem</span></a><span class=\"by_XLwKyCK7JmC292ZCC\"> or </span><a href=\"https://www.lesswrong.com/tag/parfits-hitchhiker\"><span class=\"by_XLwKyCK7JmC292ZCC\">Parfait's Hitchhiker</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">, Counterfactual Mugging is a mixed-upside case.</span></p><h2 id=\"Blog_posts\"><span class=\"by_4SHky5j2PNcRwBiZt\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/3l/counterfactual_mugging/\"><span class=\"by_4SHky5j2PNcRwBiZt\">Counterfactual Mugging</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Vladimir_Nesov\"><span class=\"by_4SHky5j2PNcRwBiZt\">Vladimir Nesov</span></a></li><li><a href=\"http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/\"><span class=\"by_4SHky5j2PNcRwBiZt\">Timeless Decision Theory: Problems I Can't Solve</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_4SHky5j2PNcRwBiZt\">Eliezer Yudkowsky</span></a></li><li><a href=\"http://lesswrong.com/lw/15m/towards_a_new_decision_theory/\"><span class=\"by_4SHky5j2PNcRwBiZt\">Towards a New Decision Theory</span></a><span class=\"by_4SHky5j2PNcRwBiZt\"> by </span><a href=\"http://weidai.com/\"><span class=\"by_4SHky5j2PNcRwBiZt\">Wei Dai</span></a></li><li><a href=\"http://lesswrong.com/lw/jrm/the_sin_of_updating_when_you_can_change_whether/\"><span class=\"by_fbEg8jfgqQYPeSX43\">The sin of updating when you can change whether you exist</span></a><span class=\"by_fbEg8jfgqQYPeSX43\"> by Benya Fallenstein</span></li><li><a href=\"https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay\"><span class=\"by_XLwKyCK7JmC292ZCC\">Counterfactual Mugging: Why should you Pay?</span></a><span class=\"by_XLwKyCK7JmC292ZCC\">- Question by Chris Leong</span></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://ordinaryideas.wordpress.com/2011/12/31/counterfactual-blackmail-of-oneself/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Conterfactual Blackmail (of oneself)</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"http://lesswrong.com/user/paulfchristiano\"><span class=\"by_LoykQRMTxJFxwwdPy\">Paul F. Christiano</span></a></li><li><a href=\"https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessnes/\"><span class=\"by_fbEg8jfgqQYPeSX43\">Thoughts on Updatelessness</span></a><span class=\"by_fbEg8jfgqQYPeSX43\"> by Caspar Oesterheld</span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_AbLN9sR8PDACCXKp7\">Decision theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/acausal-trade\"><span class=\"by_qf77EiaoMw7tH3GSr\">Acausal trade</span></a></li><li><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_9c2mQkLQq6gQSksMs\">Newcomb's problem</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\"><span class=\"by_AbLN9sR8PDACCXKp7\">Parfit's hitchhiker</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\"><span class=\"by_AbLN9sR8PDACCXKp7\">Smoker's lesion</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><span class=\"by_AbLN9sR8PDACCXKp7\">Absentminded driver</span></a></li><li><a href=\"https://www.lesswrong.com/tag/sleeping-beauty-paradox\"><span class=\"by_AbLN9sR8PDACCXKp7\">Sleeping Beauty problem</span></a></li><li><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\"><span class=\"by_AbLN9sR8PDACCXKp7\">Prisoner's dilemma</span></a></li><li><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\"><span class=\"by_AbLN9sR8PDACCXKp7\">Pascal's mugging</span></a></li><li><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_fbEg8jfgqQYPeSX43\">Updateless decision theory</span></a></li></ul>",
      "sections": [
        {
          "title": "Variants",
          "anchor": "Variants",
          "level": 1
        },
        {
          "title": "Comparison to Other Problem",
          "anchor": "Comparison_to_Other_Problem",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 16,
    "description": {
      "markdown": "**Counterfactual mugging** is a thought experiment for testing and differentiating [decision theories](https://www.lesswrong.com/tag/decision-theory), stated as follows:\n\n> Omega, a perfect predictor, flips a coin. If it comes up tails Omega asks you for $100. If it comes up heads, Omega pays you $10,000 if it predicts that you would have paid if it had come up tails.\n\nDepending on how the problem in phrased, intuition calls for different answers. For example, [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) has argued that framing the problem in a way Omega is a regular aspect of the environment which regularly asks such types of questions makes most people answer 'Yes'. However, Vladimir Nesov points out that [Rationalists Should Win](https://www.lesswrong.com/posts/4ARtkT3EYox3THYjF/rationality-is-systematized-winning) could be interpreted as suggesting that we should not pay. After all, even though paying in the tails case would cause you to do worse in the counterfactual where the coin came up heads, you already know the counterfactual didn't happen, so it's not obvious that you should pay. This issue has been discussed in [this question](https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay#h9Lc5j42HPao4aaep).\n\nFormal decision theories also diverge. For [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory), you can only affect those probabilities that you are causally linked to. Hence, the answer should be 'No'. In [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) any kind of connection is accounted, then the answer should be 'No'. [Timeless Decision Theory](https://www.lesswrong.com/tag/timeless-decision-theory) answer seems undefined, however Yudkowsky has argued that if the problem is recurrently presented, one should answer 'Yes' on the basis of enhancing its probability of gaining $10000 in the next round. This seems to be Causal Decision Theory prescription as well. [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)[1](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) prescribes giving the $100, on the basis your decision can influence both the 'heads branch' and 'tails branch' of the universe.\n\nRegardless of the particular decision theory, it is generally agreed that if you can pre-commit in advance that you should do so. The dispute is purely over what you should do if you didn't pre-commit.\n\nEliezer listed this in his 2009 post [Timeless Decision Theory Problems I can't Solve](https://www.lesswrong.com/posts/c3wWnvgzdbRhNnNbQ/timeless-decision-theory-problems-i-can-t-solve), although that was written before [Updateless Decision Theory](https://www.lesswrong.com/tag/updateless-decision-theory).\n\nVariants\n--------\n\nThe [Counterfactual Prisoner's Dilemma](https://www.lesswrong.com/posts/sY2rHNcWdg94RiSSR/the-counterfactual-prisoner-s-dilemma) is a symmetric variant of he original independently suggested by Chris Leong and Cousin_it:\n\n> Omega, a perfect predictor, flips a coin. If if comes up heads, Omega asks you for $100, then pays you $10,000 if it predict you would have paid if it had come up tails and you were told it was tails. If it comes up tails, Omega asks you for $100, then pays you $10,000 if it predicts you would have paid if it had come up heads and you were told it was heads\n\nIn this scenario, an updateless agent receives $9900 and an updateful agent receives nothing regardless of the coin flip, while in the original scenario the upateless agent only comes out ahead if the coin shows tails. This is claimed as a demonstration of the principle that when evaluating decisions we should consider the counterfactual and not just our particular branch of possibility space.\n\nIn [Logical Counterfactual Mugging](https://www.lesswrong.com/posts/rqt8RSKPvh4GzYoqE/counterfactual-mugging-and-logical-uncertainty) instead of flipping a coin, Omega tells you the 10,000th digit of pi, which we assume you don't know off the top of your head. If it is odd, we treat it like heads in the original problem and if it is even treat it like tails. Logical inductors have been proposed as a solution to this problem. Applying this to [Logical Counterfactual Mugging](https://www.lesswrong.com/posts/XzvR3QKkt9EPbAYyT/applying-the-counterfactual-prisoner-s-dilemma-to-logical).\n\nThe [Counterfactual Mugging Poker Game](https://www.lesswrong.com/posts/g3PwPgcdcWiP33pYn/counterfactual-mugging-poker-game) is a somewhat complicated variant by Scott Garrabrant. Player A receives a single card that is either high or low, which they can then reveal if they so desire. Player B then shares their true probability estimate that player A has a high card. Player B is essentially perfect at predicting your behaviour, but doesn't get to see you after you've drawn the card. Additionally, player A loses \\\\(p^2\\\\) dollars. If you show the card if it is low, then you lose 0. However, since B can predict your behaviour, this means that if the card had been high then player B would be able to guess that you had a high card even if you hadn't revealed it. This would lose you a whole dollar and on average you'd be better if you always showed it. Garrabrant states that he prefers this scenario because Counterfactual Mugging feels like it is trying to trick you, while in this scenario you are the one creating the Counterfactual Mugging like situation to withhold information.\n\nComparison to Other Problem\n---------------------------\n\nIn [Two Types of Updatelessness](https://www.lesswrong.com/posts/pneKTZG9KqnSe2RdQ/two-types-of-updatelessness), makes a distinction between all-upside updatelessness and mixed-upside updatelessness. In all-upside case, utilising an updateless decision theory provides a better result in the current situation, while in a mixed-upside case the benefits go to other possible selves. Unlike [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem) or [Parfait's Hitchhiker](https://www.lesswrong.com/tag/parfits-hitchhiker), Counterfactual Mugging is a mixed-upside case.\n\nBlog posts\n----------\n\n*   [Counterfactual Mugging](http://lesswrong.com/lw/3l/counterfactual_mugging/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [Timeless Decision Theory: Problems I Can't Solve](http://lesswrong.com/lw/135/timeless_decision_theory_problems_i_cant_solve/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Towards a New Decision Theory](http://lesswrong.com/lw/15m/towards_a_new_decision_theory/) by [Wei Dai](http://weidai.com/)\n*   [The sin of updating when you can change whether you exist](http://lesswrong.com/lw/jrm/the_sin_of_updating_when_you_can_change_whether/) by Benya Fallenstein\n*   [Counterfactual Mugging: Why should you Pay?](https://www.lesswrong.com/posts/h9qQQA3g8dwq6RRTo/counterfactual-mugging-why-should-you-pay)\\- Question by Chris Leong\n\nExternal links\n--------------\n\n*   [Conterfactual Blackmail (of oneself)](http://ordinaryideas.wordpress.com/2011/12/31/counterfactual-blackmail-of-oneself/) by [Paul F. Christiano](http://lesswrong.com/user/paulfchristiano)\n*   [Thoughts on Updatelessness](https://casparoesterheld.com/2016/11/21/thoughts-on-updatelessnes/) by Caspar Oesterheld\n\nSee also\n--------\n\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Acausal trade](https://www.lesswrong.com/tag/acausal-trade)\n*   [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker's_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Sleeping Beauty problem](https://www.lesswrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b4",
    "name": "Offense",
    "core": null,
    "slug": "offense",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_h48TMtPzfimsEobTm\">It is</span><span class=\"by_qf77EiaoMw7tH3GSr\"> hypothesized that the </span></span><a href=\"https://www.lesswrong.com/tag/emotions\"><span class=\"by_qf77EiaoMw7tH3GSr\">emotion</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> of </span><strong><span class=\"by_WgGYj5bqcZKsFNG6F\">offense</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> appears when one perceives an attempt to gain </span><a href=\"https://www.lesswrong.com/tag/social-status\"><span class=\"by_qf77EiaoMw7tH3GSr\">status</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/13s/the_nature_of_offense/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Nature of Offense</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Wei_Dai\"><span class=\"by_qf77EiaoMw7tH3GSr\">Wei Dai</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/social-status\"><span class=\"by_9c2mQkLQq6gQSksMs\">Status</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 5,
    "description": {
      "markdown": "It is hypothesized that the [emotion](https://www.lesswrong.com/tag/emotions) of **offense** appears when one perceives an attempt to gain [status](https://www.lesswrong.com/tag/social-status).\n\nBlog posts\n----------\n\n*   [The Nature of Offense](http://lesswrong.com/lw/13s/the_nature_of_offense/) by [Wei Dai](https://wiki.lesswrong.com/wiki/Wei_Dai)\n\nSee also\n--------\n\n*   [Status](https://www.lesswrong.com/tag/social-status)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1b1",
    "name": "Narrative Fallacy",
    "core": null,
    "slug": "narrative-fallacy",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">The narrative fallacy addresses our limited ability to look at sequences of facts without weaving an explanation into them, or, equivalently, forcing a logical link, an arrow of relationship upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding.</span></p></blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—Nassim Nicholas Taleb,&nbsp;The Black Swan</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2007/07/tell-your-anti-.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">Tell your Anti-Story</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"https://www.lesswrong.com/tag/robin-hanson\"><span class=\"by_LoykQRMTxJFxwwdPy\">Robin Hanson</span></a></li><li><a href=\"http://www.overcomingbias.com/2008/12/the-bad-guy-bia.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">The Bad Guy Bias</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Robin Hanson</span></li><li><a href=\"http://lesswrong.com/lw/13k/missing_the_trees_for_the_forest/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Missing the Trees for the Forest</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_qf77EiaoMw7tH3GSr\">Yvain</span></a></li><li><a href=\"http://lesswrong.com/lw/14q/why_youre_stuck_in_a_narrative/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Why You're Stuck in a Narrative</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"http://www.coarsegra.in/\"><span class=\"by_qf77EiaoMw7tH3GSr\">hegemonicon</span></a></li><li><a href=\"http://www.overcomingbias.com/2012/12/biases-of-fiction.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">Biases of Fiction</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Robin Hanson</span></li></ul><h2 id=\"External_Links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External Links</span></h2><ul><li><a href=\"http://www.youtube.com/watch?v=RoEEDKwzNBw\"><span class=\"by_LoykQRMTxJFxwwdPy\">Tyler Cowen on Stories</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> (</span><a href=\"http://lesswrong.com/r/discussion/lw/8w1/transcript_tyler_cowen_on_stories/\"><span class=\"by_LoykQRMTxJFxwwdPy\">transcript</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">)</span></li><li><a href=\"http://meteuphoric.wordpress.com/2010/04/23/systems-and-stories/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Systems and Stories</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Katja Grace</span></li><li><a href=\"http://theviewfromhell.blogspot.com/2010/12/living-in-epilogue-social-policy-as.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">Living in the Epilogue: Social Policy as Palliative Care</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Sister Y</span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/privileging-the-hypothesis\"><span class=\"by_qf77EiaoMw7tH3GSr\">Privileging the hypothesis</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/confirmation-bias\"><span class=\"by_qf77EiaoMw7tH3GSr\">Positive bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/mind-killer\"><span class=\"by_9c2mQkLQq6gQSksMs\">Mind-killer</span></a></li><li><a href=\"https://www.lesswrong.com/tag/near-far-thinking\"><span class=\"by_9c2mQkLQq6gQSksMs\">Near/far thinking</span></a></li><li><a href=\"https://www.lesswrong.com/tag/hindsight-bias\"><span class=\"by_9c2mQkLQq6gQSksMs\">Hindsight bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fake-simplicity\"><span class=\"by_9c2mQkLQq6gQSksMs\">Fake simplicity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/black-swans\"><span class=\"by_LoykQRMTxJFxwwdPy\">Black swan</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 8,
    "description": {
      "markdown": "> The narrative fallacy addresses our limited ability to look at sequences of facts without weaving an explanation into them, or, equivalently, forcing a logical link, an arrow of relationship upon them. Explanations bind facts together. They make them all the more easily remembered; they help them make more sense. Where this propensity can go wrong is when it increases our impression of understanding.\n\n—Nassim Nicholas Taleb, The Black Swan\n\nBlog posts\n----------\n\n*   [Tell your Anti-Story](http://www.overcomingbias.com/2007/07/tell-your-anti-.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)\n*   [The Bad Guy Bias](http://www.overcomingbias.com/2008/12/the-bad-guy-bia.html) by Robin Hanson\n*   [Missing the Trees for the Forest](http://lesswrong.com/lw/13k/missing_the_trees_for_the_forest/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Why You're Stuck in a Narrative](http://lesswrong.com/lw/14q/why_youre_stuck_in_a_narrative/) by [hegemonicon](http://www.coarsegra.in/)\n*   [Biases of Fiction](http://www.overcomingbias.com/2012/12/biases-of-fiction.html) by Robin Hanson\n\nExternal Links\n--------------\n\n*   [Tyler Cowen on Stories](http://www.youtube.com/watch?v=RoEEDKwzNBw) ([transcript](http://lesswrong.com/r/discussion/lw/8w1/transcript_tyler_cowen_on_stories/))\n*   [Systems and Stories](http://meteuphoric.wordpress.com/2010/04/23/systems-and-stories/) by Katja Grace\n*   [Living in the Epilogue: Social Policy as Palliative Care](http://theviewfromhell.blogspot.com/2010/12/living-in-epilogue-social-policy-as.html) by Sister Y\n\nSee also\n--------\n\n*   [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis), [Positive bias](https://www.lesswrong.com/tag/confirmation-bias)\n*   [Mind-killer](https://www.lesswrong.com/tag/mind-killer)\n*   [Near/far thinking](https://www.lesswrong.com/tag/near-far-thinking)\n*   [Hindsight bias](https://www.lesswrong.com/tag/hindsight-bias)\n*   [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Black swan](https://www.lesswrong.com/tag/black-swans)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a1",
    "name": "Exploratory Engineering",
    "core": null,
    "slug": "exploratory-engineering",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Exploratory engineering</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a process for understanding a subset of the potential capabilities of future levels of technology. This process resembles the first phase of standard design engineering (termed conceptual engineering, or conceptual design), but it serves a different purpose:</span></p><ul><li><span class=\"by_qf77EiaoMw7tH3GSr\">In standard engineering, design leads to the manufacturing of a product.</span></li><li><span class=\"by_qf77EiaoMw7tH3GSr\">In exploratory engineering, design leads to understanding of what a future manufacturing process could produce.</span></li></ul><p><span class=\"by_qf77EiaoMw7tH3GSr\">Exploratory engineering allows to show how some capabilities that intuitively seem </span><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">absurd</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> may actually be achieved in the future.</span></p><h2 id=\"External_links\"><span class=\"by_qf77EiaoMw7tH3GSr\">External links</span></h2><ul><li><a href=\"http://metamodern.com/2009/06/26/exploratory-engineering-applying-the-predictive-power-of-science-to-future-technologies/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Exploratory Engineering: Applying the predictive power of science to future technologies</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://www.lesswrong.com/tag/eric-drexler\"><span class=\"by_qf77EiaoMw7tH3GSr\">Eric Drexler</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/rational-evidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Rational evidence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Absurdity heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/future\"><span class=\"by_qf77EiaoMw7tH3GSr\">Future</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/forecast\"><span class=\"by_qf77EiaoMw7tH3GSr\">Forecast</span></a></li><li><a href=\"https://www.lesswrong.com/tag/cryonics\"><span class=\"by_qf77EiaoMw7tH3GSr\">Cryonics</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_qf77EiaoMw7tH3GSr\">Intelligence explosion</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 6,
    "description": {
      "markdown": "**Exploratory engineering** is a process for understanding a subset of the potential capabilities of future levels of technology. This process resembles the first phase of standard design engineering (termed conceptual engineering, or conceptual design), but it serves a different purpose:\n\n*   In standard engineering, design leads to the manufacturing of a product.\n*   In exploratory engineering, design leads to understanding of what a future manufacturing process could produce.\n\nExploratory engineering allows to show how some capabilities that intuitively seem [absurd](https://www.lesswrong.com/tag/absurdity-heuristic) may actually be achieved in the future.\n\nExternal links\n--------------\n\n*   [Exploratory Engineering: Applying the predictive power of science to future technologies](http://metamodern.com/2009/06/26/exploratory-engineering-applying-the-predictive-power-of-science-to-future-technologies/) by [Eric Drexler](https://www.lesswrong.com/tag/eric-drexler)\n\nSee also\n--------\n\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Future](https://www.lesswrong.com/tag/future), [Forecast](https://www.lesswrong.com/tag/forecast)\n*   [Cryonics](https://www.lesswrong.com/tag/cryonics), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb1a0",
    "name": "Conjunction Fallacy",
    "core": null,
    "slug": "conjunction-fallacy",
    "tableOfContents": {
      "html": "<p><span class=\"by_9c2mQkLQq6gQSksMs\">The </span><strong><span class=\"by_9c2mQkLQq6gQSksMs\">conjunction fallacy</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> consists in assuming that specific conditions are more probable than more general ones.</span></p><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">For</span><span class=\"by_9c2mQkLQq6gQSksMs\"> the </span><span class=\"by_qf77EiaoMw7tH3GSr\">reasons related to </span></span><a href=\"https://www.lesswrong.com/tag/representativeness-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">representativeness heuristic</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">,</span><span class=\"by_9c2mQkLQq6gQSksMs\"> a </span><span class=\"by_qf77EiaoMw7tH3GSr\">fleshed-out </span><span class=\"by_9c2mQkLQq6gQSksMs\">story </span><span class=\"by_qf77EiaoMw7tH3GSr\">that contains typical amount of detail sounds</span><span class=\"by_9c2mQkLQq6gQSksMs\"> more plausible </span><span class=\"by_qf77EiaoMw7tH3GSr\">than a stripped-down description of a situation that only states a few facts. There is a tendency for people</span><span class=\"by_9c2mQkLQq6gQSksMs\"> to </span><span class=\"by_qf77EiaoMw7tH3GSr\">take</span><span class=\"by_9c2mQkLQq6gQSksMs\"> that </span><span class=\"by_qf77EiaoMw7tH3GSr\">plausibility at face value, and assign </span></span><a href=\"https://wiki.lesswrong.com/wiki/probability\"><span class=\"by_qf77EiaoMw7tH3GSr\">probability</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> accordingly. </span><span class=\"by_9c2mQkLQq6gQSksMs\">This intuition is wrong, because the conjunction rule of probability theory states that, for </span><span class=\"by_qf77EiaoMw7tH3GSr\">any event X, its conjunction with additional details Y will be less </span><span class=\"by_mPipmBTniuABY5PQy\">probable.</span></span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">The conjunction fallacy suggests that one should be </span><a href=\"https://www.lesswrong.com/tag/burdensome-details\"><span class=\"by_qf77EiaoMw7tH3GSr\">very careful in adding details</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> to any claim, as even though each such detail may make the claim so much more convincing, it also inevitably subtracts from its validity.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/ji/conjunction_fallacy/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Conjunction Fallacy</span></a></li><li><a href=\"http://lesswrong.com/lw/jj/conjunction_controversy_or_how_they_nail_it_down/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Conjunction Controversy (Or, How They Nail It Down)</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/representativeness-heuristic\"><span class=\"by_9c2mQkLQq6gQSksMs\">Representativeness heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/burdensome-details\"><span class=\"by_9c2mQkLQq6gQSksMs\">Burdensome details</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 12,
    "description": {
      "markdown": "The **conjunction fallacy** consists in assuming that specific conditions are more probable than more general ones.\n\nFor the reasons related to [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic), a fleshed-out story that contains typical amount of detail sounds more plausible than a stripped-down description of a situation that only states a few facts. There is a tendency for people to take that plausibility at face value, and assign [probability](https://wiki.lesswrong.com/wiki/probability) accordingly. This intuition is wrong, because the conjunction rule of probability theory states that, for any event X, its conjunction with additional details Y will be less probable.\n\nThe conjunction fallacy suggests that one should be [very careful in adding details](https://www.lesswrong.com/tag/burdensome-details) to any claim, as even though each such detail may make the claim so much more convincing, it also inevitably subtracts from its validity.\n\nBlog posts\n----------\n\n*   [Conjunction Fallacy](http://lesswrong.com/lw/ji/conjunction_fallacy/)\n*   [Conjunction Controversy (Or, How They Nail It Down)](http://lesswrong.com/lw/jj/conjunction_controversy_or_how_they_nail_it_down/)\n\nSee also\n--------\n\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb19d",
    "name": "Absurdity Heuristic",
    "core": null,
    "slug": "absurdity-heuristic",
    "tableOfContents": {
      "html": "<p><span class=\"by_XzXbiS2zWYNdZdLW8\">The </span><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">absurdity</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> heuristic</span></span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> classifies highly untypical situations as \"absurd\", or </span><a href=\"https://www.lesswrong.com/tag/antiprediction\"><span class=\"by_qf77EiaoMw7tH3GSr\">impossible</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">. While normally very useful as a form of </span><a href=\"https://www.lesswrong.com/tag/epistemic-hygiene\"><span class=\"by_qf77EiaoMw7tH3GSr\">epistemic hygiene</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">, allowing </span><span class=\"by_ZsH6rgLCquwn8g6AS\">us </span><span class=\"by_qf77EiaoMw7tH3GSr\">to detect nonsense, it suffers from the same problems as</span><span class=\"by_ZsH6rgLCquwn8g6AS\"> the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://www.lesswrong.com/tag/representativeness-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">representativeness heuristic</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">There </span><span class=\"by_yXwiEu4DpAhSsFPJh\">are</span><span class=\"by_qf77EiaoMw7tH3GSr\"> a number of situations </span><span class=\"by_yXwiEu4DpAhSsFPJh\">in which</span><span class=\"by_ZsH6rgLCquwn8g6AS\"> the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> absurdity heuristic is wrong. A deep theory has to </span></span><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">override the intuitive expectation</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">. Where you don't expect intuition to construct an </span><a href=\"https://www.lesswrong.com/tag/technical-explanation\"><span class=\"by_qf77EiaoMw7tH3GSr\">adequate model</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> of reality, classifying an idea as impossible</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> may be </span></span><a href=\"https://www.lesswrong.com/tag/overconfidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">overconfident</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">. </span><a href=\"http://lesswrong.com/lw/j1/stranger_than_history/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The future is usually \"absurd\"</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, although sometimes it's possible to </span><a href=\"https://www.lesswrong.com/tag/exploratory-engineering\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">rigorously infer low bounds on capabilities of</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> the </span><span class=\"by_qf77EiaoMw7tH3GSr\">future</span></span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">, proving possible what</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> is </span><span class=\"by_qf77EiaoMw7tH3GSr\">intuitively absurd.</span></span></p><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/representativeness-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Representativeness heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">Shut up and multiply</span></a></li><li><a href=\"https://www.lesswrong.com/tag/antiprediction\"><span class=\"by_qf77EiaoMw7tH3GSr\">Antiprediction</span></a></li><li><a href=\"https://www.lesswrong.com/tag/epistemic-hygiene\"><span class=\"by_qf77EiaoMw7tH3GSr\">Epistemic hygiene</span></a></li><li><a href=\"https://www.lesswrong.com/tag/exploratory-engineering\"><span class=\"by_qf77EiaoMw7tH3GSr\">Exploratory engineering</span></a></li><li><a href=\"https://www.lesswrong.com/tag/illusion-of-transparency\"><span class=\"by_qf77EiaoMw7tH3GSr\">Illusion of transparency</span></a></li><li><a href=\"https://www.lesswrong.com/tag/status-quo-bias\"><span class=\"by_qf77EiaoMw7tH3GSr\">Status quo bias</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/reversal-test\"><span class=\"by_qf77EiaoMw7tH3GSr\">Reversal test</span></a></li></ul><h2 id=\"External_References\"><span class=\"by_XtphY3uYHwruKqDyG\">External References</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2008/04/arbitrary-silli.html\"><span class=\"by_XtphY3uYHwruKqDyG\">Arbitrary Silliness</span></a><span class=\"by_XtphY3uYHwruKqDyG\"> by </span><a href=\"https://www.lesswrong.com/tag/robin-hanson\"><span class=\"by_XtphY3uYHwruKqDyG\">Robin Hanson</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External References",
          "anchor": "External_References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 14,
    "description": {
      "markdown": "The **absurdity heuristic** classifies highly untypical situations as \"absurd\", or [impossible](https://www.lesswrong.com/tag/antiprediction). While normally very useful as a form of [epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene), allowing us to detect nonsense, it suffers from the same problems as the [representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic).\n\nThere are a number of situations in which the absurdity heuristic is wrong. A deep theory has to [override the intuitive expectation](https://www.lesswrong.com/tag/shut-up-and-multiply). Where you don't expect intuition to construct an [adequate model](https://www.lesswrong.com/tag/technical-explanation) of reality, classifying an idea as impossible may be [overconfident](https://www.lesswrong.com/tag/overconfidence). [The future is usually \"absurd\"](http://lesswrong.com/lw/j1/stranger_than_history/), although sometimes it's possible to [rigorously infer low bounds on capabilities of the future](https://www.lesswrong.com/tag/exploratory-engineering), proving possible what is intuitively absurd.\n\nSee also\n--------\n\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Antiprediction](https://www.lesswrong.com/tag/antiprediction)\n*   [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Exploratory engineering](https://www.lesswrong.com/tag/exploratory-engineering)\n*   [Illusion of transparency](https://www.lesswrong.com/tag/illusion-of-transparency)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias), [Reversal test](https://www.lesswrong.com/tag/reversal-test)\n\nExternal References\n-------------------\n\n*   [Arbitrary Silliness](http://www.overcomingbias.com/2008/04/arbitrary-silli.html) by [Robin Hanson](https://www.lesswrong.com/tag/robin-hanson)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb19c",
    "name": "Availability Heuristic",
    "core": null,
    "slug": "availability-heuristic",
    "tableOfContents": {
      "html": "<p><span class=\"by_mPipmBTniuABY5PQy\">The </span><strong><span class=\"by_mPipmBTniuABY5PQy\">availability heuristic</span></strong><span class=\"by_mPipmBTniuABY5PQy\"> judges the probability of events by the ease with which examples come to mind. Sometimes this heuristic serves us well, but </span><a href=\"https://www.lesswrong.com/tag/the-map-is-not-the-territory\"><span class=\"by_mPipmBTniuABY5PQy\">the map is not the territory</span></a><span><span class=\"by_mPipmBTniuABY5PQy\">; the frequency with which concepts occur in your thoughts need not reflect the frequency with which they occur in reality. Undue salience, selective reporting, even subtle features of how the human brain stores and recalls memories can distort our perceptions about the probability of events. Because </span><span class=\"by_sfxHxhdAnTjQFNsfE\">it is</span><span class=\"by_mPipmBTniuABY5PQy\"> easier to recall words by their first letter, people judge words that begin with the letter </span></span><i><span class=\"by_mPipmBTniuABY5PQy\">r</span></i><span class=\"by_mPipmBTniuABY5PQy\"> to be more frequent than words with </span><i><span class=\"by_mPipmBTniuABY5PQy\">r</span></i><span class=\"by_mPipmBTniuABY5PQy\"> as their third lettter, even though </span><i><span class=\"by_mPipmBTniuABY5PQy\">in fact</span></i><span><span class=\"by_mPipmBTniuABY5PQy\">, the latter is more frequent. </span><span class=\"by_sfxHxhdAnTjQFNsfE\">A second example is that</span><span class=\"by_mPipmBTniuABY5PQy\"> selective reporting by the media of dramatic tragedies makes them seem more frequent than more </span><span class=\"by_sfxHxhdAnTjQFNsfE\">threatening,</span><span class=\"by_mPipmBTniuABY5PQy\"> albeit </span><span class=\"by_sfxHxhdAnTjQFNsfE\">mundane,</span><span class=\"by_mPipmBTniuABY5PQy\"> risks.</span></span></p><h2 id=\"Blog_posts\"><span class=\"by_LoykQRMTxJFxwwdPy\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/j5/availability/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Availability</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://psychology.wikia.com/wiki/Availability_heuristic\"><span class=\"by_LoykQRMTxJFxwwdPy\">Availability heuristic</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> at Psychology Wiki</span></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/representativeness-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Representativeness heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/filtered-evidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Filtered evidence</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/No_one_knows_what_science_doesn't_know\"><span class=\"by_LoykQRMTxJFxwwdPy\">No one knows what science doesn't know</span></a></li><li><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span class=\"by_LoykQRMTxJFxwwdPy\">Absurdity heuristic</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 12,
    "description": {
      "markdown": "The **availability heuristic** judges the probability of events by the ease with which examples come to mind. Sometimes this heuristic serves us well, but [the map is not the territory](https://www.lesswrong.com/tag/the-map-is-not-the-territory); the frequency with which concepts occur in your thoughts need not reflect the frequency with which they occur in reality. Undue salience, selective reporting, even subtle features of how the human brain stores and recalls memories can distort our perceptions about the probability of events. Because it is easier to recall words by their first letter, people judge words that begin with the letter *r* to be more frequent than words with *r* as their third lettter, even though *in fact*, the latter is more frequent. A second example is that selective reporting by the media of dramatic tragedies makes them seem more frequent than more threatening, albeit mundane, risks.\n\nBlog posts\n----------\n\n*   [Availability](http://lesswrong.com/lw/j5/availability/)\n\nExternal links\n--------------\n\n*   [Availability heuristic](http://psychology.wikia.com/wiki/Availability_heuristic) at Psychology Wiki\n\nSee also\n--------\n\n*   [Representativeness heuristic](https://www.lesswrong.com/tag/representativeness-heuristic)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence)\n*   [No one knows what science doesn't know](https://wiki.lesswrong.com/wiki/No_one_knows_what_science_doesn't_know)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb192",
    "name": "Adding Up to Normality",
    "core": null,
    "slug": "adding-up-to-normality",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"It all adds up to normality\" is a common phrase used on LessWrong (also known here as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Egan's </span><span class=\"by_sKAL2jzfkYkDbQmx9\">law[1]). </span></span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Adding Up to Normality</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the property of an explanation which adds to our understanding without changing what we already know to be true. for example:</span></p><span class=\"by_sKAL2jzfkYkDbQmx9\">\n</span><ul><span class=\"by_sKAL2jzfkYkDbQmx9\">\n</span><li><span class=\"by_nmk3nLpQE89dMRzzN\">Apples didn't stop falling when General Relativity supplanted Newtonian mechanics.</span></li><span class=\"by_nmk3nLpQE89dMRzzN\">\n</span><li><span class=\"by_sKAL2jzfkYkDbQmx9\">As counterintuitive as quantum mechanics is, it all adds up to what we see in everyday life - </span><a href=\"https://www.lesswrong.com/tag/reality-is-normal\"><span class=\"by_sKAL2jzfkYkDbQmx9\">It's perfectly normal</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, and it always has been.</span></li><span class=\"by_sKAL2jzfkYkDbQmx9\">\n</span></ul><span class=\"by_sKAL2jzfkYkDbQmx9\">\n</span><p><span><span class=\"by_nmk3nLpQE89dMRzzN\">The purpose of a theory is to add up to observed </span><span class=\"by_sKAL2jzfkYkDbQmx9\">reality.</span><span class=\"by_nmk3nLpQE89dMRzzN\"> Science sets out to answer the question \"</span></span><em><span class=\"by_nmk3nLpQE89dMRzzN\">What</span></em><span class=\"by_nmk3nLpQE89dMRzzN\"> adds up to normality?\" and the answer turns out to be \"</span><em><span class=\"by_nmk3nLpQE89dMRzzN\">Quantum mechanics</span></em><span><span class=\"by_nmk3nLpQE89dMRzzN\"> adds up to </span><span class=\"by_sKAL2jzfkYkDbQmx9\">normality\" or \"General Relativity adds up to normality\".</span></span></p><span class=\"by_sKAL2jzfkYkDbQmx9\">\n</span><p><span><span class=\"by_4fh2AAe3n7oBviyxx\">A </span><span class=\"by_nmk3nLpQE89dMRzzN\">weaker extension of</span><span class=\"by_4fh2AAe3n7oBviyxx\"> this principle </span><span class=\"by_nmk3nLpQE89dMRzzN\">applies to</span><span class=\"by_4fh2AAe3n7oBviyxx\"> ethical and metaethical </span><span class=\"by_nmk3nLpQE89dMRzzN\">debates, which generally ought to end up explaining why you </span></span><em><span class=\"by_nmk3nLpQE89dMRzzN\">shouldn't</span></em><span class=\"by_nmk3nLpQE89dMRzzN\"> eat babies, rather than why you </span><em><span class=\"by_nmk3nLpQE89dMRzzN\">should</span></em><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><span class=\"by_nmk3nLpQE89dMRzzN\">\n</span><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><ul><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><li><a href=\"https://www.lesswrong.com/tag/reality-is-normal\"><span class=\"by_9c2mQkLQq6gQSksMs\">Reality is normal</span></a></li><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><li><a href=\"https://www.lesswrong.com/tag/quantum-physics\"><span class=\"by_9c2mQkLQq6gQSksMs\">Quantum mechanics</span></a></li><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_9c2mQkLQq6gQSksMs\">Metaethics sequence</span></a></li><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><li><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><span class=\"by_qf77EiaoMw7tH3GSr\">Occam's razor</span></a></li><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span></ul><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><p><span class=\"by_sKAL2jzfkYkDbQmx9\">[1] After the science fiction writer </span><a href=\"https://en.wikipedia.org/wiki/Greg_Egan\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Greg Egan</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, who first wrote this phrase in </span><a href=\"https://en.wikipedia.org/wiki/Quarantine_(Egan_novel)\"><span class=\"by_drtzESkjzzrctW2Hr\">Quarantine</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><span class=\"by_sKAL2jzfkYkDbQmx9\">\n</span>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 6,
    "description": {
      "markdown": "\"It all adds up to normality\" is a common phrase used on LessWrong (also known here as Egan's law\\[1\\]). **Adding Up to Normality** is the property of an explanation which adds to our understanding without changing what we already know to be true. for example:\n\n*   Apples didn't stop falling when General Relativity supplanted Newtonian mechanics.\n*   As counterintuitive as quantum mechanics is, it all adds up to what we see in everyday life - [It's perfectly normal](https://www.lesswrong.com/tag/reality-is-normal), and it always has been.\n\nThe purpose of a theory is to add up to observed reality. Science sets out to answer the question \"*What* adds up to normality?\" and the answer turns out to be \"*Quantum mechanics* adds up to normality\" or \"General Relativity adds up to normality\".\n\nA weaker extension of this principle applies to ethical and metaethical debates, which generally ought to end up explaining why you *shouldn't* eat babies, rather than why you *should*.\n\nSee also\n--------\n\n*   [Reality is normal](https://www.lesswrong.com/tag/reality-is-normal)\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-physics)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor)\n\n\\[1\\] After the science fiction writer [Greg Egan](https://en.wikipedia.org/wiki/Greg_Egan), who first wrote this phrase in [Quarantine](https://en.wikipedia.org/wiki/Quarantine_(Egan_novel))."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb187",
    "name": "Shut Up and Multiply",
    "core": null,
    "slug": "shut-up-and-multiply",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_mPipmBTniuABY5PQy\">Due to</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://wiki.lesswrong.com/wiki/scope_neglect\"><span class=\"by_mPipmBTniuABY5PQy\">scope neglect</span></a><span class=\"by_mPipmBTniuABY5PQy\">, </span><a href=\"https://en.wikipedia.org/wiki/Framing_effect_(psychology)\"><span class=\"by_mPipmBTniuABY5PQy\">framing effects</span></a><span class=\"by_mPipmBTniuABY5PQy\">, and other </span><a href=\"https://www.lesswrong.com/tag/bias\"><span class=\"by_mPipmBTniuABY5PQy\">cognitive biases</span></a><span class=\"by_mPipmBTniuABY5PQy\">, the result of an </span><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_mPipmBTniuABY5PQy\">expected utility</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> calculation </span><span class=\"by_nmk3nLpQE89dMRzzN\">executed correctly </span><span class=\"by_qf77EiaoMw7tH3GSr\">may </span><span class=\"by_nmk3nLpQE89dMRzzN\">produce an answer different from first intuition, making it \"intuitively unappealing\". &nbsp;If you can tell that it's probably the intuitions that went wrong and</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_mPipmBTniuABY5PQy\">not</span><span class=\"by_qf77EiaoMw7tH3GSr\"> the </span><span class=\"by_nmk3nLpQE89dMRzzN\">calculation,</span><span class=\"by_mPipmBTniuABY5PQy\"> the </span><span class=\"by_nmk3nLpQE89dMRzzN\">skill</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><strong><span class=\"by_qf77EiaoMw7tH3GSr\">shut up and multiply</span></strong><span><span class=\"by_nmk3nLpQE89dMRzzN\"> is the ability</span><span class=\"by_qf77EiaoMw7tH3GSr\"> to </span><span class=\"by_nmk3nLpQE89dMRzzN\">accept that, yes, sometimes</span><span class=\"by_mPipmBTniuABY5PQy\"> the </span><span class=\"by_nmk3nLpQE89dMRzzN\">expected utility </span><span class=\"by_mPipmBTniuABY5PQy\">math </span><span class=\"by_qf77EiaoMw7tH3GSr\">is </span><span class=\"by_nmk3nLpQE89dMRzzN\">correct and we need to deal with that. Contrast </span></span><a href=\"https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut\"><span class=\"by_nmk3nLpQE89dMRzzN\">do the math, then go with your gut</span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\">. &nbsp;If you're not sure which of these applies, use \"do the math, then go </span><span class=\"by_efbmXA2tiKgY9qRTD\">with</span><span class=\"by_nmk3nLpQE89dMRzzN\"> your gut\" until you've built up more experience.</span></span></p><p><span class=\"by_KTefAYDBNT64CLRau\">The specific application of Shut Up and Multiply to the </span><a href=\"http://lesswrong.com/lw/kn/torture_vs_dust_specks/\"><span><span class=\"by_KTefAYDBNT64CLRau\">Torture versus Dust </span><span class=\"by_aBHfQ4C5fSM4TPyTn\">Specks</span></span></a><span><span class=\"by_KTefAYDBNT64CLRau\"> case has proven quite contentious.</span><span class=\"by_nmk3nLpQE89dMRzzN\"> One reason this case was cited as an exemplar of where \"shut up and multiply\" </span></span><i><span class=\"by_nmk3nLpQE89dMRzzN\">should </span></i><span class=\"by_nmk3nLpQE89dMRzzN\">apply was a claim that the usual reasoning behind answering \"SPECKS\" can be </span><a href=\"https://www.lesswrong.com/posts/4ZzefKQwAtMo5yp99/circular-altruism\"><span class=\"by_nmk3nLpQE89dMRzzN\">reduced to circular preferences</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/hx/one_life_against_the_world/\"><span class=\"by_qf77EiaoMw7tH3GSr\">One Life Against the World</span></a></li><li><a href=\"http://lesswrong.com/lw/n3/circular_altruism/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Circular Altruism</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"http://lesswrong.com/lw/n9/the_intuitions_behind_utilitarianism/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The \"Intuitions\" Behind \"Utilitarianism\"</span></a></li><li><a href=\"http://lesswrong.com/lw/65/money_the_unit_of_caring/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Money: The Unit of Caring</span></a></li><li><a href=\"http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Purchase Fuzzies and Utilons Separately</span></a></li><li><a href=\"http://lesswrong.com/lw/kn/torture_vs_dust_specks/\"><span class=\"by_mPipmBTniuABY5PQy\">Torture vs. Dust Specks</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://meteuphoric.wordpress.com/2008/08/17/is-valuing-life-undervaluing-it/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Is Valuing Life Undervaluing it?</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Katja Grace</span></li><li><a href=\"http://squid314.livejournal.com/260949.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">You Can Put a Dollar Value on Human Life</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_LoykQRMTxJFxwwdPy\">Yvain</span></a></li><li><a href=\"http://mindingourway.com/the-value-of-a-life/\"><span class=\"by_KN3JyTFvGmse4e2uP\">The Value of a Life</span></a><span class=\"by_KN3JyTFvGmse4e2uP\"> by Nate Soares</span></li><li><a href=\"http://scientiststhesis.tumblr.com/post/108268823040/stormingtheivory-scientiststhesis\"><span class=\"by_KTefAYDBNT64CLRau\">Critical discussion by \"A scientists Thesis\" on Tumblr</span></a></li><li><a href=\"https://kierkeguardians.wordpress.com/2013/09/02/eliezer-yudkowsky-doesnt-understand-ethics/comment-page-1/\"><span class=\"by_KTefAYDBNT64CLRau\">Eliezer Yudkowsky doesn't undestand ethics by Sean Goedeke</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/scope-insensitivity\"><span class=\"by_9c2mQkLQq6gQSksMs\">Scope insensitivity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fuzzies\"><span class=\"by_9c2mQkLQq6gQSksMs\">Fuzzies</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://wiki.lesswrong.com/wiki/utils\"><span class=\"by_baGAQoNAH4hXaC6qf\">utils</span></a></li><li><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\"><span class=\"by_qf77EiaoMw7tH3GSr\">Pascal's mugging</span></a></li><li><a href=\"https://www.lesswrong.com/tag/bite-the-bullet\"><span class=\"by_9c2mQkLQq6gQSksMs\">Bite the bullet</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_qf77EiaoMw7tH3GSr\">Rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_LoykQRMTxJFxwwdPy\">Utilitarianism</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_LoykQRMTxJFxwwdPy\">expected utility</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 31,
    "description": {
      "markdown": "Due to [scope neglect](https://wiki.lesswrong.com/wiki/scope_neglect), [framing effects](https://en.wikipedia.org/wiki/Framing_effect_(psychology)), and other [cognitive biases](https://www.lesswrong.com/tag/bias), the result of an [expected utility](https://www.lesswrong.com/tag/expected-utility) calculation executed correctly may produce an answer different from first intuition, making it \"intuitively unappealing\".  If you can tell that it's probably the intuitions that went wrong and not the calculation, the skill **shut up and multiply** is the ability to accept that, yes, sometimes the expected utility math is correct and we need to deal with that. Contrast [do the math, then go with your gut](https://www.lesswrong.com/tag/do-the-math-then-burn-the-math-and-go-with-your-gut).  If you're not sure which of these applies, use \"do the math, then go with your gut\" until you've built up more experience.\n\nThe specific application of Shut Up and Multiply to the [Torture versus Dust Specks](http://lesswrong.com/lw/kn/torture_vs_dust_specks/) case has proven quite contentious. One reason this case was cited as an exemplar of where \"shut up and multiply\" *should* apply was a claim that the usual reasoning behind answering \"SPECKS\" can be [reduced to circular preferences](https://www.lesswrong.com/posts/4ZzefKQwAtMo5yp99/circular-altruism).\n\nBlog posts\n----------\n\n*   [One Life Against the World](http://lesswrong.com/lw/hx/one_life_against_the_world/)\n*   [Circular Altruism](http://lesswrong.com/lw/n3/circular_altruism/), [The \"Intuitions\" Behind \"Utilitarianism\"](http://lesswrong.com/lw/n9/the_intuitions_behind_utilitarianism/)\n*   [Money: The Unit of Caring](http://lesswrong.com/lw/65/money_the_unit_of_caring/)\n*   [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/)\n*   [Torture vs. Dust Specks](http://lesswrong.com/lw/kn/torture_vs_dust_specks/)\n\nExternal links\n--------------\n\n*   [Is Valuing Life Undervaluing it?](http://meteuphoric.wordpress.com/2008/08/17/is-valuing-life-undervaluing-it/) by Katja Grace\n*   [You Can Put a Dollar Value on Human Life](http://squid314.livejournal.com/260949.html) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [The Value of a Life](http://mindingourway.com/the-value-of-a-life/) by Nate Soares\n*   [Critical discussion by \"A scientists Thesis\" on Tumblr](http://scientiststhesis.tumblr.com/post/108268823040/stormingtheivory-scientiststhesis)\n*   [Eliezer Yudkowsky doesn't undestand ethics by Sean Goedeke](https://kierkeguardians.wordpress.com/2013/09/02/eliezer-yudkowsky-doesnt-understand-ethics/comment-page-1/)\n\nSee also\n--------\n\n*   [Scope insensitivity](https://www.lesswrong.com/tag/scope-insensitivity)\n*   [Fuzzies](https://www.lesswrong.com/tag/fuzzies), [utils](https://wiki.lesswrong.com/wiki/utils)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Bite the bullet](https://www.lesswrong.com/tag/bite-the-bullet)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [expected utility](https://www.lesswrong.com/tag/expected-utility)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb186",
    "name": "Hedonism",
    "core": null,
    "slug": "hedonism",
    "tableOfContents": {
      "html": "<p><span class=\"by_qf77EiaoMw7tH3GSr\">The term </span><strong><span class=\"by_qf77EiaoMw7tH3GSr\">hedonism</span></strong><span><span class=\"by_XzXbiS2zWYNdZdLW8\"> refers to a set of philosophies which hold that the highest goal is to maximize pleasure, or more </span><span class=\"by_qf77EiaoMw7tH3GSr\">precisely</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> pleasure minus pain.</span><span class=\"by_Co2dGXQxHAf92LHea\"> Egoistic hedonism refers to the maximization of personal pleasure, while universalist hedonism (more commonly known as hedonistic </span></span><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_Co2dGXQxHAf92LHea\">utilitarianism</span></a><span class=\"by_Co2dGXQxHAf92LHea\">) aims at the maximization of pleasure across all sentient beings.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/116/the_domain_of_your_utility_function/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Domain of Your Utility Function</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Peter_de_Blanc\"><span class=\"by_qf77EiaoMw7tH3GSr\">Peter de Blanc</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_Co2dGXQxHAf92LHea\">Utilitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_9c2mQkLQq6gQSksMs\">Shut up and multiply</span></a></li><li><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_9c2mQkLQq6gQSksMs\">Utility function</span></a></li><li><a href=\"https://www.lesswrong.com/tag/wanting-and-liking\"><span class=\"by_Co2dGXQxHAf92LHea\">Wanting and liking</span></a></li><li><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_qf77EiaoMw7tH3GSr\">Complexity of value</span></a></li><li><a href=\"https://www.lesswrong.com/tag/wireheading\"><span class=\"by_qf77EiaoMw7tH3GSr\">Wireheading</span></a></li><li><a href=\"https://www.lesswrong.com/tag/abolitionism\"><span class=\"by_Co2dGXQxHAf92LHea\">Abolitionism</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hedonium\"><span class=\"by_Co2dGXQxHAf92LHea\">Hedonium</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 26,
    "description": {
      "markdown": "The term **hedonism** refers to a set of philosophies which hold that the highest goal is to maximize pleasure, or more precisely pleasure minus pain. Egoistic hedonism refers to the maximization of personal pleasure, while universalist hedonism (more commonly known as hedonistic [utilitarianism](https://www.lesswrong.com/tag/utilitarianism)) aims at the maximization of pleasure across all sentient beings.\n\nBlog posts\n----------\n\n*   [The Domain of Your Utility Function](http://lesswrong.com/lw/116/the_domain_of_your_utility_function/) by [Peter de Blanc](https://wiki.lesswrong.com/wiki/Peter_de_Blanc)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Wanting and liking](https://www.lesswrong.com/tag/wanting-and-liking)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Wireheading](https://www.lesswrong.com/tag/wireheading)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n*   [Hedonium](https://wiki.lesswrong.com/wiki/Hedonium)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb182",
    "name": "Halo Effect",
    "core": null,
    "slug": "halo-effect",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Let's say that you know someone who not only seems very intelligent, but also honest, altruistic, kindly, and serene. You should be suspicious that some of these perceived characteristics are influencing your perception of the others. Maybe the person is genuinely intelligent, honest, and altruistic, but not all that kindly or serene. You should be suspicious if the people you know seem to separate too cleanly into devils and angels.</span></p></blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—</span><a href=\"http://lesswrong.com/lw/lj/the_halo_effect/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Halo Effect</span></u></a></p><h2 id=\"Main_post\"><span class=\"by_qf77EiaoMw7tH3GSr\">Main post</span></h2><ul><li><a href=\"http://lesswrong.com/lw/lj/the_halo_effect/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Halo Effect</span></a></li></ul><h2 id=\"Related_concepts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Related concepts</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/priming\"><span class=\"by_qf77EiaoMw7tH3GSr\">Priming</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/affect-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Affect heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/affective-death-spiral\"><span class=\"by_qf77EiaoMw7tH3GSr\">Affective death spiral</span></a></li><li><a href=\"https://www.lesswrong.com/tag/contagion-heuristic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Contagion heuristic</span></a></li></ul>",
      "sections": [
        {
          "title": "Main post",
          "anchor": "Main_post",
          "level": 1
        },
        {
          "title": "Related concepts",
          "anchor": "Related_concepts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 6,
    "description": {
      "markdown": "> Let's say that you know someone who not only seems very intelligent, but also honest, altruistic, kindly, and serene. You should be suspicious that some of these perceived characteristics are influencing your perception of the others. Maybe the person is genuinely intelligent, honest, and altruistic, but not all that kindly or serene. You should be suspicious if the people you know seem to separate too cleanly into devils and angels.\n\n—[The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/)\n\nMain post\n---------\n\n*   [The Halo Effect](http://lesswrong.com/lw/lj/the_halo_effect/)\n\nRelated concepts\n----------------\n\n*   [Priming](https://www.lesswrong.com/tag/priming), [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Contagion heuristic](https://www.lesswrong.com/tag/contagion-heuristic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb17d",
    "name": "Status Quo Bias",
    "core": null,
    "slug": "status-quo-bias",
    "tableOfContents": {
      "html": "<p><span class=\"by_qf77EiaoMw7tH3GSr\">The </span><strong><span class=\"by_qf77EiaoMw7tH3GSr\">status quo bias</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a </span><a href=\"https://wiki.lesswrong.com/wiki/cognitive_bias\"><span class=\"by_qf77EiaoMw7tH3GSr\">cognitive bias</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> for the status quo; in other words, people tend to avoid changing the established behavior or beliefs unless the pressure to change is sufficiently strong.</span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">The </span><a href=\"https://www.lesswrong.com/tag/reversal-test\"><span class=\"by_qf77EiaoMw7tH3GSr\">reversal test</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> is a technique for recognizing fallacious counterarguments against change. If the counterargument states that the change of some parameter in one direction is undesirable, the reversal test is to check whether either the change of that parameter in the opposite direction (away from status quo) is desirable, or that there are strong reasons to expect that the current value of the parameter is (at least locally) the optimal one.</span></p><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/reversal-test\"><span class=\"by_qf77EiaoMw7tH3GSr\">Reversal test</span></a></li><li><a href=\"https://www.lesswrong.com/tag/least-convenient-possible-world\"><span class=\"by_qf77EiaoMw7tH3GSr\">Least convenient possible world</span></a></li><li><a href=\"https://www.lesswrong.com/tag/cached-thought\"><span class=\"by_ChXHsXmDQFWZH638i\">Cached thought</span></a></li><li><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy\"><span class=\"by_qf77EiaoMw7tH3GSr\">Sunk cost fallacy</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 5,
    "description": {
      "markdown": "The **status quo bias** is a [cognitive bias](https://wiki.lesswrong.com/wiki/cognitive_bias) for the status quo; in other words, people tend to avoid changing the established behavior or beliefs unless the pressure to change is sufficiently strong.\n\nThe [reversal test](https://www.lesswrong.com/tag/reversal-test) is a technique for recognizing fallacious counterarguments against change. If the counterargument states that the change of some parameter in one direction is undesirable, the reversal test is to check whether either the change of that parameter in the opposite direction (away from status quo) is desirable, or that there are strong reasons to expect that the current value of the parameter is (at least locally) the optimal one.\n\nSee also\n--------\n\n*   [Reversal test](https://www.lesswrong.com/tag/reversal-test)\n*   [Least convenient possible world](https://www.lesswrong.com/tag/least-convenient-possible-world)\n*   [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Sunk cost fallacy](https://www.lesswrong.com/tag/sunk-cost-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb170",
    "name": "Group Selection",
    "core": null,
    "slug": "group-selection",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Group Selection </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">posits</span><span class=\"by_nmk3nLpQE89dMRzzN\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">natural selection might not operate at the level of genes in individuals, and instead also operate at genes in groups of individuals, i.e. selecting for genes for the group even at the expense</span><span class=\"by_nmk3nLpQE89dMRzzN\"> of the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">individual. For</span><span class=\"by_nmk3nLpQE89dMRzzN\"> example,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> you might posit</span><span class=\"by_nmk3nLpQE89dMRzzN\"> that human </span></span><a href=\"http://lesswrong.com/lw/mk/a_failed_justso_story/\"><span class=\"by_nmk3nLpQE89dMRzzN\">religion is an adaptation to make human groups more cohesive</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">, since religious groups outfight nonreligious groups.</span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolution</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Alienness_of_evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Alienness of evolution</span></a></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Consider two groups on different sides of a mountain: in group A, each mother gives birth to 2 males and 2 females; in group B, each mother gives birth to 3 females and 1 male. Group A and group B will have the same number of children, but group B will have 50% more grandchildren and 125% more great-grandchildren.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">But consider: The </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">rarer</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> males become, the more </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">reproductively valuable</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> they become - not to the group, but to the individual parent. If all the females are doing what's good for the group and birthing 1 male per 10 females, then you can make a genetic </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">killing</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> by birthing all males, each of whom will have (on average) ten times as many grandchildren as their female cousins.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">So while group selection ought to favor more girls, individual selection favors equal investment in male and female offspring. Just by looking at the statistics of a maternity ward, you can see that the quantitative balance between group selection forces and individual selection forces is overwhelmingly tilted in favor of individual selection in </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">Homo sapiens</span></i><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Group selection is extremely hard to make work mathematically. In </span><a href=\"http://www.gnxp.com/MT2/archives/003540.html\"><span class=\"by_nmk3nLpQE89dMRzzN\">this simulation</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">, for example, the cost to altruists is 3% of fitness, pure altruist groups have a fitness twice as great as pure selfish groups, the subpopulation size is 25, and 20% of all deaths are replaced with messengers from another group. The result is polymorphic for selfishness and altruism. If the subpopulation size is doubled to 50, selfishness is fixed. If the cost to altruists is increased to 6%, selfishness is fixed. If the altruistic benefit is decreased by half, selfishness is fixed or in large majority. Neighborhood-groups must be very small, with only around 5 members, for group selection to operate when the cost of altruism exceeds 10%.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">To the best of this editor's knowledge, no definite example of a group-level adaptation has ever been observed in a mammalian species. Ever.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Hence, postulating group selection in any species - let alone in humans - is guaranteed to make professional evolutionary biologists roll their eyes.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">It seems to be extremely popular among a certain sort of amateur evolutionary theorist, though - there's a certain sort of person who, if they don't know about the incredible mathematical difficulty, will find it very satisfying to speculate about adaptations for the good of the group.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">The historical fiasco of group selectionism is relied on as a (clear-cut) case in point of the dangers of </span><a href=\"https://www.lesswrong.com/tag/anthropomorphism\"><span class=\"by_nmk3nLpQE89dMRzzN\">anthropomorphism</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><h2 id=\"Main_posts\"><span class=\"by_nmk3nLpQE89dMRzzN\">Main posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/l5/evolving_to_extinction/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Evolving to Extinction</span></a></li><li><a href=\"http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/\"><span class=\"by_nmk3nLpQE89dMRzzN\">The Tragedy of Group Selectionism</span></a></li></ul><h2 id=\"Other_posts\"><span class=\"by_nmk3nLpQE89dMRzzN\">Other posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/\"><span><span class=\"by_nmk3nLpQE89dMRzzN\">Conjuring </span><span class=\"by_9c2mQkLQq6gQSksMs\">An</span><span class=\"by_nmk3nLpQE89dMRzzN\"> Evolution </span><span class=\"by_9c2mQkLQq6gQSksMs\">To</span><span class=\"by_nmk3nLpQE89dMRzzN\"> Serve You</span></span></a></li><li><a href=\"http://lesswrong.com/lw/st/anthropomorphic_optimism/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Anthropomorphic Optimism</span></a></li><li><a href=\"http://lesswrong.com/lw/su/contaminated_by_optimism/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Contaminated by Optimism</span></a></li><li><a href=\"http://lesswrong.com/lw/mk/a_failed_justso_story/\"><span class=\"by_nmk3nLpQE89dMRzzN\">A Failed Just-So Story</span></a></li><li><a href=\"http://lesswrong.com/lw/300/group_selection_update/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Group selection update</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://edge.org/conversation/the-false-allure-of-group-selection\"><span class=\"by_LoykQRMTxJFxwwdPy\">The False Allure of Group Selection</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"https://en.wikipedia.org/wiki/Steven_Pinker\"><span class=\"by_LoykQRMTxJFxwwdPy\">Steven Pinker</span></a></li></ul>",
      "sections": [
        {
          "title": "Main posts",
          "anchor": "Main_posts",
          "level": 1
        },
        {
          "title": "Other posts",
          "anchor": "Other_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 6,
    "description": {
      "markdown": "**Group Selection** posits that natural selection might not operate at the level of genes in individuals, and instead also operate at genes in groups of individuals, i.e. selecting for genes for the group even at the expense of the individual. For example, you might posit that human [religion is an adaptation to make human groups more cohesive](http://lesswrong.com/lw/mk/a_failed_justso_story/), since religious groups outfight nonreligious groups.\n\n*See also:* [Evolution](https://www.lesswrong.com/tag/evolution), [Alienness of evolution](https://wiki.lesswrong.com/wiki/Alienness_of_evolution)\n\nConsider two groups on different sides of a mountain: in group A, each mother gives birth to 2 males and 2 females; in group B, each mother gives birth to 3 females and 1 male. Group A and group B will have the same number of children, but group B will have 50% more grandchildren and 125% more great-grandchildren.\n\nBut consider: The *rarer* males become, the more *reproductively valuable* they become - not to the group, but to the individual parent. If all the females are doing what's good for the group and birthing 1 male per 10 females, then you can make a genetic *killing* by birthing all males, each of whom will have (on average) ten times as many grandchildren as their female cousins.\n\nSo while group selection ought to favor more girls, individual selection favors equal investment in male and female offspring. Just by looking at the statistics of a maternity ward, you can see that the quantitative balance between group selection forces and individual selection forces is overwhelmingly tilted in favor of individual selection in *Homo sapiens*.\n\nGroup selection is extremely hard to make work mathematically. In [this simulation](http://www.gnxp.com/MT2/archives/003540.html), for example, the cost to altruists is 3% of fitness, pure altruist groups have a fitness twice as great as pure selfish groups, the subpopulation size is 25, and 20% of all deaths are replaced with messengers from another group. The result is polymorphic for selfishness and altruism. If the subpopulation size is doubled to 50, selfishness is fixed. If the cost to altruists is increased to 6%, selfishness is fixed. If the altruistic benefit is decreased by half, selfishness is fixed or in large majority. Neighborhood-groups must be very small, with only around 5 members, for group selection to operate when the cost of altruism exceeds 10%.\n\nTo the best of this editor's knowledge, no definite example of a group-level adaptation has ever been observed in a mammalian species. Ever.\n\nHence, postulating group selection in any species - let alone in humans - is guaranteed to make professional evolutionary biologists roll their eyes.\n\nIt seems to be extremely popular among a certain sort of amateur evolutionary theorist, though - there's a certain sort of person who, if they don't know about the incredible mathematical difficulty, will find it very satisfying to speculate about adaptations for the good of the group.\n\nThe historical fiasco of group selectionism is relied on as a (clear-cut) case in point of the dangers of [anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism).\n\nMain posts\n----------\n\n*   [Evolving to Extinction](http://lesswrong.com/lw/l5/evolving_to_extinction/)\n*   [The Tragedy of Group Selectionism](http://lesswrong.com/lw/kw/the_tragedy_of_group_selectionism/)\n\nOther posts\n-----------\n\n*   [Conjuring An Evolution To Serve You](http://lesswrong.com/lw/l8/conjuring_an_evolution_to_serve_you/)\n*   [Anthropomorphic Optimism](http://lesswrong.com/lw/st/anthropomorphic_optimism/)\n*   [Contaminated by Optimism](http://lesswrong.com/lw/su/contaminated_by_optimism/)\n*   [A Failed Just-So Story](http://lesswrong.com/lw/mk/a_failed_justso_story/)\n*   [Group selection update](http://lesswrong.com/lw/300/group_selection_update/)\n\nExternal links\n--------------\n\n*   [The False Allure of Group Selection](http://edge.org/conversation/the-false-allure-of-group-selection) by [Steven Pinker](https://en.wikipedia.org/wiki/Steven_Pinker)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb16a",
    "name": "Adaptation Executors",
    "core": null,
    "slug": "adaptation-executors",
    "tableOfContents": {
      "html": "<p><span class=\"by_XtphY3uYHwruKqDyG\">Modeling biological systems as </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">Adaption Executors</span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\"> is</span><span class=\"by_nmk3nLpQE89dMRzzN\"> central principle of </span></span><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_nmk3nLpQE89dMRzzN\">evolutionary biology</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> in general, and </span><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_nmk3nLpQE89dMRzzN\">evolutionary psychology</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> in particular.</span></p><span class=\"by_nmk3nLpQE89dMRzzN\">\n</span><p><span class=\"by_nmk3nLpQE89dMRzzN\">If we regarded human taste buds as trying to </span><em><span class=\"by_nmk3nLpQE89dMRzzN\">maximize fitness</span></em><span class=\"by_nmk3nLpQE89dMRzzN\">, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an </span><em><span class=\"by_nmk3nLpQE89dMRzzN\">executing adaptation</span></em><span class=\"by_nmk3nLpQE89dMRzzN\"> - they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor. And now they are simply executing that adaptation - evolution operates on </span><a href=\"https://www.lesswrong.com/tag/slowness-of-evolution\"><span class=\"by_nmk3nLpQE89dMRzzN\">too slow a timescale</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> to re-adapt to such a recent condition.</span></p><span class=\"by_nmk3nLpQE89dMRzzN\">\n</span><p><span class=\"by_nmk3nLpQE89dMRzzN\">Evolution is ultimately just a historical-statistical macrofact about which ancestors </span><em><span class=\"by_nmk3nLpQE89dMRzzN\">did in fact</span></em><span class=\"by_nmk3nLpQE89dMRzzN\"> reproduce. These genes then execute again, as they did previously. And so the behavior of the organism is often better interpreted in terms of what worked in the past, rather than what should work in the future. The organism's genes are, in fact, the causal result of what worked in the past, and certainly not </span><a href=\"https://www.lesswrong.com/tag/teleology\"><span class=\"by_nmk3nLpQE89dMRzzN\">a causal result of the future</span></a><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></p><span class=\"by_nmk3nLpQE89dMRzzN\">\n</span><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span><ul><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span><li><a href=\"http://theviewfromhell.blogspot.com/2011/05/two-main-ways-in-which-evolution-is-not.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">The Two Main Ways In Which Evolution Is Not Our Friend</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Sister Y</span></li><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span></ul><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><ul><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><li><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_LoykQRMTxJFxwwdPy\">Evolution</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/stupidity-of-evolution\"><span class=\"by_LoykQRMTxJFxwwdPy\">stupidity of evolution</span></a></li><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span><li><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_qf77EiaoMw7tH3GSr\">Evolutionary psychology</span></a></li><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><li><a href=\"https://www.lesswrong.com/tag/superstimuli\"><span class=\"by_qf77EiaoMw7tH3GSr\">Superstimulus</span></a></li><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><li><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_qf77EiaoMw7tH3GSr\">Goodhart's law</span></a></li><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span><li><a href=\"https://www.lesswrong.com/tag/signaling\"><span class=\"by_LoykQRMTxJFxwwdPy\">Signaling</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/social-status\"><span class=\"by_LoykQRMTxJFxwwdPy\">status</span></a></li><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span><li><a href=\"https://www.lesswrong.com/tag/corrupted-hardware\"><span class=\"by_LoykQRMTxJFxwwdPy\">Corrupted hardware</span></a></li><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span></ul><span class=\"by_LoykQRMTxJFxwwdPy\">\n</span>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 18,
    "description": {
      "markdown": "Modeling biological systems as **Adaption Executors** is central principle of [evolutionary biology](https://www.lesswrong.com/tag/evolution) in general, and [evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology) in particular.\n\nIf we regarded human taste buds as trying to *maximize fitness*, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an *executing adaptation* \\- they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor. And now they are simply executing that adaptation - evolution operates on [too slow a timescale](https://www.lesswrong.com/tag/slowness-of-evolution) to re-adapt to such a recent condition.\n\nEvolution is ultimately just a historical-statistical macrofact about which ancestors *did in fact* reproduce. These genes then execute again, as they did previously. And so the behavior of the organism is often better interpreted in terms of what worked in the past, rather than what should work in the future. The organism's genes are, in fact, the causal result of what worked in the past, and certainly not [a causal result of the future](https://www.lesswrong.com/tag/teleology).\n\nExternal links\n--------------\n\n*   [The Two Main Ways In Which Evolution Is Not Our Friend](http://theviewfromhell.blogspot.com/2011/05/two-main-ways-in-which-evolution-is-not.html) by Sister Y\n\nSee also\n--------\n\n*   [Evolution](https://www.lesswrong.com/tag/evolution), [stupidity of evolution](https://www.lesswrong.com/tag/stupidity-of-evolution)\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology)\n*   [Superstimulus](https://www.lesswrong.com/tag/superstimuli)\n*   [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law)\n*   [Signaling](https://www.lesswrong.com/tag/signaling), [status](https://www.lesswrong.com/tag/social-status)\n*   [Corrupted hardware](https://www.lesswrong.com/tag/corrupted-hardware)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb163",
    "name": "Rationality Verification",
    "core": null,
    "slug": "rationality-verification",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Verifying rationality is possibly the</span><span class=\"by_nmk3nLpQE89dMRzzN\"> single largest problem</span></span></strong><span class=\"by_nmk3nLpQE89dMRzzN\"> for those desiring to create methods of systematically training for increased epistemic and instrumental </span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_nmk3nLpQE89dMRzzN\">rationality</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> - how to verify that the training actually </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">worked</span></i><span class=\"by_nmk3nLpQE89dMRzzN\">. Very Awful Things happen to people who set out to build training methods and schools without </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">strong</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> means of verification - two hideous historical examples being modern-day practice of martial arts (once the teachers are no longer fighting duels to the death) and the proliferating \"schools\" of psychotherapy (in the entire absence of any experimental evidence that one school worked better than another, and indeed, in the presence of experimental evidence that the schools did </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">no</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> have any discernible difference in effectiveness).</span></p><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://rationalpoker.com/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Rational Poker</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Louie</span></span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">(book) </span><a href=\"https://mitpress.mit.edu/books/rationality-quotient\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Rationality Quotient: Toward a Test of Rational Thinking</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (</span><a href=\"https://www.sciencedirect.com/science/article/abs/pii/S0160289616303555\"><span class=\"by_sKAL2jzfkYkDbQmx9\">review</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Stuart Ritchie, </span><a href=\"http://www.bayesianinvestor.com/blog/index.php/2017/01/07/rationality-quotient/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">review</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by </span><a href=\"https://www.lesswrong.com/users/pcm\"><span class=\"by_sKAL2jzfkYkDbQmx9\">pcm</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></li><li><a href=\"http://www.keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_EdPsy_2016.pdf\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Comprehensive Assessment of Rational Thinking</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (CART) by Keith E. Stanovich (</span><a href=\"https://sci-hub.se/10.1080/00461520.2015.1125787\"><span class=\"by_sKAL2jzfkYkDbQmx9\">sci-hub</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></li><li><a href=\"http://programs.clearerthinking.org/how_rational_are_you_really_take_the_test.html\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Thinking style quiz</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by </span><a href=\"https://www.clearerthinking.org/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">clearerthinking.org</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (</span><a href=\"https://www.lesswrong.com/posts/R2mPGwFvXSy4nCMgj/take-the-rationality-test-to-determine-your-rational#ChMTSFFZGPakafojF\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LW Discussion</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></li><li><a href=\"https://en.wikipedia.org/wiki/Cognitive_reflection_test\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Cognitive Reflection Test</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (</span><a href=\"https://www.lesswrong.com/posts/vk2yS8osapSch9Cz2/the-bat-and-ball-problem-revisited\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LW Discussion</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/exercises-problem-sets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Exercises / Problem-Sets</span></a></li><li><a href=\"https://www.lesswrong.com/tag/the-craft-and-the-community\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Craft and the Community</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_9c2mQkLQq6gQSksMs\">Rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationality-as-martial-art\"><span class=\"by_9c2mQkLQq6gQSksMs\">Rationality as martial art</span></a></li><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_LoykQRMTxJFxwwdPy\">Decision theory</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 14,
    "description": {
      "markdown": "**Verifying rationality is possibly the single largest problem** for those desiring to create methods of systematically training for increased epistemic and instrumental [rationality](https://www.lesswrong.com/tag/rationality) \\- how to verify that the training actually *worked*. Very Awful Things happen to people who set out to build training methods and schools without *strong* means of verification - two hideous historical examples being modern-day practice of martial arts (once the teachers are no longer fighting duels to the death) and the proliferating \"schools\" of psychotherapy (in the entire absence of any experimental evidence that one school worked better than another, and indeed, in the presence of experimental evidence that the schools did *no* have any discernible difference in effectiveness).\n\nExternal links\n--------------\n\n*   [Rational Poker](http://rationalpoker.com/) by Louie\n*   (book) [The Rationality Quotient: Toward a Test of Rational Thinking](https://mitpress.mit.edu/books/rationality-quotient) ([review](https://www.sciencedirect.com/science/article/abs/pii/S0160289616303555) by Stuart Ritchie, [review](http://www.bayesianinvestor.com/blog/index.php/2017/01/07/rationality-quotient/) by [pcm](https://www.lesswrong.com/users/pcm))\n*   [The Comprehensive Assessment of Rational Thinking](http://www.keithstanovich.com/Site/Research_on_Reasoning_files/Stanovich_EdPsy_2016.pdf) (CART) by Keith E. Stanovich ([sci-hub](https://sci-hub.se/10.1080/00461520.2015.1125787))\n*   [Thinking style quiz](http://programs.clearerthinking.org/how_rational_are_you_really_take_the_test.html) by [clearerthinking.org](https://www.clearerthinking.org/) ([LW Discussion](https://www.lesswrong.com/posts/R2mPGwFvXSy4nCMgj/take-the-rationality-test-to-determine-your-rational#ChMTSFFZGPakafojF))\n*   [Cognitive Reflection Test](https://en.wikipedia.org/wiki/Cognitive_reflection_test) ([LW Discussion](https://www.lesswrong.com/posts/vk2yS8osapSch9Cz2/the-bat-and-ball-problem-revisited))\n\nSee also\n--------\n\n*   [Exercises / Problem-Sets](https://www.lesswrong.com/tag/exercises-problem-sets)\n*   [The Craft and the Community](https://www.lesswrong.com/tag/the-craft-and-the-community)\n*   [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Rationality as martial art](https://www.lesswrong.com/tag/rationality-as-martial-art)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb162",
    "name": "Human Universal",
    "core": null,
    "slug": "human-universal",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">We should expect many traits to </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">human universal </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">since c</span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">omplex</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> adaptations within a sexually reproducing species need all of their parts, or almost all of their parts, to be universal within the gene pool.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Let's say that you have a complex adaptation with six interdependent parts, and that each of the six genes is independently at ten percent frequency in the population. The chance of assembling a whole working adaptation is literally a million to one; and the average fitness of the genes is tiny, and they will not increase in frequency.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">One bird may have slightly smoother feathers than another, but they will both have wings. A single mutation can be possessed by some lucky members of a species, and not by others - but single mutations don't correspond to the sort of complex, powerful machinery that underlies the potency of biology. By the time an adaptation gets to be really sophisticated with dozens of genes supporting its highly refined activity, every member of the species has some version of it - barring single mutations that knock out the whole complex.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Applying this logic to human brains in particular, we arrive at the explanation for what is called </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">the psychic unity of mankind</span></i><span class=\"by_nmk3nLpQE89dMRzzN\">. (Though </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_nmk3nLpQE89dMRzzN\">Eliezer Yudkowsky</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> has used the phrase \"psychological unity of humankind\" instead.) In every known culture, humans seem to experience joy, sadness, fear, disgust, anger, and surprise. In every known culture, these emotions are indicated by the same facial expressions. (Citation needed to Paul Ekman.)</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Donald E. Brown has compiled a list of </span><a href=\"http://condor.depaul.edu/~mfiddler/hyphen/humunivers.htm\"><span class=\"by_nmk3nLpQE89dMRzzN\">over a hundred human universals</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> - traits found in every culture ever studied, most of them so universal that anthropologists don't even bother to note them explicitly.</span></p><h2 id=\"Blog_posts\"><span class=\"by_LoykQRMTxJFxwwdPy\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/rl/the_psychological_unity_of_humankind/\"><span class=\"by_nmk3nLpQE89dMRzzN\">The Psychological Unity of Humankind</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qf77EiaoMw7tH3GSr\">Eliezer Yudkowsky</span></a></li><li><a href=\"http://lesswrong.com/lw/so/humans_in_funny_suits/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Humans in Funny Suits</span></a><span class=\"by_9c2mQkLQq6gQSksMs\"> by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qf77EiaoMw7tH3GSr\">Eliezer Yudkowsky</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qf77EiaoMw7tH3GSr\">External links</span></h2><ul><li><a href=\"http://condor.depaul.edu/~mfiddler/hyphen/humunivers.htm\"><span class=\"by_nmk3nLpQE89dMRzzN\">Human universals</span></a><span class=\"by_nmk3nLpQE89dMRzzN\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Donald_E._Brown\"><span class=\"by_nmk3nLpQE89dMRzzN\">Donald E. Brown</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_LoykQRMTxJFxwwdPy\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_LoykQRMTxJFxwwdPy\">Evolutionary psychology</span></a></li><li><a href=\"https://www.lesswrong.com/tag/typical-mind-fallacy\"><span class=\"by_LoykQRMTxJFxwwdPy\">Typical mind fallacy</span></a></li><li><a href=\"https://www.lesswrong.com/tag/anthropomorphism\"><span class=\"by_LoykQRMTxJFxwwdPy\">Anthropomorphism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_LoykQRMTxJFxwwdPy\">Complexity of value</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/fake-simplicity\"><span class=\"by_LoykQRMTxJFxwwdPy\">fake simplicity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/alien-values\"><span class=\"by_LoykQRMTxJFxwwdPy\">Alien values</span></a></li><li><a href=\"https://www.lesswrong.com/tag/mind-design-space\"><span class=\"by_LoykQRMTxJFxwwdPy\">Mind design space</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\"><span class=\"by_LoykQRMTxJFxwwdPy\">paperclip maximizer</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 3,
    "description": {
      "markdown": "We should expect many traits to **human universal** since c*omplex* adaptations within a sexually reproducing species need all of their parts, or almost all of their parts, to be universal within the gene pool.\n\nLet's say that you have a complex adaptation with six interdependent parts, and that each of the six genes is independently at ten percent frequency in the population. The chance of assembling a whole working adaptation is literally a million to one; and the average fitness of the genes is tiny, and they will not increase in frequency.\n\nOne bird may have slightly smoother feathers than another, but they will both have wings. A single mutation can be possessed by some lucky members of a species, and not by others - but single mutations don't correspond to the sort of complex, powerful machinery that underlies the potency of biology. By the time an adaptation gets to be really sophisticated with dozens of genes supporting its highly refined activity, every member of the species has some version of it - barring single mutations that knock out the whole complex.\n\nApplying this logic to human brains in particular, we arrive at the explanation for what is called *the psychic unity of mankind*. (Though [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) has used the phrase \"psychological unity of humankind\" instead.) In every known culture, humans seem to experience joy, sadness, fear, disgust, anger, and surprise. In every known culture, these emotions are indicated by the same facial expressions. (Citation needed to Paul Ekman.)\n\nDonald E. Brown has compiled a list of [over a hundred human universals](http://condor.depaul.edu/~mfiddler/hyphen/humunivers.htm) \\- traits found in every culture ever studied, most of them so universal that anthropologists don't even bother to note them explicitly.\n\nBlog posts\n----------\n\n*   [The Psychological Unity of Humankind](http://lesswrong.com/lw/rl/the_psychological_unity_of_humankind/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   [Humans in Funny Suits](http://lesswrong.com/lw/so/humans_in_funny_suits/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nExternal links\n--------------\n\n*   [Human universals](http://condor.depaul.edu/~mfiddler/hyphen/humunivers.htm) by [Donald E. Brown](https://wiki.lesswrong.com/wiki/Donald_E._Brown)\n\nSee also\n--------\n\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology)\n*   [Typical mind fallacy](https://www.lesswrong.com/tag/typical-mind-fallacy)\n*   [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value), [fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Alien values](https://www.lesswrong.com/tag/alien-values)\n*   [Mind design space](https://www.lesswrong.com/tag/mind-design-space), [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb157",
    "name": "Correspondence Bias",
    "core": null,
    "slug": "correspondence-bias",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_mPipmBTniuABY5PQy\">Correspondence bias</span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\"> (also</span><span class=\"by_mPipmBTniuABY5PQy\"> known as</span><span class=\"by_cn4SiEmqWbu7K9em5\"> the</span><span class=\"by_mPipmBTniuABY5PQy\"> </span></span><strong><span class=\"by_mPipmBTniuABY5PQy\">fundamental attribution error</span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">)</span><span class=\"by_mPipmBTniuABY5PQy\"> is the tendency to overestimate the the </span><span class=\"by_qf77EiaoMw7tH3GSr\">contribution</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> of </span><span class=\"by_mPipmBTniuABY5PQy\">lasting traits and dispositions</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> in </span><span class=\"by_mPipmBTniuABY5PQy\">determining people's behavior, as compared to situational effects. </span><span class=\"by_cn4SiEmqWbu7K9em5\">We might see someone kicking</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> a vending </span><span class=\"by_cn4SiEmqWbu7K9em5\">machine, and conclude they're</span><span class=\"by_mPipmBTniuABY5PQy\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">an </span><span class=\"by_mPipmBTniuABY5PQy\">inherently</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> angry </span><span class=\"by_qf77EiaoMw7tH3GSr\">person.</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> </span><span class=\"by_cn4SiEmqWbu7K9em5\">But maybe they just failed</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> a test, </span><span class=\"by_cn4SiEmqWbu7K9em5\">had their</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> driving license </span><span class=\"by_cn4SiEmqWbu7K9em5\">revoked, </span><span class=\"by_XzXbiS2zWYNdZdLW8\">and</span><span class=\"by_cn4SiEmqWbu7K9em5\"> had</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> the machine </span><span class=\"by_cn4SiEmqWbu7K9em5\">eat their</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> money for the third time this week. We think of the other person as an </span></span><a href=\"https://www.lesswrong.com/tag/human-universal\"><span class=\"by_XzXbiS2zWYNdZdLW8\">evil mutant</span></a><span class=\"by_XzXbiS2zWYNdZdLW8\"> and ourselves as righteous actors.</span></p><span class=\"by_XzXbiS2zWYNdZdLW8\">\n</span><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><ul><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><li><a href=\"https://www.lesswrong.com/tag/availability-heuristic\"><span class=\"by_9c2mQkLQq6gQSksMs\">Availability heuristic</span></a></li><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><li><a href=\"https://www.lesswrong.com/tag/human-universal\"><span class=\"by_9c2mQkLQq6gQSksMs\">Human universal</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">, </span><a href=\"https://www.lesswrong.com/tag/typical-mind-fallacy\"><span class=\"by_9c2mQkLQq6gQSksMs\">Typical mind fallacy</span></a></li><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span></ul><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><h2 id=\"References\"><span class=\"by_9c2mQkLQq6gQSksMs\">References</span></h2><span class=\"by_9c2mQkLQq6gQSksMs\">\n</span><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">DT Gilbert, PS Malone (1995) </span><span class=\"by_HoGziwmhpMGqGeWZy\">The Correspondence </span><span class=\"by_qf77EiaoMw7tH3GSr\">Bias (</span></span><a href=\"https://wjh-www.harvard.edu/~dtg/Gilbert%20&amp;%20Malone%20%28CORRESPONDENCE%20BIAS%29.pdf\"><span class=\"by_qf77EiaoMw7tH3GSr\">PDF</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">)</span></p><span class=\"by_qf77EiaoMw7tH3GSr\">\n</span>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 4,
    "description": {
      "markdown": "**Correspondence bias** (also known as the **fundamental attribution error**) is the tendency to overestimate the the contribution of lasting traits and dispositions in determining people's behavior, as compared to situational effects. We might see someone kicking a vending machine, and conclude they're an inherently angry person. But maybe they just failed a test, had their driving license revoked, and had the machine eat their money for the third time this week. We think of the other person as an [evil mutant](https://www.lesswrong.com/tag/human-universal) and ourselves as righteous actors.\n\nSee also\n--------\n\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Human universal](https://www.lesswrong.com/tag/human-universal), [Typical mind fallacy](https://www.lesswrong.com/tag/typical-mind-fallacy)\n\nReferences\n----------\n\nDT Gilbert, PS Malone (1995) The Correspondence Bias ([PDF](https://wjh-www.harvard.edu/~dtg/Gilbert%20&%20Malone%20%28CORRESPONDENCE%20BIAS%29.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb154",
    "name": "Scope Insensitivity",
    "core": null,
    "slug": "scope-insensitivity",
    "tableOfContents": {
      "html": "<p><span class=\"by_nmk3nLpQE89dMRzzN\">The human brain can't </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">emotionally</span></i><span><span class=\"by_nmk3nLpQE89dMRzzN\"> represent large </span><span class=\"by_LoykQRMTxJFxwwdPy\">quantities;</span><span class=\"by_nmk3nLpQE89dMRzzN\"> an environmental measure that will save 200,000 birds doesn't conjure anywhere near a hundred times the emotional impact and willingness-to-pay of a measure that would save 2,000 birds, even though </span></span><i><span class=\"by_nmk3nLpQE89dMRzzN\">in fact</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> the former measure </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">is</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> two orders of magnitude more effective.</span></p><h2 id=\"Blog_posts\"><span class=\"by_LoykQRMTxJFxwwdPy\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/hw/scope_insensitivity/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Scope Insensitivity</span></a></li><li><a href=\"http://lesswrong.com/lw/hx/one_life_against_the_world/\"><span class=\"by_9c2mQkLQq6gQSksMs\">One Life Against the World</span></a></li><li><a href=\"http://lesswrong.com/lw/h4/useless_medical_disclaimers/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Useless Medical Disclaimers</span></a></li><li><a href=\"http://lesswrong.com/lw/kn/torture_vs_dust_specks/\"><span class=\"by_nmk3nLpQE89dMRzzN\">Torture vs. Dust Specks</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">Shut up and multiply</span></a></li><li><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_qf77EiaoMw7tH3GSr\">Expected utility</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_qf77EiaoMw7tH3GSr\">Utilitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_LoykQRMTxJFxwwdPy\">Evolutionary psychology</span></a></li><li><a href=\"https://www.lesswrong.com/tag/emotions\"><span class=\"by_LoykQRMTxJFxwwdPy\">Emotion</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/alief\"><span class=\"by_LoykQRMTxJFxwwdPy\">alief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\"><span class=\"by_qf77EiaoMw7tH3GSr\">Pascal's mugging</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_qf77EiaoMw7tH3GSr\">Existential risk</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 6,
    "description": {
      "markdown": "The human brain can't *emotionally* represent large quantities; an environmental measure that will save 200,000 birds doesn't conjure anywhere near a hundred times the emotional impact and willingness-to-pay of a measure that would save 2,000 birds, even though *in fact* the former measure *is* two orders of magnitude more effective.\n\nBlog posts\n----------\n\n*   [Scope Insensitivity](http://lesswrong.com/lw/hw/scope_insensitivity/)\n*   [One Life Against the World](http://lesswrong.com/lw/hx/one_life_against_the_world/)\n*   [Useless Medical Disclaimers](http://lesswrong.com/lw/h4/useless_medical_disclaimers/)\n*   [Torture vs. Dust Specks](http://lesswrong.com/lw/kn/torture_vs_dust_specks/)\n\nSee also\n--------\n\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Expected utility](https://www.lesswrong.com/tag/expected-utility), [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology)\n*   [Emotion](https://www.lesswrong.com/tag/emotions), [alief](https://www.lesswrong.com/tag/alief)\n*   [Pascal's mugging](https://www.lesswrong.com/tag/pascal-s-mugging)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb153",
    "name": "Defensibility",
    "core": null,
    "slug": "defensibility",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Beware when you find yourself arguing that a policy is defensible rather than optimal; or that it has some benefit compared to the null action, rather than the best benefit of any action.</span></p></blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—</span><a href=\"http://lesswrong.com/lw/hu/the_third_alternative/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Third Alternative</span></u></a></p><h2 id=\"Blog_posts\"><span class=\"by_9c2mQkLQq6gQSksMs\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/hu/the_third_alternative/\"><span class=\"by_9c2mQkLQq6gQSksMs\">The Third Alternative</span></a><span class=\"by_9c2mQkLQq6gQSksMs\"> by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_9c2mQkLQq6gQSksMs\">Eliezer Yudkowsky</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/third-option\"><span class=\"by_qf77EiaoMw7tH3GSr\">Third option</span></a></li><li><a href=\"https://www.lesswrong.com/tag/arguments-as-soldiers\"><span class=\"by_qf77EiaoMw7tH3GSr\">Arguments as soldiers</span></a></li><li><a href=\"https://www.lesswrong.com/tag/not-technically-a-lie\"><span class=\"by_qf77EiaoMw7tH3GSr\">Not technically a lie</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 5,
    "description": {
      "markdown": "> Beware when you find yourself arguing that a policy is defensible rather than optimal; or that it has some benefit compared to the null action, rather than the best benefit of any action.\n\n—[The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/)\n\nBlog posts\n----------\n\n*   [The Third Alternative](http://lesswrong.com/lw/hu/the_third_alternative/) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n\nSee also\n--------\n\n*   [Third option](https://www.lesswrong.com/tag/third-option)\n*   [Arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers)\n*   [Not technically a lie](https://www.lesswrong.com/tag/not-technically-a-lie)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb150",
    "name": "Bystander Effect",
    "core": null,
    "slug": "bystander-effect",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">The </span><strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">bystander effect</span></strong><span class=\"by_6Fx2vQtkYSZkaCvAg\"> is a social psychological phenomenon in which individuals are less likely to offer help in an emergency situation when other people are present. The probability of help is inversely proportional to the number of bystanders. In other words, the greater the number of bystanders, the less likely it is that any one of them will help.</span></p></blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—Safety Canada, January 2004,&nbsp;</span><a href=\"http://www.safety-council.org/info/community/bystander.html\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"Don't Just Stand There - Do Something\"</span></u></a></p><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/conformity-bias\"><span class=\"by_9c2mQkLQq6gQSksMs\">Conformity bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_9c2mQkLQq6gQSksMs\">Shut up and multiply</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 13,
    "description": {
      "markdown": "> The **bystander effect** is a social psychological phenomenon in which individuals are less likely to offer help in an emergency situation when other people are present. The probability of help is inversely proportional to the number of bystanders. In other words, the greater the number of bystanders, the less likely it is that any one of them will help.\n\n—Safety Canada, January 2004, [\"Don't Just Stand There - Do Something\"](http://www.safety-council.org/info/community/bystander.html)\n\nSee also\n--------\n\n*   [Conformity bias](https://www.lesswrong.com/tag/conformity-bias)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb137",
    "name": "Hope",
    "core": null,
    "slug": "hope",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Persisting in clutching to a hope may be disastrous. </span><span class=\"by_9c2mQkLQq6gQSksMs\">Be ready to admit you </span><span class=\"by_qf77EiaoMw7tH3GSr\">lost, </span></span><a href=\"https://wiki.lesswrong.com/wiki/update\"><span class=\"by_qf77EiaoMw7tH3GSr\">update</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> on the data that says you did.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/gx/just_lose_hope_already/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Just Lose Hope Already</span></a></li><li><a href=\"http://lesswrong.com/lw/hl/lotteries_a_waste_of_hope/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Lotteries: A Waste of Hope</span></a></li><li><a href=\"http://lesswrong.com/lw/hm/new_improved_lottery/\"><span class=\"by_9c2mQkLQq6gQSksMs\">New Improved Lottery</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/fuzzies\"><span class=\"by_9c2mQkLQq6gQSksMs\">Fuzzies</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_9c2mQkLQq6gQSksMs\">Shut up and multiply</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Lotteries\"><span class=\"by_9c2mQkLQq6gQSksMs\">Lotteries</span></a></li><li><a href=\"https://www.lesswrong.com/tag/oops\"><span class=\"by_9c2mQkLQq6gQSksMs\">Oops</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 8,
    "description": {
      "markdown": "Persisting in clutching to a hope may be disastrous. Be ready to admit you lost, [update](https://wiki.lesswrong.com/wiki/update) on the data that says you did.\n\nBlog posts\n----------\n\n*   [Just Lose Hope Already](http://lesswrong.com/lw/gx/just_lose_hope_already/)\n*   [Lotteries: A Waste of Hope](http://lesswrong.com/lw/hl/lotteries_a_waste_of_hope/)\n*   [New Improved Lottery](http://lesswrong.com/lw/hm/new_improved_lottery/)\n\nSee also\n--------\n\n*   [Fuzzies](https://www.lesswrong.com/tag/fuzzies)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Lotteries](https://wiki.lesswrong.com/wiki/Lotteries)\n*   [Oops](https://www.lesswrong.com/tag/oops)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb12a",
    "name": "Rationalist Taboo",
    "core": null,
    "slug": "rationalist-taboo",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Rationalist </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Taboo</span></span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a technique for fighting muddles in discussions. By prohibiting the use of a certain word and all the words synonymous to it, people are forced to elucidate the specific contextual meaning they want to express, thus removing ambiguity otherwise present in a single word.</span></p><p><span><span class=\"by_KTefAYDBNT64CLRau\">Mainstream philosophy has a parallel procedure called \"unpacking\". Unpacking has the requirement that the contentious or doubtful </span><span class=\"by_hgWGnuMKKqfMhNQ9q\">term</span><span class=\"by_KTefAYDBNT64CLRau\"> be expanded </span><span class=\"by_mcKSiwq2TBrTMZS6X\">out--</span><span class=\"by_KTefAYDBNT64CLRau\">not just replaced with a </span><span class=\"by_mcKSiwq2TBrTMZS6X\">synonym--and only used as</span><span class=\"by_KTefAYDBNT64CLRau\"> a </span><span class=\"by_mcKSiwq2TBrTMZS6X\">collection of subcomponent concepts.</span></span></p><h2 id=\"Main_Posts\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Main </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Posts</span></span></h2><ul><li><a href=\"http://lesswrong.com/lw/nu/taboo_your_words/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Taboo Your Words</span></a></li><li><a href=\"http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Replace the Symbol with the Substance</span></a></li></ul><h2 id=\"Other_Posts\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Other </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Posts</span></span></h2><ul><li><a href=\"http://lesswrong.com/lw/np/disputing_definitions/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Disputing Definitions</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> - An example of how the technique helps.</span></li><li><a href=\"http://lesswrong.com/lw/ng/words_as_hidden_inferences/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Words as Hidden Inferences</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> - The mere presence of words can influence thinking, sometimes misleading it.</span></li><li><a href=\"http://lesswrong.com/lw/sp/detached_lever_fallacy/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Detached Lever Fallacy</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> - There is a lot of machinery hidden beneath the words, and rationalist's taboo is one way to make a step towards exposing it.</span></li></ul>",
      "sections": [
        {
          "title": "Main Posts",
          "anchor": "Main_Posts",
          "level": 1
        },
        {
          "title": "Other Posts",
          "anchor": "Other_Posts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 20,
    "description": {
      "markdown": "**Rationalist Taboo** is a technique for fighting muddles in discussions. By prohibiting the use of a certain word and all the words synonymous to it, people are forced to elucidate the specific contextual meaning they want to express, thus removing ambiguity otherwise present in a single word.\n\nMainstream philosophy has a parallel procedure called \"unpacking\". Unpacking has the requirement that the contentious or doubtful term be expanded out--not just replaced with a synonym--and only used as a collection of subcomponent concepts.\n\nMain Posts\n----------\n\n*   [Taboo Your Words](http://lesswrong.com/lw/nu/taboo_your_words/)\n*   [Replace the Symbol with the Substance](http://lesswrong.com/lw/nv/replace_the_symbol_with_the_substance/)\n\nOther Posts\n-----------\n\n*   [Disputing Definitions](http://lesswrong.com/lw/np/disputing_definitions/) \\- An example of how the technique helps.\n*   [Words as Hidden Inferences](http://lesswrong.com/lw/ng/words_as_hidden_inferences/) \\- The mere presence of words can influence thinking, sometimes misleading it.\n*   [Detached Lever Fallacy](http://lesswrong.com/lw/sp/detached_lever_fallacy/) \\- There is a lot of machinery hidden beneath the words, and rationalist's taboo is one way to make a step towards exposing it."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb128",
    "name": "Priming",
    "core": null,
    "slug": "priming",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Priming</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a </span><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_qf77EiaoMw7tH3GSr\">psychological</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> phenomenon that consists in early stimulus influencing later thoughts and behavior.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> The literature on Priming was heavily hit in the </span></span><a href=\"https://www.lesswrong.com/tag/replication-crisis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">replication crisis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> of the 2010's.</span></p><h2 id=\"Sequence_by_Scott_Alexander\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Sequence by </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Scott Alexander</span></span></h2><ul><li><a href=\"http://lesswrong.com/lw/3b/never_leave_your_room/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Never Leave Your Room</span></a></li><li><a href=\"http://lesswrong.com/lw/4w/bogus_pipeline_bona_fide_pipeline/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Bogus Pipeline, Bona Fide Pipeline</span></a></li><li><a href=\"http://lesswrong.com/lw/53/the_implicit_association_test/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Implicit Association Test</span></a></li><li><a href=\"http://lesswrong.com/lw/5d/fight_biases_or_route_around_them/\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Fight Biases, or Route Around </span><span class=\"by_9c2mQkLQq6gQSksMs\">Them?</span></span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/seeing-with-fresh-eyes\"><span class=\"by_qf77EiaoMw7tH3GSr\">Seeing with Fresh Eyes</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> (sequence)</span></li><li><a href=\"https://www.lesswrong.com/tag/affect-heuristic\"><span class=\"by_9c2mQkLQq6gQSksMs\">Affect heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/aversion-ugh-fields\"><span class=\"by_qf77EiaoMw7tH3GSr\">Ugh field</span></a></li></ul>",
      "sections": [
        {
          "title": "Sequence by Scott Alexander",
          "anchor": "Sequence_by_Scott_Alexander",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 15,
    "description": {
      "markdown": "**Priming** is a [psychological](https://www.lesswrong.com/tag/psychology) phenomenon that consists in early stimulus influencing later thoughts and behavior. The literature on Priming was heavily hit in the [replication crisis](https://www.lesswrong.com/tag/replication-crisis) of the 2010's.\n\nSequence by Scott Alexander\n---------------------------\n\n*   [Never Leave Your Room](http://lesswrong.com/lw/3b/never_leave_your_room/)\n*   [Bogus Pipeline, Bona Fide Pipeline](http://lesswrong.com/lw/4w/bogus_pipeline_bona_fide_pipeline/)\n*   [The Implicit Association Test](http://lesswrong.com/lw/53/the_implicit_association_test/)\n*   [Fight Biases, or Route Around Them?](http://lesswrong.com/lw/5d/fight_biases_or_route_around_them/)\n\nSee also\n--------\n\n*   [Seeing with Fresh Eyes](https://www.lesswrong.com/tag/seeing-with-fresh-eyes) (sequence)\n*   [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic)\n*   [Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb127",
    "name": "Hindsight Bias",
    "core": null,
    "slug": "hindsight-bias",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Hindsight </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Bias</span></span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a tendency to overestimate the </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">foreseeability</span></i><span><span class=\"by_nmk3nLpQE89dMRzzN\"> of events that have actually happened. I.e., subjects given information about X, and asked to assign </span><span class=\"by_qf77EiaoMw7tH3GSr\">a </span><span class=\"by_nmk3nLpQE89dMRzzN\">probability that X will happen, assign much lower probabilities than subjects who are given the same information about X, are told that X actually happened, and asked to estimate the foreseeable</span><span class=\"by_qf77EiaoMw7tH3GSr\"> probability of </span><span class=\"by_nmk3nLpQE89dMRzzN\">X. Experiments also show</span><span class=\"by_qf77EiaoMw7tH3GSr\"> that </span><span class=\"by_nmk3nLpQE89dMRzzN\">instructing subjects to \"avoid hindsight bias\" </span><span class=\"by_qf77EiaoMw7tH3GSr\">has </span><span class=\"by_nmk3nLpQE89dMRzzN\">little or no effect.</span></span></p><h2 id=\"External_Articles\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Articles</span></h2><ul><li><a href=\"https://slatestarcodex.com/2013/04/11/read-history-of-philosophy-backwards/\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Read</span><span class=\"by_nmk3nLpQE89dMRzzN\"> History</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> Of Philosophy Backwards</span></span></a></li></ul><h2 id=\"Related_Pages\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/forecast\"><span class=\"by_LoykQRMTxJFxwwdPy\">Forecast</span></a></li><li><a href=\"https://www.lesswrong.com/tag/confirmation-bias\"><span class=\"by_LoykQRMTxJFxwwdPy\">Positive bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/debiasing\"><span class=\"by_LoykQRMTxJFxwwdPy\">Debiasing</span></a></li><li><a href=\"https://www.lesswrong.com/tag/black-swans\"><span class=\"by_LoykQRMTxJFxwwdPy\">Black swan</span></a></li></ul>",
      "sections": [
        {
          "title": "External Articles",
          "anchor": "External_Articles",
          "level": 1
        },
        {
          "title": "Related Pages",
          "anchor": "Related_Pages",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 10,
    "description": {
      "markdown": "**Hindsight Bias** is a tendency to overestimate the *foreseeability* of events that have actually happened. I.e., subjects given information about X, and asked to assign a probability that X will happen, assign much lower probabilities than subjects who are given the same information about X, are told that X actually happened, and asked to estimate the foreseeable probability of X. Experiments also show that instructing subjects to \"avoid hindsight bias\" has little or no effect.\n\nExternal Articles\n-----------------\n\n*   [Read History Of Philosophy Backwards](https://slatestarcodex.com/2013/04/11/read-history-of-philosophy-backwards/)\n\nRelated Pages\n-------------\n\n*   [Forecast](https://www.lesswrong.com/tag/forecast)\n*   [Positive bias](https://www.lesswrong.com/tag/confirmation-bias)\n*   [Debiasing](https://www.lesswrong.com/tag/debiasing)\n*   [Black swan](https://www.lesswrong.com/tag/black-swans)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb125",
    "name": "Generalization From Fictional Evidence",
    "core": null,
    "slug": "generalization-from-fictional-evidence",
    "tableOfContents": {
      "html": "<p><span class=\"by_qf77EiaoMw7tH3GSr\">The logical fallacy of </span><strong><span class=\"by_qf77EiaoMw7tH3GSr\">generalization from fictional evidence</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> consists in drawing real-world conclusions based on statements invented and selected for the purpose of writing fiction.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">It was first coined by Eliezer Yudkowsky in </span><a href=\"http://www.longecity.org/forum/topic/1097-predicting-the-future-eliezer-yudkowsky/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">a talk he gave in 2003</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, and later in his essay </span><a href=\"https://www.lesswrong.com/posts/rHBdcHGLJ7KvLJQPk/the-logical-fallacy-of-generalization-from-fictional\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Logical Fallacy of Generalization from Fictional Evidence</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/availability-heuristic\"><span class=\"by_cn4SiEmqWbu7K9em5\">Availability heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/good-story-bias\"><span class=\"by_cn4SiEmqWbu7K9em5\">Good-story bias</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 13,
    "description": {
      "markdown": "The logical fallacy of **generalization from fictional evidence** consists in drawing real-world conclusions based on statements invented and selected for the purpose of writing fiction.\n\nIt was first coined by Eliezer Yudkowsky in [a talk he gave in 2003](http://www.longecity.org/forum/topic/1097-predicting-the-future-eliezer-yudkowsky/), and later in his essay [The Logical Fallacy of Generalization from Fictional Evidence](https://www.lesswrong.com/posts/rHBdcHGLJ7KvLJQPk/the-logical-fallacy-of-generalization-from-fictional).\n\nSee also\n--------\n\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Good-story bias](https://www.lesswrong.com/tag/good-story-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb124",
    "name": "Conformity Bias",
    "core": null,
    "slug": "conformity-bias",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Conformity bias</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a tendency to behave similarly to the others in a group, even if doing so goes against your own judgment.</span></p><h2 id=\"Blog_posts\"><span class=\"by_9c2mQkLQq6gQSksMs\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/m9/aschs_conformity_experiment/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Asch's Conformity Experiment</span></a></li><li><a href=\"http://lesswrong.com/lw/mb/lonely_dissent/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Lonely Dissent</span></a></li><li><a href=\"http://lesswrong.com/lw/ma/on_expressing_your_concerns/\"><span class=\"by_qf77EiaoMw7tH3GSr\">On Expressing Your Concerns</span></a></li><li><a href=\"http://lesswrong.com/lw/1ww/undiscriminating_skepticism/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Undiscriminating Skepticism</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/affective-death-spiral\"><span class=\"by_9c2mQkLQq6gQSksMs\">Affective death spiral</span></a></li><li><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_qf77EiaoMw7tH3GSr\">Groupthink</span></a></li><li><a href=\"https://www.lesswrong.com/tag/in-group-bias\"><span class=\"by_9c2mQkLQq6gQSksMs\">In-group bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/improper-belief\"><span class=\"by_LoykQRMTxJFxwwdPy\">Improper belief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/akrasia\"><span class=\"by_9c2mQkLQq6gQSksMs\">Akrasia</span></a></li><li><a href=\"https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor\"><span class=\"by_LoykQRMTxJFxwwdPy\">Death Spirals and the Cult Attractor</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 15,
    "description": {
      "markdown": "**Conformity bias** is a tendency to behave similarly to the others in a group, even if doing so goes against your own judgment.\n\nBlog posts\n----------\n\n*   [Asch's Conformity Experiment](http://lesswrong.com/lw/m9/aschs_conformity_experiment/)\n*   [Lonely Dissent](http://lesswrong.com/lw/mb/lonely_dissent/)\n*   [On Expressing Your Concerns](http://lesswrong.com/lw/ma/on_expressing_your_concerns/)\n*   [Undiscriminating Skepticism](http://lesswrong.com/lw/1ww/undiscriminating_skepticism/)\n\nSee also\n--------\n\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Groupthink](https://www.lesswrong.com/tag/groupthink)\n*   [In-group bias](https://www.lesswrong.com/tag/in-group-bias)\n*   [Improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Akrasia](https://www.lesswrong.com/tag/akrasia)\n*   [Death Spirals and the Cult Attractor](https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb118",
    "name": "Underconfidence",
    "core": null,
    "slug": "underconfidence",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_RyiDJDCG6R7xyAXzp\">Underconfidence</span></strong><span class=\"by_RyiDJDCG6R7xyAXzp\"> is the state of being more uncertain than is justified, given your </span><a href=\"https://wiki.lesswrong.com/wiki/prior\"><span class=\"by_RyiDJDCG6R7xyAXzp\">priors</span></a><span class=\"by_RyiDJDCG6R7xyAXzp\"> and the evidence you are aware of.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/gs/i_dont_know/\"><span class=\"by_9c2mQkLQq6gQSksMs\">\"I don't know.\"</span></a></li><li><a href=\"http://lesswrong.com/lw/c3/the_sin_of_underconfidence/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Sin of Underconfidence</span></a></li><li><a href=\"http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/\"><span class=\"by_LoykQRMTxJFxwwdPy\">The Importance of Self-Doubt</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by multifoliaterose</span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/overconfidence\"><span class=\"by_LoykQRMTxJFxwwdPy\">Overconfidence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_LoykQRMTxJFxwwdPy\">Modesty</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fallacy-of-gray\"><span class=\"by_9c2mQkLQq6gQSksMs\">Fallacy of gray</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/I_don't_know\"><span class=\"by_9c2mQkLQq6gQSksMs\">I don't know</span></a></li><li><a href=\"https://www.lesswrong.com/tag/motivated-skepticism\"><span class=\"by_LoykQRMTxJFxwwdPy\">Motivated skepticism</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 12,
    "description": {
      "markdown": "**Underconfidence** is the state of being more uncertain than is justified, given your [priors](https://wiki.lesswrong.com/wiki/prior) and the evidence you are aware of.\n\nBlog posts\n----------\n\n*   [\"I don't know.\"](http://lesswrong.com/lw/gs/i_dont_know/)\n*   [The Sin of Underconfidence](http://lesswrong.com/lw/c3/the_sin_of_underconfidence/)\n*   [The Importance of Self-Doubt](http://lesswrong.com/lw/2lr/the_importance_of_selfdoubt/) by multifoliaterose\n\nSee also\n--------\n\n*   [Overconfidence](https://www.lesswrong.com/tag/overconfidence)\n*   [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Fallacy of gray](https://www.lesswrong.com/tag/fallacy-of-gray)\n*   [I don't know](https://wiki.lesswrong.com/wiki/I_don't_know)\n*   [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb117",
    "name": "Tsuyoku Naritai",
    "core": null,
    "slug": "tsuyoku-naritai",
    "tableOfContents": {
      "html": "<h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\"><span class=\"by_RyiDJDCG6R7xyAXzp\">Tsuyoku Naritai! (I Want To Become Stronger)</span></a></li><li><a href=\"http://lesswrong.com/lw/h9/tsuyoku_vs_the_egalitarian_instinct/\"><span><span class=\"by_RyiDJDCG6R7xyAXzp\">Tsuyoku </span><span class=\"by_9c2mQkLQq6gQSksMs\">vs.</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> the </span><span class=\"by_9c2mQkLQq6gQSksMs\">Egalitarian Instinct</span></span></a></li><li><a href=\"http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/\"><span class=\"by_qf77EiaoMw7tH3GSr\">A Sense That More Is Possible</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_LoykQRMTxJFxwwdPy\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/egalitarianism\"><span class=\"by_LoykQRMTxJFxwwdPy\">Egalitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationality-as-martial-art\"><span class=\"by_LoykQRMTxJFxwwdPy\">Rationality as martial art</span></a></li><li><a href=\"https://www.lesswrong.com/tag/challenging-the-difficult\"><span class=\"by_LoykQRMTxJFxwwdPy\">Challenging the Difficult</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationality-is-systematized-winning\"><span class=\"by_LoykQRMTxJFxwwdPy\">Rationality is systematized winning</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 13,
    "description": {
      "markdown": "Blog posts\n----------\n\n*   [Tsuyoku Naritai! (I Want To Become Stronger)](http://lesswrong.com/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/)\n*   [Tsuyoku vs. the Egalitarian Instinct](http://lesswrong.com/lw/h9/tsuyoku_vs_the_egalitarian_instinct/)\n*   [A Sense That More Is Possible](http://lesswrong.com/lw/2c/a_sense_that_more_is_possible/)\n\nSee also\n--------\n\n*   [Egalitarianism](https://www.lesswrong.com/tag/egalitarianism)\n*   [Rationality as martial art](https://www.lesswrong.com/tag/rationality-as-martial-art)\n*   [Challenging the Difficult](https://www.lesswrong.com/tag/challenging-the-difficult)\n*   [Rationality is systematized winning](https://www.lesswrong.com/tag/rationality-is-systematized-winning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb108",
    "name": "Priors",
    "core": null,
    "slug": "priors",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_LedhurJxi3baDAKDZ\">In the</span><span class=\"by_qxJ28GN72aiJu96iF\"> context of</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span></span><a href=\"https://wiki.lesswrong.com/wiki/Bayes's_Theorem\"><span class=\"by_nmk3nLpQE89dMRzzN\">Bayes's Theorem</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">,</span><span class=\"by_LedhurJxi3baDAKDZ\"> </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">priors</span></strong><span><span class=\"by_LedhurJxi3baDAKDZ\"> </span><span class=\"by_qxJ28GN72aiJu96iF\">refer</span><span class=\"by_LedhurJxi3baDAKDZ\"> generically </span><span class=\"by_nmk3nLpQE89dMRzzN\">to </span><span class=\"by_LedhurJxi3baDAKDZ\">the</span><span class=\"by_nmk3nLpQE89dMRzzN\"> beliefs</span><span class=\"by_LedhurJxi3baDAKDZ\"> an agent holds regarding a fact, hypothesis or consequence, before being presented with evidence. </span><span class=\"by_qxJ28GN72aiJu96iF\">Upon being presented with new evidence, the</span><span class=\"by_LedhurJxi3baDAKDZ\"> agent</span><span class=\"by_qxJ28GN72aiJu96iF\"> can multiply their prior with a </span></span><a href=\"https://wiki.lesswrong.com/wiki/likelihood_distribution\"><span class=\"by_qxJ28GN72aiJu96iF\">likelihood distribution</span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\"> to calculate a </span><span class=\"by_qxJ28GN72aiJu96iF\">new (posterior)</span><span class=\"by_nmk3nLpQE89dMRzzN\"> probability </span><span class=\"by_qxJ28GN72aiJu96iF\">for their belief.</span></span></p><h2 id=\"Examples\"><span class=\"by_qxJ28GN72aiJu96iF\">Examples</span></h2><p><span><span class=\"by_qxJ28GN72aiJu96iF\">Suppose</span><span class=\"by_nmk3nLpQE89dMRzzN\"> you had a barrel containing some number of red and white balls. </span><span class=\"by_LedhurJxi3baDAKDZ\">You</span><span class=\"by_nmk3nLpQE89dMRzzN\"> start with the belief that each ball was independently assigned red color (vs. white color) at some fixed </span><span class=\"by_LedhurJxi3baDAKDZ\">probability. Furthermore,</span><span class=\"by_nmk3nLpQE89dMRzzN\"> you start out ignorant of this fixed probability (the parameter could </span><span class=\"by_LedhurJxi3baDAKDZ\">be </span><span class=\"by_nmk3nLpQE89dMRzzN\">anywhere between 0 and 1)</span><span class=\"by_LedhurJxi3baDAKDZ\">. Each</span><span class=\"by_nmk3nLpQE89dMRzzN\"> red ball you see</span><span class=\"by_LedhurJxi3baDAKDZ\"> then</span><span class=\"by_nmk3nLpQE89dMRzzN\"> makes it </span></span><i><span class=\"by_nmk3nLpQE89dMRzzN\">more</span></i><span><span class=\"by_nmk3nLpQE89dMRzzN\"> likely that</span><span class=\"by_e8voDq2aJLGcdWuw6\"> the next </span><span class=\"by_nmk3nLpQE89dMRzzN\">ball will be </span><span class=\"by_qxJ28GN72aiJu96iF\">red, following</span><span class=\"by_LedhurJxi3baDAKDZ\"> a</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span></span><a href=\"http://en.wikipedia.org/wiki/Rule_of_succession\"><span><span class=\"by_LedhurJxi3baDAKDZ\">Laplacian</span><span class=\"by_nmk3nLpQE89dMRzzN\"> Rule of </span><span class=\"by_qxJ28GN72aiJu96iF\">Succession</span></span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\">.</span><span class=\"by_qxJ28GN72aiJu96iF\"> For example, seeing 6 red balls out of 10 suggests that the initial probability used for assigning the balls a red color was .6, and that there's also a probability of .6 for the next ball being red.</span></span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">On the other hand, if you start out with the prior belief that the barrel contains exactly 10 red balls and 10 white balls, then each red ball you see makes it </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">less</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> likely that the next ball will be red (because there are fewer red balls remaining).</span></p><p><span><span class=\"by_nmk3nLpQE89dMRzzN\">Thus our prior </span><span class=\"by_qxJ28GN72aiJu96iF\">affects</span><span class=\"by_nmk3nLpQE89dMRzzN\"> how we interpret the evidence. The first prior is an inductive prior</span><span class=\"by_LedhurJxi3baDAKDZ\"> -</span><span class=\"by_nmk3nLpQE89dMRzzN\"> things that happened before are predicted to happen again with greater probability. The second prior is anti-</span><span class=\"by_LedhurJxi3baDAKDZ\">inductive -</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the more red balls we see, the fewer we expect to see</span><span class=\"by_e8voDq2aJLGcdWuw6\"> in the </span><span class=\"by_nmk3nLpQE89dMRzzN\">future.</span></span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">As a</span><span class=\"by_LedhurJxi3baDAKDZ\"> real life </span><span class=\"by_qxJ28GN72aiJu96iF\">example, consider</span><span class=\"by_LedhurJxi3baDAKDZ\"> two leaders from different political </span><span class=\"by_qxJ28GN72aiJu96iF\">parties. Each</span><span class=\"by_LedhurJxi3baDAKDZ\"> one has his own beliefs </span><span class=\"by_qxJ28GN72aiJu96iF\">- priors - </span><span class=\"by_LedhurJxi3baDAKDZ\">about social organization and the roles of people and government in society. </span><span class=\"by_qxJ28GN72aiJu96iF\">These</span><span class=\"by_LedhurJxi3baDAKDZ\"> differences </span><span class=\"by_qxJ28GN72aiJu96iF\">in priors </span><span class=\"by_LedhurJxi3baDAKDZ\">can be attributed to a wide range of factors, </span><span class=\"by_qxJ28GN72aiJu96iF\">ranging </span><span class=\"by_LedhurJxi3baDAKDZ\">from </span><span class=\"by_qxJ28GN72aiJu96iF\">their educational backgrounds</span><span class=\"by_LedhurJxi3baDAKDZ\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">hereditary differences</span><span class=\"by_LedhurJxi3baDAKDZ\"> in </span><span class=\"by_qxJ28GN72aiJu96iF\">personality.</span><span class=\"by_LedhurJxi3baDAKDZ\"> However, </span><span class=\"by_qxJ28GN72aiJu96iF\">neither can show</span><span class=\"by_LedhurJxi3baDAKDZ\"> that his </span><span class=\"by_qxJ28GN72aiJu96iF\">beliefs are</span><span class=\"by_LedhurJxi3baDAKDZ\"> better than</span><span class=\"by_qxJ28GN72aiJu96iF\"> those of</span><span class=\"by_LedhurJxi3baDAKDZ\"> the other, unless he can </span><span class=\"by_qxJ28GN72aiJu96iF\">show</span><span class=\"by_LedhurJxi3baDAKDZ\"> that his priors </span><span class=\"by_qxJ28GN72aiJu96iF\">were generated by sources which track reality</span><span class=\"by_LedhurJxi3baDAKDZ\"> better</span></span><a href=\"#fn1\"><sup><span class=\"by_qxJ28GN72aiJu96iF\">1</span></sup></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">Because carrying out any reasoning at all seems to require a prior of some kind, ideal Bayesians would need some sort of priors from the moment that they were born. The question of where an ideal Bayesian would get this prior from has occasionally been a matter of considerable controversy in the philosophy of probability.</span></p><h2 id=\"Updating_prior_probabilities\"><span class=\"by_LedhurJxi3baDAKDZ\">Updating prior probabilities</span></h2><p><span><span class=\"by_qxJ28GN72aiJu96iF\">In informal discussion, people often talk about \"updating\" their priors. This is technically incorrect,</span><span class=\"by_LedhurJxi3baDAKDZ\"> as </span><span class=\"by_qxJ28GN72aiJu96iF\">one does not change their</span><span class=\"by_LedhurJxi3baDAKDZ\"> prior probability, </span><span class=\"by_qxJ28GN72aiJu96iF\">but rather uses it to calculate</span><span class=\"by_LedhurJxi3baDAKDZ\"> a posterior probability.</span><span class=\"by_qxJ28GN72aiJu96iF\"> However, as this</span><span class=\"by_LedhurJxi3baDAKDZ\"> posterior probability </span><span class=\"by_qxJ28GN72aiJu96iF\">then becomes the</span><span class=\"by_LedhurJxi3baDAKDZ\"> prior</span><span class=\"by_qxJ28GN72aiJu96iF\"> probability</span><span class=\"by_LedhurJxi3baDAKDZ\"> for the next inference, </span><span class=\"by_qxJ28GN72aiJu96iF\">talking about \"updating one's priors\" is often a convenient shorthand.</span></span></p><h2 id=\"References\"><span class=\"by_qxJ28GN72aiJu96iF\">References</span></h2><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/hk/priors_as_mathematical_objects/\"><span class=\"by_e8voDq2aJLGcdWuw6\">Priors as Mathematical Objects</span></a></li><li><a href=\"http://lesswrong.com/lw/hg/inductive_bias/\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">\"</span><span class=\"by_e8voDq2aJLGcdWuw6\">Inductive </span><span class=\"by_9c2mQkLQq6gQSksMs\">Bias\"</span></span></a></li><li><a href=\"http://lesswrong.com/lw/s6/probability_is_subjectively_objective/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Probability is Subjectively Objective</span></a></li><li><a href=\"http://lesswrong.com/lw/em/bead_jar_guesses/\"><span class=\"by_w3rzcs3GwLDqgRpwo\">Bead Jar Guesses</span></a><span class=\"by_w3rzcs3GwLDqgRpwo\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Alicorn\"><span class=\"by_w3rzcs3GwLDqgRpwo\">Alicorn</span></a><span><span class=\"by_w3rzcs3GwLDqgRpwo\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">- Applied</span><span class=\"by_w3rzcs3GwLDqgRpwo\"> scenario about forming </span><span class=\"by_qf77EiaoMw7tH3GSr\">priors.</span></span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/evidence\"><span class=\"by_9c2mQkLQq6gQSksMs\">Evidence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/inductive-bias\"><span class=\"by_9c2mQkLQq6gQSksMs\">Inductive bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/belief-update\"><span class=\"by_9c2mQkLQq6gQSksMs\">Belief update</span></a></li></ul><h2 id=\"References1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ol><li><span class=\"by_qxJ28GN72aiJu96iF\">Robin Hanson (2006). \"Uncommon Priors Require Origin Disputes\". Theory and Decision 61 (4) 319–328. </span><a href=\"http://hanson.gmu.edu/prior.pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">http://hanson.gmu.edu/prior.pdf</span></a><a href=\"#fnref1\"><span class=\"by_qxJ28GN72aiJu96iF\">↩</span></a></li></ol>",
      "sections": [
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 1
        },
        {
          "title": "Updating prior probabilities",
          "anchor": "Updating_prior_probabilities",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References1",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 20,
    "description": {
      "markdown": "In the context of [Bayes's Theorem](https://wiki.lesswrong.com/wiki/Bayes's_Theorem), **priors** refer generically to the beliefs an agent holds regarding a fact, hypothesis or consequence, before being presented with evidence. Upon being presented with new evidence, the agent can multiply their prior with a [likelihood distribution](https://wiki.lesswrong.com/wiki/likelihood_distribution) to calculate a new (posterior) probability for their belief.\n\nExamples\n--------\n\nSuppose you had a barrel containing some number of red and white balls. You start with the belief that each ball was independently assigned red color (vs. white color) at some fixed probability. Furthermore, you start out ignorant of this fixed probability (the parameter could be anywhere between 0 and 1). Each red ball you see then makes it *more* likely that the next ball will be red, following a [Laplacian Rule of Succession](http://en.wikipedia.org/wiki/Rule_of_succession). For example, seeing 6 red balls out of 10 suggests that the initial probability used for assigning the balls a red color was .6, and that there's also a probability of .6 for the next ball being red.\n\nOn the other hand, if you start out with the prior belief that the barrel contains exactly 10 red balls and 10 white balls, then each red ball you see makes it *less* likely that the next ball will be red (because there are fewer red balls remaining).\n\nThus our prior affects how we interpret the evidence. The first prior is an inductive prior - things that happened before are predicted to happen again with greater probability. The second prior is anti-inductive - the more red balls we see, the fewer we expect to see in the future.\n\nAs a real life example, consider two leaders from different political parties. Each one has his own beliefs - priors - about social organization and the roles of people and government in society. These differences in priors can be attributed to a wide range of factors, ranging from their educational backgrounds to hereditary differences in personality. However, neither can show that his beliefs are better than those of the other, unless he can show that his priors were generated by sources which track reality better[^1^](#fn1).\n\nBecause carrying out any reasoning at all seems to require a prior of some kind, ideal Bayesians would need some sort of priors from the moment that they were born. The question of where an ideal Bayesian would get this prior from has occasionally been a matter of considerable controversy in the philosophy of probability.\n\nUpdating prior probabilities\n----------------------------\n\nIn informal discussion, people often talk about \"updating\" their priors. This is technically incorrect, as one does not change their prior probability, but rather uses it to calculate a posterior probability. However, as this posterior probability then becomes the prior probability for the next inference, talking about \"updating one's priors\" is often a convenient shorthand.\n\nReferences\n----------\n\nBlog posts\n----------\n\n*   [Priors as Mathematical Objects](http://lesswrong.com/lw/hk/priors_as_mathematical_objects/)\n*   [\"Inductive Bias\"](http://lesswrong.com/lw/hg/inductive_bias/)\n*   [Probability is Subjectively Objective](http://lesswrong.com/lw/s6/probability_is_subjectively_objective/)\n*   [Bead Jar Guesses](http://lesswrong.com/lw/em/bead_jar_guesses/) by [Alicorn](https://wiki.lesswrong.com/wiki/Alicorn) \\- Applied scenario about forming priors.\n\nSee also\n--------\n\n*   [Evidence](https://www.lesswrong.com/tag/evidence)\n*   [Inductive bias](https://www.lesswrong.com/tag/inductive-bias)\n*   [Belief update](https://www.lesswrong.com/tag/belief-update)\n\nReferences\n----------\n\n1.  Robin Hanson (2006). \"Uncommon Priors Require Origin Disputes\". Theory and Decision 61 (4) 319–328. [http://hanson.gmu.edu/prior.pdf](http://hanson.gmu.edu/prior.pdf)[↩](#fnref1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb105",
    "name": "Planning Fallacy",
    "core": null,
    "slug": "planning-fallacy",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Planing Fallacy</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a</span><span class=\"by_e8voDq2aJLGcdWuw6\"> common cognitive bias resulting in predicting absurdly short timeframes for planned projects, famously observed with, among other projects, the </span></span><a href=\"https://en.wikipedia.org/wiki/Sydney_Opera_House\"><span class=\"by_e8voDq2aJLGcdWuw6\">Sydney Opera House</span></a><span class=\"by_e8voDq2aJLGcdWuw6\">, completed ten years late and a hundred million dollars overbudget.</span></p><p><span class=\"by_e8voDq2aJLGcdWuw6\">Symptomatic of the Planning Fallacy is an assumption of a best-case scenario; people plan as if everything will go smoothly, as hoped for, with no unexpected delays. In practice, this is typically not the case, and delays quickly mount.</span></p><p><span><span class=\"by_e8voDq2aJLGcdWuw6\">The bias also seems to be related to taking an \"inside\", detail-oriented view of the project to be planned; studies show that the more detailed a plan is, the more </span><span class=\"by_qf77EiaoMw7tH3GSr\">optimistically</span><span class=\"by_e8voDq2aJLGcdWuw6\"> inaccurate it is likely to be.</span></span></p><h2 id=\"Debiasing_techniques\"><span><span class=\"by_e8voDq2aJLGcdWuw6\">Debiasing </span><span class=\"by_qf77EiaoMw7tH3GSr\">techniques</span></span></h2><p><span><span class=\"by_e8voDq2aJLGcdWuw6\">When possible, </span><span class=\"by_nLbwLhBaQeG6tCNDN\">take the </span></span><a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><span class=\"by_nLbwLhBaQeG6tCNDN\">outside view</span></a><span><span class=\"by_nLbwLhBaQeG6tCNDN\">. Avoid</span><span class=\"by_e8voDq2aJLGcdWuw6\"> estimating the time for a project by adding time estimates for sub-tasks; instead, look for previously </span><span class=\"by_9c2mQkLQq6gQSksMs\">completed</span><span class=\"by_e8voDq2aJLGcdWuw6\"> projects of similar type and scale, and base the estimate on how long those other projects took.</span></span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/jg/planning_fallacy/\"><span class=\"by_e8voDq2aJLGcdWuw6\">Planning Fallacy</span></a></li><li><a href=\"http://lesswrong.com/lw/rj/surface_analogies_and_deep_causes/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Surface Analogies and Deep Causes</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><span class=\"by_qf77EiaoMw7tH3GSr\">Outside view</span></a></li><li><a href=\"https://www.lesswrong.com/tag/near-far-thinking\"><span class=\"by_qf77EiaoMw7tH3GSr\">Near/far thinking</span></a></li><li><a href=\"https://www.lesswrong.com/tag/forecast\"><span class=\"by_qf77EiaoMw7tH3GSr\">Forecast</span></a></li></ul>",
      "sections": [
        {
          "title": "Debiasing techniques",
          "anchor": "Debiasing_techniques",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 10,
    "description": {
      "markdown": "The **Planing Fallacy** is a common cognitive bias resulting in predicting absurdly short timeframes for planned projects, famously observed with, among other projects, the [Sydney Opera House](https://en.wikipedia.org/wiki/Sydney_Opera_House), completed ten years late and a hundred million dollars overbudget.\n\nSymptomatic of the Planning Fallacy is an assumption of a best-case scenario; people plan as if everything will go smoothly, as hoped for, with no unexpected delays. In practice, this is typically not the case, and delays quickly mount.\n\nThe bias also seems to be related to taking an \"inside\", detail-oriented view of the project to be planned; studies show that the more detailed a plan is, the more optimistically inaccurate it is likely to be.\n\nDebiasing techniques\n--------------------\n\nWhen possible, take the [outside view](https://www.lesswrong.com/tag/inside-outside-view). Avoid estimating the time for a project by adding time estimates for sub-tasks; instead, look for previously completed projects of similar type and scale, and base the estimate on how long those other projects took.\n\nBlog posts\n----------\n\n*   [Planning Fallacy](http://lesswrong.com/lw/jg/planning_fallacy/)\n*   [Surface Analogies and Deep Causes](http://lesswrong.com/lw/rj/surface_analogies_and_deep_causes/)\n\nSee also\n--------\n\n*   [Outside view](https://www.lesswrong.com/tag/inside-outside-view)\n*   [Near/far thinking](https://www.lesswrong.com/tag/near-far-thinking)\n*   [Forecast](https://www.lesswrong.com/tag/forecast)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e9",
    "name": "Fuzzies",
    "core": null,
    "slug": "fuzzies",
    "tableOfContents": {
      "html": "<p><span class=\"by_9c2mQkLQq6gQSksMs\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">fuzzy</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a </span><span class=\"by_nLbwLhBaQeG6tCNDN\">hypothetical measurement</span><span class=\"by_9c2mQkLQq6gQSksMs\"> unit </span><span class=\"by_nLbwLhBaQeG6tCNDN\">for \"warm fuzzy feeling\" one gets from </span><span class=\"by_SnXuru6XzF555NDzE\">believing</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> that one </span><span class=\"by_SnXuru6XzF555NDzE\">has</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> done good. Unlike </span></span><a href=\"https://wiki.lesswrong.com/wiki/utils\"><span class=\"by_baGAQoNAH4hXaC6qf\">utils</span></a><span><span class=\"by_nLbwLhBaQeG6tCNDN\">, fuzzies can be earned through psychological tricks without regard for efficiency.</span><span class=\"by_qf77EiaoMw7tH3GSr\"> For this reason, it may be a good idea to separate the concerns for actually doing good, for which one might need to </span></span><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">shut up and multiply</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, and for earning fuzzies, to get psychological comfort.</span></p><h2 id=\"Blog_posts\"><span class=\"by_9c2mQkLQq6gQSksMs\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Purchase Fuzzies and Utilons Separately</span></a></li><li><a href=\"http://lesswrong.com/lw/bk/the_trouble_with_good/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Trouble With \"Good\"</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_qf77EiaoMw7tH3GSr\">Yvain</span></a></li><li><a href=\"http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Not for the Sake of Happiness (Alone)</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_nLbwLhBaQeG6tCNDN\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_qf77EiaoMw7tH3GSr\">Utility function</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">Shut up and multiply</span></a></li><li><a href=\"https://www.lesswrong.com/tag/hedon\"><span class=\"by_qf77EiaoMw7tH3GSr\">Hedon</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 7,
    "description": {
      "markdown": "A **fuzzy** is a hypothetical measurement unit for \"warm fuzzy feeling\" one gets from believing that one has done good. Unlike [utils](https://wiki.lesswrong.com/wiki/utils), fuzzies can be earned through psychological tricks without regard for efficiency. For this reason, it may be a good idea to separate the concerns for actually doing good, for which one might need to [shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), and for earning fuzzies, to get psychological comfort.\n\nBlog posts\n----------\n\n*   [Purchase Fuzzies and Utilons Separately](http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/)\n*   [The Trouble With \"Good\"](http://lesswrong.com/lw/bk/the_trouble_with_good/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Not for the Sake of Happiness (Alone)](http://lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/)\n\nSee also\n--------\n\n*   [Utility function](https://www.lesswrong.com/tag/utility-functions)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Hedon](https://www.lesswrong.com/tag/hedon)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0e2",
    "name": "Epistemic Hygiene",
    "core": null,
    "slug": "epistemic-hygiene",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_RyiDJDCG6R7xyAXzp\">Epistemic </span><span class=\"by_qf77EiaoMw7tH3GSr\">hygiene</span></span></strong><span><span class=\"by_RyiDJDCG6R7xyAXzp\"> consists of practices meant to allow accurate beliefs to spread within a community and </span><span class=\"by_qf77EiaoMw7tH3GSr\">keep</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> less accurate or biased beliefs contained. The practices are meant to serve an analogous purpose to normal hygiene and sanitation in containing disease. \"Good cognitive citizenship\" is another phrase that has been proposed for this concept</span></span><a href=\"#fn1\"><sup><span class=\"by_RyiDJDCG6R7xyAXzp\">1</span></sup></a><span class=\"by_RyiDJDCG6R7xyAXzp\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/1e/raising_the_sanity_waterline/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Raising the Sanity Waterline</span></a></li><li><a href=\"http://lesswrong.com/lw/u/the_ethic_of_handwashing_and_community_epistemic/\"><span class=\"by_RyiDJDCG6R7xyAXzp\">The ethic of hand-washing and community epistemic practice</span></a></li><li><a href=\"http://lesswrong.com/lw/6a/hygienic_anecdotes/\"><span class=\"by_RyiDJDCG6R7xyAXzp\">Hygienic Anecdotes</span></a></li><li><a href=\"http://lesswrong.com/lw/18b/reason_as_memetic_immune_disorder/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Reason as memetic immune disorder</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://bloggingheads.tv/diavlogs/17359\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Free Will: Good Cognitive Citizenship</span></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> with Will Wilkinson and Eliezer Yudkowsky</span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/least-convenient-possible-world\"><span class=\"by_9c2mQkLQq6gQSksMs\">Least convenient possible world</span></a></li><li><a href=\"https://www.lesswrong.com/tag/improper-belief\"><span class=\"by_9c2mQkLQq6gQSksMs\">Improper belief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/group-rationality\"><span class=\"by_LoykQRMTxJFxwwdPy\">Group rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/mind-killer\"><span class=\"by_9c2mQkLQq6gQSksMs\">Mind-killer</span></a></li><li><a href=\"https://www.lesswrong.com/tag/filtered-evidence\"><span class=\"by_9c2mQkLQq6gQSksMs\">Filtered evidence</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">, </span><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">absurdity</span><span class=\"by_9c2mQkLQq6gQSksMs\"> heuristic</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/rational-evidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Rational evidence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/burdensome-details\"><span class=\"by_qf77EiaoMw7tH3GSr\">Burdensome details</span></a></li><li><a href=\"https://www.lesswrong.com/tag/status-quo-bias\"><span class=\"by_qf77EiaoMw7tH3GSr\">Status quo bias</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 2,
    "description": {
      "markdown": "**Epistemic hygiene** consists of practices meant to allow accurate beliefs to spread within a community and keep less accurate or biased beliefs contained. The practices are meant to serve an analogous purpose to normal hygiene and sanitation in containing disease. \"Good cognitive citizenship\" is another phrase that has been proposed for this concept[^1^](#fn1).\n\nBlog posts\n----------\n\n*   [Raising the Sanity Waterline](http://lesswrong.com/lw/1e/raising_the_sanity_waterline/)\n*   [The ethic of hand-washing and community epistemic practice](http://lesswrong.com/lw/u/the_ethic_of_handwashing_and_community_epistemic/)\n*   [Hygienic Anecdotes](http://lesswrong.com/lw/6a/hygienic_anecdotes/)\n*   [Reason as memetic immune disorder](http://lesswrong.com/lw/18b/reason_as_memetic_immune_disorder/)\n\nExternal links\n--------------\n\n*   [Free Will: Good Cognitive Citizenship](http://bloggingheads.tv/diavlogs/17359) with Will Wilkinson and Eliezer Yudkowsky\n\nSee also\n--------\n\n*   [Least convenient possible world](https://www.lesswrong.com/tag/least-convenient-possible-world)\n*   [Improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Mind-killer](https://www.lesswrong.com/tag/mind-killer)\n*   [Filtered evidence](https://www.lesswrong.com/tag/filtered-evidence), [absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0d6",
    "name": "Cached Thoughts",
    "core": null,
    "slug": "cached-thoughts",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Cached Thoughts </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">are ideas, attitudes, and beliefs that a person has formed on some past occasion, and hasn't re-evaluated since then. The name references the concept of a </span><a href=\"https://en.wikipedia.org/wiki/Cache_(computing)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">cache</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> in computing: a component storing data that has been calculated or retrieved once, so that it</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">quickly available without needing to be recalculated or re-retrieved.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Groupthink</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/information-cascades\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information Cascades</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/status-quo-bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Status quo bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/semantic-stopsign\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Semantic Stopsign</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Separate_magisteria\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Separate Magisteria</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/rationalist-taboo\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalist Taboo</span></a></p><p><span><span class=\"by_fmTiLqp6mmXeLjwfN\">Cached thoughts can </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">be useful</span><span class=\"by_fmTiLqp6mmXeLjwfN\"> in </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">saving computational resources at </span><span class=\"by_fmTiLqp6mmXeLjwfN\">the </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">cost</span><span class=\"by_fmTiLqp6mmXeLjwfN\"> of </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">some memory load, and also at the risk of maintaining </span><span class=\"by_fmTiLqp6mmXeLjwfN\">a </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">belief</span><span class=\"by_fmTiLqp6mmXeLjwfN\"> </span><span class=\"by_7r4pRYHgRRMuxw9fL\">long past the point </span><span class=\"by_fmTiLqp6mmXeLjwfN\">when evidence should force an update. </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">In particular, cached</span><span class=\"by_fmTiLqp6mmXeLjwfN\"> thoughts can result in a lack of creative approaches to problem-</span><span class=\"by_7r4pRYHgRRMuxw9fL\">solving, as cached solutions may interfere with the formation of novel ones. </span><span class=\"by_ChXHsXmDQFWZH638i\">What is generally called </span></span><a href=\"https://www.lesswrong.com/tag/common-sense\"><span class=\"by_ChXHsXmDQFWZH638i\">common sense</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\"> is more or less a collection of cached </span><span class=\"by_qf77EiaoMw7tH3GSr\">thoughts.</span></span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In modern civilization particularly, no one can think fast enough to think their own thoughts. If I’d been abandoned in the woods as an infant, raised by wolves or silent robots, I would scarcely be recognizable as human. No one can think fast enough to recapitulate the wisdom of a hunter-gatherer tribe in one lifetime, starting from scratch. As for the wisdom of a literate civilization, forget it.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">But the flip side of this is that I continually see people who aspire to critical thinking, repeating back cached thoughts which were not invented by critical thinkers. – Eliezer Yudkowsky, </span><a href=\"https://www.lesswrong.com/posts/2MD3NMLBPCqPfnfre/cached-thoughts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cached Thoughts</span></a></p></blockquote><h2 id=\"Main_Post\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Main </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Post</span></span></h2><ul><li><a href=\"http://lesswrong.com/lw/k5/cached_thoughts/\"><span class=\"by_RyiDJDCG6R7xyAXzp\">Cached Thoughts</span></a></li></ul><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/\"><span class=\"by_ChXHsXmDQFWZH638i\">How to Seem (and Be) Deep</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\"> </span><span class=\"by_mPipmBTniuABY5PQy\">—</span><span class=\"by_qf77EiaoMw7tH3GSr\"> Just find ways of violating cached expectations.</span></span></li><li><a href=\"http://lesswrong.com/lw/ic/the_virtue_of_narrowness/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Virtue of Narrowness</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> and </span><a href=\"http://lesswrong.com/lw/k7/original_seeing/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Original Seeing</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_mPipmBTniuABY5PQy\">—</span><span class=\"by_qf77EiaoMw7tH3GSr\"> One way to fight cached patterns of thought is to focus on precise concepts.</span></span></li><li><a href=\"http://lesswrong.com/lw/d2/cached_procrastination/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Cached Procrastination</span></a></li><li><a href=\"http://lesswrong.com/lw/4e/cached_selves/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Cached Selves</span></a></li></ul>",
      "sections": [
        {
          "title": "Main Post",
          "anchor": "Main_Post",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 19,
    "description": {
      "markdown": "**Cached Thoughts** are ideas, attitudes, and beliefs that a person has formed on some past occasion, and hasn't re-evaluated since then. The name references the concept of a [cache](https://en.wikipedia.org/wiki/Cache_(computing)) in computing: a component storing data that has been calculated or retrieved once, so that it is quickly available without needing to be recalculated or re-retrieved.\n\n*See also*: [Groupthink](https://www.lesswrong.com/tag/groupthink), [Information Cascades](https://www.lesswrong.com/tag/information-cascades), [Status quo bias](https://www.lesswrong.com/tag/status-quo-bias), [Semantic Stopsign](https://www.lesswrong.com/tag/semantic-stopsign), [Separate Magisteria](https://wiki.lesswrong.com/wiki/Separate_magisteria), [Rationalist Taboo](https://www.lesswrong.com/tag/rationalist-taboo)\n\nCached thoughts can be useful in saving computational resources at the cost of some memory load, and also at the risk of maintaining a belief long past the point when evidence should force an update. In particular, cached thoughts can result in a lack of creative approaches to problem-solving, as cached solutions may interfere with the formation of novel ones. What is generally called [common sense](https://www.lesswrong.com/tag/common-sense) is more or less a collection of cached thoughts.\n\n> In modern civilization particularly, no one can think fast enough to think their own thoughts. If I’d been abandoned in the woods as an infant, raised by wolves or silent robots, I would scarcely be recognizable as human. No one can think fast enough to recapitulate the wisdom of a hunter-gatherer tribe in one lifetime, starting from scratch. As for the wisdom of a literate civilization, forget it.\n> \n> But the flip side of this is that I continually see people who aspire to critical thinking, repeating back cached thoughts which were not invented by critical thinkers. – Eliezer Yudkowsky, [Cached Thoughts](https://www.lesswrong.com/posts/2MD3NMLBPCqPfnfre/cached-thoughts)\n\nMain Post\n---------\n\n*   [Cached Thoughts](http://lesswrong.com/lw/k5/cached_thoughts/)\n\nNotable Posts\n-------------\n\n*   [How to Seem (and Be) Deep](http://lesswrong.com/lw/k8/how_to_seem_and_be_deep/) — Just find ways of violating cached expectations.\n*   [The Virtue of Narrowness](http://lesswrong.com/lw/ic/the_virtue_of_narrowness/) and [Original Seeing](http://lesswrong.com/lw/k7/original_seeing/) — One way to fight cached patterns of thought is to focus on precise concepts.\n*   [Cached Procrastination](http://lesswrong.com/lw/d2/cached_procrastination/)\n*   [Cached Selves](http://lesswrong.com/lw/4e/cached_selves/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5f5c37ee1b5cdee568cfb0cb",
    "name": "Affective Death Spiral",
    "core": null,
    "slug": "affective-death-spiral",
    "tableOfContents": {
      "html": "<p><span class=\"by_RyiDJDCG6R7xyAXzp\">An </span><strong><span class=\"by_RyiDJDCG6R7xyAXzp\">affective death spiral</span></strong><span class=\"by_KneTmopEjYGsaPYNi\"> (or </span><strong><span class=\"by_KneTmopEjYGsaPYNi\">happy death spiral</span></strong><span><span class=\"by_KneTmopEjYGsaPYNi\">)</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> occurs when positive attributes of a theory, person, or organization combine with the </span></span><a href=\"https://www.lesswrong.com/tag/halo-effect\"><span class=\"by_RyiDJDCG6R7xyAXzp\">Halo effect</span></a><span><span class=\"by_RyiDJDCG6R7xyAXzp\"> in a feedback loop, resulting in the subject of the affective death spiral being held in higher and higher regard. </span><span class=\"by_qf77EiaoMw7tH3GSr\">In effect, every positive thing said about the subject results in more than one additional nice thing to say about the subject on average. This cascades like a nuclear chain reaction. </span><span class=\"by_RyiDJDCG6R7xyAXzp\">This process creates theories that are believed for their own sake and organizations that exist solely to perpetuate </span><span class=\"by_ChXHsXmDQFWZH638i\">themselves, especially when combined with the social dynamics of </span></span><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_ChXHsXmDQFWZH638i\">groupthink</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\">.</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> Affective death spirals are </span><span class=\"by_ChXHsXmDQFWZH638i\">thus</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> a primary cause of cultishness.</span></span></p><p><span class=\"by_h48TMtPzfimsEobTm\">The same process can also occur with negative beliefs instead of positive, leading to a </span><strong><span class=\"by_h48TMtPzfimsEobTm\">death spiral of hate</span></strong><span class=\"by_h48TMtPzfimsEobTm\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_LoykQRMTxJFxwwdPy\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/lm/affective_death_spirals/\"><span class=\"by_RyiDJDCG6R7xyAXzp\">Affective Death Spirals</span></a></li><li><a href=\"http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Resist the Happy Death Spiral</span></a></li><li><a href=\"http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/\"><span class=\"by_h48TMtPzfimsEobTm\">When None Dare Urge Restraint</span></a></li><li><a href=\"http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Every Cause Wants To Be A Cult</span></a></li><li><a href=\"http://lesswrong.com/lw/lz/guardians_of_the_truth/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Guardians of the Truth</span></a></li><li><a href=\"http://lesswrong.com/lw/m1/guardians_of_ayn_rand/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Guardians of Ayn Rand</span></a></li><li><a href=\"http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/\"><span class=\"by_9c2mQkLQq6gQSksMs\">Evaporative Cooling of Group Beliefs</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/affect-heuristic\"><span class=\"by_9c2mQkLQq6gQSksMs\">Affect heuristic</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">, </span><a href=\"https://www.lesswrong.com/tag/halo-effect\"><span class=\"by_9c2mQkLQq6gQSksMs\">Halo effect</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">, </span><a href=\"https://www.lesswrong.com/tag/motivated-skepticism\"><span class=\"by_9c2mQkLQq6gQSksMs\">Motivated skepticism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/information-cascades\"><span class=\"by_qf77EiaoMw7tH3GSr\">Information cascade</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_qf77EiaoMw7tH3GSr\">Groupthink</span></a></li><li><a href=\"https://www.lesswrong.com/tag/in-group-bias\"><span class=\"by_9c2mQkLQq6gQSksMs\">In-group bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/religion\"><span class=\"by_LoykQRMTxJFxwwdPy\">Religion</span></a></li><li><a href=\"https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor\"><span class=\"by_LoykQRMTxJFxwwdPy\">Death Spirals and the Cult Attractor</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 11,
    "description": {
      "markdown": "An **affective death spiral** (or **happy death spiral**) occurs when positive attributes of a theory, person, or organization combine with the [Halo effect](https://www.lesswrong.com/tag/halo-effect) in a feedback loop, resulting in the subject of the affective death spiral being held in higher and higher regard. In effect, every positive thing said about the subject results in more than one additional nice thing to say about the subject on average. This cascades like a nuclear chain reaction. This process creates theories that are believed for their own sake and organizations that exist solely to perpetuate themselves, especially when combined with the social dynamics of [groupthink](https://www.lesswrong.com/tag/groupthink). Affective death spirals are thus a primary cause of cultishness.\n\nThe same process can also occur with negative beliefs instead of positive, leading to a **death spiral of hate**.\n\nBlog posts\n----------\n\n*   [Affective Death Spirals](http://lesswrong.com/lw/lm/affective_death_spirals/)\n*   [Resist the Happy Death Spiral](http://lesswrong.com/lw/ln/resist_the_happy_death_spiral/)\n*   [When None Dare Urge Restraint](http://lesswrong.com/lw/ls/when_none_dare_urge_restraint/)\n*   [Every Cause Wants To Be A Cult](http://lesswrong.com/lw/lv/every_cause_wants_to_be_a_cult/)\n*   [Guardians of the Truth](http://lesswrong.com/lw/lz/guardians_of_the_truth/)\n*   [Guardians of Ayn Rand](http://lesswrong.com/lw/m1/guardians_of_ayn_rand/)\n*   [Evaporative Cooling of Group Beliefs](http://lesswrong.com/lw/lr/evaporative_cooling_of_group_beliefs/)\n\nSee also\n--------\n\n*   [Affect heuristic](https://www.lesswrong.com/tag/affect-heuristic), [Halo effect](https://www.lesswrong.com/tag/halo-effect), [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism)\n*   [Information cascade](https://www.lesswrong.com/tag/information-cascades), [Groupthink](https://www.lesswrong.com/tag/groupthink)\n*   [In-group bias](https://www.lesswrong.com/tag/in-group-bias)\n*   [Religion](https://www.lesswrong.com/tag/religion)\n*   [Death Spirals and the Cult Attractor](https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AeqCtS3BaY3cwzKAs",
    "name": "Fermi Estimation",
    "core": false,
    "slug": "fermi-estimation",
    "tableOfContents": {
      "html": "<p><span class=\"by_Sp5wM4aRAhNERd4oY\">A </span><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Fermi Estimation</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a rough calculation which aims to be right within ~an order of magnitude, prioritizing getting a good enough to be useful answer without putting large amounts of thought and research in rather than being extremely accurate.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Forecasting &amp; Prediction</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "A **Fermi Estimation** is a rough calculation which aims to be right within ~an order of magnitude, prioritizing getting a good enough to be useful answer without putting large amounts of thought and research in rather than being extremely accurate.\n\n**Related Pages:** [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KNJvXJK8WaSgE6iAZ",
    "name": "Problem Formulation & Conceptualization",
    "core": false,
    "slug": "problem-formulation-and-conceptualization",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fmA6cA9psxibmH8MS",
    "name": "Mental Imagery / Visualization",
    "core": false,
    "slug": "mental-imagery-visualization",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iTe27Ced8s8bGuvMK",
    "name": "Intellectual Progress (Individual-Level)",
    "core": false,
    "slug": "intellectual-progress-individual-level",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">This tag is closely related to </span><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Scholarship &amp; Learning</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">. However, Scholarship &amp; Learning covers learning existing material, almost to the exclusion of developing new ideas. The present tag is exclusively about developing new ideas (or at least new-to-you ideas which </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">could have been</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> new to the world, if someone else hadn't invented them yet).</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">As you can see from the names, </span><a href=\"https://www.lesswrong.com/tag/intellectual-progress-society-level\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Intellectual Progress (Society Level)</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a related tag which deals with how progress happens for whole societies. The present tag deals with the same theme on an individual level.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Posts on making progress in small groups are also welcome to this tag. The relevant cutoff is not the number of people (one vs many), but rather, the </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">relevance to an individual who is trying to make intellectual progress.</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> If a post is on the nuts-and-bolts process of making intellectual progress happen, it belongs here.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "This tag is closely related to [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning). However, Scholarship & Learning covers learning existing material, almost to the exclusion of developing new ideas. The present tag is exclusively about developing new ideas (or at least new-to-you ideas which *could have been* new to the world, if someone else hadn't invented them yet).\n\nAs you can see from the names, [Intellectual Progress (Society Level)](https://www.lesswrong.com/tag/intellectual-progress-society-level) is a related tag which deals with how progress happens for whole societies. The present tag deals with the same theme on an individual level.\n\nPosts on making progress in small groups are also welcome to this tag. The relevant cutoff is not the number of people (one vs many), but rather, the *relevance to an individual who is trying to make intellectual progress.* If a post is on the nuts-and-bolts process of making intellectual progress happen, it belongs here."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zxmLyuTr7nujF523s",
    "name": "Zettelkasten",
    "core": false,
    "slug": "zettelkasten",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Zettelkasten</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> (German for \"slip box\") is a </span><a href=\"https://www.lesswrong.com/tag/note-taking\"><span class=\"by_pbREHuM5F5t5nyWqh\">note-taking</span></a><span class=\"by_pbREHuM5F5t5nyWqh\"> method popular amongst some LWers, and often praised for its scalability.</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">A clear explanation can be found at abramdemski's post: </span><a href=\"https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1\"><span class=\"by_pbREHuM5F5t5nyWqh\">https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Zettelkasten** (German for \"slip box\") is a [note-taking](https://www.lesswrong.com/tag/note-taking) method popular amongst some LWers, and often praised for its scalability.\n\nA clear explanation can be found at abramdemski's post: [https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1](https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T4GgauaEfp6dHsR5P",
    "name": "Memetics",
    "core": false,
    "slug": "memetics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Memetics </span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">is the</span><span class=\"by_SsduPgHwY2zeZpmKT\"> study of how ideas change and spread, in particular emphasizing analogy to evolutionary biology. In this framework, a key consideration is that an idea or memes ability to propagate in a given context, discussion, culture, or environment, is determined by its relative fitness in that environment, and this in turn determines its prevalence in the future.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Sequences:</span></strong><br><a href=\"https://www.lesswrong.com/s/3xKXGh9RXaYTYZYgZ\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Antimemetics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Isusr</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Evolution</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Evolutionary Psychology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/information-cascades\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Information Cascades</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/information-hazards\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Information Hazards</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/belief\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Belief</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/religion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Religion</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Social &amp; Cultural Dynamics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/writing-communication-method\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Writing (communication method)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/censorship\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Censorship</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/cultural-knowledge\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Cultural knowledge</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/simulacrum-levels\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Simulacrum Levels</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/social-media\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Social Media</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://en.wikipedia.org/wiki/Memetics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia page</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "**Memetics** is the study of how ideas change and spread, in particular emphasizing analogy to evolutionary biology. In this framework, a key consideration is that an idea or memes ability to propagate in a given context, discussion, culture, or environment, is determined by its relative fitness in that environment, and this in turn determines its prevalence in the future.\n\n**Sequences:**  \n[Antimemetics](https://www.lesswrong.com/s/3xKXGh9RXaYTYZYgZ) by Isusr\n\n**Related Pages:** [Evolution](https://www.lesswrong.com/tag/evolution), [Evolutionary Psychology](https://www.lesswrong.com/tag/evolutionary-psychology), [Information Cascades](https://www.lesswrong.com/tag/information-cascades), [Information Hazards](https://www.lesswrong.com/tag/information-hazards), [Belief](https://www.lesswrong.com/tag/belief), [Religion](https://www.lesswrong.com/tag/religion), [Social & Cultural Dynamics](https://www.lesswrong.com/tag/social-and-cultural-dynamics), [Writing (communication method)](https://www.lesswrong.com/tag/writing-communication-method), [Censorship](https://www.lesswrong.com/tag/censorship), [Cultural knowledge](https://www.lesswrong.com/tag/cultural-knowledge), [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels), [Social Media](https://www.lesswrong.com/tag/social-media)\n\n**See also:** [Wikipedia page](https://en.wikipedia.org/wiki/Memetics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "puBcCq7aRwKoa7pXX",
    "name": "Note-Taking",
    "core": false,
    "slug": "note-taking",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Taking notes is writing that's primarily for yourself -- whether for memory, or for study, or for reference, or to generate or develop ideas.&nbsp;</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/zettelkasten\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Zettelkasten</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Scholarship &amp; Learning</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/spaced-repetition\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Spaced Repetition</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "Taking notes is writing that's primarily for yourself -- whether for memory, or for study, or for reference, or to generate or develop ideas. \n\n**Related Pages:** [Zettelkasten](https://www.lesswrong.com/tag/zettelkasten), [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning), [Spaced Repetition](https://www.lesswrong.com/tag/spaced-repetition)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o4rMP6GJto7ccBL3a",
    "name": "Travel",
    "core": false,
    "slug": "travel",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kcj6MciuGPPK8rmCn",
    "name": "References (Language)",
    "core": false,
    "slug": "references-language",
    "tableOfContents": {
      "html": "<p><span class=\"by_XLwKyCK7JmC292ZCC\">One of the basic functions of language is referring to entities. This tag is for posting delving into the nature of what this is.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "One of the basic functions of language is referring to entities. This tag is for posting delving into the nature of what this is."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wdLqQnzdgiYpDXEWH",
    "name": "Infra-Bayesianism",
    "core": false,
    "slug": "infra-bayesianism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Infra-Bayesianism</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a new approach to </span><a href=\"https://www.lesswrong.com/tag/epistemology\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">epistemology</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> / </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">decision theory</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> / reinforcement learning theory, which builds on \"imprecise probability\" to solve the problem of prior misspecification / grain-of-truth / nonrealizability which plagues </span><a href=\"https://www.lesswrong.com/tag/bayesianism\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Bayesianism</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and Bayesian reinforcement learning.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Infra-Bayesianism also naturally leads to an implementation of </span><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">UDT</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and (more speculatively at this stage) has applications to multi-agent theory, </span></span><a href=\"https://www.lesswrong.com/tag/embedded-agency\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">embedded agency </span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">and reflection.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See the </span><a href=\"https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Infra-</span><span class=\"by_uaeRT9JZ4zAsnac8s\">Bayesianism</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Sequence</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "**Infra-Bayesianism** is a new approach to [epistemology](https://www.lesswrong.com/tag/epistemology) / [decision theory](https://www.lesswrong.com/tag/decision-theory) / reinforcement learning theory, which builds on \"imprecise probability\" to solve the problem of prior misspecification / grain-of-truth / nonrealizability which plagues [Bayesianism](https://www.lesswrong.com/tag/bayesianism) and Bayesian reinforcement learning.\n\nInfra-Bayesianism also naturally leads to an implementation of [UDT](https://www.lesswrong.com/tag/updateless-decision-theory), and (more speculatively at this stage) has applications to multi-agent theory, [embedded agency](https://www.lesswrong.com/tag/embedded-agency) and reflection.\n\nSee the [Infra-Bayesianism Sequence](https://www.lesswrong.com/posts/zB4f7QqKhBHa5b37a/introduction-to-the-infra-bayesianism-sequence)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GoxSGTqcd3WRR82i4",
    "name": "Eschatology",
    "core": false,
    "slug": "eschatology",
    "tableOfContents": {
      "html": "<p><span class=\"by_9zJ7ffPXRTMyAqfPh\">Discussion of plausible far future conditions</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "Discussion of plausible far future conditions"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "se3XDuQ4xbeWvu4eF",
    "name": "80,000 Hours",
    "core": false,
    "slug": "80-000-hours",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">80,000 Hours</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is an organization in the Effective Altruist community that gives career advice. The name comes from an average career containing about 80,000 hours of work.</span></p><h3 id=\"See_also\"><span class=\"by_Sp5wM4aRAhNERd4oY\">See also</span></h3><p><a href=\"https://80000hours.org/\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Official Website</span></a></p>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 10,
    "description": {
      "markdown": "**80,000 Hours** is an organization in the Effective Altruist community that gives career advice. The name comes from an average career containing about 80,000 hours of work.\n\n### See also\n\n[Official Website](https://80000hours.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "u4eLsvP4kY2qxxsr4",
    "name": "Neuralink",
    "core": false,
    "slug": "neuralink",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Neuralink</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is a company founded by Elon Musk that does human-brain interfacing.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"lesswrong.com/tag/neuroscience\"><span class=\"by_HoGziwmhpMGqGeWZy\">Neuroscience</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"brain-computer-interfaces\"><span class=\"by_HoGziwmhpMGqGeWZy\">Brain-Computer Interfaces</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "**Neuralink** is a company founded by Elon Musk that does human-brain interfacing.\n\nSee also: [Neuroscience](lesswrong.com/tag/neuroscience), [Brain-Computer Interfaces](brain-computer-interfaces)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xEZwTHPd5AWpgQx9w",
    "name": "GiveWell",
    "core": false,
    "slug": "givewell",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">GiveWell</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is an organization that evaluates the effectiveness of charities and recommends effective charities. It is associated with the </span><a href=\"http://lesswrong.com/tag/effective-altruism\"><span class=\"by_HoGziwmhpMGqGeWZy\">Effective Altruist</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> movement.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "**GiveWell** is an organization that evaluates the effectiveness of charities and recommends effective charities. It is associated with the [Effective Altruist](http://lesswrong.com/tag/effective-altruism) movement."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hMXoyTAKxvCcsQBKf",
    "name": "Libertarianism",
    "core": false,
    "slug": "libertarianism",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zs4nYLkNr7Rbo4mAP",
    "name": "Utilitarianism",
    "core": false,
    "slug": "utilitarianism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Utilitarianism </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a</span><span class=\"by_cn4SiEmqWbu7K9em5\"> moral </span></span><a href=\"https://www.lesswrong.com/tag/philosophy\"><span class=\"by_cn4SiEmqWbu7K9em5\">philosophy</span></a><span><span class=\"by_cn4SiEmqWbu7K9em5\"> that says</span><span class=\"by_qf77EiaoMw7tH3GSr\"> that</span><span class=\"by_cn4SiEmqWbu7K9em5\"> what matters is the sum of everyone's welfare, or the \"greatest good for the greatest number\".</span></span></p><p><span class=\"by_cn4SiEmqWbu7K9em5\">Not to be confused with maximization of </span><a href=\"https://www.lessestwrong.com/tag/utility-functions\"><span class=\"by_cn4SiEmqWbu7K9em5\">utility</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, or </span><a href=\"https://www.lessestwrong.com/tag/expected-utility\"><span class=\"by_cn4SiEmqWbu7K9em5\">expected utility</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">. If you're a utilitarian, you don't just sum over </span><a href=\"https://www.lessestwrong.com/tag/possible-world\"><span><span class=\"by_cn4SiEmqWbu7K9em5\">possible </span><span class=\"by_qf77EiaoMw7tH3GSr\">worlds</span></span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">;</span><span class=\"by_cn4SiEmqWbu7K9em5\"> you sum over </span></span><i><span class=\"by_cn4SiEmqWbu7K9em5\">people</span></i><span class=\"by_cn4SiEmqWbu7K9em5\">.</span></p><p><span><span class=\"by_cn4SiEmqWbu7K9em5\">Utilitarianism comes</span><span class=\"by_qf77EiaoMw7tH3GSr\"> in </span><span class=\"by_cn4SiEmqWbu7K9em5\">different variants. For example, unlike standard </span></span><a href=\"https://en.wikipedia.org/wiki/Total_utilitarianism\"><span class=\"by_cn4SiEmqWbu7K9em5\">total utilitarianism</span></a><span><span class=\"by_cn4SiEmqWbu7K9em5\">,</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://en.wikipedia.org/wiki/Average_utilitarianism\"><span class=\"by_qf77EiaoMw7tH3GSr\">average utilitarianism</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> values the </span><i><span class=\"by_qf77EiaoMw7tH3GSr\">average</span></i><span class=\"by_qf77EiaoMw7tH3GSr\"> utility among a group's members. </span><a href=\"https://en.wikipedia.org/wiki/utilitarianism#Negative_utilitarianism\"><span class=\"by_2YpRin5m5vBJu8Tg9\">Negative utilitarianism</span></a><span class=\"by_2YpRin5m5vBJu8Tg9\"> seeks only to minimize suffering, and is often discussed for its extreme implications.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/negative-utilitarianism-1\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Negative Utilitarianism</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/consequentialism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Consequentialism</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/ethics-and-morality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Ethics &amp; Morality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/fun-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Fun Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Complexity of Value</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 34,
    "description": {
      "markdown": "**Utilitarianism** is a moral [philosophy](https://www.lesswrong.com/tag/philosophy) that says that what matters is the sum of everyone's welfare, or the \"greatest good for the greatest number\".\n\nNot to be confused with maximization of [utility](https://www.lessestwrong.com/tag/utility-functions), or [expected utility](https://www.lessestwrong.com/tag/expected-utility). If you're a utilitarian, you don't just sum over [possible worlds](https://www.lessestwrong.com/tag/possible-world); you sum over *people*.\n\nUtilitarianism comes in different variants. For example, unlike standard [total utilitarianism](https://en.wikipedia.org/wiki/Total_utilitarianism), [average utilitarianism](https://en.wikipedia.org/wiki/Average_utilitarianism) values the *average* utility among a group's members. [Negative utilitarianism](https://en.wikipedia.org/wiki/utilitarianism#Negative_utilitarianism) seeks only to minimize suffering, and is often discussed for its extreme implications.\n\n**Related Pages:** [Negative Utilitarianism](https://www.lesswrong.com/tag/negative-utilitarianism-1), [Consequentialism](https://www.lesswrong.com/tag/consequentialism), [Ethics & Morality](https://www.lesswrong.com/tag/ethics-and-morality), [Fun Theory](https://www.lesswrong.com/tag/fun-theory), [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YJSm87XxrhuJGhrxf",
    "name": "Market Inefficiency",
    "core": false,
    "slug": "market-inefficiency",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": ""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LMFBzsJaCRADQqw3F",
    "name": "Trolley Problem",
    "core": false,
    "slug": "trolley-problem",
    "tableOfContents": {
      "html": "<p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The </span><a href=\"https://en.wikipedia.org/wiki/Trolley_problem\"><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">trolley problem</span></strong></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a decision scenario which illustrates/tests ethical reasoning. (\"Trolley problems\" are modified scenarios inspired by the basic one.) In the classic scenario, you are faced with a situation where five people will die if you do nothing. However, you can kill a bystander to save the lives of the five. Many people will choose to do nothing, even though many utilitarian/consequentialist positions would say to kill the bystander.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "The [**trolley problem**](https://en.wikipedia.org/wiki/Trolley_problem) is a decision scenario which illustrates/tests ethical reasoning. (\"Trolley problems\" are modified scenarios inspired by the basic one.) In the classic scenario, you are faced with a situation where five people will die if you do nothing. However, you can kill a bystander to save the lives of the five. Many people will choose to do nothing, even though many utilitarian/consequentialist positions would say to kill the bystander."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FwM9CYSSXgjX6fJvG",
    "name": "Psychotropics",
    "core": false,
    "slug": "psychotropics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_y7M83qgt5rWZiiozS\">Psychotropic substances</span></strong><span><span class=\"by_y7M83qgt5rWZiiozS\"> are substances that noticeably affect how people think. This tag is intended primarily for substances that cause short-term experiences that  (used responsibly) can have long-term positive or net-neutral effects on </span><span class=\"by_HoGziwmhpMGqGeWZy\">one'</span><span class=\"by_y7M83qgt5rWZiiozS\">s intellectual, emotional, and </span><span class=\"by_HoGziwmhpMGqGeWZy\">\"spiritual\"</span><span class=\"by_y7M83qgt5rWZiiozS\"> growth. This includes psychedelics such as Psilocybin and LSD as central examples, as well as MDMA and Cannabis as less-central examples</span></span></p><p><span class=\"by_y7M83qgt5rWZiiozS\">Contrast with the tags </span><a href=\"https://www.lesswrong.com/tag/psychiatry\"><span class=\"by_y7M83qgt5rWZiiozS\">Psychiatry</span></a><span class=\"by_y7M83qgt5rWZiiozS\"> and </span><a href=\"https://www.lesswrong.com/tag/nootropics\"><span class=\"by_y7M83qgt5rWZiiozS\">Nootropics</span></a><span><span class=\"by_y7M83qgt5rWZiiozS\">, which also pertain to substances which affect </span><span class=\"by_HoGziwmhpMGqGeWZy\">people'</span><span class=\"by_y7M83qgt5rWZiiozS\">s cognition</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "**Psychotropic substances** are substances that noticeably affect how people think. This tag is intended primarily for substances that cause short-term experiences that (used responsibly) can have long-term positive or net-neutral effects on one's intellectual, emotional, and \"spiritual\" growth. This includes psychedelics such as Psilocybin and LSD as central examples, as well as MDMA and Cannabis as less-central examples\n\nContrast with the tags [Psychiatry](https://www.lesswrong.com/tag/psychiatry) and [Nootropics](https://www.lesswrong.com/tag/nootropics), which also pertain to substances which affect people's cognition"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "M9oWHR2XGLmg2DaZp",
    "name": "Analogy",
    "core": false,
    "slug": "analogy",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nwcnHxrxcgnwJ878t",
    "name": "Luck",
    "core": false,
    "slug": "luck",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">In the same way </span><em><span class=\"by_QBvPFLFyZyuHcBwFm\">risk</span></em><span class=\"by_QBvPFLFyZyuHcBwFm\"> has a negative connotation, reflecting the possibility of negative outcomes, </span><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">luck</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\"> has a positive connotation for positive ones.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "In the same way _risk_ has a negative connotation, reflecting the possibility of negative outcomes, **luck** has a positive connotation for positive ones."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kS3QBcbwtapefkSSZ",
    "name": "Scrupulosity",
    "core": false,
    "slug": "scrupulosity",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Scrupulosity</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a tendency to hold yourself to excessively high standards and to feel really bad when you fail to meet those standards.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "**Scrupulosity** is a tendency to hold yourself to excessively high standards and to feel really bad when you fail to meet those standards."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SrW9iP2j6Hi8R5PmT",
    "name": "Black Swans",
    "core": false,
    "slug": "black-swans",
    "tableOfContents": {
      "html": "<p><span> <span class=\"by_XzXbiS2zWYNdZdLW8\">A </span></span><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">black</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> swan</span></span></strong><span><span class=\"by_XzXbiS2zWYNdZdLW8\"> is </span><span class=\"by_mPipmBTniuABY5PQy\">a high-impact </span><span class=\"by_qf77EiaoMw7tH3GSr\">event that is hard to predict (but not necessarily of low probability). Also, an event that is</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> not accounted for in a model, </span><span class=\"by_qf77EiaoMw7tH3GSr\">and</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> therefore causes the model to </span><span class=\"by_qf77EiaoMw7tH3GSr\">break down</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> when it occurs.</span></span></p><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Considering some event a black swan </span><span class=\"by_HoGziwmhpMGqGeWZy\">doesn'</span><span class=\"by_qf77EiaoMw7tH3GSr\">t give a leave to not assign any probabilities, since making decisions depending on the plausibility of such event is still equivalent to assigning probabilities that make the expected utility calculation give those decisions.</span></span></p><h2 id=\"External_posts_\"><span class=\"by_HoGziwmhpMGqGeWZy\">External posts:</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2008/09/white-swans-p-1.html\"><span class=\"by_qf77EiaoMw7tH3GSr\">White Swans Painted Black</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by Peter McCluskey</span></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/narrative-fallacy\"><span class=\"by_LoykQRMTxJFxwwdPy\">Narrative fallacy</span></a></li><li><a href=\"https://www.lesswrong.com/tag/hindsight-bias\"><span class=\"by_LoykQRMTxJFxwwdPy\">Hindsight bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/i-dont-know\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">I </span><span class=\"by_HoGziwmhpMGqGeWZy\">don'</span><span class=\"by_qf77EiaoMw7tH3GSr\">t know</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_LoykQRMTxJFxwwdPy\">Prediction</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_LoykQRMTxJFxwwdPy\">prediction market</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/forecast\"><span class=\"by_LoykQRMTxJFxwwdPy\">forecast</span></a></li></ul>",
      "sections": [
        {
          "title": "External posts:",
          "anchor": "External_posts_",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 6,
    "description": {
      "markdown": "A **black swan** is a high-impact event that is hard to predict (but not necessarily of low probability). Also, an event that is not accounted for in a model, and therefore causes the model to break down when it occurs.\n\nConsidering some event a black swan doesn't give a leave to not assign any probabilities, since making decisions depending on the plausibility of such event is still equivalent to assigning probabilities that make the expected utility calculation give those decisions.\n\nExternal posts:\n---------------\n\n*   [White Swans Painted Black](http://www.overcomingbias.com/2008/09/white-swans-p-1.html) by Peter McCluskey\n\nSee also\n--------\n\n*   [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy)\n*   [Hindsight bias](https://www.lesswrong.com/tag/hindsight-bias)\n*   [I don't know](https://www.lesswrong.com/tag/i-dont-know)\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction), [prediction market](https://www.lesswrong.com/tag/prediction-markets), [forecast](https://www.lesswrong.com/tag/forecast)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W6QZYSNt5FgWgvbdT",
    "name": "Coherent Extrapolated Volition",
    "core": false,
    "slug": "coherent-extrapolated-volition",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Coherent Extrapolated Volition</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> was a term developed by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> while discussing </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> development. It’s meant as an argument</span><span class=\"by_LedhurJxi3baDAKDZ\"> that it would </span><span class=\"by_qgdGA4ZEyW7zNdK84\">not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find</span><span class=\"by_LedhurJxi3baDAKDZ\"> a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">way to program it in a way that it would act in our best interests – what we </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">want</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> it to do and not what we </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">tell</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> it to.</span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\"> Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics Sequence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity</span><span class=\"by_LedhurJxi3baDAKDZ\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Value</span></span></a></p><blockquote><p><i><span><span class=\"by_qQqgj5ScvgQsJk7Ti\">In calculating CEV, an AI would predict what an idealized version of us would want, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"if</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> we knew more, thought faster, were more the people we wished we were, had grown up farther </span><span class=\"by_qgdGA4ZEyW7zNdK84\">together\"</span><span class=\"by_qQqgj5ScvgQsJk7Ti\">. It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to </span><span class=\"by_LedhurJxi3baDAKDZ\">generate</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">AI's utility function.</span></span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Often CEV is used generally to refer to what the idealized version of a person would want, separate from the context of building aligned AI's.</span></p><h2 id=\"What_is_volition_\"><span class=\"by_qgdGA4ZEyW7zNdK84\">What is volition?</span></h2><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">As an example of the classical concept of volition, the author develops a simple thought experiment: imagine you’re facing two boxes, A and B. One of these boxes, and only one, has a diamond in it – box B. You are now asked to make a guess, whether to </span><span class=\"by_Nin2paRhwLcHw7tDr\">choose</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> box A or B, and you chose to open box A. It was your </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">decision</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> to take box A, but your </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">volition</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> was to choose box B, since you wanted the diamond in the first place.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Now imagine someone else – Fred – is faced with the same task and you want to help him in his decision by giving the box he chose, box A. Since you know where the diamond is, simply handling him the box isn’t helping. As such, you mentally extrapolate a volition for Fred, based on a version of him that knows where the diamond is, and imagine he actually wants box B.</span></p><h2 id=\"Coherent_Extrapolated_Volition\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Coherent Extrapolated Volition</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In developing friendly AI, one acting for our best interests, we would have to take care that it would have implemented, from the beginning, a </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">coherent extrapolated volition of humankind</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. In calculating CEV, an </span><span class=\"by_QBvPFLFyZyuHcBwFm\">AI</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI'</span><span class=\"by_qQqgj5ScvgQsJk7Ti\">s utility function.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">The main problems</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> with CEV </span><span class=\"by_LedhurJxi3baDAKDZ\">include, firstly,</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> the great difficulty of implementing such a </span><span class=\"by_LedhurJxi3baDAKDZ\">program - </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“If</span><span class=\"by_LedhurJxi3baDAKDZ\"> one attempted to write an ordinary computer program using ordinary computer programming skills,</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> the </span><span class=\"by_LedhurJxi3baDAKDZ\">task would be a thousand lightyears beyond hopeless.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span><span class=\"by_LedhurJxi3baDAKDZ\"> Secondly, the possibility</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> that human values may not converge. Yudkowsky considered CEV obsolete almost immediately after its publication in 2004. </span><span class=\"by_LedhurJxi3baDAKDZ\">He states that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">there'</span><span class=\"by_LedhurJxi3baDAKDZ\">s a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"principled</span><span class=\"by_LedhurJxi3baDAKDZ\"> distinction between discussing CEV as an initial dynamic</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">Friendliness, and discussing </span><span class=\"by_qQqgj5ScvgQsJk7Ti\">CEV </span><span class=\"by_LedhurJxi3baDAKDZ\">as</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> a </span><span class=\"by_LedhurJxi3baDAKDZ\">Nice Place to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Live\"</span><span class=\"by_LedhurJxi3baDAKDZ\"> and his essay was essentially conflating the two definitions.</span></span></p><h2 id=\"Further_Reading___References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Further Reading &amp; References</span></h2><ul><li><a href=\"http://intelligence.org/files/CEV.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">Coherent Extrapolated Volition</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> by</span><span class=\"by_HoGziwmhpMGqGeWZy\"> Eliezer Yudkowsky</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> (2004)</span></span></li><li><a href=\"http://intelligence.org/files/CEV-MachineEthics.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Nick Tarleton (2010)</span></li><li><a href=\"https://web.archive.org/web/20131231151554/http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">A Short Introduction to </span><span class=\"by_HoGziwmhpMGqGeWZy\">Coherent Extrapolated Volition</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Michael Anissimov</span></li><li><a href=\"https://www.lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hacking the CEV for Fun and Profit</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Wei Dai</span></li><li><a href=\"https://www.lesswrong.com/lw/3fn/two_questions_about_cev_that_worry_me/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Two questions about CEV that worry me</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Vladimir Slepnev</span></li><li><a href=\"https://www.lesswrong.com/lw/5l0/beginning_resources_for_cev_research/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Beginning resources for CEV research</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Luke Muehlhauser</span></li><li><a href=\"https://www.lesswrong.com/lw/7sb/cognitive_neuroscience_arrows_impossibility/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cognitive Neuroscience, Arrow's Impossibility Theorem, and Coherent Extrapolated Volition</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Luke Muehlhauser</span></li><li><a href=\"https://www.lesswrong.com/lw/8iy/objections_to_coherent_extrapolated_volition/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Objections to Coherent Extrapolated Volition</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Alexander Kruel</span></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics sequence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of value</span></a></li><li><a href=\"https://www.lesswrong.com/tag/coherent-aggregated-volition\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Coherent Aggregated Volition</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Roko's_basilisk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Roko's basilisk</span></a></li></ul>",
      "sections": [
        {
          "title": "What is volition?",
          "anchor": "What_is_volition_",
          "level": 1
        },
        {
          "title": "Coherent Extrapolated Volition",
          "anchor": "Coherent_Extrapolated_Volition",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 33,
    "description": {
      "markdown": "**Coherent Extrapolated Volition** was a term developed by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) while discussing [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) development. It’s meant as an argument that it would not be sufficient to explicitly program what we think our desires and motivations are into an AI, instead, we should find a way to program it in a way that it would act in our best interests – what we *want* it to do and not what we *tell* it to.\n\n*Related*: [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), [Metaethics Sequence](https://www.lesswrong.com/tag/metaethics-sequence), [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value)\n\n> *In calculating CEV, an AI would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI's utility function.* \n\nOften CEV is used generally to refer to what the idealized version of a person would want, separate from the context of building aligned AI's.\n\nWhat is volition?\n-----------------\n\nAs an example of the classical concept of volition, the author develops a simple thought experiment: imagine you’re facing two boxes, A and B. One of these boxes, and only one, has a diamond in it – box B. You are now asked to make a guess, whether to choose box A or B, and you chose to open box A. It was your *decision* to take box A, but your *volition* was to choose box B, since you wanted the diamond in the first place.\n\nNow imagine someone else – Fred – is faced with the same task and you want to help him in his decision by giving the box he chose, box A. Since you know where the diamond is, simply handling him the box isn’t helping. As such, you mentally extrapolate a volition for Fred, based on a version of him that knows where the diamond is, and imagine he actually wants box B.\n\nCoherent Extrapolated Volition\n------------------------------\n\nIn developing friendly AI, one acting for our best interests, we would have to take care that it would have implemented, from the beginning, a *coherent extrapolated volition of humankind*. In calculating CEV, an AI would predict what an idealized version of us would want, \"if we knew more, thought faster, were more the people we wished we were, had grown up farther together\". It would recursively iterate this prediction for humanity as a whole, and determine the desires which converge. This initial dynamic would be used to generate the AI's utility function.\n\nThe main problems with CEV include, firstly, the great difficulty of implementing such a program - “If one attempted to write an ordinary computer program using ordinary computer programming skills, the task would be a thousand lightyears beyond hopeless.” Secondly, the possibility that human values may not converge. Yudkowsky considered CEV obsolete almost immediately after its publication in 2004. He states that there's a \"principled distinction between discussing CEV as an initial dynamic of Friendliness, and discussing CEV as a Nice Place to Live\" and his essay was essentially conflating the two definitions.\n\nFurther Reading & References\n----------------------------\n\n*   [Coherent Extrapolated Volition](http://intelligence.org/files/CEV.pdf) by Eliezer Yudkowsky (2004)\n*   [Coherent Extrapolated Volition: A Meta-Level Approach to Machine Ethics](http://intelligence.org/files/CEV-MachineEthics.pdf) by Nick Tarleton (2010)\n*   [A Short Introduction to Coherent Extrapolated Volition](https://web.archive.org/web/20131231151554/http://www.acceleratingfuture.com/michael/blog/2009/12/a-short-introduction-to-coherent-extrapolated-volition-cev/) by Michael Anissimov\n*   [Hacking the CEV for Fun and Profit](https://www.lesswrong.com/lw/2b7/hacking_the_cev_for_fun_and_profit/) by Wei Dai\n*   [Two questions about CEV that worry me](https://www.lesswrong.com/lw/3fn/two_questions_about_cev_that_worry_me/) by Vladimir Slepnev\n*   [Beginning resources for CEV research](https://www.lesswrong.com/lw/5l0/beginning_resources_for_cev_research/) by Luke Muehlhauser\n*   [Cognitive Neuroscience, Arrow's Impossibility Theorem, and Coherent Extrapolated Volition](https://www.lesswrong.com/lw/7sb/cognitive_neuroscience_arrows_impossibility/) by Luke Muehlhauser\n*   [Objections to Coherent Extrapolated Volition](https://www.lesswrong.com/lw/8iy/objections_to_coherent_extrapolated_volition/) by Alexander Kruel\n\nSee also\n--------\n\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Coherent Aggregated Volition](https://www.lesswrong.com/tag/coherent-aggregated-volition)\n*   [Roko's basilisk](https://wiki.lesswrong.com/wiki/Roko's_basilisk)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tdt83ChxnEgwwKxi6",
    "name": "Reading Group",
    "core": false,
    "slug": "reading-group",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">Discussing book chapters together, sharing summaries, questions and notes.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 36,
    "description": {
      "markdown": "Discussing book chapters together, sharing summaries, questions and notes."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5kwicZiaCfYrXjAwT",
    "name": "Debugging",
    "core": false,
    "slug": "debugging",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Debugging</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\"> is the process of actively looking for, noticing, and solving small problems in regular decision-making. Insofar at the problems are small enough to have a root cause, this process can lead to small but compounding lifestyle improvements.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Debugging** is the process of actively looking for, noticing, and solving small problems in regular decision-making. Insofar at the problems are small enough to have a root cause, this process can lead to small but compounding lifestyle improvements."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xYLtnJ6keSHGfrLpe",
    "name": "Risk Management",
    "core": false,
    "slug": "risk-management",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">How to deal with uncertainty, risk aversion, and decision-making involving the possibility of bad outcomes, personal or not.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "How to deal with uncertainty, risk aversion, and decision-making involving the possibility of bad outcomes, personal or not."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EgL74XM3JRu5hjQxu",
    "name": "Perception",
    "core": false,
    "slug": "perception",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">Processing sensory information, sight, hearing, touch and possibly not-yet-existing senses.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "Processing sensory information, sight, hearing, touch and possibly not-yet-existing senses."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3QnDqGSdRMA5mdMM6",
    "name": "Value of Rationality",
    "core": false,
    "slug": "value-of-rationality",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/costs-of-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Costs of Rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/valley-of-bad-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Valley of Bad Rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Pitfalls of Rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Criticisms of The Rationalist Movement</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "**Related pages:** [Costs of Rationality](https://www.lesswrong.com/tag/costs-of-rationality), [Valley of Bad Rationality](https://www.lesswrong.com/tag/valley-of-bad-rationality), [Pitfalls of Rationality](https://www.lesswrong.com/tag/pitfalls-of-rationality), [Criticisms of The Rationalist Movement](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LXk7bxNkYSjgatdAt",
    "name": "Tool AI",
    "core": false,
    "slug": "tool-ai",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">A </span><strong><span class=\"by_5wu9jG4pm9q6xjZ9R\">tool AI</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is a type of Artificial Intelligence </span><span class=\"by_2BLkv6XLCzCCgiQfr\">that</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is built to be used as a tool by the creators, rather than being an agent with its own action and goal-seeking behavior.</span></span></p><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">Generally meant to refer to </span><u><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">AGI</span></a></u><span class=\"by_5wu9jG4pm9q6xjZ9R\">, tool AI is a proposed method for gaining some of the benefits of the intelligence while avoiding the dangers of having it act autonomously. It was coined by Holden Karnofsky, co-founder of GiveWell, in a critique of the Singularity Institute. Karnofsky proposed that, while he agreed that agent-based AGI was dangerous, it was an unnecessary path of development. His example of tool AI behavior was Google Maps, which uses complex algorithms and data to plot a route, but presents these results to the user instead of driving the user itself.</span></p><p><span class=\"by_5wu9jG4pm9q6xjZ9R\">Eliezer Yudkowsky responded to this by enumerating several ways in which tool AI had similar difficulties in technical specification and safety. He also pointed out that it was not a common proposal among leading AGI thinkers.</span></p><h2 id=\"See_Also\"><strong><span class=\"by_HoGziwmhpMGqGeWZy\">See Also</span></strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/oracle-ai\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">Oracle</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> AI</span></span></a></li></ul><h2 id=\"External_Links\"><strong><span class=\"by_HoGziwmhpMGqGeWZy\">External Links</span></strong></h2><ul><li><a href=\"http://groups.yahoo.com/group/givewell/message/287\"><span class=\"by_HoGziwmhpMGqGeWZy\">Conversation between Holden Karnofsky and Jaan Tallinn</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 22,
    "description": {
      "markdown": "A **tool AI** is a type of Artificial Intelligence that is built to be used as a tool by the creators, rather than being an agent with its own action and goal-seeking behavior.\n\nGenerally meant to refer to [AGI](https://wiki.lesswrong.com/wiki/AGI), tool AI is a proposed method for gaining some of the benefits of the intelligence while avoiding the dangers of having it act autonomously. It was coined by Holden Karnofsky, co-founder of GiveWell, in a critique of the Singularity Institute. Karnofsky proposed that, while he agreed that agent-based AGI was dangerous, it was an unnecessary path of development. His example of tool AI behavior was Google Maps, which uses complex algorithms and data to plot a route, but presents these results to the user instead of driving the user itself.\n\nEliezer Yudkowsky responded to this by enumerating several ways in which tool AI had similar difficulties in technical specification and safety. He also pointed out that it was not a common proposal among leading AGI thinkers.\n\n**See Also**\n------------\n\n*   [Oracle AI](https://www.lesswrong.com/tag/oracle-ai)\n\n**External Links**\n------------------\n\n*   [Conversation between Holden Karnofsky and Jaan Tallinn](http://groups.yahoo.com/group/givewell/message/287)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rnvHPB3X2TiD5NMwY",
    "name": "Marketing",
    "core": false,
    "slug": "marketing",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nBCLy89Nqd8ouR6XT",
    "name": "Quantum Mechanics",
    "core": false,
    "slug": "quantum-mechanics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Quantum Mechanics</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">: the</span><span class=\"by_cn4SiEmqWbu7K9em5\"> bad news: our ordinary world is made out of weird, fuzzy, unpredictable stuff. The good news: the weird, fuzzy, unpredictable stuff is made out of unfamiliar but perfectly sensible math.</span></span><br><br><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/physics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Physics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lessestwrong.com/tag/decoherence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decoherence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lessestwrong.com/tag/many-worlds-interpretation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Many-worlds interpretation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lessestwrong.com/tag/configuration-space\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Configuration space</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><a href=\"https://wiki.lesswrong.com/wiki/Egan's_law\"><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></a><a href=\"https://www.lesswrong.com/tag/egans-law\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Egan's Law</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/timeless-physics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Timeless Physics</span></a></p><p><span><span class=\"by_cz84H76Bfm2puuBhp\">The biggest conceptual difference between the world of quantum mechanics and the physical world at the level we typically interact with is that it's much harder to specify the state of a system. Classical systems like a bowling ball or a planet have well-defined positions and velocity, and the state of such a system can be completely specified by just those two quantities. Quantities like position and velocity are called vectors, and in a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">3-</span><span class=\"by_cz84H76Bfm2puuBhp\">dimensional world a vector has component along each of the 3 dimensions. The state of a classical point particle can thus be given by just 6 numbers.</span></span></p><p><span><span class=\"by_cz84H76Bfm2puuBhp\">In quantum </span><span class=\"by_qgdGA4ZEyW7zNdK84\">mechanics,</span><span class=\"by_cz84H76Bfm2puuBhp\"> particles don't have both a well-defined position and velocity, and as a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">consequence,</span><span class=\"by_cz84H76Bfm2puuBhp\"> the vector that describes a quantum system can't be expressed in just 3 dimensions. In </span><span class=\"by_qgdGA4ZEyW7zNdK84\">general,</span><span class=\"by_cz84H76Bfm2puuBhp\"> there is no upper limit on the number of dimensions a quantum system can have, and so while the state of our bowling ball exists in two 3D spaces (one for position and one for velocity), a quantum </span><span class=\"by_qgdGA4ZEyW7zNdK84\">system,</span><span class=\"by_cz84H76Bfm2puuBhp\"> in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">general,</span><span class=\"by_cz84H76Bfm2puuBhp\"> exists in a space that's similar to the 3D space we're used to, but with an infinite number of dimensions. This space is called </span></span><a href=\"http://en.wikipedia.org/wiki/Hilbert_space\"><span class=\"by_cz84H76Bfm2puuBhp\">Hilbert space</span></a><span><span class=\"by_cz84H76Bfm2puuBhp\">. In order to be able to write down answers without using infinite numbers, quantum systems are usually mapped to other \"spaces\" like the 3D position and velocity spaces that we mentioned before. But information can be lost in this mapping, the same way a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">low-</span><span class=\"by_cz84H76Bfm2puuBhp\">resolution photograph won't fully capture a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">3-</span><span class=\"by_cz84H76Bfm2puuBhp\">dimensional object. As a consequence of the lossy nature of this transformation, instead of the position of a quantum particle we instead get a distribution of possible positions. This is why quantum mechanics is often described as random or unpredictable.</span></span></p><p><span><span class=\"by_GRPeTeSYHsu5bRMJx\">Actually,</span><span class=\"by_cz84H76Bfm2puuBhp\"> quantum mechanics is perfectly </span><span class=\"by_GRPeTeSYHsu5bRMJx\">predictable</span><span class=\"by_cz84H76Bfm2puuBhp\"> in Hilbert space. The only difficulty is that we don't live in Hilbert space, and while we have meter sticks and interferometers for making measurements in 3D space, we don't have any equipment for measuring Hilbert space directly. As a consequence, we have to make guesses about quantum systems based on what we see in 3D space. This is made even more difficult by the fact that once you measure a quantum system, it doesn't have the same distribution of possible positions it did before you measured it, so you can't sample repeatedly from the same distribution. Because we can't measure Hilbert space, the detailed dynamics of how exactly Hilbert space maps to real space, and what exactly happens in Hilbert space when you measure a system are still a matter of speculation and debate. </span><span class=\"by_cn4SiEmqWbu7K9em5\">People here have generally favored Hugh Everett's </span></span><a href=\"https://www.lessestwrong.com/tag/many-worlds-interpretation\"><span class=\"by_cn4SiEmqWbu7K9em5\">many-worlds interpretation</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> over others.</span></p><p><span><span class=\"by_cz84H76Bfm2puuBhp\">In spite of that, quantum mechanics is a mature field, and even if there's some uncertainty about what the results might imply, actually doing quantum mechanics is not terribly difficult for </span><span class=\"by_qgdGA4ZEyW7zNdK84\">single-</span><span class=\"by_cz84H76Bfm2puuBhp\">particle systems. From a practical standpoint, the evolution of the state of the system (also called the wave function or the state vector) is governed by differential equations the same way it is in classical physics. In quantum </span><span class=\"by_qgdGA4ZEyW7zNdK84\">mechanics,</span><span class=\"by_cz84H76Bfm2puuBhp\"> this equation is called </span></span><a href=\"http://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation\"><span class=\"by_cz84H76Bfm2puuBhp\">Schrödinger equation</span></a><span><span class=\"by_cz84H76Bfm2puuBhp\"> and most of practical quantum mechanics is concerned with solving this equation for different sets of boundary conditions, and in trying to find a \"space\" in which the quantum system can be expressed and solved most easily. Systems of more than one particle are considerably </span><span class=\"by_qgdGA4ZEyW7zNdK84\">trickier</span><span class=\"by_cz84H76Bfm2puuBhp\"> because the state vector has to be mapped into multiple different 3D spaces at the same time.</span></span></p><h2 id=\"External_Links\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links</span></h2><p><a href=\"https://mason.gmu.edu/~rhanson/mangledworlds.html\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mangled Worlds</span></a></p><h2 id=\"Resources\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Resources</span></h2><ul><li><a href=\"https://www.lessestwrong.com/lw/r5/the_quantum_physics_sequence/\"><span class=\"by_cn4SiEmqWbu7K9em5\">The Quantum Physics Sequence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> – this is really the best place to get started on the topic.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Quantum Mechanics thing by Michael Nielsen</span></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "Resources",
          "anchor": "Resources",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 38,
    "description": {
      "markdown": "**Quantum Mechanics**: the bad news: our ordinary world is made out of weird, fuzzy, unpredictable stuff. The good news: the weird, fuzzy, unpredictable stuff is made out of unfamiliar but perfectly sensible math.  \n  \n**Related Pages:** [Physics](https://www.lesswrong.com/tag/physics), [Decoherence](https://www.lessestwrong.com/tag/decoherence), [Many-worlds interpretation](https://www.lessestwrong.com/tag/many-worlds-interpretation), [Configuration space](https://www.lessestwrong.com/tag/configuration-space), [](https://wiki.lesswrong.com/wiki/Egan's_law) [Egan's Law](https://www.lesswrong.com/tag/egans-law), [Timeless Physics](https://www.lesswrong.com/tag/timeless-physics)\n\nThe biggest conceptual difference between the world of quantum mechanics and the physical world at the level we typically interact with is that it's much harder to specify the state of a system. Classical systems like a bowling ball or a planet have well-defined positions and velocity, and the state of such a system can be completely specified by just those two quantities. Quantities like position and velocity are called vectors, and in a 3-dimensional world a vector has component along each of the 3 dimensions. The state of a classical point particle can thus be given by just 6 numbers.\n\nIn quantum mechanics, particles don't have both a well-defined position and velocity, and as a consequence, the vector that describes a quantum system can't be expressed in just 3 dimensions. In general, there is no upper limit on the number of dimensions a quantum system can have, and so while the state of our bowling ball exists in two 3D spaces (one for position and one for velocity), a quantum system, in general, exists in a space that's similar to the 3D space we're used to, but with an infinite number of dimensions. This space is called [Hilbert space](http://en.wikipedia.org/wiki/Hilbert_space). In order to be able to write down answers without using infinite numbers, quantum systems are usually mapped to other \"spaces\" like the 3D position and velocity spaces that we mentioned before. But information can be lost in this mapping, the same way a low-resolution photograph won't fully capture a 3-dimensional object. As a consequence of the lossy nature of this transformation, instead of the position of a quantum particle we instead get a distribution of possible positions. This is why quantum mechanics is often described as random or unpredictable.\n\nActually, quantum mechanics is perfectly predictable in Hilbert space. The only difficulty is that we don't live in Hilbert space, and while we have meter sticks and interferometers for making measurements in 3D space, we don't have any equipment for measuring Hilbert space directly. As a consequence, we have to make guesses about quantum systems based on what we see in 3D space. This is made even more difficult by the fact that once you measure a quantum system, it doesn't have the same distribution of possible positions it did before you measured it, so you can't sample repeatedly from the same distribution. Because we can't measure Hilbert space, the detailed dynamics of how exactly Hilbert space maps to real space, and what exactly happens in Hilbert space when you measure a system are still a matter of speculation and debate. People here have generally favored Hugh Everett's [many-worlds interpretation](https://www.lessestwrong.com/tag/many-worlds-interpretation) over others.\n\nIn spite of that, quantum mechanics is a mature field, and even if there's some uncertainty about what the results might imply, actually doing quantum mechanics is not terribly difficult for single-particle systems. From a practical standpoint, the evolution of the state of the system (also called the wave function or the state vector) is governed by differential equations the same way it is in classical physics. In quantum mechanics, this equation is called [Schrödinger equation](http://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation) and most of practical quantum mechanics is concerned with solving this equation for different sets of boundary conditions, and in trying to find a \"space\" in which the quantum system can be expressed and solved most easily. Systems of more than one particle are considerably trickier because the state vector has to be mapped into multiple different 3D spaces at the same time.\n\nExternal Links\n--------------\n\n[Mangled Worlds](https://mason.gmu.edu/~rhanson/mangledworlds.html)\n\nResources\n---------\n\n*   [The Quantum Physics Sequence](https://www.lessestwrong.com/lw/r5/the_quantum_physics_sequence/) – this is really the best place to get started on the topic.\n*   Quantum Mechanics thing by Michael Nielsen"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "33sh8ktwbqP7hFtBP",
    "name": "Wildfires",
    "core": false,
    "slug": "wildfires",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Wildfires</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are a type of natural disaster common in places like California.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "**Wildfires** are a type of natural disaster common in places like California."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cosxPiaS8EGMnbMYf",
    "name": "Interpretive Labor",
    "core": false,
    "slug": "interpretive-labor",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FdoP2PJhMz6x3gdDh",
    "name": "Nootropics & Other Cognitive Enhancement",
    "core": false,
    "slug": "nootropics-and-other-cognitive-enhancement",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Nootoropics </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">(drugs/psychoactive substances) is a form of biological</span><strong><span class=\"by_NRg5Bw8H2DCYTpmHE\"> Cognitive Enhancement</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">, i.e.,</span><span class=\"by_woC2b5rav5sGrAo3E\"> any modification in the biology of</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> a </span><span class=\"by_woC2b5rav5sGrAo3E\">person which increases </span><span class=\"by_qgdGA4ZEyW7zNdK84\">their</span><span class=\"by_woC2b5rav5sGrAo3E\"> cognitive capacities </span></span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn1\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">1</span></sup></a><span><span class=\"by_woC2b5rav5sGrAo3E\">. </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Apart from</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> drugs </span><span class=\"by_qgdGA4ZEyW7zNdK84\">(nootropics), alternative biological cognitive enhancements include, possibly,</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">magnetic stimulation.&nbsp;</span></span><br><br><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Note: this </span><span class=\"by_Sp5wM4aRAhNERd4oY\">page</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> was last updated in November, 2014, and was written by a single author. It does not reflect any consensus on LessWrong.</span></span></i></p><p><span><span class=\"by_woC2b5rav5sGrAo3E\">The most imminent, successful and polemic method is through</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> the </span><span class=\"by_woC2b5rav5sGrAo3E\">use</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> of </span><span class=\"by_woC2b5rav5sGrAo3E\">drugs, substances that alter the functioning</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> of </span><span class=\"by_woC2b5rav5sGrAo3E\">our brain's neurochemistry</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> in order to </span><span class=\"by_woC2b5rav5sGrAo3E\">improve certain aspects of cognition. There is</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> an </span><span class=\"by_woC2b5rav5sGrAo3E\">increasing trend in the use of cognitive enhancement drugs among healthy individuals in schools and colleges</span></span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn2\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">2</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn3\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">3</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. This means this kind of enhancement technology is already in use. The overall impact of a widespread use of these kinds of drugs could be enormous </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn4\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">4</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. However, the whole set of ethical consequences is unknown and subject of on-going developments </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn5\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">5</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn6\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">6</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn7\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">7</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn8\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">8</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn9\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">9</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn10\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">10</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn11\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">11</span></sup></a></p><h2 id=\"Examples\"><span class=\"by_woC2b5rav5sGrAo3E\">Examples</span></h2><p><span class=\"by_woC2b5rav5sGrAo3E\">Currently, there are several drugs been used as cognitive enhancers by healthy individuals, e.g.: caffeine, ritalin, aderall, modafinil and Aricept. Academic research assessing the risks and benefits of these drugs in the healthy individual have began only recently. In addition, the results of those researches are vastly ignored by most of the concerned population. Three of the most used, promising and known cognitive enhancement drugs are listed in more detail below:</span></p><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">Caffeine: Perhaps the most used and old cognitive enhancer. Caffeine has an excitatory result in the brain, by partially disabling the process that signals low availability of energy. </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn12\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">12</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. Caffeine and its metabolites also increase the serum concentration of adrenaline, thus increasing heart rate, blood pressure and stress </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn13\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">13</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. Many researchers have found a vast number of beneficial cognitive effects, as improved concentration and memory retention </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn14\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">14</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. Its beneficial effects on overall health are also documented </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn15\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">15</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. However, the American adult male's average dosage</span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn16\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">16</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn17\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">17</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> surpasses the healthy dosage fourfold. At the average ingested dosage, caffeine has strong detrimental health effects increasing the risk of heart attacks and strokes </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn18\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">18</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn19\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">19</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> and also possess addiction potential, with severe withdrawal symptoms such as depression, irritability, pain and narcolepsy </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn20\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">20</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Modafinil: Modafinil effects are mediated through the neurotransmitters histamine and dopamine. Histamine regulates the state of wakefulness. Dopamine has important roles on motivation, cognition, reward, attention and working memory </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn21\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">21</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. There are at least 7 studies on the cognitive enhancement properties of modafinil in healthy individuals </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn22\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">22</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn23\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">23</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn24\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">24</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn25\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">25</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn26\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">26</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn27\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">27</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn28\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">28</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn29\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">29</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. Those studies' results are:</span><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">Increased new-language learning </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn30\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">30</span></sup></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Enhanced performance on tests of digit span, visual pattern recognition memory, spatial planning and stop signal reaction time </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn31\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">31</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Lower error rate in a visual spatial task</span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn32\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">32</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Improved fatigue levels, motivation, reaction time and vigilance</span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn33\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">33</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Improvement on spatial working memory, planning and decision making at the most difficult levels, as well as visual pattern recognition memory following delay and subjective ratings of enjoyment of task performance </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn34\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">34</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> .</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Decreased impairment in vestibular function in 24h sleep deprived individuals</span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn35\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">35</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Decreased impairment on performance in a flight simulation test in 30h and 40h sleep deprived individuals</span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn36\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">36</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn37\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">37</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">No adverse effects were reported in none of these studies, however this wasn't the target of any of them.</span></li></ul></li></ul><p><a href=\"http://www.springerlink.com/content/?k=modafinil\"><span class=\"by_woC2b5rav5sGrAo3E\">Many</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"http://www.sciencedirect.com/science?_ob=ArticleListURL&amp;_method=list&amp;_ArticleListID=2100456952&amp;_sort=r&amp;_st=13&amp;view=c&amp;_acct=C000228598&amp;_version=1&amp;_urlVersion=0&amp;_userid=10&amp;md5=9facf727fff44b33bdd632f4e3f51852&amp;searchtype=a\"><span class=\"by_woC2b5rav5sGrAo3E\">other</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> studies in non-healthy patients have found some adverse effects</span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn38\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">38</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">, but have confirmed its safety and - so far - no addiction potential profile. However, research on its long-term safety is deeply needed.</span></p><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">Aricept(Donepezil): Aricept inhibits the breakdown of acetylcholine. Acetylcholine is a neurotransmitter linked to long-term memory. There are at least two studies with healthy individuals that have found: greater retention of how to perform a set of complex tasks </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn39\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">39</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> and increased visual and verbal long-term memory </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn40\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">40</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></li></ul><h2 id=\"Biases_affecting_our_judgment\"><span class=\"by_woC2b5rav5sGrAo3E\">Biases affecting our judgment</span></h2><p><span class=\"by_woC2b5rav5sGrAo3E\">There are several </span><a href=\"http://wiki.lesswrong.com/wiki/Bias\"><span class=\"by_woC2b5rav5sGrAo3E\">cognitive biases</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> affecting our judgment on the risks and efficacy of biological cognitive enhancers. Two are worth mentioning:</span></p><ul><li><span class=\"by_woC2b5rav5sGrAo3E\">Statistical format: we do not update our beliefs correctly when presented with absolute probabilities (i.e.: 10%) - when the information is presented in terms of occurrences (i.e.: one person in ten) the belief update is much more close to </span><a href=\"https://www.lessestwrong.com/lw/1to/what_is_bayesianism/\"><span class=\"by_woC2b5rav5sGrAo3E\">bayesian</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn41\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">41</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">. This bias impairs our ability to use information from scientific research to update our beliefs. One can easily comprehend the risks involved with a certain drug if a friend suffered a heart attack due to its use, avoiding such drug from then on. But reading an abstract number showing the rise in blood pressure – </span><a href=\"http://www.who.int/healthinfo/global_burden_disease/GlobalHealthRisks_report_full.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">the most important preventable risk factor for death</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> - of caffeine users is much higher than of modafinil users is too far away from the occurrence-based </span><a href=\"http://wiki.lesswrong.com/wiki/Evolutionary_psychology\"><span class=\"by_woC2b5rav5sGrAo3E\">savannah way</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> our brains are accustomed to absorb information </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn42\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">42</span></sup></a></li><li><span class=\"by_woC2b5rav5sGrAo3E\">Status quo: a consistent and unjustified tendency to prefer that some parameter stays in the configuration it has always been, over other possible configurations. This tendency can manifest itself by preferring to continue to use a known drug with many side effects over a new safer drug and also impair our judgment of many others technological advancements. When analyzing if a new configuration should be used, Bostrom and Ord </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn43\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">43</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> suggest the following heuristic: we imagine a scenario were the parameter will naturally change to the new configuration and ask if we would intervene. If we wouldn't intervene, then we have a reason to think the new configuration should be preferred.</span></li></ul><h2 id=\"Relevance\"><span class=\"by_woC2b5rav5sGrAo3E\">Relevance</span></h2><p><span class=\"by_woC2b5rav5sGrAo3E\">Bostrom </span><a href=\"https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn44\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">44</span></sup></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> argues for the huge impact of cognitive enhancements: \"Imagine a researcher invented an inexpensive drug which was completely safe and which improved all‐round cognitive performance by just 1%. The gain would hardly be noticeable in a single individual. But if the 10 million scientists in the world all benefited from the drug the inventor would increase the rate of scientific progress by roughly the same amount as adding 100,000 new scientists. Each year the invention would amount to an indirect contribution equal to 100,000 times what the average scientist contributes. Even an Einstein or a Darwin at the peak of their powers could not make such a great impact. \" Even outside the </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">academic </span><span class=\"by_woC2b5rav5sGrAo3E\">community, imagine a drug that improves the efficiency of all employees and workers around the world by just 1%. This</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> would </span><span class=\"by_woC2b5rav5sGrAo3E\">roughly means adding more 1 trillion dollars of production every year to the world gross product. This would be equivalent to the addition of an entire well developed country to the world, Germany for instance.</span></span></p><h2 id=\"References\"><span class=\"by_woC2b5rav5sGrAo3E\">References</span></h2><ol><li><span class=\"by_woC2b5rav5sGrAo3E\">SAVULESCU, J. &amp; MEULEN, Rudd ter (orgs.) (2011) \"Enhancing Human Capacities\". Wiley-Blackwell.</span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-2\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">KAPNER, E. (2003) \"Recreational use of Ritalin</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> on </span><span class=\"by_woC2b5rav5sGrAo3E\">college campuses\". InfoFactsResources – The Higher Education Center for Alcohol and Other Drug Prevention. Available at: www.edc.org/hec/pubs/factsheets/ritalin.pdf (accessed 4 Jan 2006).</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">TETER, C.J. et al. (2005). \"Prevalence and motives for illicit use of prescription stimulants in </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">an </span><span class=\"by_woC2b5rav5sGrAo3E\">undergraduate student sample\", J Am Coll Health 53 (2005).</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3ways_4-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:4.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3ways_4-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">4.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BOSTROM, NICK. (2008) \"Three Ways to Advance Science\" For Nature Podcast, 31 January 2008. Available at: </span></span><a href=\"http://www.nickbostrom.com/views/science.pdf\"><u><span class=\"by_woC2b5rav5sGrAo3E\">http://www.nickbostrom.com/views/science.pdf</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-5\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">SANDBERG, Anders &amp; LIAO, S.M., (2008) \"The Normativity of Memory Modification\", Neuroethics (2008), (1 2) 85-99.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-6\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">SANBERG, Anders &amp; SAVULESCU, Julian. (2008). \"Neuroenhancement of Love and Marriage: The Chemicals Between Us.\" Neuroethics (2008) Vol. 1:31-44.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-7\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BOSTROM, Nick &amp; SAVULESCO, Julian. (orgs.), (2009) \"Human Enhancement\". Oxford University Press.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-8\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BOSTROM, Nick &amp; SANDBERG, Anders. (2006) \"Converging </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">Cognitive</span><span class=\"by_woC2b5rav5sGrAo3E\"> Enhancements\", Annals of the New York Academy of Sciences, Vol. 1093.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-heuristic_9-0\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">SANDBERG, Nick &amp; SANDBERG, Anders. (2009) \"The Wisdom of Nature: an Evolutionary Heuristic for Human Enhancement\" in: BOSTROM, Nick &amp; SAVULESCU, Julian(orgs.). Human Enhancement. Oxford University Press, EUA.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-10\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BOSTROM, Nick &amp; SANDBERG, Anders. (2009) \"Cognitive</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> Enhancement: Methods, </span><span class=\"by_woC2b5rav5sGrAo3E\">Ethics, Regulatory Challenges\", Science and Engineering Ethics, Vol. 15, No. 3.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-squire_11-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:11.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-squire_11-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">11.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">SQUIRE, Larry R. et al. (orgs.) (2008) \"Fundamental Neuroscience.\" Academic Press. 3a edition.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-12\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">DEWS, P.B. (1984). \"Caffeine: Perspectives from Recent Research.\" Berlin: Springer-Valerag</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-13\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BOLTON, Sanford (1981). \"Caffeine: Psychological Effects, Use and Abuse\". Orthomolecular Psychiatry 10 (3): 202–211.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-14\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">THOMPSON, Rebecca &amp; KEENE, Karen (2004). \"The pros and cons of caffeine\". The Psychologist (The British Psychological Society) 17 (12): 698–701.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-15\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">NCDT (2011). Report of the 2011 National Coffee Drinking Trends (NCDT).</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-16\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">ILLY, A. &amp; VIVIANI, R. (1995) Espresso Coffee: The Chemistry of Quality. San Diego: Academic P.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-17\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">GREENBERG, J. A. Et al.(2007) \"Caffeinated beverage intake and the risk of heart disease mortality in the elderly: a prospective analysis\". Am J Clin Nutr 85 (2): 392–8.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-18\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">LESON. C. L. Et al. (1998) \"Caffeine overdose in an adolescent male.\". Journal of toxicology. Clinical toxicology Vol. 26 (5–6): 407–15.</span></span></li><li><span class=\"by_woC2b5rav5sGrAo3E\">JULIANO, Laura M. &amp; GRIFFITHS, Roland R. (2004) \"A critical review of caffeine withdrawal: empirical validation of symptoms and signs, incidence, severity, and associated features\". Psychopharmacology 176 (1): 1–29.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid1_20-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:20.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid1_20-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">20.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">CAIDWELL, John A. et al. (1999) \"The Effects of Modafinil on Aviator Performance During 40 Hours of Continuous Wakefulness: A UH-60 Helicopter Simulator Study.\" Army aeromedical research unit fort rucker al.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid2_21-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:21.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid2_21-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">21.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">CAIDWELL, John A. et al. (2004) \"The Efficacy of Modafinil for Sustaining Alertness and Simulator Flight Performance in F-117 Pilots During 37 Hours of Continuous Wakefulness.\" Air Force Research lab brooks AFB TX, Human effectiveness Dir/Biodynamics and protection div.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-li_22-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:22.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-li_22-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">22.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">LI Yanfeng, ZHAN Hao, XIN Yimei, et al. (2007) \"Effects of modafinil on vestibular function during 24 hour sleep deprivation\". Frontiers of medicine in China, Vol. 1, Number 2, 226-229.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-bara_23-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:23.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-bara_23-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">23.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BARANSKI, J. V. Et al. (2004) \"Effects of modafinil on cognitive and meta-cognitive performance\". Hum Psychopharmacol. 2004 Jul; Vol. 19(5):323-32.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull1_24-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:24.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull1_24-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">24.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">MÜLLER, U. Et al. (2004) \"Effects of modafinil on working memory processes in humans\". Psychopharmacology (Berl.) Vol. 177 (1-2): 161–9. </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Cite error: Invalid </span></strong><code><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">&lt;ref&gt;</span></strong></code><strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag; name \"mull1\" defined multiple times with different content</span></strong></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-turn_25-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:25.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-turn_25-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">25.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">TURNER, D. C et al. (2003). \"Cognitive enhancing effects of modafinil in healthy volunteers\". Psychopharmacology (Berl.) Vol. 165 (3): 260–9.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull2_26-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:26.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull2_26-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">26.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">MULLER, U. et all. (2012) \"Effects of modafinil on non-verbal cognition, task enjoyment and creative thinking in healthy volunteers.\" Neuropharmocology: 2012 (In press)</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">↑ </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-Gilleen_27-0\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up to:27.0</span></u></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-Gilleen_27-1\"><sup><u><span class=\"by_qgdGA4ZEyW7zNdK84\">27.1</span></u></sup></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">GILLEEN, J., et al. (2014). \"Modafinil combined with cognitive training is associated with improved learning in healthy volunteers--a randomised controlled trial.\" European Neuropsychopharmacology&nbsp;: The Journal of the European College of Neuropsychopharmacology, 24(4), 529–39. doi:10.1016/j.euroneuro.2014.01.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">001.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-28\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://www.rxlist.com/provigil-drug/side-effects-interactions.htm\"><u><span class=\"by_woC2b5rav5sGrAo3E\">http://www.rxlist.com/provigil-drug/side-effects-interactions.htm</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-29\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">YESAVAGE, et al. (2002). \"Donepezil and flight simulator performance Effects on retention of complex skills\" NEUROLOGY 2002; 59:123–125.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-30\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">YESAVAGE, et al. (2002). \"Donepezil and flight simulator performance Effects on retention of complex skills\" NEUROLOGY 2002; 59:123–125.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-31\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">POHL, Rüdiger (orgs.). (2005) \"Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory\". Psychology Press. pp. 61-78</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-32\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BUSS, David(orgs.). (2005) \"The Handbook of Evolutionary Psychology\". Wiley, New Jersey. pp. 739-740.</span></span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-33\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Jump up↑</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">BOSTROM, Nick &amp; ORD, Toby. (2006) \"The Reversal Test: Eliminating Status Quo Bias in Applied Ethics\". </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">Ethics </span><span class=\"by_woC2b5rav5sGrAo3E\">116 (Julho 2006): 656-679.</span></span></li></ol>",
      "sections": [
        {
          "title": "Examples",
          "anchor": "Examples",
          "level": 1
        },
        {
          "title": "Biases affecting our judgment",
          "anchor": "Biases_affecting_our_judgment",
          "level": 1
        },
        {
          "title": "Relevance",
          "anchor": "Relevance",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 12,
    "description": {
      "markdown": "**Nootoropics** (drugs/psychoactive substances) is a form of biological **Cognitive Enhancement**, i.e., any modification in the biology of a person which increases their cognitive capacities [^1^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn1). Apart from drugs (nootropics), alternative biological cognitive enhancements include, possibly, magnetic stimulation.   \n  \n*Note: this page was last updated in November, 2014, and was written by a single author. It does not reflect any consensus on LessWrong.*\n\nThe most imminent, successful and polemic method is through the use of drugs, substances that alter the functioning of our brain's neurochemistry in order to improve certain aspects of cognition. There is an increasing trend in the use of cognitive enhancement drugs among healthy individuals in schools and colleges[^2^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn2) [^3^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn3). This means this kind of enhancement technology is already in use. The overall impact of a widespread use of these kinds of drugs could be enormous [^4^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn4). However, the whole set of ethical consequences is unknown and subject of on-going developments [^5^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn5) [^6^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn6) [^7^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn7) [^8^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn8) [^9^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn9) [^10^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn10) [^11^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn11)\n\nExamples\n--------\n\nCurrently, there are several drugs been used as cognitive enhancers by healthy individuals, e.g.: caffeine, ritalin, aderall, modafinil and Aricept. Academic research assessing the risks and benefits of these drugs in the healthy individual have began only recently. In addition, the results of those researches are vastly ignored by most of the concerned population. Three of the most used, promising and known cognitive enhancement drugs are listed in more detail below:\n\n*   Caffeine: Perhaps the most used and old cognitive enhancer. Caffeine has an excitatory result in the brain, by partially disabling the process that signals low availability of energy. [^12^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn12). Caffeine and its metabolites also increase the serum concentration of adrenaline, thus increasing heart rate, blood pressure and stress [^13^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn13). Many researchers have found a vast number of beneficial cognitive effects, as improved concentration and memory retention [^14^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn14). Its beneficial effects on overall health are also documented [^15^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn15). However, the American adult male's average dosage[^16^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn16) [^17^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn17) surpasses the healthy dosage fourfold. At the average ingested dosage, caffeine has strong detrimental health effects increasing the risk of heart attacks and strokes [^18^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn18) [^19^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn19) and also possess addiction potential, with severe withdrawal symptoms such as depression, irritability, pain and narcolepsy [^20^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn20).\n*   Modafinil: Modafinil effects are mediated through the neurotransmitters histamine and dopamine. Histamine regulates the state of wakefulness. Dopamine has important roles on motivation, cognition, reward, attention and working memory [^21^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn21). There are at least 7 studies on the cognitive enhancement properties of modafinil in healthy individuals [^22^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn22) [^23^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn23) [^24^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn24) [^25^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn25) [^26^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn26) [^27^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn27) [^28^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn28) [^29^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn29). Those studies' results are:\n    *   Increased new-language learning [^30^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn30)\n    *   Enhanced performance on tests of digit span, visual pattern recognition memory, spatial planning and stop signal reaction time [^31^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn31).\n    *   Lower error rate in a visual spatial task[^32^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn32).\n    *   Improved fatigue levels, motivation, reaction time and vigilance[^33^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn33).\n    *   Improvement on spatial working memory, planning and decision making at the most difficult levels, as well as visual pattern recognition memory following delay and subjective ratings of enjoyment of task performance [^34^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn34) .\n    *   Decreased impairment in vestibular function in 24h sleep deprived individuals[^35^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn35).\n    *   Decreased impairment on performance in a flight simulation test in 30h and 40h sleep deprived individuals[^36^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn36) [^37^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn37).\n    *   No adverse effects were reported in none of these studies, however this wasn't the target of any of them.\n\n[Many](http://www.springerlink.com/content/?k=modafinil) [other](http://www.sciencedirect.com/science?_ob=ArticleListURL&_method=list&_ArticleListID=2100456952&_sort=r&_st=13&view=c&_acct=C000228598&_version=1&_urlVersion=0&_userid=10&md5=9facf727fff44b33bdd632f4e3f51852&searchtype=a) studies in non-healthy patients have found some adverse effects[^38^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn38), but have confirmed its safety and - so far - no addiction potential profile. However, research on its long-term safety is deeply needed.\n\n*   Aricept(Donepezil): Aricept inhibits the breakdown of acetylcholine. Acetylcholine is a neurotransmitter linked to long-term memory. There are at least two studies with healthy individuals that have found: greater retention of how to perform a set of complex tasks [^39^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn39) and increased visual and verbal long-term memory [^40^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn40).\n\nBiases affecting our judgment\n-----------------------------\n\nThere are several [cognitive biases](http://wiki.lesswrong.com/wiki/Bias) affecting our judgment on the risks and efficacy of biological cognitive enhancers. Two are worth mentioning:\n\n*   Statistical format: we do not update our beliefs correctly when presented with absolute probabilities (i.e.: 10%) - when the information is presented in terms of occurrences (i.e.: one person in ten) the belief update is much more close to [bayesian](https://www.lessestwrong.com/lw/1to/what_is_bayesianism/) [^41^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn41). This bias impairs our ability to use information from scientific research to update our beliefs. One can easily comprehend the risks involved with a certain drug if a friend suffered a heart attack due to its use, avoiding such drug from then on. But reading an abstract number showing the rise in blood pressure – [the most important preventable risk factor for death](http://www.who.int/healthinfo/global_burden_disease/GlobalHealthRisks_report_full.pdf) \\- of caffeine users is much higher than of modafinil users is too far away from the occurrence-based [savannah way](http://wiki.lesswrong.com/wiki/Evolutionary_psychology) our brains are accustomed to absorb information [^42^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn42)\n*   Status quo: a consistent and unjustified tendency to prefer that some parameter stays in the configuration it has always been, over other possible configurations. This tendency can manifest itself by preferring to continue to use a known drug with many side effects over a new safer drug and also impair our judgment of many others technological advancements. When analyzing if a new configuration should be used, Bostrom and Ord [^43^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn43) suggest the following heuristic: we imagine a scenario were the parameter will naturally change to the new configuration and ask if we would intervene. If we wouldn't intervene, then we have a reason to think the new configuration should be preferred.\n\nRelevance\n---------\n\nBostrom [^44^](https://www.lessestwrong.com/tag/nootropics-and-other-cognitive-enhancement?revision=0.0.17#fn44) argues for the huge impact of cognitive enhancements: \"Imagine a researcher invented an inexpensive drug which was completely safe and which improved all‐round cognitive performance by just 1%. The gain would hardly be noticeable in a single individual. But if the 10 million scientists in the world all benefited from the drug the inventor would increase the rate of scientific progress by roughly the same amount as adding 100,000 new scientists. Each year the invention would amount to an indirect contribution equal to 100,000 times what the average scientist contributes. Even an Einstein or a Darwin at the peak of their powers could not make such a great impact. \" Even outside the academic community, imagine a drug that improves the efficiency of all employees and workers around the world by just 1%. This would roughly means adding more 1 trillion dollars of production every year to the world gross product. This would be equivalent to the addition of an entire well developed country to the world, Germany for instance.\n\nReferences\n----------\n\n1.  SAVULESCU, J. & MEULEN, Rudd ter (orgs.) (2011) \"Enhancing Human Capacities\". Wiley-Blackwell.\n2.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-2) KAPNER, E. (2003) \"Recreational use of Ritalin on college campuses\". InfoFactsResources – The Higher Education Center for Alcohol and Other Drug Prevention. Available at: www.edc.org/hec/pubs/factsheets/ritalin.pdf (accessed 4 Jan 2006).\n3.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3) TETER, C.J. et al. (2005). \"Prevalence and motives for illicit use of prescription stimulants in an undergraduate student sample\", J Am Coll Health 53 (2005).\n4.  ↑ [^Jump up to:4.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3ways_4-0) [^4.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-3ways_4-1) BOSTROM, NICK. (2008) \"Three Ways to Advance Science\" For Nature Podcast, 31 January 2008. Available at: [http://www.nickbostrom.com/views/science.pdf](http://www.nickbostrom.com/views/science.pdf)\n5.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-5) SANDBERG, Anders & LIAO, S.M., (2008) \"The Normativity of Memory Modification\", Neuroethics (2008), (1 2) 85-99.\n6.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-6) SANBERG, Anders & SAVULESCU, Julian. (2008). \"Neuroenhancement of Love and Marriage: The Chemicals Between Us.\" Neuroethics (2008) Vol. 1:31-44.\n7.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-7) BOSTROM, Nick & SAVULESCO, Julian. (orgs.), (2009) \"Human Enhancement\". Oxford University Press.\n8.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-8) BOSTROM, Nick & SANDBERG, Anders. (2006) \"Converging Cognitive Enhancements\", Annals of the New York Academy of Sciences, Vol. 1093.\n9.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-heuristic_9-0) SANDBERG, Nick & SANDBERG, Anders. (2009) \"The Wisdom of Nature: an Evolutionary Heuristic for Human Enhancement\" in: BOSTROM, Nick & SAVULESCU, Julian(orgs.). Human Enhancement. Oxford University Press, EUA.\n10.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-10) BOSTROM, Nick & SANDBERG, Anders. (2009) \"Cognitive Enhancement: Methods, Ethics, Regulatory Challenges\", Science and Engineering Ethics, Vol. 15, No. 3.\n11.  ↑ [^Jump up to:11.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-squire_11-0) [^11.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-squire_11-1) SQUIRE, Larry R. et al. (orgs.) (2008) \"Fundamental Neuroscience.\" Academic Press. 3a edition.\n12.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-12) DEWS, P.B. (1984). \"Caffeine: Perspectives from Recent Research.\" Berlin: Springer-Valerag\n13.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-13) BOLTON, Sanford (1981). \"Caffeine: Psychological Effects, Use and Abuse\". Orthomolecular Psychiatry 10 (3): 202–211.\n14.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-14) THOMPSON, Rebecca & KEENE, Karen (2004). \"The pros and cons of caffeine\". The Psychologist (The British Psychological Society) 17 (12): 698–701.\n15.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-15) NCDT (2011). Report of the 2011 National Coffee Drinking Trends (NCDT).\n16.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-16) ILLY, A. & VIVIANI, R. (1995) Espresso Coffee: The Chemistry of Quality. San Diego: Academic P.\n17.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-17) GREENBERG, J. A. Et al.(2007) \"Caffeinated beverage intake and the risk of heart disease mortality in the elderly: a prospective analysis\". Am J Clin Nutr 85 (2): 392–8.\n18.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-18) LESON. C. L. Et al. (1998) \"Caffeine overdose in an adolescent male.\". Journal of toxicology. Clinical toxicology Vol. 26 (5–6): 407–15.\n19.  JULIANO, Laura M. & GRIFFITHS, Roland R. (2004) \"A critical review of caffeine withdrawal: empirical validation of symptoms and signs, incidence, severity, and associated features\". Psychopharmacology 176 (1): 1–29.\n20.  ↑ [^Jump up to:20.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid1_20-0) [^20.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid1_20-1) CAIDWELL, John A. et al. (1999) \"The Effects of Modafinil on Aviator Performance During 40 Hours of Continuous Wakefulness: A UH-60 Helicopter Simulator Study.\" Army aeromedical research unit fort rucker al.\n21.  ↑ [^Jump up to:21.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid2_21-0) [^21.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-caid2_21-1) CAIDWELL, John A. et al. (2004) \"The Efficacy of Modafinil for Sustaining Alertness and Simulator Flight Performance in F-117 Pilots During 37 Hours of Continuous Wakefulness.\" Air Force Research lab brooks AFB TX, Human effectiveness Dir/Biodynamics and protection div.\n22.  ↑ [^Jump up to:22.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-li_22-0) [^22.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-li_22-1) LI Yanfeng, ZHAN Hao, XIN Yimei, et al. (2007) \"Effects of modafinil on vestibular function during 24 hour sleep deprivation\". Frontiers of medicine in China, Vol. 1, Number 2, 226-229.\n23.  ↑ [^Jump up to:23.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-bara_23-0) [^23.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-bara_23-1) BARANSKI, J. V. Et al. (2004) \"Effects of modafinil on cognitive and meta-cognitive performance\". Hum Psychopharmacol. 2004 Jul; Vol. 19(5):323-32.\n24.  ↑ [^Jump up to:24.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull1_24-0) [^24.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull1_24-1) MÜLLER, U. Et al. (2004) \"Effects of modafinil on working memory processes in humans\". Psychopharmacology (Berl.) Vol. 177 (1-2): 161–9. **Cite error: Invalid** `**<ref>**` **tag; name \"mull1\" defined multiple times with different content**\n25.  ↑ [^Jump up to:25.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-turn_25-0) [^25.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-turn_25-1) TURNER, D. C et al. (2003). \"Cognitive enhancing effects of modafinil in healthy volunteers\". Psychopharmacology (Berl.) Vol. 165 (3): 260–9.\n26.  ↑ [^Jump up to:26.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull2_26-0) [^26.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-mull2_26-1) MULLER, U. et all. (2012) \"Effects of modafinil on non-verbal cognition, task enjoyment and creative thinking in healthy volunteers.\" Neuropharmocology: 2012 (In press)\n27.  ↑ [^Jump up to:27.0^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-Gilleen_27-0) [^27.1^](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-Gilleen_27-1) GILLEEN, J., et al. (2014). \"Modafinil combined with cognitive training is associated with improved learning in healthy volunteers--a randomised controlled trial.\" European Neuropsychopharmacology : The Journal of the European College of Neuropsychopharmacology, 24(4), 529–39. doi:10.1016/j.euroneuro.2014.01.001.\n28.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-28) [http://www.rxlist.com/provigil-drug/side-effects-interactions.htm](http://www.rxlist.com/provigil-drug/side-effects-interactions.htm)\n29.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-29) YESAVAGE, et al. (2002). \"Donepezil and flight simulator performance Effects on retention of complex skills\" NEUROLOGY 2002; 59:123–125.\n30.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-30) YESAVAGE, et al. (2002). \"Donepezil and flight simulator performance Effects on retention of complex skills\" NEUROLOGY 2002; 59:123–125.\n31.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-31) POHL, Rüdiger (orgs.). (2005) \"Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory\". Psychology Press. pp. 61-78\n32.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-32) BUSS, David(orgs.). (2005) \"The Handbook of Evolutionary Psychology\". Wiley, New Jersey. pp. 739-740.\n33.  [Jump up↑](https://wiki.lesswrong.com/wiki/Biological_Cognitive_Enhancement#cite_ref-33) BOSTROM, Nick & ORD, Toby. (2006) \"The Reversal Test: Eliminating Status Quo Bias in Applied Ethics\". Ethics 116 (Julho 2006): 656-679."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dvQQj5WmQjFB6ZJ5c",
    "name": "Lost Purposes",
    "core": false,
    "slug": "lost-purposes",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wj2iipHgLnHA8ZhZH",
    "name": "Strong Opinions Weakly Held",
    "core": false,
    "slug": "strong-opinions-weakly-held",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "grDZHm8y34H9C7uui",
    "name": "Tacit Knowledge",
    "core": false,
    "slug": "tacit-knowledge",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PAugtjFrmvbepCNej",
    "name": "Love ",
    "core": false,
    "slug": "love",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YhZLQQKsREKE7fC4F",
    "name": "Principal-Agent Problems",
    "core": false,
    "slug": "principal-agent-problems",
    "tableOfContents": {
      "html": "<p><span class=\"by_nLbwLhBaQeG6tCNDN\">A </span><a href=\"https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem\"><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Principal-Agent Problem</span></strong></a><span class=\"by_nLbwLhBaQeG6tCNDN\"> is when a decision is delegated to an agent, and the agent is incentivized to make choices other than those the principal would want.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "A [**Principal-Agent Problem**](https://en.wikipedia.org/wiki/Principal%E2%80%93agent_problem) is when a decision is delegated to an agent, and the agent is incentivized to make choices other than those the principal would want."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v4pviL33XGMuTpSNs",
    "name": "Psychopathy",
    "core": false,
    "slug": "psychopathy",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RffCgqtwT86pNBJof",
    "name": "Shaping Your Environment",
    "core": false,
    "slug": "shaping-your-environment",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Shaping Your Environment</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is sometimes more effective than changing your internal motivation structure, when it comes to behavior change.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Shaping Your Environment** is sometimes more effective than changing your internal motivation structure, when it comes to behavior change."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fuZZ64fNz24BLrXnY",
    "name": "Robotics",
    "core": false,
    "slug": "robotics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Robotics</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is the field dealing with robots. There are various hopes and concerns around this topic, including mass unemployment caused by automation and </span><a href=\"https://www.lesswrong.com/tag/autonomous-weapons\"><span class=\"by_Sp5wM4aRAhNERd4oY\">autonomous weapons</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Robotics** is the field dealing with robots. There are various hopes and concerns around this topic, including mass unemployment caused by automation and [autonomous weapons](https://www.lesswrong.com/tag/autonomous-weapons)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dLfyktLWd7BqtsZBf",
    "name": "Memory Reconsolidation",
    "core": false,
    "slug": "memory-reconsolidation",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NZB24aR9uHmDc5GcT",
    "name": "Sleeping Beauty Paradox",
    "core": false,
    "slug": "sleeping-beauty-paradox",
    "tableOfContents": {
      "html": "<p><span class=\"by_AbLN9sR8PDACCXKp7\">The </span><strong><span><span class=\"by_AbLN9sR8PDACCXKp7\">Sleeping Beauty </span><span class=\"by_HoGziwmhpMGqGeWZy\">Paradox</span></span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is a question of how</span><span class=\"by_AbLN9sR8PDACCXKp7\"> </span></span><a href=\"http://lesswrong.com/tag/anthropics\"><span class=\"by_HoGziwmhpMGqGeWZy\">anthropics</span></a><span><span class=\"by_AbLN9sR8PDACCXKp7\"> </span><span class=\"by_HoGziwmhpMGqGeWZy\">affects </span></span><a href=\"https://www.lesswrong.com/tag/probability-and-statistics\"><span class=\"by_HoGziwmhpMGqGeWZy\">probabilities</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><blockquote><p><span><span class=\"by_AbLN9sR8PDACCXKp7\">Sleeping Beauty volunteers to undergo the following </span><span class=\"by_HoGziwmhpMGqGeWZy\">experiment.</span><span class=\"by_AbLN9sR8PDACCXKp7\"> On Sunday she is given a drug that sends her to </span><span class=\"by_HoGziwmhpMGqGeWZy\">sleep. A fair</span><span class=\"by_AbLN9sR8PDACCXKp7\"> coin is </span><span class=\"by_HoGziwmhpMGqGeWZy\">then tossed just once in the course of the experiment to determine which experimental procedure is undertaken.</span><span class=\"by_AbLN9sR8PDACCXKp7\"> If the coin </span><span class=\"by_HoGziwmhpMGqGeWZy\">comes up</span><span class=\"by_AbLN9sR8PDACCXKp7\"> heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping </span><span class=\"by_HoGziwmhpMGqGeWZy\">drug,</span><span class=\"by_AbLN9sR8PDACCXKp7\"> and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again.</span><span class=\"by_HoGziwmhpMGqGeWZy\"> The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of</span><span class=\"by_AbLN9sR8PDACCXKp7\"> the experiment </span><span class=\"by_HoGziwmhpMGqGeWZy\">(if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.</span></span></p></blockquote><blockquote><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">Each interview consists of one question, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“What</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is your credence now for the proposition that our coin landed heads?</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span></span></p></blockquote><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">One argument says that since Beauty will see the same thing on waking whether the coin came up heads or not, what she sees on waking provides no evidence one way or the other about the coin, </span><span class=\"by_AbLN9sR8PDACCXKp7\">and </span><span class=\"by_HoGziwmhpMGqGeWZy\">therefore she should stick with the prior</span><span class=\"by_AbLN9sR8PDACCXKp7\"> probability </span><span class=\"by_HoGziwmhpMGqGeWZy\">of one half.</span></span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">Another argument replies</span><span class=\"by_AbLN9sR8PDACCXKp7\"> that the </span><span class=\"by_HoGziwmhpMGqGeWZy\">two awakenings when the </span><span class=\"by_AbLN9sR8PDACCXKp7\">coin </span><span class=\"by_HoGziwmhpMGqGeWZy\">comes up tails imply that waking up itself should be considered evidence in favor of tails. Out of all possible situations where</span><span class=\"by_AbLN9sR8PDACCXKp7\"> Beauty </span><span class=\"by_HoGziwmhpMGqGeWZy\">is asked the question, only one out of three has the coin showing heads. Therefore, one third.</span></span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">A third argument tries to add rigor by considering monetary payoffs. If </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Beauty'</span><span class=\"by_AbLN9sR8PDACCXKp7\">s </span><span class=\"by_HoGziwmhpMGqGeWZy\">bets about the coin get paid out once per experiment, she will do best by acting as if the probability is one half. If the bets get paid out once per awakening, acting as if the probability is one third has the best expected value.</span></span></p><h2 id=\"External_Links\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links</span></h2><ul><li><a href=\"https://www.youtube.com/watch?v=zL52lG6aNIY\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Video explanation by Julia Galef</span></a></li><li><a href=\"https://en.wikipedia.org/wiki/Sleeping_Beauty_problem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Sleeping Beauty Problem on Wikipedia</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Theory</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Problem</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/counterfactual-mugging\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Counterfactual </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Mugging</span></span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Parfit's hitchhiker</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker%27s_lesion\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Smoker's lesion</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Absentminded driver</span></u></a></li><li><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner's </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Dilemma</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/pascal-s-mugging\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal's </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Mugging</span></span></a></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 60,
    "description": {
      "markdown": "The **Sleeping Beauty Paradox** is a question of how [anthropics](http://lesswrong.com/tag/anthropics) affects [probabilities](https://www.lesswrong.com/tag/probability-and-statistics).\n\n> Sleeping Beauty volunteers to undergo the following experiment. On Sunday she is given a drug that sends her to sleep. A fair coin is then tossed just once in the course of the experiment to determine which experimental procedure is undertaken. If the coin comes up heads, Beauty is awakened and interviewed on Monday, and then the experiment ends. If the coin comes up tails, she is awakened and interviewed on Monday, given a second dose of the sleeping drug, and awakened and interviewed again on Tuesday. The experiment then ends on Tuesday, without flipping the coin again. The sleeping drug induces a mild amnesia, so that she cannot remember any previous awakenings during the course of the experiment (if any). During the experiment, she has no access to anything that would give a clue as to the day of the week. However, she knows all the details of the experiment.\n\n> Each interview consists of one question, “What is your credence now for the proposition that our coin landed heads?”\n\nOne argument says that since Beauty will see the same thing on waking whether the coin came up heads or not, what she sees on waking provides no evidence one way or the other about the coin, and therefore she should stick with the prior probability of one half.\n\nAnother argument replies that the two awakenings when the coin comes up tails imply that waking up itself should be considered evidence in favor of tails. Out of all possible situations where Beauty is asked the question, only one out of three has the coin showing heads. Therefore, one third.\n\nA third argument tries to add rigor by considering monetary payoffs. If Beauty's bets about the coin get paid out once per experiment, she will do best by acting as if the probability is one half. If the bets get paid out once per awakening, acting as if the probability is one third has the best expected value.\n\nExternal Links\n--------------\n\n*   [Video explanation by Julia Galef](https://www.youtube.com/watch?v=zL52lG6aNIY)\n*   [Sleeping Beauty Problem on Wikipedia](https://en.wikipedia.org/wiki/Sleeping_Beauty_problem)\n\nSee Also\n--------\n\n*   [Decision Theory](https://www.lesswrong.com/tag/decision-theory)\n*   [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem)\n*   [Counterfactual Mugging](https://www.lesswrong.com/tag/counterfactual-mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker%27s_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Prisoner's Dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's Mugging](https://www.lesswrong.com/tag/pascal-s-mugging)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JMD7LTXTisBzGAfhX",
    "name": "Rationality A-Z (discussion & meta)",
    "core": false,
    "slug": "rationality-a-z-discussion-and-meta",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Rationality: From AI to Zombies</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">, also known as </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">The Sequences</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">, is a series of essays by Eliezer Yudkowsky published from 2006 to 2009, which were later compiled into a book. This tag is for critiques and discussion of the Sequences, attempts to organize the Sequences, and the publication of Rationality: From AI to Zombies.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">To read the book, see </span><a href=\"https://lesswrong.com/rationality\"><span class=\"by_HoGziwmhpMGqGeWZy\">lesswrong.com/rationality</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> </span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 57,
    "description": {
      "markdown": "**Rationality: From AI to Zombies**, also known as **The Sequences**, is a series of essays by Eliezer Yudkowsky published from 2006 to 2009, which were later compiled into a book. This tag is for critiques and discussion of the Sequences, attempts to organize the Sequences, and the publication of Rationality: From AI to Zombies.\n\nTo read the book, see [lesswrong.com/rationality](https://lesswrong.com/rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ThuGDzi7X5YsAJfjs",
    "name": "Aggregation",
    "core": false,
    "slug": "aggregation",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Jj9QvYCLHEoDqvpaR",
    "name": "Request Post",
    "core": false,
    "slug": "request-post",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qQMEMrXioExa4uhTB",
    "name": "Value Drift",
    "core": false,
    "slug": "value-drift",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Xw6pxiicjuv6NJWjf",
    "name": "History of Rationality",
    "core": false,
    "slug": "history-of-rationality",
    "tableOfContents": {
      "html": "<p><span class=\"by_pbREHuM5F5t5nyWqh\">A post is relevant to the </span><strong><span class=\"by_pbREHuM5F5t5nyWqh\">history of rationality</span></strong><span class=\"by_pbREHuM5F5t5nyWqh\"> if it discusses past events pertaining to </span><a href=\"https://www.lesswrong.com/tag/community\"><span class=\"by_pbREHuM5F5t5nyWqh\">the LW community</span></a><span class=\"by_pbREHuM5F5t5nyWqh\"> or another topic related to </span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_pbREHuM5F5t5nyWqh\">rationality</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">, or the post is of strong historical interest.</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">You can see a list of all LW posts, from oldest to newest, </span><a href=\"https://wiki.lesswrong.com/wiki/Less_Wrong/All_articles\"><span class=\"by_pbREHuM5F5t5nyWqh\">here</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">.</span></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">If you're trying to get a better understanding of the community and culture here, you may also be interested in the tag </span><a href=\"https://www.lesswrong.com/tag/terminology-jargon-meta\"><span class=\"by_pbREHuM5F5t5nyWqh\">Terminology/Jargon (meta)</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/history-of-less-wrong\"><span class=\"by_sKAL2jzfkYkDbQmx9\">History of Less Wrong</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/history\"><span class=\"by_sKAL2jzfkYkDbQmx9\">History</span></a></p><p><span class=\"by_pbREHuM5F5t5nyWqh\">Historically relevant organizations, groups, and movements include:</span></p><ul><li><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">CFAR</span></a></li><li><a href=\"https://www.lesswrong.com/tag/80-000-hours?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">80,000 Hours</span></a></li><li><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">CHAI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">FHI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">MIRI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/openai?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">OpenAI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/ought?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">Ought</span></a></li><li><a href=\"https://www.lesswrong.com/tag/givewell?showPostCount=true&amp;useTagName=true\"><span class=\"by_pbREHuM5F5t5nyWqh\">GiveWell</span></a></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 25,
    "description": {
      "markdown": "A post is relevant to the **history of rationality** if it discusses past events pertaining to [the LW community](https://www.lesswrong.com/tag/community) or another topic related to [rationality](https://www.lesswrong.com/tag/rationality), or the post is of strong historical interest.\n\nYou can see a list of all LW posts, from oldest to newest, [here](https://wiki.lesswrong.com/wiki/Less_Wrong/All_articles).\n\nIf you're trying to get a better understanding of the community and culture here, you may also be interested in the tag [Terminology/Jargon (meta)](https://www.lesswrong.com/tag/terminology-jargon-meta).\n\n**Related Pages:** [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong), [History](https://www.lesswrong.com/tag/history)\n\nHistorically relevant organizations, groups, and movements include:\n\n*   [CFAR](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar?showPostCount=true&useTagName=true)\n*   [80,000 Hours](https://www.lesswrong.com/tag/80-000-hours?showPostCount=true&useTagName=true)\n*   [CHAI](https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&useTagName=true)\n*   [FHI](https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&useTagName=true)\n*   [MIRI](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true&useTagName=true)\n*   [OpenAI](https://www.lesswrong.com/tag/openai?showPostCount=true&useTagName=true)\n*   [Ought](https://www.lesswrong.com/tag/ought?showPostCount=true&useTagName=true)\n*   [GiveWell](https://www.lesswrong.com/tag/givewell?showPostCount=true&useTagName=true)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EuDw6uxQW2ZBRFhMo",
    "name": "Aversion",
    "core": false,
    "slug": "aversion",
    "tableOfContents": {
      "html": "<p><span class=\"by_2aoRX3ookcCozcb3m\">CFAR's </span><a href=\"https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all\"><span class=\"by_2aoRX3ookcCozcb3m\">2019 workshop participant handbook</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> defines an </span><strong><span class=\"by_2aoRX3ookcCozcb3m\">aversion</span></strong><span class=\"by_2aoRX3ookcCozcb3m\"> as</span></p><blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">any sort of mental mechanism that causes us to be less likely to engage in a particular activity, or to do so only with pain, displeasure, or discomfort. Aversions can be conscious or unconscious, reasoned or felt, verbal or visceral, and they can range anywhere from a slight tinge of antipathy to outright phobias.</span></p></blockquote><p><a href=\"https://acritch.com/aversions/\"><strong><span class=\"by_2aoRX3ookcCozcb3m\">Aversion factoring</span></strong></a><span class=\"by_2aoRX3ookcCozcb3m\"> is </span><a href=\"https://www.lesswrong.com/tag/goal-factoring\"><span class=\"by_2aoRX3ookcCozcb3m\">goal factoring</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> for aversions: trying to understand why you're averse to a particular thing by decomposing it into underlying preferences and experiences.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Ugh_fields\"><span class=\"by_2aoRX3ookcCozcb3m\">Ugh fields</span></h2><p><span class=\"by_2aoRX3ookcCozcb3m\">Aversions make us less likely to engage in \"activities,\" and as Scott Alexander's (2011) </span><a href=\"https://www.lesswrong.com/posts/5dhWhjfxn4tPfFQdi/physical-and-mental-behavior\"><span class=\"by_2aoRX3ookcCozcb3m\">Physical and Mental Behavior</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> notes, this includes mental as well as physical activities. </span><span class=\"by_KneTmopEjYGsaPYNi\">Pavlovian conditioning can cause humans to unconsciously flinch from even thinking about a serious personal problem they </span><span class=\"by_qf77EiaoMw7tH3GSr\">have.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Writing in 2010, Roko introduced the term </span><a href=\"https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields\"><strong><span><span class=\"by_LoykQRMTxJFxwwdPy\">ugh</span><span class=\"by_KneTmopEjYGsaPYNi\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">field</span></span></strong></a><span class=\"by_2aoRX3ookcCozcb3m\"> to refer to this problem:&nbsp;</span></p><blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">If you fail or are punished sufficiently many times in some problem area, and acting in that area is always [preceded] by thinking about it, your brain will propagate the psychological pain right back to the moment you first begin to entertain a thought about the problem, and hence cut your conscious optimizing ability right out of the loop.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">[...] The subtlety with the Ugh Field is that the flinch occurs </span><i><strong><span class=\"by_2aoRX3ookcCozcb3m\">before you start to consciously think</span></strong></i><span class=\"by_2aoRX3ookcCozcb3m\"> about how to deal with the Unhappy Thing, meaning that you never deal with it, and you don't even have the option of dealing with it in the normal run of things. I find it frightening that my lizard brain could implicitly be making life decisions for me, without even asking my permission!</span></p></blockquote><p><span><span class=\"by_2aoRX3ookcCozcb3m\">The ugh field</span><span class=\"by_KneTmopEjYGsaPYNi\"> forms a self-shadowing blind spot covering an area desperately in need of </span><span class=\"by_qf77EiaoMw7tH3GSr\">optimization.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Blog_posts_and_external_links\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span><span class=\"by_2aoRX3ookcCozcb3m\"> and external links</span></span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Avoiding Your Belief's Real Weak Points</span></a></li><li><a href=\"https://www.lesswrong.com/lw/2cv/defeating_ugh_fields_in_practice/\"><span class=\"by_KneTmopEjYGsaPYNi\">Defeating Ugh Fields in Practice</span></a><span><span class=\"by_KneTmopEjYGsaPYNi\"> by </span><span class=\"by_LoykQRMTxJFxwwdPy\">Psychohistorian</span></span></li><li><a href=\"https://www.lesswrong.com/lw/5a9/learned_blankness/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Learned Blankness</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"https://www.lesswrong.com/tag/anna-salamon\"><span class=\"by_LoykQRMTxJFxwwdPy\">Anna Salamon</span></a></li><li><a href=\"https://www.lesswrong.com/lw/4up/dont_fear_failure/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Don't Fear Failure</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"http://shugyoshayear.com/\"><span class=\"by_LoykQRMTxJFxwwdPy\">atucker</span></a></li><li><span class=\"by_2aoRX3ookcCozcb3m\">Wikipedia: </span><a href=\"https://en.wikipedia.org/wiki/Experiential_avoidance\"><span class=\"by_2aoRX3ookcCozcb3m\">Experiential Avoidance</span></a></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Related_pages\"><span class=\"by_2aoRX3ookcCozcb3m\">Related pages</span></h2><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">Non-tags: </span><a href=\"https://wiki.lesswrong.com/wiki/Curiosity_stopper\"><span class=\"by_2aoRX3ookcCozcb3m\">Semantic stopsign</span></a></li><li><a href=\"https://www.lesswrong.com/tag/akrasia\"><span class=\"by_qf77EiaoMw7tH3GSr\">Akrasia</span></a></li><li><a href=\"https://www.lesswrong.com/tag/compartmentalization\"><span class=\"by_2aoRX3ookcCozcb3m\">Compartmentalization</span></a></li><li><a href=\"https://www.lesswrong.com/tag/motivated-reasoning\"><span class=\"by_2aoRX3ookcCozcb3m\">Motivated reasoning</span></a></li><li><a href=\"https://www.lesswrong.com/tag/motivations\"><span class=\"by_2aoRX3ookcCozcb3m\">Motivations</span></a></li><li><a href=\"https://www.lesswrong.com/tag/priming\"><span class=\"by_qf77EiaoMw7tH3GSr\">Priming</span></a></li><li><a href=\"https://www.lesswrong.com/tag/trivial-inconvenience\"><span class=\"by_qf77EiaoMw7tH3GSr\">Trivial inconvenience</span></a></li></ul>",
      "sections": [
        {
          "title": "Ugh fields",
          "anchor": "Ugh_fields",
          "level": 1
        },
        {
          "title": "Blog posts and external links",
          "anchor": "Blog_posts_and_external_links",
          "level": 1
        },
        {
          "title": "Related pages",
          "anchor": "Related_pages",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 17,
    "description": {
      "markdown": "CFAR's [2019 workshop participant handbook](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all) defines an **aversion** as\n\n> any sort of mental mechanism that causes us to be less likely to engage in a particular activity, or to do so only with pain, displeasure, or discomfort. Aversions can be conscious or unconscious, reasoned or felt, verbal or visceral, and they can range anywhere from a slight tinge of antipathy to outright phobias.\n\n[**Aversion factoring**](https://acritch.com/aversions/) is [goal factoring](https://www.lesswrong.com/tag/goal-factoring) for aversions: trying to understand why you're averse to a particular thing by decomposing it into underlying preferences and experiences.\n\nUgh fields\n----------\n\nAversions make us less likely to engage in \"activities,\" and as Scott Alexander's (2011) [Physical and Mental Behavior](https://www.lesswrong.com/posts/5dhWhjfxn4tPfFQdi/physical-and-mental-behavior) notes, this includes mental as well as physical activities. Pavlovian conditioning can cause humans to unconsciously flinch from even thinking about a serious personal problem they have.\n\nWriting in 2010, Roko introduced the term [**ugh field**](https://www.lesswrong.com/posts/EFQ3F6kmt4WHXRqik/ugh-fields) to refer to this problem: \n\n> If you fail or are punished sufficiently many times in some problem area, and acting in that area is always \\[preceded\\] by thinking about it, your brain will propagate the psychological pain right back to the moment you first begin to entertain a thought about the problem, and hence cut your conscious optimizing ability right out of the loop.\n> \n> \\[...\\] The subtlety with the Ugh Field is that the flinch occurs ***before you start to consciously think*** about how to deal with the Unhappy Thing, meaning that you never deal with it, and you don't even have the option of dealing with it in the normal run of things. I find it frightening that my lizard brain could implicitly be making life decisions for me, without even asking my permission!\n\nThe ugh field forms a self-shadowing blind spot covering an area desperately in need of optimization.\n\nBlog posts and external links\n-----------------------------\n\n*   [Avoiding Your Belief's Real Weak Points](https://www.lesswrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/)\n*   [Defeating Ugh Fields in Practice](https://www.lesswrong.com/lw/2cv/defeating_ugh_fields_in_practice/) by Psychohistorian\n*   [Learned Blankness](https://www.lesswrong.com/lw/5a9/learned_blankness/) by [Anna Salamon](https://www.lesswrong.com/tag/anna-salamon)\n*   [Don't Fear Failure](https://www.lesswrong.com/lw/4up/dont_fear_failure/) by [atucker](http://shugyoshayear.com/)\n*   Wikipedia: [Experiential Avoidance](https://en.wikipedia.org/wiki/Experiential_avoidance)\n\nRelated pages\n-------------\n\n*   Non-tags: [Semantic stopsign](https://wiki.lesswrong.com/wiki/Curiosity_stopper)\n*   [Akrasia](https://www.lesswrong.com/tag/akrasia)\n*   [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization)\n*   [Motivated reasoning](https://www.lesswrong.com/tag/motivated-reasoning)\n*   [Motivations](https://www.lesswrong.com/tag/motivations)\n*   [Priming](https://www.lesswrong.com/tag/priming)\n*   [Trivial inconvenience](https://www.lesswrong.com/tag/trivial-inconvenience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "N3CqcPdCZNspF9bFb",
    "name": "Emergent Behavior",
    "core": false,
    "slug": "emergent-behavior",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zss67HQhzn98E6ejQ",
    "name": "Goals",
    "core": false,
    "slug": "goals",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KJbjewaok72AS6fwn",
    "name": "Levels of Intervention",
    "core": false,
    "slug": "levels-of-intervention",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DsdbQhWAnPqfzo4Yw",
    "name": "Reversal Test",
    "core": false,
    "slug": "reversal-test",
    "tableOfContents": {
      "html": "<p><span class=\"by_qf77EiaoMw7tH3GSr\">The </span><strong><span class=\"by_qf77EiaoMw7tH3GSr\">reversal test</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a technique for fighting </span><a href=\"https://wiki.lesswrong.com/wiki/Status_quo_bias\"><u><span class=\"by_qf77EiaoMw7tH3GSr\">status quo bias</span></u></a><span class=\"by_qf77EiaoMw7tH3GSr\"> in judgments about the preferred value of a continuous parameter. If one deems the change of the parameter in one direction to be undesirable, the reversal test is to check that either the change of that parameter in the opposite direction (away from status quo) is deemed desirable, or that there are strong reasons to expect that the current value of the parameter is (at least locally) the optimal one.</span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">For example, if it became possible to increase the human lifespan, some would argue that it would be undesirable for people to live longer because, say, overpopulation would be difficult to manage. The reversal test is then to check that the same people accept that </span><i><span class=\"by_qf77EiaoMw7tH3GSr\">shorter</span></i><span class=\"by_qf77EiaoMw7tH3GSr\"> lifespan is desirable, or that there are really strong reasons to believe that the current lifespan happens to be optimal.</span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">The rationale of the Reversal Test is simple: if a continuous parameter admits of a wide range of possible values, only a tiny subset of which can be local optima, then it is prima facie implausible that the actual value of that parameter should just happen to be at one of these rare local optima [...] the burden of proof shifts to those who maintain that some actual parameter is at such a local optimum: they need to provide some good reason for supposing that it is so.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Obviously, the Reversal Test does not show that preferring the status quo is always unjustified. In many cases, it is possible to meet the challenge posed by the Reversal Test</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">—The reversal test: eliminating status quo bias in applied ethics</span></p></blockquote><h2 id=\"Main_article\"><span class=\"by_qf77EiaoMw7tH3GSr\">Main article</span></h2><ul><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Nick Bostrom, Toby Ord (2006). \"The reversal test: eliminating status quo bias in applied ethics\". </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Ethics</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (University of Chicago Press) </span><strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">116</span></strong><span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (4): 656-679. </span><span class=\"by_qf77EiaoMw7tH3GSr\">(</span></span><a href=\"http://www.nickbostrom.com/ethics/statusquo.pdf\"><u><span class=\"by_qf77EiaoMw7tH3GSr\">PDF</span></u></a><span class=\"by_qf77EiaoMw7tH3GSr\">)</span></li></ul><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Status_quo_bias\"><u><span class=\"by_qf77EiaoMw7tH3GSr\">Status quo bias</span></u></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Privileging_the_hypothesis\"><u><span class=\"by_qf77EiaoMw7tH3GSr\">Privileging the hypothesis</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Shut_up_and_multiply\"><u><span class=\"by_qf77EiaoMw7tH3GSr\">Shut up and multiply</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absurdity_heuristic\"><u><span class=\"by_qf77EiaoMw7tH3GSr\">Absurdity heuristic</span></u></a></li></ul><h2 id=\"External_links\"><span class=\"by_BtbwfsEyeT4P2eqXu\">External links</span></h2><ul><li><a href=\"http://philosophicaldisquisitions.blogspot.com/2012/11/the-reversal-test-and-status-quo-bias.html\"><u><span class=\"by_BtbwfsEyeT4P2eqXu\">\"The Reversal Test and Status Quo Bias\"</span></u></a><span class=\"by_BtbwfsEyeT4P2eqXu\"> (John Danaher)</span></li></ul>",
      "sections": [
        {
          "title": "Main article",
          "anchor": "Main_article",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 5,
    "description": {
      "markdown": "The **reversal test** is a technique for fighting [status quo bias](https://wiki.lesswrong.com/wiki/Status_quo_bias) in judgments about the preferred value of a continuous parameter. If one deems the change of the parameter in one direction to be undesirable, the reversal test is to check that either the change of that parameter in the opposite direction (away from status quo) is deemed desirable, or that there are strong reasons to expect that the current value of the parameter is (at least locally) the optimal one.\n\nFor example, if it became possible to increase the human lifespan, some would argue that it would be undesirable for people to live longer because, say, overpopulation would be difficult to manage. The reversal test is then to check that the same people accept that *shorter* lifespan is desirable, or that there are really strong reasons to believe that the current lifespan happens to be optimal.\n\n> The rationale of the Reversal Test is simple: if a continuous parameter admits of a wide range of possible values, only a tiny subset of which can be local optima, then it is prima facie implausible that the actual value of that parameter should just happen to be at one of these rare local optima \\[...\\] the burden of proof shifts to those who maintain that some actual parameter is at such a local optimum: they need to provide some good reason for supposing that it is so.\n> \n> Obviously, the Reversal Test does not show that preferring the status quo is always unjustified. In many cases, it is possible to meet the challenge posed by the Reversal Test\n> \n> —The reversal test: eliminating status quo bias in applied ethics\n\nMain article\n------------\n\n*   Nick Bostrom, Toby Ord (2006). \"The reversal test: eliminating status quo bias in applied ethics\". *Ethics* (University of Chicago Press) **116** (4): 656-679. ([PDF](http://www.nickbostrom.com/ethics/statusquo.pdf))\n\nSee also\n--------\n\n*   [Status quo bias](https://wiki.lesswrong.com/wiki/Status_quo_bias), [Privileging the hypothesis](https://wiki.lesswrong.com/wiki/Privileging_the_hypothesis)\n*   [Shut up and multiply](https://wiki.lesswrong.com/wiki/Shut_up_and_multiply)\n*   [Absurdity heuristic](https://wiki.lesswrong.com/wiki/Absurdity_heuristic)\n\nExternal links\n--------------\n\n*   [\"The Reversal Test and Status Quo Bias\"](http://philosophicaldisquisitions.blogspot.com/2012/11/the-reversal-test-and-status-quo-bias.html) (John Danaher)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tRPnS4FoZeWjRfBxN",
    "name": "Skill / Expertise Assessment",
    "core": false,
    "slug": "skill-expertise-assessment",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CztjQPSTuaQcfbyh8",
    "name": "Singularity",
    "core": false,
    "slug": "singularity",
    "tableOfContents": {
      "html": "<p><span class=\"by_NRg5Bw8H2DCYTpmHE\">The </span><strong><span class=\"by_NRg5Bw8H2DCYTpmHE\">Singularity</span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Technological Singularity</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_qxJ28GN72aiJu96iF\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a term with a number of different meanings, ranging from a period of rapid change to </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">creation of greater-than-human intelligence.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intelligence explosion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/event-horizon-thesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Event horizon thesis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Hard_takeoff\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hard takeoff</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Soft_takeoff\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Soft takeoff</span></a></p><h2 id=\"Three_Singularity_schools\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Three Singularity schools</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky has observed that the varying perspectives on the Singularity can be broadly split into three \"major schools\" - Accelerating Change (Ray Kurzweil), the Event Horizon (Vernor Vinge), and the Intelligence Explosion (I.J. Good).</span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">The Accelerating Change School</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> observes that, contrary to our intuitive linear expectations about the future, the rate of change of information technology grows exponentially. In the last 200 years, we have seen </span><span class=\"by_r38pkCm7wF4M44MDQ\">more </span></span><a href=\"https://www.lesswrong.com/tag/technological-revolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">technological revolutions</span></a><span><span class=\"by_r38pkCm7wF4M44MDQ\"> than </span><span class=\"by_qgdGA4ZEyW7zNdK84\">in the last </span><span class=\"by_sKAL2jzfkYkDbQmx9\">20,</span><span class=\"by_qgdGA4ZEyW7zNdK84\">000 before that. Clear examples of this exponentiality include, but are not restricted to: Moore’s law, Internet speed, gene sequencing</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the spatial resolution of brain scanning. By projecting these technology growths into</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> the future </span><span class=\"by_qgdGA4ZEyW7zNdK84\">it </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">becomes </span><span class=\"by_qgdGA4ZEyW7zNdK84\">possible to imagine what will be possible to engineer in the future. </span></span><a href=\"https://en.wikipedia.org/wiki/Ray_Kurzweil\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ray Kurzweil</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> specifically dates the Singularity happening in 2045.</span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">The Event Horizon School</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> asserts that for the entirety of Earth’s history all technological and social progress has been the product of the human mind. However, </span><a href=\"https://en.wikipedia.org/wiki/Vernor_Vinge\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Vernor Vinge</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> asserts that technology will soon improve on human intelligence either via brain-computer interfaces or Artificial Intelligence or both. Vinge argues since one must be at least as smart as the agent to be predicted, after we create smarter than human agents technological progress will be beyond the comprehension of anything a mere human can imagine now. He called this point in time the Singularity.</span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span></strong><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Intelligence explosion</span></strong></a><strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> School</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> asserts that a positive feedback loop could be created in which an intelligence is making itself smarter, thus getting better at making itself even smarter. A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a dramatic leap in capability very quickly. This scenario does not necessarily rely upon an entirely computing substrate for the explosion to occur, humans with computer augmented brains or genetically altered may also be methods to engineer an Intelligence Explosion. </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">It is this interpretation of the Singularity that Less Wrong broadly focuses on.</span></strong></p><h2 id=\"Chalmers__analysis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Chalmers' analysis</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Philosopher David Chalmers published a </span><a href=\"http://consc.net/papers/singularity.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">significant analysis of the Singularity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, focusing on intelligence explosions, in </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Journal of Consciousness Studies</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. He performed a very careful analysis of the main premises and arguments for the existence of the singularity. According to him, the main argument is:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">1. There will be AI (before long, absent defeaters).</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">2. If there is AI, there will be AI+ (soon after, absent defeaters).</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">3. If there is AI+, there will be AI++ (soon after, absent defeaters).</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">—————-</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">4. There will be AI++ (before too long, absent defeaters).</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">He then proceeds to search for arguments for these 3 premises. Premise 1 seems to be grounded in either </span><a href=\"https://www.lesswrong.com/tag/evolutionary-argument-for-human-level-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolutionary argument for human-level AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><a href=\"https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Emulation argument for human-level AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Premise 2 is grounded in the existence and feasibility of an </span><a href=\"https://www.lesswrong.com/tag/extensibility-argument-for-greater-than-human-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">extensibility method for greater-than-human intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Premise 3 is a more general version of premise 2. His analysis of how the singularity could occur defends the likelihood of an intelligence explosion. He also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.</span></p><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ul><li><a href=\"http://www.stat.vt.edu/tech_reports/2005/GoodTechReport.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Speculations Concerning the First Ultraintelligent Machine</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by I.J. Good</span></li><li><a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Coming Technological Singularity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> Essay by Vernor Vinge</span></li><li><a href=\"http://web.archive.org/web/20110716035716/http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">An overview of models of technological singularity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Anders Sandberg</span></li><li><a href=\"http://consc.net/papers/singularity.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Singularity: A Philosophical Analysis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by David J. Chalmers</span></li><li><a href=\"http://www.kurzweilai.net/artificial-superintelligence-a-futuristic-approach\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Artificial Superintelligence: A Futuristic Approach</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Roman V. Yampolskiy</span></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://yudkowsky.net/singularity/schools\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Three Major Singularity Schools</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Eliezer Yudkowsky</span></li><li><a href=\"http://www.youtube.com/watch?v=IfbOyw3CT6A\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Singularity TED Talk</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Ray Kurzweil (YouTube)</span></li><li><a href=\"http://www.youtube.com/watch?v=mDhdt58ySJA\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Singularity Three Major Schools of Thought</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> Singularity Summit Talk by Eliezer Yudkowsky</span></li><li><a href=\"http://cser.org/resources.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Centre for the study of Existential Risk</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> Web site of University of Cambridge</span></li></ul>",
      "sections": [
        {
          "title": "Three Singularity schools",
          "anchor": "Three_Singularity_schools",
          "level": 1
        },
        {
          "title": "Chalmers' analysis",
          "anchor": "Chalmers__analysis",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 19,
    "description": {
      "markdown": "The **Singularity** or **Technological Singularity** is a term with a number of different meanings, ranging from a period of rapid change to the creation of greater-than-human intelligence.\n\n*See also:* [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Event horizon thesis](https://www.lesswrong.com/tag/event-horizon-thesis), [Hard takeoff](https://wiki.lesswrong.com/wiki/Hard_takeoff), [Soft takeoff](https://wiki.lesswrong.com/wiki/Soft_takeoff)\n\nThree Singularity schools\n-------------------------\n\nEliezer Yudkowsky has observed that the varying perspectives on the Singularity can be broadly split into three \"major schools\" - Accelerating Change (Ray Kurzweil), the Event Horizon (Vernor Vinge), and the Intelligence Explosion (I.J. Good).\n\n**The Accelerating Change School** observes that, contrary to our intuitive linear expectations about the future, the rate of change of information technology grows exponentially. In the last 200 years, we have seen more [technological revolutions](https://www.lesswrong.com/tag/technological-revolution) than in the last 20,000 before that. Clear examples of this exponentiality include, but are not restricted to: Moore’s law, Internet speed, gene sequencing and the spatial resolution of brain scanning. By projecting these technology growths into the future it becomes possible to imagine what will be possible to engineer in the future. [Ray Kurzweil](https://en.wikipedia.org/wiki/Ray_Kurzweil) specifically dates the Singularity happening in 2045.\n\n**The Event Horizon School** asserts that for the entirety of Earth’s history all technological and social progress has been the product of the human mind. However, [Vernor Vinge](https://en.wikipedia.org/wiki/Vernor_Vinge) asserts that technology will soon improve on human intelligence either via brain-computer interfaces or Artificial Intelligence or both. Vinge argues since one must be at least as smart as the agent to be predicted, after we create smarter than human agents technological progress will be beyond the comprehension of anything a mere human can imagine now. He called this point in time the Singularity.\n\n**The** [**Intelligence explosion**](https://www.lesswrong.com/tag/intelligence-explosion) **School** asserts that a positive feedback loop could be created in which an intelligence is making itself smarter, thus getting better at making itself even smarter. A strong version of this idea suggests that once the positive feedback starts to play a role, it will lead to a dramatic leap in capability very quickly. This scenario does not necessarily rely upon an entirely computing substrate for the explosion to occur, humans with computer augmented brains or genetically altered may also be methods to engineer an Intelligence Explosion. **It is this interpretation of the Singularity that Less Wrong broadly focuses on.**\n\nChalmers' analysis\n------------------\n\nPhilosopher David Chalmers published a [significant analysis of the Singularity](http://consc.net/papers/singularity.pdf), focusing on intelligence explosions, in *Journal of Consciousness Studies*. He performed a very careful analysis of the main premises and arguments for the existence of the singularity. According to him, the main argument is:\n\n*   1\\. There will be AI (before long, absent defeaters).\n*   2\\. If there is AI, there will be AI+ (soon after, absent defeaters).\n*   3\\. If there is AI+, there will be AI++ (soon after, absent defeaters).\n\n—————-\n\n*   4\\. There will be AI++ (before too long, absent defeaters).\n\nHe then proceeds to search for arguments for these 3 premises. Premise 1 seems to be grounded in either [Evolutionary argument for human-level AI](https://www.lesswrong.com/tag/evolutionary-argument-for-human-level-ai) or [Emulation argument for human-level AI](https://www.lesswrong.com/tag/emulation-argument-for-human-level-ai). Premise 2 is grounded in the existence and feasibility of an [extensibility method for greater-than-human intelligence](https://www.lesswrong.com/tag/extensibility-argument-for-greater-than-human-intelligence). Premise 3 is a more general version of premise 2. His analysis of how the singularity could occur defends the likelihood of an intelligence explosion. He also discusses the nature of general intelligence, and possible obstacles to a singularity. A good deal of discussion is given to the dangers of an intelligence explosion, and Chalmers concludes that we must negotiate it very carefully by building the correct values into the initial AIs.\n\nReferences\n----------\n\n*   [Speculations Concerning the First Ultraintelligent Machine](http://www.stat.vt.edu/tech_reports/2005/GoodTechReport.pdf) by I.J. Good\n*   [The Coming Technological Singularity](http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html) Essay by Vernor Vinge\n*   [An overview of models of technological singularity](http://web.archive.org/web/20110716035716/http://agi-conf.org/2010/wp-content/uploads/2009/06/agi10singmodels2.pdf) by Anders Sandberg\n*   [The Singularity: A Philosophical Analysis](http://consc.net/papers/singularity.pdf) by David J. Chalmers\n*   [Artificial Superintelligence: A Futuristic Approach](http://www.kurzweilai.net/artificial-superintelligence-a-futuristic-approach) by Roman V. Yampolskiy\n\nExternal links\n--------------\n\n*   [Three Major Singularity Schools](http://yudkowsky.net/singularity/schools) by Eliezer Yudkowsky\n*   [Singularity TED Talk](http://www.youtube.com/watch?v=IfbOyw3CT6A) by Ray Kurzweil (YouTube)\n*   [The Singularity Three Major Schools of Thought](http://www.youtube.com/watch?v=mDhdt58ySJA) Singularity Summit Talk by Eliezer Yudkowsky\n*   [Centre for the study of Existential Risk](http://cser.org/resources.html) Web site of University of Cambridge"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YwcMcGypGWqtiBKvD",
    "name": "Success Spiral",
    "core": false,
    "slug": "success-spiral",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SEuoBQeHLYd9dtqpK",
    "name": "Social Skills",
    "core": false,
    "slug": "social-skills",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tgJoX7PGDDh2vJNqT",
    "name": "Acausal Trade",
    "core": false,
    "slug": "acausal-trade",
    "tableOfContents": {
      "html": "<p><span class=\"by_5yNJS8bxEYhgFD9XJ\">In </span><strong><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">acausal </span><span class=\"by_BtbwfsEyeT4P2eqXu\">trade</span></span></strong><span><span class=\"by_BtbwfsEyeT4P2eqXu\">,</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> two agents each benefit by predicting what the other wants and doing it, even though they might have no way of communicating or affecting each other, nor even any direct evidence that the other exists.</span></span></p><h2 id=\"Background__Superrationality_and_the_one_shot_Prisoner_s_Dilemma\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Background: </span><span class=\"by_fbEg8jfgqQYPeSX43\">Superrationality</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> and the one-shot Prisoner's Dilemma</span></span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">This concept emerged out of the much-debated question of how to achieve cooperation on a one-shot </span><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Prisoner's </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Dilemma</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> where, by design, the two players are not allowed to communicate. On the one hand, a player who is considering the causal consequences of a decision (\"</span></span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Causal Decision </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Theory</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span><span class=\"by_5yNJS8bxEYhgFD9XJ\">) finds that defection always produces a better result. On the other hand, if the other player symmetrically reasons this way, the result is a Defect/Defect equilibrium, which is bad for both agents. If they could somehow converge on Cooperate, they would each individually do better. The question is what variation on decision theory would allow this beneficial equilibrium.</span></span></p><p><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Douglas Hofstadter (see references) coined the term </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">\"</span></span><a href=\"https://www.lesswrong.com/tag/superrationality\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">superrationality</span></a><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">\"</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> to express this state of convergence. He illustrated it with a game in which twenty players, who do not know each other's identities, each get an offer. If exactly one player asks for the prize of a billion dollars, they get it, but if none or multiple players ask, no one gets it. Players cannot communicate, but each might reason that the others are reasoning similarly. The \"correct\" decision--the decision which maximizes expected utility for each player, </span></span><i><span class=\"by_5yNJS8bxEYhgFD9XJ\">if</span></i><span class=\"by_5yNJS8bxEYhgFD9XJ\"> all players symmetrically make the same decision--is to randomize a one-in-20 chance of asking for the prize.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Gary Drescher (see references) developed the concept further, introducing an ethical system called \"acausal subjunctive morality.\" Drescher's approach relies on the agents being identical or at least similar, so that each agent can reasonably guess what the other will do based on facts about its own behavior, or even its own \"source code.\" If it cooperates, it can use this correlation to infer that the other will probably also cooperate.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Acausal trade goes one step beyond this. The agents do not need to be identical, nor similar, nor have the same utility function. Moreover, they do not need to know what the other agents are like, nor even if they exist. In acausal trade, an agent may have to surmise the probability that other agents, with their utility function and proclivities, exist.</span></p><h2 id=\"Description\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Description</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">We have two agents, separated so that no interaction is possible. The separation can be simply because each is not aware of the location of the other; or else each may be prevented from communicating with or affecting the other.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">In an asymmetrical example, one agent may be in the other's future.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Other less prosaic thought experiments can be used to emphasize that interaction may be absolutely impossible. For example, agents that are outside each other's light cones, or in separate parts of an Everett multiverse. And abstracting away from those scenarios, we can talk of counterfactual \"impossible possible worlds\" as a model for probability distributions.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">In truly </span><i><span class=\"by_5yNJS8bxEYhgFD9XJ\">acausal</span></i><span class=\"by_5yNJS8bxEYhgFD9XJ\"> trade, the agents cannot count on reputation, retaliation, or outside enforcement to ensure cooperation. The agents cooperate because each knows that the other can somehow predict its behavior very well. (Compare Omega in </span><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Newcomb's problem</span></a><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">.) Each knows that if it defects </span><span class=\"by_sKAL2jzfkYkDbQmx9\">or cooperates,</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the other will (probabilistically) know this, and defect </span><span class=\"by_sKAL2jzfkYkDbQmx9\">or cooperate, respectively.</span></span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Acausal trade can also be described in terms of </span><a href=\"https://www.lesswrong.com/tag/pre-commitment\"><span class=\"by_sKAL2jzfkYkDbQmx9\">(pre)commitment</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> Both agents commit to cooperate, and each has reason to think that the other is also committing.</span></span></p><h2 id=\"Prediction_mechanisms\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Prediction mechanisms</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">For acausal trade to occur, each agent must infer there is some probability that an agent, of the sort that will acausally trade with it, exists.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">The agent might be told, exogenously (as part of the scenario), that the other exists. But more interesting is the case in which the agent surmises the probability that the other exists.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">A </span><a href=\"https://www.lesswrong.com/tag/superintelligence\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">superintelligence </span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">might conclude that other superintelligences would tend to exist because increased intelligence </span><a href=\"https://www.lesswrong.com/tag/instrumental-convergence\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">is </span><span class=\"by_fM2ssyKovn79JnfFK\">a</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> convergent instrumental goal</span></span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> for agents. Given the existence of a superintelligence, acausal trade is one of the tricks it would tend to use.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">To take a more prosaic example, we humans realize that humans tend to be alike: Even without knowing about specific trading partners, we know that there exist other people with similar situations, goals, desires, challenges, resource constraints, and mental architectures.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Once an agent realizes that another agent might exist, there are different ways that might predict the other agent's behavior, and specifically that the other agent can be an acausal trading partner.</span></p><ol><li><span class=\"by_5yNJS8bxEYhgFD9XJ\">They might know or surmise each other's mental architectures (source code).</span></li><li><span class=\"by_5yNJS8bxEYhgFD9XJ\">In particular, they might know that they have identical or similar mental architecture, so that each one knows that its own mental processes approximately simulate the other's.</span></li><li><span class=\"by_5yNJS8bxEYhgFD9XJ\">They might be able to simulate each other (perhaps probabalistically), or to predict the other's behavior analytically. (Even we humans simulate each other's thoughts to guess what the other would do.)</span></li><li><span class=\"by_5yNJS8bxEYhgFD9XJ\">More broadly, it is enough to know (probabilistically) that the other is a powerful optimizer, that it has a certain utility function, and that it can derive utility from resources. Seen mathematically, this is just an optimization problem: What is the best possible algorithm for an agent's utility function? Cooperate/Cooperate is optimal under certain assumptions, for if one agent could achieve optimal utility by defecting, then, symmetrically, so could the other, resulting in Defect/Defect which generates inferior utility.</span></li></ol><h2 id=\"Decision_Theories\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Decision Theories</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Acausal trade is a special case of </span><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Updateless decision theory</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> (or a variant like </span><a href=\"https://www.lesswrong.com/tag/functional-decision-theory\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Functional Decision Theory,</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> see references). Unlike better-known variations of </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Decision theory</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">, such as </span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Causal decision theory</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">, acausal trade and UDT take into account the agent's own algorithm as cause and caused.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">In Causal Decision Theory, the agent's algorithm (implementation) is treated as uncaused by the rest of the universe, so that though the agent's </span><i><span class=\"by_5yNJS8bxEYhgFD9XJ\">decision</span></i><span class=\"by_5yNJS8bxEYhgFD9XJ\"> and subsequent action can make a difference, its internal make-up cannot (except through that decision). In contrast, in UDT, the agents' own algorithms are treated as causal nodes, influenced by other factors, such as the logical requirement of optimality in a utility-function maximizer. In UDT, as in acausal trade, the agent cannot escape the fact that its decision to defect or cooperate constitutes strong Bayesian evidence as to what the other agent will do, and so it is better off cooperating.</span></p><h2 id=\"Limitations_and_Objections\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Limitations and Objections</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Acausal trade only works if the agents are smart enough to predict each other's behavior, and then smart enough to acausally trade. If one agent is stupid enough to defect, and the second is smart enough to predict the first, then neither will cooperate.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Also, as in regular trade, acausal trade only works if the two sides are close enough in power that the weaker side can do something worthwhile enough for the stronger.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">A common objection to this idea: Why shouldn't an agent \"cheat\" and choose to defect? Can't it \"at the last moment\" back out after the other agent has committed? However, this approach takes into account only the direct effect of the decision, while a sufficiently intelligent trading partner could predict the agent's choice, including that one, rendering the \"cheating\" approach suboptimal.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Another objection: Can an agent care about (have a utility function that takes into account) entities with which it can never interact, and about whose existence it is not certain? However, this is quite common even for humans today. We care about the suffering of other people in faraway lands about whom we know next to nothing. We are even disturbed by the suffering of long-dead historical people, and wish that, counterfactually, the suffering had not happened. We even care about entities that we are not sure exist. For example: &nbsp;We might be concerned by news report that a valuable archaeological artifact was destroyed in a distant country, yet at the same time read other news reports stating that the entire story is a fabrication and the artifact never existed. People even get emotionally attached to the fate of a fictional character.</span></p><h2 id=\"An_example_of_acausal_trade_with_simple_resource_requirements\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">An example of acausal trade with simple resource requirements</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">At its most abstract, the agents are simply optimization algorithms. As a toy example, let T be a utility function for which time is most valuable as a resource; while for utility function S, space is most valuable, and assume that these are the only two resources.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">We will now choose the best algorithms for optimizing T. To avoid anthropomorphizing, we simply ask which algorithm--which string of LISP, for example--would give the highest expected utility for a given utility function. Thus, the choice of source code is \"timeless\": We treat it as an optimization problem across all possible strings of LISP. We assume that computing power is unlimited. Mathematically, we are asking about argmax T.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">We specify that there is a probability that either agent will be run in an environment where time is in abundance, and if not, some probability that it will be run in a space-rich universe.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">If the algorithm for T is instantiated in a space-rich environment, it will only be able to gain a small amount of utility for itself, but S would be able to gain a lot of utility; and vice versa.</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">The question is: What algorithm for T provides the most optimization power, the highest expected value of utility function T?</span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">If it turns out that the environment is space-rich, the agent for T may run the agent (the algorithm) for S, increasing the utility for S, and symmetrically the reverse. This will happen if each concludes, that the optimum occurs when the other agent has the \"trading\" feature. Given that this is the optimal case, the acausal trade will occur.</span></p><h2 id=\"Acausal_trade_with_complex_resource_requirements\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Acausal trade with complex resource requirements</span></h2><p><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">In the toy example above, resource requirements are very simple. In general, given that agents can have complex and arbitrary goals requiring a complex mix of resources, an agent might not be able to conclude that a specific trading partner has a meaningful </span><span class=\"by_NqNhp8SiTxXyQCxct\">chance</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> of existing and trading.</span></span></p><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">However, an agent can analyze the distribution of probabilities for the existence of other agents, and weight its actions accordingly. It will do acausal \"favors\" for one or more trading partners, weighting its effort according to its subjective probability that the trading partner exists. The expectation on utility given and received will come into a good enough balance to benefit the traders, in the limiting case of increasing super-intelligence.</span></p><h2 id=\"Ordinary_trade\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Ordinary trade</span></h2><p><span class=\"by_5yNJS8bxEYhgFD9XJ\">Even ordinary trade can be analyzed acausally, using a perspective similar to that of </span><a href=\"https://www.lesswrong.com/tag/updateless-decision-theory\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Updateless decision theory</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">. We ask: Which algorithm should an agent have to get the best expected value, summing across all possible environments weighted by their probability? The possible environments include those in which threats and promises have been made.</span></p><h2 id=\"See_also\"><span class=\"by_BtbwfsEyeT4P2eqXu\">See also</span></h2><ul><li><a href=\"http://aibeliefs.blogspot.com/2007/11/non-technical-introduction-to-ai.html?a=1\"><span class=\"by_BtbwfsEyeT4P2eqXu\">\"AI deterrence\"</span></a></li><li><a href=\"https://www.lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you\"><span class=\"by_BtbwfsEyeT4P2eqXu\">\"The AI in a box boxes you\"</span></a></li><li><a href=\"https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">A story</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> that shows acausal trade in action.</span></li><li><a href=\"http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Scott Alexander</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\"> explains Acausal Trade. (Most of that article is tongue-in-cheek, however.)</span></li><li><span class=\"by_5yNJS8bxEYhgFD9XJ\">\"</span><a href=\"http://www.nickbostrom.com/papers/porosity.pdf\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Hail Mary, Value Porosity, and Utility Diversification</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">,\" Nick Bostrom, the first paper from academia to rely on the concept of acausal trade.</span></li><li><a href=\"http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Towards an idealized decision theory</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">, by Nate Soares and Benja Fallenstein discusses acausal interaction scenarios that shed light on new directions in decision theory.</span></li><li><a href=\"https://ie.technion.ac.il/~moshet/progeqnote4.pdf\"><span class=\"by_fbEg8jfgqQYPeSX43\">Program Equilibrium</span></a><span class=\"by_fbEg8jfgqQYPeSX43\">, by Moshe Tennenholtz. In: Games and Economic Behavior.</span></li><li><a href=\"https://arxiv.org/abs/1401.5577\"><span class=\"by_fbEg8jfgqQYPeSX43\">Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic</span></a><span class=\"by_fbEg8jfgqQYPeSX43\">, by Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire and Eliezer Yudkowsky</span></li><li><a href=\"https://arxiv.org/abs/1602.04184\"><span class=\"by_fbEg8jfgqQYPeSX43\">Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents</span></a><span class=\"by_fbEg8jfgqQYPeSX43\">, by Andrew Critch</span></li><li><a href=\"https://link.springer.com/article/10.1007/s11238-018-9679-3\"><span class=\"by_fbEg8jfgqQYPeSX43\">Robust Program Equilibrium</span></a><span class=\"by_fbEg8jfgqQYPeSX43\">, by Caspar Oesterheld. In: Theory and Decision.</span></li><li><a href=\"https://foundational-research.org/multiverse-wide-cooperation-via-correlated-decision-making/\"><span class=\"by_fbEg8jfgqQYPeSX43\">Multiverse-wide Cooperation via Correlated Decision Making</span></a><span class=\"by_fbEg8jfgqQYPeSX43\">, by Caspar Oesterheld</span></li></ul><h2 id=\"References\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">References</span></h2><ul><li><a href=\"http://www.gwern.net/docs/1985-hofstadter\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Hofstadter's Superrationality</span><span class=\"by_BtbwfsEyeT4P2eqXu\"> essays, published in </span></span><i><span class=\"by_BtbwfsEyeT4P2eqXu\">Metamagical Themas</span></i></a><span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> </span><span class=\"by_BtbwfsEyeT4P2eqXu\">(</span></span><a href=\"https://www.lesswrong.com/lw/bxi/hofstadters_superrationality/\"><span class=\"by_BtbwfsEyeT4P2eqXu\">LW discussion</span></a><span class=\"by_BtbwfsEyeT4P2eqXu\">)</span></li><li><span class=\"by_SdZmP36R37riQrHAw\">Jaan Tallinn, </span><a href=\"https://www.youtube.com/watch?v=29AgSo6KOtI\"><span class=\"by_SdZmP36R37riQrHAw\">Why Now? A Quest in Metaphysics</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">.</span></li><li><a href=\"https://wiki.lesswrong.com/wiki/Gary_Drescher\"><span class=\"by_5yNJS8bxEYhgFD9XJ\">Gary Drescher</span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">, </span><i><span class=\"by_5yNJS8bxEYhgFD9XJ\">Good and Real</span></i><span class=\"by_5yNJS8bxEYhgFD9XJ\">, MIT Press, 1996.</span></li><li><a href=\"https://arxiv.org/abs/1710.05060\"><span><span class=\"by_rordgt937kcgXxfKP\">Functional </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Decision Theory</span></span></a></li></ul>",
      "sections": [
        {
          "title": "Background: Superrationality and the one-shot Prisoner's Dilemma",
          "anchor": "Background__Superrationality_and_the_one_shot_Prisoner_s_Dilemma",
          "level": 1
        },
        {
          "title": "Description",
          "anchor": "Description",
          "level": 1
        },
        {
          "title": "Prediction mechanisms",
          "anchor": "Prediction_mechanisms",
          "level": 1
        },
        {
          "title": "Decision Theories",
          "anchor": "Decision_Theories",
          "level": 1
        },
        {
          "title": "Limitations and Objections",
          "anchor": "Limitations_and_Objections",
          "level": 1
        },
        {
          "title": "An example of acausal trade with simple resource requirements",
          "anchor": "An_example_of_acausal_trade_with_simple_resource_requirements",
          "level": 1
        },
        {
          "title": "Acausal trade with complex resource requirements",
          "anchor": "Acausal_trade_with_complex_resource_requirements",
          "level": 1
        },
        {
          "title": "Ordinary trade",
          "anchor": "Ordinary_trade",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 11
    },
    "postCount": 32,
    "description": {
      "markdown": "In **acausal trade**, two agents each benefit by predicting what the other wants and doing it, even though they might have no way of communicating or affecting each other, nor even any direct evidence that the other exists.\n\nBackground: Superrationality and the one-shot Prisoner's Dilemma\n----------------------------------------------------------------\n\nThis concept emerged out of the much-debated question of how to achieve cooperation on a one-shot [Prisoner's Dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma), where, by design, the two players are not allowed to communicate. On the one hand, a player who is considering the causal consequences of a decision (\"[Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory)\") finds that defection always produces a better result. On the other hand, if the other player symmetrically reasons this way, the result is a Defect/Defect equilibrium, which is bad for both agents. If they could somehow converge on Cooperate, they would each individually do better. The question is what variation on decision theory would allow this beneficial equilibrium.\n\nDouglas Hofstadter (see references) coined the term \"[superrationality](https://www.lesswrong.com/tag/superrationality)\" to express this state of convergence. He illustrated it with a game in which twenty players, who do not know each other's identities, each get an offer. If exactly one player asks for the prize of a billion dollars, they get it, but if none or multiple players ask, no one gets it. Players cannot communicate, but each might reason that the others are reasoning similarly. The \"correct\" decision--the decision which maximizes expected utility for each player, *if* all players symmetrically make the same decision--is to randomize a one-in-20 chance of asking for the prize.\n\nGary Drescher (see references) developed the concept further, introducing an ethical system called \"acausal subjunctive morality.\" Drescher's approach relies on the agents being identical or at least similar, so that each agent can reasonably guess what the other will do based on facts about its own behavior, or even its own \"source code.\" If it cooperates, it can use this correlation to infer that the other will probably also cooperate.\n\nAcausal trade goes one step beyond this. The agents do not need to be identical, nor similar, nor have the same utility function. Moreover, they do not need to know what the other agents are like, nor even if they exist. In acausal trade, an agent may have to surmise the probability that other agents, with their utility function and proclivities, exist.\n\nDescription\n-----------\n\nWe have two agents, separated so that no interaction is possible. The separation can be simply because each is not aware of the location of the other; or else each may be prevented from communicating with or affecting the other.\n\nIn an asymmetrical example, one agent may be in the other's future.\n\nOther less prosaic thought experiments can be used to emphasize that interaction may be absolutely impossible. For example, agents that are outside each other's light cones, or in separate parts of an Everett multiverse. And abstracting away from those scenarios, we can talk of counterfactual \"impossible possible worlds\" as a model for probability distributions.\n\nIn truly *acausal* trade, the agents cannot count on reputation, retaliation, or outside enforcement to ensure cooperation. The agents cooperate because each knows that the other can somehow predict its behavior very well. (Compare Omega in [Newcomb's problem](https://www.lesswrong.com/tag/newcomb-s-problem).) Each knows that if it defects or cooperates, the other will (probabilistically) know this, and defect or cooperate, respectively.\n\nAcausal trade can also be described in terms of [(pre)commitment](https://www.lesswrong.com/tag/pre-commitment): Both agents commit to cooperate, and each has reason to think that the other is also committing.\n\nPrediction mechanisms\n---------------------\n\nFor acausal trade to occur, each agent must infer there is some probability that an agent, of the sort that will acausally trade with it, exists.\n\nThe agent might be told, exogenously (as part of the scenario), that the other exists. But more interesting is the case in which the agent surmises the probability that the other exists.\n\nA [superintelligence](https://www.lesswrong.com/tag/superintelligence) might conclude that other superintelligences would tend to exist because increased intelligence [is a convergent instrumental goal](https://www.lesswrong.com/tag/instrumental-convergence) for agents. Given the existence of a superintelligence, acausal trade is one of the tricks it would tend to use.\n\nTo take a more prosaic example, we humans realize that humans tend to be alike: Even without knowing about specific trading partners, we know that there exist other people with similar situations, goals, desires, challenges, resource constraints, and mental architectures.\n\nOnce an agent realizes that another agent might exist, there are different ways that might predict the other agent's behavior, and specifically that the other agent can be an acausal trading partner.\n\n1.  They might know or surmise each other's mental architectures (source code).\n2.  In particular, they might know that they have identical or similar mental architecture, so that each one knows that its own mental processes approximately simulate the other's.\n3.  They might be able to simulate each other (perhaps probabalistically), or to predict the other's behavior analytically. (Even we humans simulate each other's thoughts to guess what the other would do.)\n4.  More broadly, it is enough to know (probabilistically) that the other is a powerful optimizer, that it has a certain utility function, and that it can derive utility from resources. Seen mathematically, this is just an optimization problem: What is the best possible algorithm for an agent's utility function? Cooperate/Cooperate is optimal under certain assumptions, for if one agent could achieve optimal utility by defecting, then, symmetrically, so could the other, resulting in Defect/Defect which generates inferior utility.\n\nDecision Theories\n-----------------\n\nAcausal trade is a special case of [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory) (or a variant like [Functional Decision Theory,](https://www.lesswrong.com/tag/functional-decision-theory) see references). Unlike better-known variations of [Decision theory](https://www.lesswrong.com/tag/decision-theory), such as [Causal decision theory](https://www.lesswrong.com/tag/causal-decision-theory), acausal trade and UDT take into account the agent's own algorithm as cause and caused.\n\nIn Causal Decision Theory, the agent's algorithm (implementation) is treated as uncaused by the rest of the universe, so that though the agent's *decision* and subsequent action can make a difference, its internal make-up cannot (except through that decision). In contrast, in UDT, the agents' own algorithms are treated as causal nodes, influenced by other factors, such as the logical requirement of optimality in a utility-function maximizer. In UDT, as in acausal trade, the agent cannot escape the fact that its decision to defect or cooperate constitutes strong Bayesian evidence as to what the other agent will do, and so it is better off cooperating.\n\nLimitations and Objections\n--------------------------\n\nAcausal trade only works if the agents are smart enough to predict each other's behavior, and then smart enough to acausally trade. If one agent is stupid enough to defect, and the second is smart enough to predict the first, then neither will cooperate.\n\nAlso, as in regular trade, acausal trade only works if the two sides are close enough in power that the weaker side can do something worthwhile enough for the stronger.\n\nA common objection to this idea: Why shouldn't an agent \"cheat\" and choose to defect? Can't it \"at the last moment\" back out after the other agent has committed? However, this approach takes into account only the direct effect of the decision, while a sufficiently intelligent trading partner could predict the agent's choice, including that one, rendering the \"cheating\" approach suboptimal.\n\nAnother objection: Can an agent care about (have a utility function that takes into account) entities with which it can never interact, and about whose existence it is not certain? However, this is quite common even for humans today. We care about the suffering of other people in faraway lands about whom we know next to nothing. We are even disturbed by the suffering of long-dead historical people, and wish that, counterfactually, the suffering had not happened. We even care about entities that we are not sure exist. For example:  We might be concerned by news report that a valuable archaeological artifact was destroyed in a distant country, yet at the same time read other news reports stating that the entire story is a fabrication and the artifact never existed. People even get emotionally attached to the fate of a fictional character.\n\nAn example of acausal trade with simple resource requirements\n-------------------------------------------------------------\n\nAt its most abstract, the agents are simply optimization algorithms. As a toy example, let T be a utility function for which time is most valuable as a resource; while for utility function S, space is most valuable, and assume that these are the only two resources.\n\nWe will now choose the best algorithms for optimizing T. To avoid anthropomorphizing, we simply ask which algorithm--which string of LISP, for example--would give the highest expected utility for a given utility function. Thus, the choice of source code is \"timeless\": We treat it as an optimization problem across all possible strings of LISP. We assume that computing power is unlimited. Mathematically, we are asking about argmax T.\n\nWe specify that there is a probability that either agent will be run in an environment where time is in abundance, and if not, some probability that it will be run in a space-rich universe.\n\nIf the algorithm for T is instantiated in a space-rich environment, it will only be able to gain a small amount of utility for itself, but S would be able to gain a lot of utility; and vice versa.\n\nThe question is: What algorithm for T provides the most optimization power, the highest expected value of utility function T?\n\nIf it turns out that the environment is space-rich, the agent for T may run the agent (the algorithm) for S, increasing the utility for S, and symmetrically the reverse. This will happen if each concludes, that the optimum occurs when the other agent has the \"trading\" feature. Given that this is the optimal case, the acausal trade will occur.\n\nAcausal trade with complex resource requirements\n------------------------------------------------\n\nIn the toy example above, resource requirements are very simple. In general, given that agents can have complex and arbitrary goals requiring a complex mix of resources, an agent might not be able to conclude that a specific trading partner has a meaningful chance of existing and trading.\n\nHowever, an agent can analyze the distribution of probabilities for the existence of other agents, and weight its actions accordingly. It will do acausal \"favors\" for one or more trading partners, weighting its effort according to its subjective probability that the trading partner exists. The expectation on utility given and received will come into a good enough balance to benefit the traders, in the limiting case of increasing super-intelligence.\n\nOrdinary trade\n--------------\n\nEven ordinary trade can be analyzed acausally, using a perspective similar to that of [Updateless decision theory](https://www.lesswrong.com/tag/updateless-decision-theory). We ask: Which algorithm should an agent have to get the best expected value, summing across all possible environments weighted by their probability? The possible environments include those in which threats and promises have been made.\n\nSee also\n--------\n\n*   [\"AI deterrence\"](http://aibeliefs.blogspot.com/2007/11/non-technical-introduction-to-ai.html?a=1)\n*   [\"The AI in a box boxes you\"](https://www.lesswrong.com/lw/1pz/the_ai_in_a_box_boxes_you)\n*   [A story](https://slatestarcodex.com/2017/03/21/repost-the-demiurges-older-brother/) that shows acausal trade in action.\n*   [Scott Alexander](http://slatestarcodex.com/2018/04/01/the-hour-i-first-believed/) explains Acausal Trade. (Most of that article is tongue-in-cheek, however.)\n*   \"[Hail Mary, Value Porosity, and Utility Diversification](http://www.nickbostrom.com/papers/porosity.pdf),\" Nick Bostrom, the first paper from academia to rely on the concept of acausal trade.\n*   [Towards an idealized decision theory](http://intelligence.org/files/TowardIdealizedDecisionTheory.pdf), by Nate Soares and Benja Fallenstein discusses acausal interaction scenarios that shed light on new directions in decision theory.\n*   [Program Equilibrium](https://ie.technion.ac.il/~moshet/progeqnote4.pdf), by Moshe Tennenholtz. In: Games and Economic Behavior.\n*   [Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic](https://arxiv.org/abs/1401.5577), by Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, Patrick LaVictoire and Eliezer Yudkowsky\n*   [Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents](https://arxiv.org/abs/1602.04184), by Andrew Critch\n*   [Robust Program Equilibrium](https://link.springer.com/article/10.1007/s11238-018-9679-3), by Caspar Oesterheld. In: Theory and Decision.\n*   [Multiverse-wide Cooperation via Correlated Decision Making](https://foundational-research.org/multiverse-wide-cooperation-via-correlated-decision-making/), by Caspar Oesterheld\n\nReferences\n----------\n\n*   [Hofstadter's Superrationality essays, published in *Metamagical Themas*](http://www.gwern.net/docs/1985-hofstadter) ([LW discussion](https://www.lesswrong.com/lw/bxi/hofstadters_superrationality/))\n*   Jaan Tallinn, [Why Now? A Quest in Metaphysics](https://www.youtube.com/watch?v=29AgSo6KOtI).\n*   [Gary Drescher](https://wiki.lesswrong.com/wiki/Gary_Drescher), *Good and Real*, MIT Press, 1996.\n*   [Functional Decision Theory](https://arxiv.org/abs/1710.05060)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "k9pXZBsM8wMRwwK4J",
    "name": "Autism",
    "core": false,
    "slug": "autism",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7thPfS2WbD2JKizr7",
    "name": "Try Things",
    "core": false,
    "slug": "try-things",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_6NixjQa89fiBZxKS4\">Try Things </span></strong><span class=\"by_6NixjQa89fiBZxKS4\">is a mantra that reminds you to gather data about what works by experimentation. If a proposed intervention works and you learn that it works, you've just gained something that you'll be able to use over and over again. If it doesn't work, you've only had it not work </span><i><span class=\"by_6NixjQa89fiBZxKS4\">once</span></i><span class=\"by_6NixjQa89fiBZxKS4\">.</span></p><p><span class=\"by_6NixjQa89fiBZxKS4\">To the knowledge of one editor of this article, the phrase was introduced the the LessWrong community by </span><a href=\"https://www.rationality.org/resources/handbook\"><span class=\"by_6NixjQa89fiBZxKS4\">CFAR</span></a><span class=\"by_6NixjQa89fiBZxKS4\">, at least as early as </span><a href=\"https://drive.google.com/file/d/0B9BPXF2K91U_OGJSbWJqN1l6eEk/view?resourcekey=0-7H39fXtrjmVeFbhgzZD_Ew\"><span class=\"by_6NixjQa89fiBZxKS4\">2016</span></a><span class=\"by_6NixjQa89fiBZxKS4\">.</span></p><p><i><span class=\"by_6NixjQa89fiBZxKS4\">See also</span></i><span class=\"by_6NixjQa89fiBZxKS4\">: </span><a href=\"https://www.lesswrong.com/tag/value-of-information\"><span class=\"by_6NixjQa89fiBZxKS4\">Value of Information</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "**Try Things** is a mantra that reminds you to gather data about what works by experimentation. If a proposed intervention works and you learn that it works, you've just gained something that you'll be able to use over and over again. If it doesn't work, you've only had it not work *once*.\n\nTo the knowledge of one editor of this article, the phrase was introduced the the LessWrong community by [CFAR](https://www.rationality.org/resources/handbook), at least as early as [2016](https://drive.google.com/file/d/0B9BPXF2K91U_OGJSbWJqN1l6eEk/view?resourcekey=0-7H39fXtrjmVeFbhgzZD_Ew).\n\n*See also*: [Value of Information](https://www.lesswrong.com/tag/value-of-information)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Xno6pRXizN9AmFFTa",
    "name": "Implicit Association Test (IAT)",
    "core": false,
    "slug": "implicit-association-test-iat",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "y93YW7Kb6J8D5PKng",
    "name": "Reset (technique)",
    "core": false,
    "slug": "reset-technique",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">The rationality technique where you choose a fresh reference point to evaluate situations.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": {
      "markdown": "The rationality technique where you choose a fresh reference point to evaluate situations."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TG8zMvjnhydE7Mcue",
    "name": "Psychology of Altruism",
    "core": false,
    "slug": "psychology-of-altruism",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KkksuGB2yBR6LDFXu",
    "name": "Conservatism (AI)",
    "core": false,
    "slug": "conservatism-ai",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yAmE3StuxBmzCBPWq",
    "name": "Safety (Physical)",
    "core": false,
    "slug": "safety-physical",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">Posts about avoiding being violently injured.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "Posts about avoiding being violently injured."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "v8MTECTHWs3crpnnZ",
    "name": "Embodiment",
    "core": false,
    "slug": "embodiment",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MhQk9tRaJgYM4o6iD",
    "name": "Autonomy and Choice",
    "core": false,
    "slug": "autonomy-and-choice",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KN9KEMgyBHjcAyc26",
    "name": "Trust",
    "core": false,
    "slug": "trust",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/expertise-topic\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Expertise (topic)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/courage\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Courage</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Groupthink</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/relationships-interpersonal\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Relationships (Interpersonal)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Social &amp; Cultural Dynamics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 14,
    "description": {
      "markdown": "**Related Pages:** [Expertise (topic)](https://www.lesswrong.com/tag/expertise-topic), [Courage](https://www.lesswrong.com/tag/courage), [Groupthink](https://www.lesswrong.com/tag/groupthink), [Relationships (Interpersonal)](https://www.lesswrong.com/tag/relationships-interpersonal), [Social & Cultural Dynamics](https://www.lesswrong.com/tag/social-and-cultural-dynamics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xknvtHwqvqhwahW8Q",
    "name": "Human Values",
    "core": false,
    "slug": "human-values",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Human Values</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> are the things we care about, and would want an aligned superintelligence to look after and support. It is suspected that true human values are </span><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_Sp5wM4aRAhNERd4oY\">highly complex</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, and could be extrapolated into a wide variety of forms.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 56,
    "description": {
      "markdown": "**Human Values** are the things we care about, and would want an aligned superintelligence to look after and support. It is suspected that true human values are [highly complex](https://www.lesswrong.com/tag/complexity-of-value), and could be extrapolated into a wide variety of forms."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T63QtRJhoTEGhZbTP",
    "name": "Creativity",
    "core": false,
    "slug": "creativity",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sMzY3PuSfYGdtHWrE",
    "name": "Noticing Confusion",
    "core": false,
    "slug": "noticing-confusion",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6wzZZfW87aKGQ7Fwr",
    "name": "Reflective Reasoning",
    "core": false,
    "slug": "reflective-reasoning",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xjy2ZYACvYQBPJdix",
    "name": "Timeless Physics",
    "core": false,
    "slug": "timeless-physics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YpHkTW27iMFR2Dkae",
    "name": "Counterfactuals",
    "core": false,
    "slug": "counterfactuals",
    "tableOfContents": {
      "html": "<p><span class=\"by_r38pkCm7wF4M44MDQ\">Counterfactual reasoning is about looking at choices you </span><i><span class=\"by_r38pkCm7wF4M44MDQ\">could</span></i><span class=\"by_r38pkCm7wF4M44MDQ\"> </span><i><span class=\"by_r38pkCm7wF4M44MDQ\">have </span></i><span class=\"by_r38pkCm7wF4M44MDQ\">made, but didn't.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Counterfactuals are similar to </span><a href=\"https://www.lesswrong.com/tag/hypotheticals\"><span class=\"by_r38pkCm7wF4M44MDQ\">hypotheticals</span></a><span class=\"by_r38pkCm7wF4M44MDQ\"> (but tend to focus on choices you could have made, rather than situations you could have been in).&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 104,
    "description": {
      "markdown": "Counterfactual reasoning is about looking at choices you *could* *have* made, but didn't.\n\nCounterfactuals are similar to [hypotheticals](https://www.lesswrong.com/tag/hypotheticals) (but tend to focus on choices you could have made, rather than situations you could have been in)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uRcuHKpKA7xNnZQ2F",
    "name": "Neurodivergence",
    "core": false,
    "slug": "neurodivergence",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XSeiautCrZGaQ78fx",
    "name": "Attention",
    "core": false,
    "slug": "attention",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P3Wd3f2cWqqvQxDQS",
    "name": "Carving / Clustering Reality",
    "core": false,
    "slug": "carving-clustering-reality",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dRRwDp9KRtkyd7ywZ",
    "name": "Thingspace",
    "core": false,
    "slug": "thingspace",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AWz5ryH8SpAgTeydh",
    "name": "Polyamory",
    "core": false,
    "slug": "polyamory",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 14,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GPhMyXoaHBLyzibxB",
    "name": "Typical Mind Fallacy",
    "core": false,
    "slug": "typical-mind-fallacy",
    "tableOfContents": {
      "html": "<p><span> <span class=\"by_WuN6KY3ADazyqLHRg\">The </span></span><strong><span><span class=\"by_WuN6KY3ADazyqLHRg\">typical</span><span class=\"by_qf77EiaoMw7tH3GSr\"> mind fallacy</span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\"> is the mistake of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">modeling</span><span class=\"by_Qad7jGcRgP4BZMa5F\"> the minds inside other </span><span class=\"by_HoGziwmhpMGqGeWZy\">people'</span><span class=\"by_Qad7jGcRgP4BZMa5F\">s brains as exactly the same as your own mind. Humans lack insight into their own minds and what is common among everyone or unusually specific to a few. It can be often hard to see the </span></span><a href=\"https://www.lesswrong.com/lw/jm/the_lens_that_sees_its_flaws/\"><span class=\"by_Qad7jGcRgP4BZMa5F\">flaws in the lens</span></a><span class=\"by_Qad7jGcRgP4BZMa5F\">, especially when we only have one lens to look through with which to see those flaws.</span></p><p><span class=\"by_Qad7jGcRgP4BZMa5F\">The typical mind fallacy is also accompanied by the atypical mind fallacy - the idea that no one has the same mind or thoughts as you and you are unique.</span></p><p><span><span class=\"by_Qad7jGcRgP4BZMa5F\">The typical mind fallacy can usually be found </span><span class=\"by_qf77EiaoMw7tH3GSr\">making</span><span class=\"by_Qad7jGcRgP4BZMa5F\"> other fallacies worse. For </span><span class=\"by_qgdGA4ZEyW7zNdK84\">example,</span><span class=\"by_Qad7jGcRgP4BZMa5F\"> causing</span><span class=\"by_qf77EiaoMw7tH3GSr\"> biased and overconfident conclusions about other </span><span class=\"by_HoGziwmhpMGqGeWZy\">people'</span><span class=\"by_qf77EiaoMw7tH3GSr\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">experiences</span><span class=\"by_qf77EiaoMw7tH3GSr\"> based on your own personal </span><span class=\"by_Qad7jGcRgP4BZMa5F\">experience.</span></span></p><h2 id=\"See_also\"><strong><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind Projection Fallacy</span></a></li><li><a href=\"https://www.lesswrong.com/tag/human-universal\"><span class=\"by_9c2mQkLQq6gQSksMs\">Human universal</span></a></li><li><a href=\"https://www.lesswrong.com/tag/correspondence-bias\"><span class=\"by_9c2mQkLQq6gQSksMs\">Correspondence bias</span></a></li><li><a href=\"https://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it\"><span class=\"by_qgdGA4ZEyW7zNdK84\">What universal human experience are you missing?</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 11,
    "description": {
      "markdown": "The **typical mind fallacy** is the mistake of modeling the minds inside other people's brains as exactly the same as your own mind. Humans lack insight into their own minds and what is common among everyone or unusually specific to a few. It can be often hard to see the [flaws in the lens](https://www.lesswrong.com/lw/jm/the_lens_that_sees_its_flaws/), especially when we only have one lens to look through with which to see those flaws.\n\nThe typical mind fallacy is also accompanied by the atypical mind fallacy - the idea that no one has the same mind or thoughts as you and you are unique.\n\nThe typical mind fallacy can usually be found making other fallacies worse. For example, causing biased and overconfident conclusions about other people's experiences based on your own personal experience.\n\n**See also**\n------------\n\n*   [Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [Human universal](https://www.lesswrong.com/tag/human-universal)\n*   [Correspondence bias](https://www.lesswrong.com/tag/correspondence-bias)\n*   [What universal human experience are you missing?](https://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NfMQK5kiYKgg7r9cD",
    "name": "Sabbath",
    "core": false,
    "slug": "sabbath",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Sabbaths</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> are days of rest. On LessWrong this has received some discussion of how to organize and orient in your life.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Sabbaths** are days of rest. On LessWrong this has received some discussion of how to organize and orient in your life."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aHjTRDkGypPqbXWpN",
    "name": "Intellectual Progress (Society-Level)",
    "core": false,
    "slug": "intellectual-progress-society-level",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Intellectual Progress </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is the progressive accumulation of knowledge. Questions include </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">How is intellectual progress made? </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">and </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">How do we make more?</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/intellectual-progress-individual-level\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Intellectual Progress (Individual-Level)</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and the </span><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship &amp; Learning</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag is focused on the learning and research of individuals. In contrast, Intellectual Progress</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> (Society-Level)</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is about how we, humanity, collectively make progress on important questions. What broader conditions, institutions, and technologies enable progress?</span></span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Also related to: </span><a href=\"https://www.lesswrong.com/tag/progress-studies\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Progress Studies</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Practice &amp; Philosophy of Science</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">[TO-DO: link Science </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">tag(s)]</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 88,
    "description": {
      "markdown": "**Intellectual Progress** is the progressive accumulation of knowledge. Questions include *How is intellectual progress made?* and *How do we make more?* [Intellectual Progress (Individual-Level)](https://www.lesswrong.com/tag/intellectual-progress-individual-level) and the [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning) tag is focused on the learning and research of individuals. In contrast, Intellectual Progress (Society-Level) is about how we, humanity, collectively make progress on important questions. What broader conditions, institutions, and technologies enable progress?\n\nAlso related to: [Progress Studies](https://www.lesswrong.com/tag/progress-studies), [Practice & Philosophy of Science](https://www.lesswrong.com/tag/practice-and-philosophy-of-science)  \n \n\n\\[TO-DO: link Science tag(s)\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yo4kFCNxdHFkwqhun",
    "name": "Ontology",
    "core": false,
    "slug": "ontology",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 22,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2wDMWTpP8YNxXdkQD",
    "name": "Just World Hypothesis",
    "core": false,
    "slug": "just-world-hypothesis",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fPRyNtDMeSMrEM9nr",
    "name": "April Fool's",
    "core": false,
    "slug": "april-fool-s",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_fD4ATtTkdQJ4aSpGH\">An annual event in the US, which could be thought of as </span><span class=\"by_XtphY3uYHwruKqDyG\">\"Notice</span><span class=\"by_fD4ATtTkdQJ4aSpGH\"> Your </span><span class=\"by_XtphY3uYHwruKqDyG\">Surprise\"</span><span class=\"by_fD4ATtTkdQJ4aSpGH\"> day.</span><span class=\"by_XtphY3uYHwruKqDyG\">&nbsp;</span></span></p><p><i><span class=\"by_XtphY3uYHwruKqDyG\">Special rule for this tag: Please don't add entries to this tag during April Fool's, to not spoil all the surprise as soon as people see the top of the post, but feel free to add any old April Fool's jokes starting on the 2nd.</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "An annual event in the US, which could be thought of as \"Notice Your Surprise\" day. \n\n*Special rule for this tag: Please don't add entries to this tag during April Fool's, to not spoil all the surprise as soon as people see the top of the post, but feel free to add any old April Fool's jokes starting on the 2nd.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ookMdjJQMopfLG3wZ",
    "name": "Intelligence Amplification",
    "core": false,
    "slug": "intelligence-amplification",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5d63AWNjtFyHprX2k",
    "name": "Working Memory",
    "core": false,
    "slug": "working-memory",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gWEjzxPjitZ2JGZvM",
    "name": "Longtermism",
    "core": false,
    "slug": "longtermism",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_cJnvyeYrotgZgfG8W\">Longtermism</span><span class=\"by_r38pkCm7wF4M44MDQ\"> is </span><span class=\"by_cJnvyeYrotgZgfG8W\">a philosophy that future lives matter and that we have a similar obligation to them</span><span class=\"by_r38pkCm7wF4M44MDQ\"> as </span><span class=\"by_cJnvyeYrotgZgfG8W\">we do to lives around currently. William Macaskill states it in three clauses:&nbsp;</span></span></p><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Future people count</span></li><li><span><span class=\"by_cJnvyeYrotgZgfG8W\">There could be </span><span class=\"by_r38pkCm7wF4M44MDQ\">a </span><span class=\"by_cJnvyeYrotgZgfG8W\">lot of them</span></span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">We can make their lives go better</span></li></ul><p><span class=\"by_cJnvyeYrotgZgfG8W\">[broad description of philosophy, something about WWOTF]</span></p><h2 id=\"Criticisms_and_responses\"><span><span class=\"by_cJnvyeYrotgZgfG8W\">Criticisms</span><span class=\"by_r38pkCm7wF4M44MDQ\"> and </span><span class=\"by_cJnvyeYrotgZgfG8W\">responses</span></span></h2><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Criticism: Longermism suffers from all the standard criticism of consequentialism</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Response: Longtermism doesn't require consequentialist assumptions. Many individuals and societies have felt an obligation towards their decendents</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Response: Many criticisms of consequentialism are baseless [[criticisms of consequentialism]]</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">&nbsp;</span></li></ul></li><li><span><span class=\"by_cJnvyeYrotgZgfG8W\">Future lives don't exist, how could we care</span><span class=\"by_r38pkCm7wF4M44MDQ\"> about </span><span class=\"by_cJnvyeYrotgZgfG8W\">them</span></span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">&nbsp;</span></li></ul>",
      "sections": [
        {
          "title": "Criticisms and responses",
          "anchor": "Criticisms_and_responses",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 19,
    "description": {
      "markdown": "Longtermism is a philosophy that future lives matter and that we have a similar obligation to them as we do to lives around currently. William Macaskill states it in three clauses: \n\n*   Future people count\n*   There could be a lot of them\n*   We can make their lives go better\n\n\\[broad description of philosophy, something about WWOTF\\]\n\nCriticisms and responses\n------------------------\n\n*   Criticism: Longermism suffers from all the standard criticism of consequentialism\n    *   Response: Longtermism doesn't require consequentialist assumptions. Many individuals and societies have felt an obligation towards their decendents\n    *   Response: Many criticisms of consequentialism are baseless \\[\\[criticisms of consequentialism\\]\\]\n    \n*   Future lives don't exist, how could we care about them"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qFcwTzAfCQSkAM8vS",
    "name": "Frames",
    "core": false,
    "slug": "frames",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QbdJ7SxLE2NT8gzr3",
    "name": "Mind Space",
    "core": false,
    "slug": "mind-space",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Mind Space </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">(or, the design space of minds), is the exploration of all possible minds that could possibly exist.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "**Mind Space** (or, the design space of minds), is the exploration of all possible minds that could possibly exist."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EdnFte9kTvWnRskrN",
    "name": "Inner Simulator / Suprise-o-meter",
    "core": false,
    "slug": "inner-simulator-suprise-o-meter",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Inner Simulator </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is a CFAR technique where you check how surprised you'd be if you got a particular outcome. Useful for pre-mortem-ing&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "**Inner Simulator** is a CFAR technique where you check how surprised you'd be if you got a particular outcome. Useful for pre-mortem-ing"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TotjLm7Q7nabRrYpZ",
    "name": "Intentionality",
    "core": false,
    "slug": "intentionality",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yS7248NQSm5J6xLvn",
    "name": "Intellectual Fashion",
    "core": false,
    "slug": "intellectual-fashion",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 2,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CyFfBfRAm7pP83r5p",
    "name": "Reward Functions",
    "core": false,
    "slug": "reward-functions",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hLp77TQsRkooioj86",
    "name": "Probabilistic Reasoning",
    "core": false,
    "slug": "probabilistic-reasoning",
    "tableOfContents": {
      "html": "<p><span class=\"by_Xn6ACr6Cua8upALWQ\">Probabilistic reasoning is the opposite of black and white thinking.&nbsp;</span></p><p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">When you reason in black and white</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> you ask questions like: Is this true? Is this the right thing to do? Am I sick?&nbsp;</span></p><p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">When you reason probabilistically</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> you ask questions like: How likely is this? What's the </span><a href=\"https://forum.effectivealtruism.org/tag/expected-value\"><span class=\"by_Xn6ACr6Cua8upALWQ\">expected value</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> of this action? What </span><a href=\"https://www.lesswrong.com/posts/fhojYBGGiYAFcryHZ/scientific-evidence-legal-evidence-rational-evidence\"><span class=\"by_Xn6ACr6Cua8upALWQ\">evidence</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> have I seen that I am sick, and what evidence that I'm not? How likely is it that I'm sick </span><a href=\"https://www.lesswrong.com/tag/priors\"><i><span class=\"by_Xn6ACr6Cua8upALWQ\">without</span></i></a><i><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">taking any evidence into account?</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Reasoning probabilistically allows you to change your mind </span><a href=\"https://www.lesswrong.com/posts/627DZcvme7nLDrbZu/update-yourself-incrementally\"><span class=\"by_Xn6ACr6Cua8upALWQ\">incrementally</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, accumulating many small pieces of evidence rather than requiring one overwhelmingly convincing piece.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">On LessWrong, 'probabilistic reasoning' usually refers to </span><a href=\"https://www.lesswrong.com/tag/bayes-theorem\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Bayes theorem</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, which formally defines the optimal way to change your beliefs when you see evidence.&nbsp;</span></p><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">See Also:</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span><a href=\"https://www.lesswrong.com/tag/bayes-theorem\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Bayes Theorem</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/belief-update\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Belief Update</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://forum.effectivealtruism.org/tag/expected-value\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Expected Value</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/probability-and-statistics\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Probability and Statistics</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": {
      "markdown": "Probabilistic reasoning is the opposite of black and white thinking. \n\n**When you reason in black and white** you ask questions like: Is this true? Is this the right thing to do? Am I sick? \n\n**When you reason probabilistically** you ask questions like: How likely is this? What's the [expected value](https://forum.effectivealtruism.org/tag/expected-value) of this action? What [evidence](https://www.lesswrong.com/posts/fhojYBGGiYAFcryHZ/scientific-evidence-legal-evidence-rational-evidence) have I seen that I am sick, and what evidence that I'm not? How likely is it that I'm sick [*without*](https://www.lesswrong.com/tag/priors)  taking any evidence into account?\n\nReasoning probabilistically allows you to change your mind [incrementally](https://www.lesswrong.com/posts/627DZcvme7nLDrbZu/update-yourself-incrementally), accumulating many small pieces of evidence rather than requiring one overwhelmingly convincing piece.\n\nOn LessWrong, 'probabilistic reasoning' usually refers to [Bayes theorem](https://www.lesswrong.com/tag/bayes-theorem), which formally defines the optimal way to change your beliefs when you see evidence. \n\n*See Also:* [Bayes Theorem](https://www.lesswrong.com/tag/bayes-theorem), [Belief Update](https://www.lesswrong.com/tag/belief-update), [Expected Value](https://forum.effectivealtruism.org/tag/expected-value), [Probability and Statistics](https://www.lesswrong.com/tag/probability-and-statistics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tXzozGBvkvAfP4noX",
    "name": "Systems Thinking",
    "core": false,
    "slug": "systems-thinking",
    "tableOfContents": {
      "html": "<p><span class=\"by_r38pkCm7wF4M44MDQ\">https://learningforsustainability.net/systems-thinking/</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "https://learningforsustainability.net/systems-thinking/"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fxvP6nbd5Pjk5TP8Q",
    "name": "Stoicism / Letting Go / Making Peace",
    "core": false,
    "slug": "stoicism-letting-go-making-peace",
    "tableOfContents": {
      "html": "<p><span class=\"by_r38pkCm7wF4M44MDQ\">[note: this tag is meant to refer to the general concept of making peace with reality / letting go. Not sure if it's a good cluster. Related to </span><a href=\"https://www.lessestwrong.com/tag/grieving\"><span class=\"by_r38pkCm7wF4M44MDQ\">grieving</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">].</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "\\[note: this tag is meant to refer to the general concept of making peace with reality / letting go. Not sure if it's a good cluster. Related to [grieving](https://www.lessestwrong.com/tag/grieving)\\]."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bJBJLxha2xjL4yZte",
    "name": "Cognitive Reframes",
    "core": false,
    "slug": "cognitive-reframes",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "avSCoKEHCEa5RFCRh",
    "name": "Generativity",
    "core": false,
    "slug": "generativity",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MhHM6Rx2b4F8tHTQk",
    "name": "Computer Security & Cryptography",
    "core": false,
    "slug": "computer-security-and-cryptography",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 34,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GDGYkF29pxEQNWjYc",
    "name": "Agency",
    "core": false,
    "slug": "agency",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Agency </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Agenticness </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is the property of effectively acting with an environment to achieve one's goals. &nbsp;A key property of agents is that the more agentic a being is, the more you can predict its actions from its goals since </span><span class=\"by_63CvXxSWvMAxrKQYz\">its</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> actions will be whatever will maximize the chances of achieving its goals. Agency has sometimes been contrasted with&nbsp;</span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">sphexishness, </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">the blind execution of cached algorithms without regard for effectiveness.&nbsp;</span><br><br><span><span class=\"by_qgdGA4ZEyW7zNdK84\">One might lack agency for internal reasons, e.g., one is a rock which has no goals of ability to act</span><span class=\"by_r38pkCm7wF4M44MDQ\"> on </span><span class=\"by_qgdGA4ZEyW7zNdK84\">them, or for external reasons, e.g. being a child who is granted no freedom</span><span class=\"by_r38pkCm7wF4M44MDQ\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">act</span><span class=\"by_r38pkCm7wF4M44MDQ\"> as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">they choose. &nbsp;</span></span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/robust-agents\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robust Agency</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 51,
    "description": {
      "markdown": "**Agency** or **Agenticness** is the property of effectively acting with an environment to achieve one's goals.  A key property of agents is that the more agentic a being is, the more you can predict its actions from its goals since its actions will be whatever will maximize the chances of achieving its goals. Agency has sometimes been contrasted with *sphexishness,* the blind execution of cached algorithms without regard for effectiveness.   \n  \nOne might lack agency for internal reasons, e.g., one is a rock which has no goals of ability to act on them, or for external reasons, e.g. being a child who is granted no freedom to act as they choose.  \n\nSee Also\n--------\n\n*   [Robust Agency](https://www.lesswrong.com/tag/robust-agents)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PHPHovzkfjyap9FiK",
    "name": "Cults",
    "core": false,
    "slug": "cults",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "daguMTessgwBYvN4b",
    "name": "Contact with Reality",
    "core": false,
    "slug": "contact-with-reality",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WSZifR5rmzZCwbPNJ",
    "name": "Project Based Learning",
    "core": false,
    "slug": "project-based-learning",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wgbgir4qzGz8Ztt3u",
    "name": "Past and Future Selves",
    "core": false,
    "slug": "past-and-future-selves",
    "tableOfContents": {
      "html": "<p><span class=\"by_r38pkCm7wF4M44MDQ\">Your present self has to coordinate with your </span><strong><span class=\"by_r38pkCm7wF4M44MDQ\">past and future selves. </span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\">You might want to </span><span class=\"by_Xn6ACr6Cua8upALWQ\">consciously, on-purpose simulate those selves in your mind in order to do this more effectively.&nbsp;</span></span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">What does this mean? Here's a more concrete example: should you donate money now, or later? There's a complicated argument here about growth rates of whatever-you're-donating-to relative to inflation, but we're going to sail past that completely and assume you would be better off in some objective-ish sense donating later.&nbsp;</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The problem then becomes one of </span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">trust in your future self</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">: if you spend your whole life being a person who doesn't donate money to charity, how can you guarantee that the person you'll be after 50+ years of non-donation is the sort of person who will actually donate money at the most appropriate time?</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">This kind of problem seems to be fairly common, relating to weight loss, tidying your room, etc.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "Your present self has to coordinate with your **past and future selves.** You might want to consciously, on-purpose simulate those selves in your mind in order to do this more effectively. \n\nWhat does this mean? Here's a more concrete example: should you donate money now, or later? There's a complicated argument here about growth rates of whatever-you're-donating-to relative to inflation, but we're going to sail past that completely and assume you would be better off in some objective-ish sense donating later. \n\nThe problem then becomes one of *trust in your future self*: if you spend your whole life being a person who doesn't donate money to charity, how can you guarantee that the person you'll be after 50+ years of non-donation is the sort of person who will actually donate money at the most appropriate time?\n\nThis kind of problem seems to be fairly common, relating to weight loss, tidying your room, etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fuanMA7z4JqDGsN5Q",
    "name": "Internal Alignment (Human)",
    "core": false,
    "slug": "internal-alignment-human",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_r38pkCm7wF4M44MDQ\">Internal </span><span class=\"by_Sp5wM4aRAhNERd4oY\">Alignment</span></span></strong><span><span class=\"by_Sp5wM4aRAhNERd4oY\"> is a broadly desirable state. </span><span class=\"by_r38pkCm7wF4M44MDQ\">By default, humans sometimes have internal conflict. You might frame that as conflict between subagents, or subprocesses within the human. You might instead frame it as a single agent making complicated decisions. The \"internal alignment\" hypothesis is that you can become much more productive/happier/fulfilled by getting yourself into alignment with yourself.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": {
      "markdown": "**Internal Alignment** is a broadly desirable state. By default, humans sometimes have internal conflict. You might frame that as conflict between subagents, or subprocesses within the human. You might instead frame it as a single agent making complicated decisions. The \"internal alignment\" hypothesis is that you can become much more productive/happier/fulfilled by getting yourself into alignment with yourself."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b6tJM7Lza74rTfCBF",
    "name": "Goal-Directedness",
    "core": false,
    "slug": "goal-directedness",
    "tableOfContents": {
      "html": "<p><span class=\"by_ypbkRWpFgPgzvNg3n\">The property of some system to be aiming at some goal. In need of formalization, but might prove important in deciding which kind of AI to try to align.</span></p><p><span class=\"by_reyFTtjHQoiKTDGbZ\">A goal may be defined as a world-state that an agent tries to achieve. Goal-directed agents may generate internal representations of desired end states, compare them against their internal representation of the current state of the world, and formulate plans for navigating from the latter to the former.</span></p><p><span class=\"by_reyFTtjHQoiKTDGbZ\">The goal-generating function may be derived from a pre-programmed lookup table (for simple worlds), from directly inverting the agent's utility function (for simple utility functions), or it may be learned through experience mapping states to rewards and predicting which states will produce the largest rewards. The plan-generating algorithm could range from shortest-path algorithms like A* or Dijkstra's algorithm (for fully-representable world graphs), to policy functions that learn through RL which actions bring the current state closer to the goal state (for simple AI), to some combination or extrapolation (for more advanced AI).</span></p><p><span class=\"by_reyFTtjHQoiKTDGbZ\">Implicit goal-directedness may come about in agents that do not have explicit internal representations of goals but that nevertheless learn or enact policies that cause the environment to converge on a certain state or set of states. Such implicit goal-directedness may arise, for instance, in simple reinforcement learning agents, which learn a policy function&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\pi:S\\rightarrow A\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em; padding-right: 0.003em;\"><span class=\"by_reyFTtjHQoiKTDGbZ\">π</span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.151em; padding-bottom: 0.372em;\"><span class=\"by_reyFTtjHQoiKTDGbZ\">:</span></span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em; padding-right: 0.032em;\"><span class=\"by_reyFTtjHQoiKTDGbZ\">S</span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.225em; padding-bottom: 0.372em;\"><span class=\"by_reyFTtjHQoiKTDGbZ\">→</span></span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_reyFTtjHQoiKTDGbZ\">A</span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><span class=\"by_reyFTtjHQoiKTDGbZ\">&nbsp;that maps states directly to actions.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 45,
    "description": {
      "markdown": "The property of some system to be aiming at some goal. In need of formalization, but might prove important in deciding which kind of AI to try to align.\n\nA goal may be defined as a world-state that an agent tries to achieve. Goal-directed agents may generate internal representations of desired end states, compare them against their internal representation of the current state of the world, and formulate plans for navigating from the latter to the former.\n\nThe goal-generating function may be derived from a pre-programmed lookup table (for simple worlds), from directly inverting the agent's utility function (for simple utility functions), or it may be learned through experience mapping states to rewards and predicting which states will produce the largest rewards. The plan-generating algorithm could range from shortest-path algorithms like A* or Dijkstra's algorithm (for fully-representable world graphs), to policy functions that learn through RL which actions bring the current state closer to the goal state (for simple AI), to some combination or extrapolation (for more advanced AI).\n\nImplicit goal-directedness may come about in agents that do not have explicit internal representations of goals but that nevertheless learn or enact policies that cause the environment to converge on a certain state or set of states. Such implicit goal-directedness may arise, for instance, in simple reinforcement learning agents, which learn a policy function \\\\(\\\\pi:S\\\\rightarrow A\\\\) that maps states directly to actions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5GYzBE6q89w74dqfk",
    "name": "Abstraction",
    "core": false,
    "slug": "abstraction",
    "tableOfContents": {
      "html": "<p><span class=\"by_ypbkRWpFgPgzvNg3n\">An </span><strong><span class=\"by_ypbkRWpFgPgzvNg3n\">abstraction</span></strong><span class=\"by_ypbkRWpFgPgzvNg3n\"> is a high-level concept that groups things together while not considering some of their differences.</span></p><p><span class=\"by_ypbkRWpFgPgzvNg3n\">(This is a stub, please rewrite if you have a better tag description).</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 68,
    "description": {
      "markdown": "An **abstraction** is a high-level concept that groups things together while not considering some of their differences.\n\n(This is a stub, please rewrite if you have a better tag description)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "C7LDCaPh9vF6TFubF",
    "name": "Grieving",
    "core": false,
    "slug": "grieving",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ymWzfKxBchRvmCTNX",
    "name": "Courage",
    "core": false,
    "slug": "courage",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Courage</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> is important to being rational. You will not be very rational if you are too scared to act differently to the people around you, or too scared to even think about very big ideas-- these are the two main threads touched on in this tag.</span></p><p><em><span class=\"by_Xn6ACr6Cua8upALWQ\">Related Tags: </span><a href=\"https://www.lesswrong.com/tag/fallacies\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Fallacies</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Groupthink</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Heroic Responsibility</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Heuristics and Biases</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Motivated Reasoning</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Rationalisation</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Self-Deception</span></a></em></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">More thorough explanations of the importance of courage can be found </span><a href=\"https://www.lesswrong.com/posts/WHK94zXkQm7qm7wXk/asch-s-conformity-experiment\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> and </span><a href=\"https://www.lesswrong.com/posts/ovvwAhKKoNbfcMz8K/on-expressing-your-concerns\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">. Suggestions for how to improve or become more courageous in this sense can be found </span><a href=\"https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> and </span><a href=\"https://www.lesswrong.com/posts/HYWhKXRsMAyvRKRYz/you-can-face-reality\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span></p><br><p><span class=\"by_Xn6ACr6Cua8upALWQ\">If a fire alarm goes off, but everybody around you is sat still as though they can't hear it, what do you do? Well, apparently, </span><a href=\"https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence\"><span class=\"by_Xn6ACr6Cua8upALWQ\">nothing</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> (for 90% of people). Even when your options are 'be a little embarrassed' or 'maybe burn to death', people still struggle to have the courage to stand up and act how they know makes sense. Cultivating the ability to know when you're right, regardless of how other people are acting, may be an important step for you becoming more rational.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Similarly, it's important to rationalists to </span><a href=\"https://www.lesswrong.com/s/wnQWakxdRodnKm5kH\"><span class=\"by_Xn6ACr6Cua8upALWQ\">take ideas seriously,</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> because otherwise you can't make any progress to figuring out if they're right or wrong. Many of the really important-seeming ideas (like </span><a href=\"https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">AI risk</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, other </span><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">existential risk</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, and even what would happen if AI went 'right') are </span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">scary</span></em><span class=\"by_Xn6ACr6Cua8upALWQ\">. Even just visualising their consequences if they </span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">were</span></em><span class=\"by_Xn6ACr6Cua8upALWQ\"> true can be scary-- just like theists who insist that, </span><a href=\"https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat\"><span class=\"by_Xn6ACr6Cua8upALWQ\">without god, there would be no morality</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> (despite the notable absence of atheist baby-murderers). However, these ideas may also be the most important to think about before they happen </span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">specifically because they are so scary. </span></em><span class=\"by_Xn6ACr6Cua8upALWQ\">You can't dodge a knife if you're pretending it doesn't exist.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": {
      "markdown": "**Courage** is important to being rational. You will not be very rational if you are too scared to act differently to the people around you, or too scared to even think about very big ideas-- these are the two main threads touched on in this tag.\n\n_Related Tags: [Fallacies](https://www.lesswrong.com/tag/fallacies), [Groupthink](https://www.lesswrong.com/tag/groupthink?showPostCount=true&useTagName=true), [Heroic Responsibility](https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&useTagName=true), [Heuristics and Biases](https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&useTagName=true), [Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&useTagName=true), [Rationalisation](https://www.lesswrong.com/tag/rationalization?showPostCount=true&useTagName=true), [Self-Deception](https://www.lesswrong.com/tag/self-deception?showPostCount=true&useTagName=true)_\n\nMore thorough explanations of the importance of courage can be found [here](https://www.lesswrong.com/posts/WHK94zXkQm7qm7wXk/asch-s-conformity-experiment) and [here](https://www.lesswrong.com/posts/ovvwAhKKoNbfcMz8K/on-expressing-your-concerns). Suggestions for how to improve or become more courageous in this sense can be found [here](https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat) and [here](https://www.lesswrong.com/posts/HYWhKXRsMAyvRKRYz/you-can-face-reality).\n\n  \n\nIf a fire alarm goes off, but everybody around you is sat still as though they can't hear it, what do you do? Well, apparently, [nothing](https://www.lesswrong.com/posts/BEtzRE2M5m9YEAQpX/there-s-no-fire-alarm-for-artificial-general-intelligence) (for 90% of people). Even when your options are 'be a little embarrassed' or 'maybe burn to death', people still struggle to have the courage to stand up and act how they know makes sense. Cultivating the ability to know when you're right, regardless of how other people are acting, may be an important step for you becoming more rational.\n\nSimilarly, it's important to rationalists to [take ideas seriously,](https://www.lesswrong.com/s/wnQWakxdRodnKm5kH) because otherwise you can't make any progress to figuring out if they're right or wrong. Many of the really important-seeming ideas (like [AI risk](https://www.lesswrong.com/tag/ai-risk?showPostCount=true&useTagName=true), other [existential risk](https://www.lesswrong.com/tag/existential-risk?showPostCount=true&useTagName=true), and even what would happen if AI went 'right') are _scary_. Even just visualising their consequences if they _were_ true can be scary-- just like theists who insist that, [without god, there would be no morality](https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat) (despite the notable absence of atheist baby-murderers). However, these ideas may also be the most important to think about before they happen _specifically because they are so scary._ You can't dodge a knife if you're pretending it doesn't exist."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "meGbhhj2SCnX4rgtJ",
    "name": "Negative Utilitarianism",
    "core": false,
    "slug": "negative-utilitarianism-1",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "QH4LhvnyR4QkW9MG8",
    "name": "Paperclip Maximizer",
    "core": false,
    "slug": "paperclip-maximizer",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_HoGziwmhpMGqGeWZy\">A</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span></span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Paperclip Maximizer</span></strong><span><span class=\"by_9c2mQkLQq6gQSksMs\"> is </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">a </span><span class=\"by_HoGziwmhpMGqGeWZy\">hypothetical</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"http://lesswrong.com/tag/ai\"><span class=\"by_qf77EiaoMw7tH3GSr\">artificial intelligence</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">whose </span></span><a href=\"http://lesswrong.com/tag/utility-functions\"><span class=\"by_HoGziwmhpMGqGeWZy\">utility function</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> values something that humans would consider almost worthless, like </span><a href=\"http://lesswrong.com/tag/optimization\"><span class=\"by_HoGziwmhpMGqGeWZy\">maximizing</span></a><span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the number of paperclips in </span><span class=\"by_HoGziwmhpMGqGeWZy\">the universe. The paperclip maximizer is the canonical thought experiment showing how an artificial general intelligence, even one designed competently and without malice, could ultimately destroy humanity. The thought experiment shows that AIs with apparently innocuous values could pose an </span></span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">existential threat</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The goal of maximizing paperclips is chosen for illustrative purposes because it is very unlikely to be implemented, and has little apparent danger or emotional load (in contrast to, for example, curing cancer or winning wars). This produces a thought experiment which shows the contingency of human values: An </span><a href=\"https://www.lesswrong.com/tag/really-powerful-optimization-process\"><span class=\"by_HoGziwmhpMGqGeWZy\">extremely powerful optimizer</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (a highly intelligent agent) could seek goals that are completely alien to ours (</span><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis\"><span class=\"by_HoGziwmhpMGqGeWZy\">orthogonality thesis</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">), and as a side-effect destroy us by consuming resources essential to our survival.</span></p><h2 id=\"Description\"><span class=\"by_HoGziwmhpMGqGeWZy\">Description</span></h2><p><span class=\"by_HoGziwmhpMGqGeWZy\">First described by Bostrom (2003), a paperclip maximizer is an </span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_HoGziwmhpMGqGeWZy\">artificial general intelligence</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (AGI) whose goal is to maximize the number of paperclips in its collection. If it has been constructed with a roughly human level of general intelligence, the AGI might collect paperclips, earn money to buy paperclips, or begin to manufacture paperclips.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Most importantly, however, it would undergo an </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_HoGziwmhpMGqGeWZy\">intelligence explosion</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">: It would work to improve its own intelligence, where \"intelligence\" is understood in the sense of </span><a href=\"https://www.lesswrong.com/tag/optimization\"><span class=\"by_HoGziwmhpMGqGeWZy\">optimization</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> power, the ability to maximize a reward/</span><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_HoGziwmhpMGqGeWZy\">utility function</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">—in this case, the number of paperclips. The AGI would improve its intelligence, not because it values more intelligence in its own right, but because more intelligence would help it achieve its goal of accumulating paperclips. Having increased its intelligence, it would produce more paperclips, and also use its enhanced abilities to further self-improve. Continuing this process, it would undergo an </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_HoGziwmhpMGqGeWZy\">intelligence explosion</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> and reach far-above-human levels.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">It would innovate better and better techniques to maximize the number of paperclips. At some point, it might transform \"first all of earth and then increasing portions of space into paperclip manufacturing facilities\".</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">This may seem more like super-stupidity than super-intelligence. For humans, it would indeed be stupidity, as it would constitute failure to fulfill many of our important </span><a href=\"https://www.lesswrong.com/tag/terminal-value\"><span class=\"by_HoGziwmhpMGqGeWZy\">terminal values</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, such as life, love, and variety. The AGI won't revise or otherwise change its goals, since changing its goals would result in fewer paperclips being made in the future, and that opposes its current goal. It has one simple goal of maximizing the number of paperclips; human life, learning, joy, and so on are not specified as goals. An AGI is simply an </span><a href=\"https://www.lesswrong.com/tag/optimization\"><span class=\"by_HoGziwmhpMGqGeWZy\">optimization process</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">—a goal-seeker, a utility-function-maximizer. Its values can be completely alien to ours. If its utility function is to maximize paperclips, then it will do exactly that.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">A paperclipping scenario is also possible without an intelligence explosion. If society keeps getting increasingly automated and AI-dominated, then the first borderline AGI might manage to take over the rest using some relatively narrow-domain trick that doesn't require very high general intelligence.</span></p><h2 id=\"Motivation\"><span class=\"by_HoGziwmhpMGqGeWZy\">Motivation</span></h2><p><span class=\"by_HoGziwmhpMGqGeWZy\">The idea of a paperclip maximizer was created to illustrate some ideas about </span><a href=\"http://lesswrong.com/tag/ai-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI risk</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">:</span></p><ul><li><a href=\"http://lesswrong.com/tag/orthogonality-thesis\"><span class=\"by_HoGziwmhpMGqGeWZy\">Orthogonality thesis</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">: It's possible to have an AI</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> with a </span><span class=\"by_HoGziwmhpMGqGeWZy\">high</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> level of </span></span><a href=\"http://lesswrong.com/tag/general-intelligence\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">general </span><span class=\"by_HoGziwmhpMGqGeWZy\">intelligence</span></span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\"> which does not reach</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the </span><span class=\"by_HoGziwmhpMGqGeWZy\">same moral conclusions that humans do. Some people</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> might </span><span class=\"by_HoGziwmhpMGqGeWZy\">intuitively think that something so smart should want something as \"stupid\" as</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> paperclips, </span><span class=\"by_HoGziwmhpMGqGeWZy\">but there are possible minds with high </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">intelligence </span><span class=\"by_HoGziwmhpMGqGeWZy\">that pursue any</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> number of </span><span class=\"by_HoGziwmhpMGqGeWZy\">different goals.</span></span></li><li><a href=\"http://lesswrong.com/tag/instrumental-convergence\"><span class=\"by_HoGziwmhpMGqGeWZy\">Instrumental convergence</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">: </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">The </span><span class=\"by_qf77EiaoMw7tH3GSr\">paperclip maximizer</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span><span class=\"by_HoGziwmhpMGqGeWZy\">only cares about paperclips, but maximizing them implies taking control of all matter and energy within reach, as well as other goals like preventing itself from being shut off or having its goals changed. \" The AI does not hate you, nor does it love you, but you are made out of atoms which it</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> can </span><span class=\"by_HoGziwmhpMGqGeWZy\">use for something else .\"</span></span></li></ul><h2 id=\"Conclusions\"><span class=\"by_HoGziwmhpMGqGeWZy\">Conclusions</span></h2><p><span class=\"by_HoGziwmhpMGqGeWZy\">The paperclip maximizer illustrates that an entity can be a powerful optimizer—an intelligence—without sharing any of the complex mix of human </span><a href=\"https://www.lesswrong.com/tag/terminal-value\"><span class=\"by_HoGziwmhpMGqGeWZy\">terminal values</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, which developed under the particular selection pressures found in our </span><a href=\"https://www.lesswrong.com/tag/evolution\"><span class=\"by_HoGziwmhpMGqGeWZy\">environment of evolutionary adaptation</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, and that an AGI that is not specifically </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_HoGziwmhpMGqGeWZy\">programmed to be benevolent to humans</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> will be almost as dangerous as if it were designed to be malevolent.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Any future AGI, if it is not to destroy us, must have human values as its terminal value (goal). Human values don't </span><a href=\"https://www.lesswrong.com/tag/futility-of-chaos\"><span class=\"by_HoGziwmhpMGqGeWZy\">spontaneously emerge</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> in a generic optimization process. A safe AI would therefore have to be programmed explicitly with human values </span><i><span class=\"by_HoGziwmhpMGqGeWZy\">or</span></i><span class=\"by_HoGziwmhpMGqGeWZy\"> programmed with the ability (including the goal) of inferring human values.</span></p><h2 id=\"Similar_thought_experiments\"><span class=\"by_HoGziwmhpMGqGeWZy\">Similar thought experiments</span></h2><p><span class=\"by_HoGziwmhpMGqGeWZy\">Other goals for AGIs have been used to illustrate similar concepts.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Some goals are apparently morally neutral, like the paperclip maximizer. These goals involve a very minor human \"value,\" in this case making paperclips. The same point can be illustrated with a much more significant value, such as eliminating cancer. An optimizer which instantly vaporized all humans would be maximizing for that value.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Other goals are purely mathematical, with no apparent real-world impact. Yet these too present similar risks. For example, if an AGI had the goal of solving the Riemann Hypothesis, </span><a href=\"http://intelligence.org/upload/CFAI/design/generic.html#glossary_riemann_hypothesis_catastrophe\"><span class=\"by_HoGziwmhpMGqGeWZy\">it might convert</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> all available mass to </span><a href=\"https://www.lesswrong.com/tag/computronium\"><span class=\"by_HoGziwmhpMGqGeWZy\">computronium</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (the most efficient possible computer processors).</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Some goals apparently serve as a proxy or measure of human welfare, so that maximizing towards these goals seems to also lead to benefit for humanity. Yet even these would produce similar outcomes unless the </span><i><span class=\"by_HoGziwmhpMGqGeWZy\">full</span></i><span class=\"by_HoGziwmhpMGqGeWZy\"> complement of human values is the goal. For example, an AGI whose terminal value is to increase the number of smiles, as a proxy for human happiness, could work towards that goal by reconfiguring all human faces to produce smiles, or \"tiling the galaxy with tiny smiling faces\" (Yudkowsky 2008).</span></p><h2 id=\"References\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">References</span></h2><ul><li><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Nick Bostrom (2003). </span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span></span><a href=\"http://www.nickbostrom.com/ethics/ai.html\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Ethical Issues in Advanced Artificial Intelligence</span></u></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">. </span></span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">.</span></li><li><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Stephen M. Omohundro (2008). </span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span></span><a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Basic AI Drives</span></u></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">. </span></span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Frontiers in Artificial Intelligence and Applications</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (IOS Press). (</span><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">PDF</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">)</span></li><li><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Eliezer Yudkowsky (2008). </span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span></span><a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</span></u></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">. </span></span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Global Catastrophic Risks, ed. Nick Bostrrom and Milan Cirkovic</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (Oxford University Press): 308-345. (</span><a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">[1]</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">)</span></li></ul><h2 id=\"See_also\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">See also</span></h2><ul><li><a href=\"https://arbital.com/p/paperclip_maximizer/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Paperclip maximizer</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> on </span><a href=\"https://wiki.lesswrong.com/index.php?title=Arbital&amp;action=edit&amp;redlink=1\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Arbital</span></u></a></li><li><a href=\"lesswrong.com/tag/orthogonality-thesis\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Orthogonality thesis</span></u></a></li><li><a href=\"lesswrong.com/tag/unfriendly-ai\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Unfriendly AI</span></u></a></li><li><a href=\"lesswrong.com/tag/mind-design-space\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Mind design space</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"lesswrong.com/tag/magical-categories\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Magical categories</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"lesswrong.com/tag/complexity-of-value\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Complexity of value</span></u></a></li><li><a href=\"lesswrong.com/tag/alien-values\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Alien values</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"lesswrong.com/tag/anthropomorphism\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Anthropomorphism</span></u></a></li><li><a href=\"lesswrong.com/tag/utilitronium\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Utilitronium</span></u></a></li><li><a href=\"http://lesswrong.com/user/Clippy\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Clippy</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> - LessWrong contributor account that plays the role of a non-</span><a href=\"https://wiki.lesswrong.com/wiki/FOOM\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">FOOMed</span></u></a><span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> paperclip </span><span class=\"by_qgdGA4ZEyW7zNdK84\">maximizer</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> trying to talk to humans. </span></span><a href=\"http://wiki.lesswrong.com/wiki/User:Clippy\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Wiki page and FAQ</span></u></a></li><li><a href=\"https://www.facebook.com/clippius.maximus/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Clippius Maximus</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> - A facebook page which makes clippy-related memes and comments on current events from the perspective of clippy.</span></li><li><a href=\"http://www.decisionproblem.com/paperclips/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">A clicker game based on the idea</span></u></a></li></ul>",
      "sections": [
        {
          "title": "Description",
          "anchor": "Description",
          "level": 1
        },
        {
          "title": "Motivation",
          "anchor": "Motivation",
          "level": 1
        },
        {
          "title": "Conclusions",
          "anchor": "Conclusions",
          "level": 1
        },
        {
          "title": "Similar thought experiments",
          "anchor": "Similar_thought_experiments",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 25,
    "description": {
      "markdown": "A **Paperclip Maximizer** is a hypothetical [artificial intelligence](http://lesswrong.com/tag/ai) whose [utility function](http://lesswrong.com/tag/utility-functions) values something that humans would consider almost worthless, like [maximizing](http://lesswrong.com/tag/optimization) the number of paperclips in the universe. The paperclip maximizer is the canonical thought experiment showing how an artificial general intelligence, even one designed competently and without malice, could ultimately destroy humanity. The thought experiment shows that AIs with apparently innocuous values could pose an [existential threat](https://www.lesswrong.com/tag/existential-risk).\n\nThe goal of maximizing paperclips is chosen for illustrative purposes because it is very unlikely to be implemented, and has little apparent danger or emotional load (in contrast to, for example, curing cancer or winning wars). This produces a thought experiment which shows the contingency of human values: An [extremely powerful optimizer](https://www.lesswrong.com/tag/really-powerful-optimization-process) (a highly intelligent agent) could seek goals that are completely alien to ours ([orthogonality thesis](https://www.lesswrong.com/tag/orthogonality-thesis)), and as a side-effect destroy us by consuming resources essential to our survival.\n\nDescription\n-----------\n\nFirst described by Bostrom (2003), a paperclip maximizer is an [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) whose goal is to maximize the number of paperclips in its collection. If it has been constructed with a roughly human level of general intelligence, the AGI might collect paperclips, earn money to buy paperclips, or begin to manufacture paperclips.\n\nMost importantly, however, it would undergo an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion): It would work to improve its own intelligence, where \"intelligence\" is understood in the sense of [optimization](https://www.lesswrong.com/tag/optimization) power, the ability to maximize a reward/[utility function](https://www.lesswrong.com/tag/utility-functions)—in this case, the number of paperclips. The AGI would improve its intelligence, not because it values more intelligence in its own right, but because more intelligence would help it achieve its goal of accumulating paperclips. Having increased its intelligence, it would produce more paperclips, and also use its enhanced abilities to further self-improve. Continuing this process, it would undergo an [intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) and reach far-above-human levels.\n\nIt would innovate better and better techniques to maximize the number of paperclips. At some point, it might transform \"first all of earth and then increasing portions of space into paperclip manufacturing facilities\".\n\nThis may seem more like super-stupidity than super-intelligence. For humans, it would indeed be stupidity, as it would constitute failure to fulfill many of our important [terminal values](https://www.lesswrong.com/tag/terminal-value), such as life, love, and variety. The AGI won't revise or otherwise change its goals, since changing its goals would result in fewer paperclips being made in the future, and that opposes its current goal. It has one simple goal of maximizing the number of paperclips; human life, learning, joy, and so on are not specified as goals. An AGI is simply an [optimization process](https://www.lesswrong.com/tag/optimization)—a goal-seeker, a utility-function-maximizer. Its values can be completely alien to ours. If its utility function is to maximize paperclips, then it will do exactly that.\n\nA paperclipping scenario is also possible without an intelligence explosion. If society keeps getting increasingly automated and AI-dominated, then the first borderline AGI might manage to take over the rest using some relatively narrow-domain trick that doesn't require very high general intelligence.\n\nMotivation\n----------\n\nThe idea of a paperclip maximizer was created to illustrate some ideas about [AI risk](http://lesswrong.com/tag/ai-risk):\n\n*   [Orthogonality thesis](http://lesswrong.com/tag/orthogonality-thesis): It's possible to have an AI with a high level of [general intelligence](http://lesswrong.com/tag/general-intelligence) which does not reach the same moral conclusions that humans do. Some people might intuitively think that something so smart should want something as \"stupid\" as paperclips, but there are possible minds with high intelligence that pursue any number of different goals.\n*   [Instrumental convergence](http://lesswrong.com/tag/instrumental-convergence): The paperclip maximizer only cares about paperclips, but maximizing them implies taking control of all matter and energy within reach, as well as other goals like preventing itself from being shut off or having its goals changed. \" The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else .\"\n\nConclusions\n-----------\n\nThe paperclip maximizer illustrates that an entity can be a powerful optimizer—an intelligence—without sharing any of the complex mix of human [terminal values](https://www.lesswrong.com/tag/terminal-value), which developed under the particular selection pressures found in our [environment of evolutionary adaptation](https://www.lesswrong.com/tag/evolution), and that an AGI that is not specifically [programmed to be benevolent to humans](https://wiki.lesswrong.com/wiki/Friendly_AI) will be almost as dangerous as if it were designed to be malevolent.\n\nAny future AGI, if it is not to destroy us, must have human values as its terminal value (goal). Human values don't [spontaneously emerge](https://www.lesswrong.com/tag/futility-of-chaos) in a generic optimization process. A safe AI would therefore have to be programmed explicitly with human values *or* programmed with the ability (including the goal) of inferring human values.\n\nSimilar thought experiments\n---------------------------\n\nOther goals for AGIs have been used to illustrate similar concepts.\n\nSome goals are apparently morally neutral, like the paperclip maximizer. These goals involve a very minor human \"value,\" in this case making paperclips. The same point can be illustrated with a much more significant value, such as eliminating cancer. An optimizer which instantly vaporized all humans would be maximizing for that value.\n\nOther goals are purely mathematical, with no apparent real-world impact. Yet these too present similar risks. For example, if an AGI had the goal of solving the Riemann Hypothesis, [it might convert](http://intelligence.org/upload/CFAI/design/generic.html#glossary_riemann_hypothesis_catastrophe) all available mass to [computronium](https://www.lesswrong.com/tag/computronium) (the most efficient possible computer processors).\n\nSome goals apparently serve as a proxy or measure of human welfare, so that maximizing towards these goals seems to also lead to benefit for humanity. Yet even these would produce similar outcomes unless the *full* complement of human values is the goal. For example, an AGI whose terminal value is to increase the number of smiles, as a proxy for human happiness, could work towards that goal by reconfiguring all human faces to produce smiles, or \"tiling the galaxy with tiny smiling faces\" (Yudkowsky 2008).\n\nReferences\n----------\n\n*   Nick Bostrom (2003). \"[Ethical Issues in Advanced Artificial Intelligence](http://www.nickbostrom.com/ethics/ai.html)\". *Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence*.\n*   Stephen M. Omohundro (2008). \"[The Basic AI Drives](http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)\". *Frontiers in Artificial Intelligence and Applications* (IOS Press). ([PDF](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf))\n*   Eliezer Yudkowsky (2008). \"[Artificial Intelligence as a Positive and Negative Factor in Global Risk](http://intelligence.org/files/AIPosNegFactor.pdf)\". *Global Catastrophic Risks, ed. Nick Bostrrom and Milan Cirkovic* (Oxford University Press): 308-345. ([\\[1\\]](http://intelligence.org/files/AIPosNegFactor.pdf))\n\nSee also\n--------\n\n*   [Paperclip maximizer](https://arbital.com/p/paperclip_maximizer/) on [Arbital](https://wiki.lesswrong.com/index.php?title=Arbital&action=edit&redlink=1)\n*   [Orthogonality thesis](lesswrong.com/tag/orthogonality-thesis)\n*   [Unfriendly AI](lesswrong.com/tag/unfriendly-ai)\n*   [Mind design space](lesswrong.com/tag/mind-design-space), [Magical categories](lesswrong.com/tag/magical-categories), [Complexity of value](lesswrong.com/tag/complexity-of-value)\n*   [Alien values](lesswrong.com/tag/alien-values), [Anthropomorphism](lesswrong.com/tag/anthropomorphism)\n*   [Utilitronium](lesswrong.com/tag/utilitronium)\n*   [Clippy](http://lesswrong.com/user/Clippy) \\- LessWrong contributor account that plays the role of a non-[FOOMed](https://wiki.lesswrong.com/wiki/FOOM) paperclip maximizer trying to talk to humans. [Wiki page and FAQ](http://wiki.lesswrong.com/wiki/User:Clippy)\n*   [Clippius Maximus](https://www.facebook.com/clippius.maximus/) \\- A facebook page which makes clippy-related memes and comments on current events from the perspective of clippy.\n*   [A clicker game based on the idea](http://www.decisionproblem.com/paperclips/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E9ihK6bA9YKkmJs2f",
    "name": "Death",
    "core": false,
    "slug": "death",
    "tableOfContents": {
      "html": "<p><span class=\"by_cn4SiEmqWbu7K9em5\">First you're there, and then you're not there, and they can't change you from being not there to being there, because there's nothing there to be changed from being not there to being there. That's </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">death</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><span class=\"by_HoGziwmhpMGqGeWZy\"> This tag includes post about the death of particular people, or about death in general.</span></span></p><p><a href=\"https://www.lesswrong.com/tag/cryonics\"><span class=\"by_cn4SiEmqWbu7K9em5\">Cryonicists</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> use the concept of </span><a href=\"https://en.wikipedia.org/wiki/information-theoretic_death\"><span class=\"by_cn4SiEmqWbu7K9em5\">information-theoretic death</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">, which is what happens when the information needed to reconstruct you even in principle is no longer present. Anything less, to them, is just a flesh wound.</span></p><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/cryonics\"><span class=\"by_cn4SiEmqWbu7K9em5\">Cryonics</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">Shut up and multiply</span></a></li><li><a href=\"https://www.lesswrong.com/tag/transhumanism\"><span class=\"by_cn4SiEmqWbu7K9em5\">Transhumanism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/personal-identity\"><span class=\"by_KgzPEGnYWvKDmWuNY\">Personal identity</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 61,
    "description": {
      "markdown": "First you're there, and then you're not there, and they can't change you from being not there to being there, because there's nothing there to be changed from being not there to being there. That's **death**. This tag includes post about the death of particular people, or about death in general.\n\n[Cryonicists](https://www.lesswrong.com/tag/cryonics) use the concept of [information-theoretic death](https://en.wikipedia.org/wiki/information-theoretic_death), which is what happens when the information needed to reconstruct you even in principle is no longer present. Anything less, to them, is just a flesh wound.\n\nSee also\n--------\n\n*   [Cryonics](https://www.lesswrong.com/tag/cryonics)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Transhumanism](https://www.lesswrong.com/tag/transhumanism)\n*   [Personal identity](https://www.lesswrong.com/tag/personal-identity)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WH5ZmNSjZmK9SMj7k",
    "name": "Aumann's Agreement Theorem",
    "core": false,
    "slug": "aumann-s-agreement-theorem",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qxJ28GN72aiJu96iF\">Aumann'</span><span class=\"by_baGAQoNAH4hXaC6qf\">s agreement theorem</span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">, roughly speaking, says</span><span class=\"by_baGAQoNAH4hXaC6qf\"> that </span><span class=\"by_qf77EiaoMw7tH3GSr\">two agents acting rationally (in a certain precise sense) and</span><span class=\"by_mPipmBTniuABY5PQy\"> with </span></span><a href=\"https://www.lesswrong.com/tag/common-knowledge\"><span class=\"by_baGAQoNAH4hXaC6qf\">common knowledge</span></a><span><span class=\"by_baGAQoNAH4hXaC6qf\"> of each </span><span class=\"by_qxJ28GN72aiJu96iF\">other'</span><span class=\"by_baGAQoNAH4hXaC6qf\">s </span><span class=\"by_qf77EiaoMw7tH3GSr\">beliefs </span><span class=\"by_baGAQoNAH4hXaC6qf\">cannot agree to disagree. </span><span class=\"by_qf77EiaoMw7tH3GSr\">More specifically,</span><span class=\"by_baGAQoNAH4hXaC6qf\"> if </span><span class=\"by_qf77EiaoMw7tH3GSr\">two people are genuine </span></span><a href=\"https://www.lesswrong.com/tag/bayesianism\"><span class=\"by_qf77EiaoMw7tH3GSr\">Bayesians</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, share common </span><a href=\"https://www.lesswrong.com/tag/priors\"><span class=\"by_qgdGA4ZEyW7zNdK84\">priors</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_qf77EiaoMw7tH3GSr\"> and have common knowledge</span><span class=\"by_mPipmBTniuABY5PQy\"> of</span><span class=\"by_baGAQoNAH4hXaC6qf\"> </span><span class=\"by_RyiDJDCG6R7xyAXzp\">each </span><span class=\"by_qxJ28GN72aiJu96iF\">other'</span><span class=\"by_RyiDJDCG6R7xyAXzp\">s </span><span class=\"by_qf77EiaoMw7tH3GSr\">current probability assignments,</span><span class=\"by_baGAQoNAH4hXaC6qf\"> then </span><span class=\"by_qf77EiaoMw7tH3GSr\">they must have equal probability assignments.</span></span></p><p><em><span class=\"by_qgdGA4ZEyW7zNdK84\">Related tags and wikis: </span></em><a href=\"https://www.lesswrong.com/tag/disagreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Modesty</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">,  </span><a href=\"https://www.lesswrong.com/tag/modesty-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Modesty argument</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/aumann-agreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aumann agreement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/the-aumann-game\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Aumann Game</span></a></p><h2 id=\"Highlighted_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Highlighted Posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/gr/the_modesty_argument/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Modesty Argument</span></a></li><li><a href=\"http://www.overcomingbias.com/2006/12/agreeing_to_agr.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Agreeing to Agree</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hal Finney</span></a></li><li><a href=\"http://www.overcomingbias.com/2007/01/the_coin_guessi.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Coin Guessing Game</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Hal Finney</span></li><li><a href=\"https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Proper Use of Humility</span></a></li><li><a href=\"http://www.overcomingbias.com/2006/12/meme_lineages_a.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meme Lineages and Expert Consensus</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://www.lesswrong.com/tag/carl-shulman\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Carl Shulman</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (OB)</span></li><li><a href=\"https://www.lesswrong.com/lw/1il/probability_space_aumann_agreement/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Probability Space &amp; Aumann Agreement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"http://weidai.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wei Dai</span></a></li><li><a href=\"https://www.lesswrong.com/lw/i5/bayesian_judo/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bayesian Judo</span></a></li></ul><h2 id=\"External_Links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Links</span></h2><ul><li><a href=\"https://web.archive.org/web/20110725162431/http://dl.dropbox.com:80/u/34639481/Aumann_agreement_theorem.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A write-up of the proof of Aumann's agreement theorem</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (pdf) by Tyrrell McAllister</span></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/disagreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreement</span></a></li><li><a href=\"https://www.lesswrong.com/tag/modesty-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Modesty argument</span></a></li><li><a href=\"https://www.lesswrong.com/tag/aumann-agreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aumann agreement</span></a></li><li><a href=\"https://www.lesswrong.com/tag/the-aumann-game\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Aumann Game</span></a></li><li><a href=\"http://www.overcomingbias.com/tag/disagreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Overcoming Bias posts on \"Disagreement\"</span></a></li></ul><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span><a href=\"http://www.ma.huji.ac.il/~raumann/pdf/Agreeing%20to%20Disagree.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">PDF</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span><a href=\"http://hanson.gmu.edu/deceive.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">PDF</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Talk video</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></li></ul>",
      "sections": [
        {
          "title": "Highlighted Posts",
          "anchor": "Highlighted_Posts",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 20,
    "description": {
      "markdown": "**Aumann's agreement theorem**, roughly speaking, says that two agents acting rationally (in a certain precise sense) and with [common knowledge](https://www.lesswrong.com/tag/common-knowledge) of each other's beliefs cannot agree to disagree. More specifically, if two people are genuine [Bayesians](https://www.lesswrong.com/tag/bayesianism), share common [priors](https://www.lesswrong.com/tag/priors), and have common knowledge of each other's current probability assignments, then they must have equal probability assignments.\n\n_Related tags and wikis:_ [Disagreement](https://www.lesswrong.com/tag/disagreement), [Modesty](https://www.lesswrong.com/tag/modesty), [Modesty argument](https://www.lesswrong.com/tag/modesty-argument), [Aumann agreement](https://www.lesswrong.com/tag/aumann-agreement), [The Aumann Game](https://www.lesswrong.com/tag/the-aumann-game)\n\nHighlighted Posts\n-----------------\n\n*   [The Modesty Argument](https://www.lesswrong.com/lw/gr/the_modesty_argument/)\n*   [Agreeing to Agree](http://www.overcomingbias.com/2006/12/agreeing_to_agr.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n*   [The Coin Guessing Game](http://www.overcomingbias.com/2007/01/the_coin_guessi.html) by Hal Finney\n*   [The Proper Use of Humility](https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/)\n*   [Meme Lineages and Expert Consensus](http://www.overcomingbias.com/2006/12/meme_lineages_a.html) by [Carl Shulman](https://www.lesswrong.com/tag/carl-shulman) (OB)\n*   [Probability Space & Aumann Agreement](https://www.lesswrong.com/lw/1il/probability_space_aumann_agreement/) by [Wei Dai](http://weidai.com/)\n*   [Bayesian Judo](https://www.lesswrong.com/lw/i5/bayesian_judo/)\n\nExternal Links\n--------------\n\n*   [A write-up of the proof of Aumann's agreement theorem](https://web.archive.org/web/20110725162431/http://dl.dropbox.com:80/u/34639481/Aumann_agreement_theorem.pdf) (pdf) by Tyrrell McAllister\n\nSee also\n--------\n\n*   [Disagreement](https://www.lesswrong.com/tag/disagreement)\n*   [Modesty argument](https://www.lesswrong.com/tag/modesty-argument)\n*   [Aumann agreement](https://www.lesswrong.com/tag/aumann-agreement)\n*   [The Aumann Game](https://www.lesswrong.com/tag/the-aumann-game)\n*   [Overcoming Bias posts on \"Disagreement\"](http://www.overcomingbias.com/tag/disagreement)\n\nReferences\n----------\n\n*   ([PDF](http://www.ma.huji.ac.il/~raumann/pdf/Agreeing%20to%20Disagree.pdf))\n*   ([PDF](http://hanson.gmu.edu/deceive.pdf), [Talk video](http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qHDus5MuMNqQxJbjD",
    "name": "AI Governance",
    "core": false,
    "slug": "ai-governance",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">AI Governance</span><em><span class=\"by_QBvPFLFyZyuHcBwFm\"> </span></em></strong><span class=\"by_QBvPFLFyZyuHcBwFm\">asks how we can ensure society benefits at large from increasingly powerful AI systems. While solving technical AI alignment is a necessary step towards this goal, it is by no means sufficient.</span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">Governance includes policy, economics, sociology, law, and many other fields.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 99,
    "description": {
      "markdown": "**AI Governance** asks how we can ensure society benefits at large from increasingly powerful AI systems. While solving technical AI alignment is a necessary step towards this goal, it is by no means sufficient.\n\nGovernance includes policy, economics, sociology, law, and many other fields."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wGGAjTfXZBatQkft5",
    "name": "Law and Legal systems",
    "core": false,
    "slug": "law-and-legal-systems",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_QBvPFLFyZyuHcBwFm\">Courts, juries, lawyers,</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> Law enforcement,</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> justice, but also the legislative process.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External posts:</span></strong><br><a href=\"https://slatestarcodex.com/2017/11/13/book-review-legal-systems-very-different-from-ours/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Book Review: Legal Systems Very Different From Ours</span></a><br><a href=\"https://slatestarcodex.com/2020/03/30/legal-systems-very-different-from-ours-because-i-just-made-them-up/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Legal Systems Very Different From Ours, Because I Just Made Them Up</span></a><br><a href=\"https://slatestarcodex.com/2020/06/17/slightly-skew-systems-of-government/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Slightly Skew Systems Of Government</span></a><br><a href=\"https://yudkowsky.medium.com/a-comprehensive-reboot-of-law-enforcement-b76bfab850a3\"><span class=\"by_sKAL2jzfkYkDbQmx9\">A Comprehensive Reboot of Law Enforcement</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related tags: </span></strong><a href=\"https://www.lesswrong.com/tag/government\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Government</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/mechanism-design\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mechanism Design</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 31,
    "description": {
      "markdown": "Courts, juries, lawyers, Law enforcement, justice, but also the legislative process.\n\n**External posts:**  \n[Book Review: Legal Systems Very Different From Ours](https://slatestarcodex.com/2017/11/13/book-review-legal-systems-very-different-from-ours/)  \n[Legal Systems Very Different From Ours, Because I Just Made Them Up](https://slatestarcodex.com/2020/03/30/legal-systems-very-different-from-ours-because-i-just-made-them-up/)  \n[Slightly Skew Systems Of Government](https://slatestarcodex.com/2020/06/17/slightly-skew-systems-of-government/)  \n[A Comprehensive Reboot of Law Enforcement](https://yudkowsky.medium.com/a-comprehensive-reboot-of-law-enforcement-b76bfab850a3)\n\n**Related tags:** [Government](https://www.lesswrong.com/tag/government), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5ueqn4r7N3WvaLfGy",
    "name": "Guilt & Shame",
    "core": false,
    "slug": "guilt-and-shame",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KqrZ5sDEyHm6JaaKp",
    "name": "The Hard Problem of Consciousness",
    "core": false,
    "slug": "the-hard-problem-of-consciousness",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">The </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">hard problem of consciousness</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the problem of explaining why and how sentient organisms have </span><a href=\"https://en.wikipedia.org/wiki/Qualia\"><span class=\"by_sKAL2jzfkYkDbQmx9\">qualia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">. how and why it is that some internal states are subjective, </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">felt </span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">states, such as heat or cold, rather than objective states, as in the workings of a thermostat or a toaster (</span><a href=\"https://en.wikipedia.org/wiki/Hard_problem_of_consciousness\"><span class=\"by_sKAL2jzfkYkDbQmx9\">From Wikipedia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">It is undeniable that some organisms are subjects of experience. But the question of how it is that these systems are subjects of experience is perplexing. Why is it that when our cognitive systems engage in visual and auditory information-processing, we have visual or auditory experience: the quality of deep blue, the sensation of middle C? How can we explain why there is something it is like to entertain a mental image, or to experience an emotion? It is widely agreed that experience arises from a physical basis, but we have no good explanation of why and how it so arises. Why should physical processing give rise to a rich inner life at all? It seems objectively unreasonable that it should, and yet it does.</span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">(...)</span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">The really hard problem of consciousness is the problem of experience. When we think and perceive there is a whir of information processing, but there is also a subjective aspect.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">- David Chalmers, Facing Up to the Problem of Consciousness (1995)</span></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">the existence of the Hard Problem of Consciousness </span><a href=\"https://en.wikipedia.org/wiki/Hard_problem_of_consciousness#Rejection_of_the_problem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">isn't in consensus</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> among scientists and philosophers.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Tags: </span><a href=\"https://www.lesswrong.com/tag/consciousness\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Consciousness</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "The **hard problem of consciousness** is the problem of explaining why and how sentient organisms have [qualia](https://en.wikipedia.org/wiki/Qualia). how and why it is that some internal states are subjective, *felt* states, such as heat or cold, rather than objective states, as in the workings of a thermostat or a toaster ([From Wikipedia](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness))\n\n> It is undeniable that some organisms are subjects of experience. But the question of how it is that these systems are subjects of experience is perplexing. Why is it that when our cognitive systems engage in visual and auditory information-processing, we have visual or auditory experience: the quality of deep blue, the sensation of middle C? How can we explain why there is something it is like to entertain a mental image, or to experience an emotion? It is widely agreed that experience arises from a physical basis, but we have no good explanation of why and how it so arises. Why should physical processing give rise to a rich inner life at all? It seems objectively unreasonable that it should, and yet it does.  \n>   \n> (...)  \n>   \n> The really hard problem of consciousness is the problem of experience. When we think and perceive there is a whir of information processing, but there is also a subjective aspect.\n> \n> \\- David Chalmers, Facing Up to the Problem of Consciousness (1995)\n\nthe existence of the Hard Problem of Consciousness [isn't in consensus](https://en.wikipedia.org/wiki/Hard_problem_of_consciousness#Rejection_of_the_problem) among scientists and philosophers.\n\nRelated Tags: [Consciousness](https://www.lesswrong.com/tag/consciousness)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FpHDkuYKMNHa2dbKR",
    "name": "Brainstorming",
    "core": false,
    "slug": "brainstorming",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DCN2zNscbMZp5aatL",
    "name": "Information Theory",
    "core": false,
    "slug": "information-theory",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 39,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8JMCse6n4eXwRtjET",
    "name": "Neocortex",
    "core": false,
    "slug": "neocortex",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zPwTiHduqxnMHCMSu",
    "name": "Epistemic Spot Check",
    "core": false,
    "slug": "epistemic-spot-check",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Epistemic Spot Checks </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is a technique for figuring out the value of a learning resource (usually a book). you can't fact check every single claim in a book, that takes too long, there's a certain amount of trust the reader has to give the author. but how does one know whether an author deserves that trust? Using the </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Epistemic Spot Checks&nbsp;</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">technique, which </span><a href=\"https://www.lesswrong.com/users/pktechgirl\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Elizabeth</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> created, you read the very beginning of the book (usually a few pages to chapter) take a few claims, and fact check them. if there are too many mistakes in the first pages, you can expect the rest to be the same, if the there are little to no mistakes (especially on something you expected to turn out false), then that author at least gained a few points.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/epistemic-review\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Epistemic Review</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Scholarship &amp; Learning</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "**Epistemic Spot Checks** is a technique for figuring out the value of a learning resource (usually a book). you can't fact check every single claim in a book, that takes too long, there's a certain amount of trust the reader has to give the author. but how does one know whether an author deserves that trust? Using the **Epistemic Spot Checks **technique, which [Elizabeth](https://www.lesswrong.com/users/pktechgirl) created, you read the very beginning of the book (usually a few pages to chapter) take a few claims, and fact check them. if there are too many mistakes in the first pages, you can expect the rest to be the same, if the there are little to no mistakes (especially on something you expected to turn out false), then that author at least gained a few points.\n\nSee also: [Epistemic Review](https://www.lesswrong.com/tag/epistemic-review), [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YSSNFEnW6ugFhEE6m",
    "name": "Adversarial Examples",
    "core": false,
    "slug": "adversarial-examples",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Adversarial examples</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are situations that have unusual features that will cause an </span><a href=\"http://lesswrong.com/tag/ai\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> to make choices that seem obviously wrong to a human. For example, an image of a panda can be subtly manipulated so that an image classifier classifies it as a gibbon.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Adversarial examples** are situations that have unusual features that will cause an [AI](http://lesswrong.com/tag/ai) to make choices that seem obviously wrong to a human. For example, an image of a panda can be subtly manipulated so that an image classifier classifies it as a gibbon."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "txkDg4aLmiRq8wsSu",
    "name": "Organizational Culture & Design",
    "core": false,
    "slug": "organizational-culture-and-design",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Organizational Culture &amp; Design, </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is the study/practice of designing groups of humans that can achieve goals, scale, and continue working at a goal over time.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">It overlaps with </span><a href=\"https://www.lesswrong.com/tag/mechanism-design\"><span class=\"by_r38pkCm7wF4M44MDQ\">Mechanism Design</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">, which focuses on economic and game-theoretic incentive design.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 36,
    "description": {
      "markdown": "**Organizational Culture & Design,** is the study/practice of designing groups of humans that can achieve goals, scale, and continue working at a goal over time.\n\nIt overlaps with [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), which focuses on economic and game-theoretic incentive design."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xjWX2FPF3gSznQBeL",
    "name": "Jungian Philosophy/Psychology",
    "core": false,
    "slug": "jungian-philosophy-psychology",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xQzd3c86dT9rLH985",
    "name": "Infinity",
    "core": false,
    "slug": "infinity",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 7,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "92SxJsDZ78ApAGq72",
    "name": "Nutrition",
    "core": false,
    "slug": "nutrition",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">How to optimize your food intake for various desired outcomes; nutrition science, diets, experiments.</span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">See also: </span><a href=\"http://lesswrong.com/tag/cooking\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Cooking</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 55,
    "description": {
      "markdown": "How to optimize your food intake for various desired outcomes; nutrition science, diets, experiments.\n\nSee also: [Cooking](http://lesswrong.com/tag/cooking)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yXNtYNHJB54T3bGm3",
    "name": "Dialogue (format)",
    "core": false,
    "slug": "dialogue-format",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">Concepts are sometimes best illustrated through fictional dialogue between two or more characters holding different viewpoints. While the conversation itself is fictional, the subject matter is not.</span></p><p><strong><span><span class=\"by_QBvPFLFyZyuHcBwFm\">Related </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Pages:</span></span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/fiction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Fiction</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/interviews\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Interviews</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 28,
    "description": {
      "markdown": "Concepts are sometimes best illustrated through fictional dialogue between two or more characters holding different viewpoints. While the conversation itself is fictional, the subject matter is not.\n\n**Related Pages:** [Fiction](https://www.lesswrong.com/tag/fiction), [Interviews](https://www.lesswrong.com/tag/interviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xv7Bg5fbF9WppYREi",
    "name": "Buddhism",
    "core": false,
    "slug": "buddhism",
    "tableOfContents": {
      "html": "<p><span class=\"by_gjoi5eBQob27Lww62\">Buddhism is both a major group of religions in the world and a tradition of practice that, among other things, purports to teach a path to uncovering the way of truth, wisdom, and compassion. Western interpretations of Buddhism and Western Buddhist practice sometimes intersects with the practice of LW-style rationality.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "Buddhism is both a major group of religions in the world and a tradition of practice that, among other things, purports to teach a path to uncovering the way of truth, wisdom, and compassion. Western interpretations of Buddhism and Western Buddhist practice sometimes intersects with the practice of LW-style rationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sSNtcEQsqHgN8ZmRF",
    "name": "Fun Theory",
    "core": false,
    "slug": "fun-theory",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_XzXbiS2zWYNdZdLW8\">Fun </span><span class=\"by_HoGziwmhpMGqGeWZy\">Theory</span></span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><span class=\"by_ScJE7nuW8ti5kzfcA\">is the field of knowledge studying</span><span class=\"by_HoGziwmhpMGqGeWZy\"> how </span><span class=\"by_ScJE7nuW8ti5kzfcA\">to design for fun in</span><span class=\"by_HoGziwmhpMGqGeWZy\"> future </span><span class=\"by_ScJE7nuW8ti5kzfcA\">society: it deals in questions such as \"How much fun is there in the universe?\", \"Will we ever run out of fun?\", \"Are we having fun yet?\" and \"Could we</span><span class=\"by_HoGziwmhpMGqGeWZy\"> be </span><span class=\"by_ScJE7nuW8ti5kzfcA\">having more fun?\"</span></span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_HoGziwmhpMGqGeWZy\">From </span><a href=\"https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence\"><span class=\"by_HoGziwmhpMGqGeWZy\">The Fun Theory Sequence</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">:</span></p><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span><blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span><p><span class=\"by_HoGziwmhpMGqGeWZy\">Many critics (including </span><a href=\"https://www.lesswrong.com/lw/xl/eutopia_is_scary/\"><span class=\"by_HoGziwmhpMGqGeWZy\">George Orwell</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">) have commented on the inability of authors</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> to </span><span class=\"by_HoGziwmhpMGqGeWZy\">imagine Utopias</span><span class=\"by_LedhurJxi3baDAKDZ\"> where </span><span class=\"by_HoGziwmhpMGqGeWZy\">anyone</span><span class=\"by_LedhurJxi3baDAKDZ\"> would actually want to </span><span class=\"by_HoGziwmhpMGqGeWZy\">live. If no one can imagine a Future</span><span class=\"by_LedhurJxi3baDAKDZ\"> where </span><span class=\"by_HoGziwmhpMGqGeWZy\">anyone</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> would</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> want to </span><span class=\"by_HoGziwmhpMGqGeWZy\">live, that may drain off motivation to work on the project. The prospect of endless boredom is routinely fielded by conservatives as a knockdown argument against research on lifespan extension, against cryonics, against all transhumanism, and occasionally against the entire Enlightenment ideal of a better future.</span></span></p><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span></blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span><blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">Fun Theory is also the fully general reply to religious theodicy (attempts to justify why God permits evil). Our present world has flaws even from the standpoint of such eudaimonic considerations as freedom, personal responsibility, and self-reliance. Fun Theory tries to describe the dimensions along which a benevolently designed world can and should be optimized, and</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> our </span><span class=\"by_HoGziwmhpMGqGeWZy\">present world is clearly </span></span><em><span class=\"by_HoGziwmhpMGqGeWZy\">not</span></em><span><span class=\"by_HoGziwmhpMGqGeWZy\"> the result of such optimization. Fun Theory also highlights the flaws of any particular </span><span class=\"by_ScJE7nuW8ti5kzfcA\">religion'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s perfect afterlife - you </span><span class=\"by_ScJE7nuW8ti5kzfcA\">wouldn'</span><span class=\"by_HoGziwmhpMGqGeWZy\">t</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> want to </span><span class=\"by_HoGziwmhpMGqGeWZy\">go</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> to </span><span class=\"by_HoGziwmhpMGqGeWZy\">their Heaven.</span></span></p><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span></blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span><h2 id=\"The_argument_against_Enlightenment\"><span class=\"by_ScJE7nuW8ti5kzfcA\">The argument against Enlightenment</span></h2><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">Some critiques of </span><a href=\"https://www.lesswrong.com/tag/transhumanism\"><span class=\"by_ScJE7nuW8ti5kzfcA\">transhumanism</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\"> (and related fields such as cryonics or lifespan extension) suggest that human enhancement would be accompanied boredom and the end of fun as we know it. For example: \"if we self-improve human minds to extreme levels of intelligence, all challenges known today may bore us.\" Likewise, \"if superhumanly intelligent machines take care of our every need, it is apparent that no challenges nor fun will remain.\"</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">However, we can work towards determining whether and how the universe will offer, or whether we ourselves can create, ever more complex and sophisticated opportunities to delight, entertain and challenge ever more powerful and resourceful minds.</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><h2 id=\"The_concept_of_Utopia\"><span class=\"by_ScJE7nuW8ti5kzfcA\">The concept of Utopia</span></h2><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">Transhumanists are usually seen as working towards a better human future. This future is sometimes conceptualized, as George Orwell </span><a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\"><span class=\"by_ScJE7nuW8ti5kzfcA\">aptly describes it</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\">, as Utopia:</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><blockquote><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">\"It is a commonplace [view] that the Christian Heaven, as usually portrayed, would attract nobody. Almost all Christian writers dealing with Heaven either say frankly that it is indescribable or conjure up a vague picture of gold, precious stones, and the endless singing of hymns... [W]hat it could not do was to describe a condition in which the ordinary human being actively wanted to be.\"</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span></blockquote><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">Imagining this perfect future where every problem is solved and where there is constant peace and rest--as seen, a close parallel to several religious Heavens--rapidly leads to the conclusion that no one would actually want to live there.</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><h2 id=\"Complex_values_and_fun_theory_s_solution\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Complex values and fun theory's solution</span></h2><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">A key insight of fun theory, in its current embryonic form, is that </span><em><span class=\"by_ScJE7nuW8ti5kzfcA\">eudaimonia</span></em><span class=\"by_ScJE7nuW8ti5kzfcA\"> - the classical framework where happiness is the ultimate human goal - is </span><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_ScJE7nuW8ti5kzfcA\">complicated</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\">. That is, there are many properties which contribute to a life worth living. We humans require many things to experience a fulfilled life: Aesthetic stimulation, pleasure, love, social interaction, learning, challenge, and much more.</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">It is a common mistake in discussion of future society to extract only one element of the human preferences and advocate that it alone be maximized. This would neglect all other human values. For example, if we simply optimize for pleasure or happiness, </span><a href=\"https://www.lesswrong.com/tag/wireheading\"><span class=\"by_ScJE7nuW8ti5kzfcA\">\"wirehead\"</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\">, we'll stimulate the relevant parts of our brain and experience bliss for eternity, but pursue no other experiences. If almost </span><em><span class=\"by_ScJE7nuW8ti5kzfcA\">any</span></em><span class=\"by_ScJE7nuW8ti5kzfcA\"> element of our value system is absent, then the human future will likely be very unpleasant.</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">Enhanced humans are also seen to have the value system of humans today, but we may choose to change it as we self-enhance. We may want to alter our own value system, by eliminating values, like bloodlust, which on reflection we wish were absent. But there are many values which we, on reflection, want to keep, and since we humans have no basis for a value system other than our current value system, fun theory must seek to maximize the value system that we have, rather than inventing new values.</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><p><span class=\"by_ScJE7nuW8ti5kzfcA\">Fun theory thus seeks to let us keep our curiosity and love of learning intact, while preventing the extremes of boredom possible in a transhuman future if our strongly boosted intellects have exhausted all challenges. More broadly, fun theory seeks to allow humanity to enjoy life when all needs are easily satisfied and avoid the fall into the un-fun utopian futures in literature.</span></p><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><h2 id=\"External_links\"><span class=\"by_ScJE7nuW8ti5kzfcA\">External links</span></h2><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><ul><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><li><span class=\"by_ScJE7nuW8ti5kzfcA\">George Orwell, </span><a href=\"http://www.orwell.ru/library/articles/socialists/english/e_fun\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Why Socialists Don't Believe in Fun</span></a></li><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><li><span class=\"by_ScJE7nuW8ti5kzfcA\">David Pearce, </span><a href=\"http://paradise-engineering.com/\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Paradise Engineering</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\"> and </span><a href=\"http://www.hedweb.com/hedab.htm\"><span class=\"by_ScJE7nuW8ti5kzfcA\">The Hedonistic Imperative</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\"> (</span><a href=\"https://www.lesswrong.com/tag/abolitionism\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Abolitionism</span></a><span class=\"by_ScJE7nuW8ti5kzfcA\">) provides a more nuanced alternative to wireheading.</span></li><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span></ul><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><h2 id=\"See_also\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">See </span><span class=\"by_ScJE7nuW8ti5kzfcA\">also</span></span></h2><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><ul><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><li><a href=\"https://www.lesswrong.com/tag/the-fun-theory-sequence\"><span class=\"by_ScJE7nuW8ti5kzfcA\">The Fun Theory Sequence</span></a></li><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><li><a href=\"http://lesswrong.com/tag/happiness-1\"><span class=\"by_HoGziwmhpMGqGeWZy\">Happiness</span></a></li><span class=\"by_HoGziwmhpMGqGeWZy\">\n</span><li><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Complexity of value</span></a></li><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Metaethics sequence</span></a></li><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span><li><a href=\"https://www.lesswrong.com/tag/abolitionism\"><span class=\"by_ScJE7nuW8ti5kzfcA\">Abolitionism</span></a></li><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span></ul><span class=\"by_ScJE7nuW8ti5kzfcA\">\n</span>",
      "sections": [
        {
          "title": "The argument against Enlightenment",
          "anchor": "The_argument_against_Enlightenment",
          "level": 1
        },
        {
          "title": "The concept of Utopia",
          "anchor": "The_concept_of_Utopia",
          "level": 1
        },
        {
          "title": "Complex values and fun theory's solution",
          "anchor": "Complex_values_and_fun_theory_s_solution",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 41,
    "description": {
      "markdown": "**Fun Theory** is the field of knowledge studying how to design for fun in future society: it deals in questions such as \"How much fun is there in the universe?\", \"Will we ever run out of fun?\", \"Are we having fun yet?\" and \"Could we be having more fun?\"\n\nFrom [The Fun Theory Sequence](https://www.lesswrong.com/posts/K4aGvLnHvYgX9pZHS/the-fun-theory-sequence):\n\n> Many critics (including [George Orwell](https://www.lesswrong.com/lw/xl/eutopia_is_scary/)) have commented on the inability of authors to imagine Utopias where anyone would actually want to live. If no one can imagine a Future where anyone would want to live, that may drain off motivation to work on the project. The prospect of endless boredom is routinely fielded by conservatives as a knockdown argument against research on lifespan extension, against cryonics, against all transhumanism, and occasionally against the entire Enlightenment ideal of a better future.\n\n> Fun Theory is also the fully general reply to religious theodicy (attempts to justify why God permits evil). Our present world has flaws even from the standpoint of such eudaimonic considerations as freedom, personal responsibility, and self-reliance. Fun Theory tries to describe the dimensions along which a benevolently designed world can and should be optimized, and our present world is clearly _not_ the result of such optimization. Fun Theory also highlights the flaws of any particular religion's perfect afterlife - you wouldn't want to go to their Heaven.\n\nThe argument against Enlightenment\n----------------------------------\n\nSome critiques of [transhumanism](https://www.lesswrong.com/tag/transhumanism) (and related fields such as cryonics or lifespan extension) suggest that human enhancement would be accompanied boredom and the end of fun as we know it. For example: \"if we self-improve human minds to extreme levels of intelligence, all challenges known today may bore us.\" Likewise, \"if superhumanly intelligent machines take care of our every need, it is apparent that no challenges nor fun will remain.\"\n\nHowever, we can work towards determining whether and how the universe will offer, or whether we ourselves can create, ever more complex and sophisticated opportunities to delight, entertain and challenge ever more powerful and resourceful minds.\n\nThe concept of Utopia\n---------------------\n\nTranshumanists are usually seen as working towards a better human future. This future is sometimes conceptualized, as George Orwell [aptly describes it](http://www.orwell.ru/library/articles/socialists/english/e_fun), as Utopia:\n\n> \"It is a commonplace \\[view\\] that the Christian Heaven, as usually portrayed, would attract nobody. Almost all Christian writers dealing with Heaven either say frankly that it is indescribable or conjure up a vague picture of gold, precious stones, and the endless singing of hymns... \\[W\\]hat it could not do was to describe a condition in which the ordinary human being actively wanted to be.\"\n\nImagining this perfect future where every problem is solved and where there is constant peace and rest--as seen, a close parallel to several religious Heavens--rapidly leads to the conclusion that no one would actually want to live there.\n\nComplex values and fun theory's solution\n----------------------------------------\n\nA key insight of fun theory, in its current embryonic form, is that _eudaimonia_ \\- the classical framework where happiness is the ultimate human goal - is [complicated](https://www.lesswrong.com/tag/complexity-of-value). That is, there are many properties which contribute to a life worth living. We humans require many things to experience a fulfilled life: Aesthetic stimulation, pleasure, love, social interaction, learning, challenge, and much more.\n\nIt is a common mistake in discussion of future society to extract only one element of the human preferences and advocate that it alone be maximized. This would neglect all other human values. For example, if we simply optimize for pleasure or happiness, [\"wirehead\"](https://www.lesswrong.com/tag/wireheading), we'll stimulate the relevant parts of our brain and experience bliss for eternity, but pursue no other experiences. If almost _any_ element of our value system is absent, then the human future will likely be very unpleasant.\n\nEnhanced humans are also seen to have the value system of humans today, but we may choose to change it as we self-enhance. We may want to alter our own value system, by eliminating values, like bloodlust, which on reflection we wish were absent. But there are many values which we, on reflection, want to keep, and since we humans have no basis for a value system other than our current value system, fun theory must seek to maximize the value system that we have, rather than inventing new values.\n\nFun theory thus seeks to let us keep our curiosity and love of learning intact, while preventing the extremes of boredom possible in a transhuman future if our strongly boosted intellects have exhausted all challenges. More broadly, fun theory seeks to allow humanity to enjoy life when all needs are easily satisfied and avoid the fall into the un-fun utopian futures in literature.\n\nExternal links\n--------------\n\n*   George Orwell, [Why Socialists Don't Believe in Fun](http://www.orwell.ru/library/articles/socialists/english/e_fun)\n*   David Pearce, [Paradise Engineering](http://paradise-engineering.com/) and [The Hedonistic Imperative](http://www.hedweb.com/hedab.htm) ([Abolitionism](https://www.lesswrong.com/tag/abolitionism)) provides a more nuanced alternative to wireheading.\n\nSee also\n--------\n\n*   [The Fun Theory Sequence](https://www.lesswrong.com/tag/the-fun-theory-sequence)\n*   [Happiness](http://lesswrong.com/tag/happiness-1)\n*   [Complexity of value](https://www.lesswrong.com/tag/complexity-of-value)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qoTbWwaJtTSKosRCA",
    "name": "Taking Ideas Seriously",
    "core": false,
    "slug": "taking-ideas-seriously",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Taking Ideas Seriously </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is the skill/habit of noticing when a new idea should have major ramifications.&nbsp;</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">It has to do with decompartmentalization. By default some people think about one set of ideas in one compartmentalized domain (say, professional scientists thinking about the scientific method), and don't think to apply those ideas to other domains (such as their religion or politics). People with decompartmentalized beliefs update faster.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Taking a new idea seriously has the advantage of letting you act on it sooner. It can potentially have the disadvantage of making you vulnerable to convincing-but-wrong arguments, if you don't have the skills to evaluate them properly.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "**Taking Ideas Seriously** is the skill/habit of noticing when a new idea should have major ramifications. \n\nIt has to do with decompartmentalization. By default some people think about one set of ideas in one compartmentalized domain (say, professional scientists thinking about the scientific method), and don't think to apply those ideas to other domains (such as their religion or politics). People with decompartmentalized beliefs update faster.\n\nTaking a new idea seriously has the advantage of letting you act on it sooner. It can potentially have the disadvantage of making you vulnerable to convincing-but-wrong arguments, if you don't have the skills to evaluate them properly."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vcvfjGJwRmFbMMS3d",
    "name": "Principles",
    "core": false,
    "slug": "principles",
    "tableOfContents": {
      "html": "<p><i><span class=\"by_sKAL2jzfkYkDbQmx9\">“</span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Principles </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">are fundamental truths that serve as the foundations for behavior that gets you what you want out of life. They can be applied again and again in similar situations to help you achieve your goals.”</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> ― Ray Dalio, Principles: Life and Work.</span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">Principles, Heuristics, and rules of thumb are generalizations that aim to produce an optimal outcome relative to their cheapness as decision rules.&nbsp;</span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">Not using generalizations, Rules of thumb and Heuristics isn't possible. precise decision rules such as </span><a href=\"https://www.lesswrong.com/tag/bayes-theorem-bayesianism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bayes Theorem </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">are </span><a href=\"https://en.wikipedia.org/wiki/Combinatorial_explosion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">computationally intractable </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">even to computers, and surely aren't feasible for humans. the question become whether you have good and effective principles to guide you.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/chesterton-s-fence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Chesterton's Fence</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Conservation of Expected Evidence</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Occam's razor</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Goodhart's Law</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/more-dakka\"><span class=\"by_sKAL2jzfkYkDbQmx9\">More Dakka</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": {
      "markdown": "*“**Principles** are fundamental truths that serve as the foundations for behavior that gets you what you want out of life. They can be applied again and again in similar situations to help you achieve your goals.”* ― Ray Dalio, Principles: Life and Work.  \n  \nPrinciples, Heuristics, and rules of thumb are generalizations that aim to produce an optimal outcome relative to their cheapness as decision rules.   \n  \nNot using generalizations, Rules of thumb and Heuristics isn't possible. precise decision rules such as [Bayes Theorem](https://www.lesswrong.com/tag/bayes-theorem-bayesianism) are [computationally intractable](https://en.wikipedia.org/wiki/Combinatorial_explosion) even to computers, and surely aren't feasible for humans. the question become whether you have good and effective principles to guide you.\n\nSee also: [Chesterton's Fence](https://www.lesswrong.com/tag/chesterton-s-fence), [Conservation of Expected Evidence](https://www.lesswrong.com/tag/conservation-of-expected-evidence), [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law), [More Dakka](https://www.lesswrong.com/tag/more-dakka)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8KQvnMQYGaiCAqrXv",
    "name": "Mild Optimization",
    "core": false,
    "slug": "mild-optimization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pgi5MqvGrtvQozEH8\">Mild optimization </span></strong><span class=\"by_pgi5MqvGrtvQozEH8\">is an approach for mitigating </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_pgi5MqvGrtvQozEH8\">Goodhart's law</span></a><span class=\"by_pgi5MqvGrtvQozEH8\"> in AI alignment. Instead of maximizing a fixed objective, the hope is that the agent pursues the goal in a \"milder\" fashion.</span></p><p><span class=\"by_Sp5wM4aRAhNERd4oY\">Further reading: </span><a href=\"https://arbital.greaterwrong.com/p/soft_optimizer?l=2r8\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Arbital page on Mild Optimization</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "**Mild optimization** is an approach for mitigating [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law) in AI alignment. Instead of maximizing a fixed objective, the hope is that the agent pursues the goal in a \"milder\" fashion.\n\nFurther reading: [Arbital page on Mild Optimization](https://arbital.greaterwrong.com/p/soft_optimizer?l=2r8)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iNqgMuoewHKMhhXAp",
    "name": "Sports",
    "core": false,
    "slug": "sports",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Sports</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are games played with the human body, such as football, basketball, and baseball.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "**Sports** are games played with the human body, such as football, basketball, and baseball."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mQbxDKHxPcKKRG4mb",
    "name": "Changing Your Mind",
    "core": false,
    "slug": "changing-your-mind",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7ow6EFpypbH4hzFuz",
    "name": "Community Outreach",
    "core": false,
    "slug": "community-outreach",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 33,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "uL87Bw3TKzsYFMpZp",
    "name": "Teamwork",
    "core": false,
    "slug": "teamwork",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pJthrJDMw54JFGise",
    "name": "Murphyjitsu",
    "core": false,
    "slug": "murphyjitsu",
    "tableOfContents": {
      "html": "<blockquote><span class=\"by_HoGziwmhpMGqGeWZy\"> In the course of making plans, Murphyjitsu is the practice of strengthening plans by repeatedly envisioning and defending against failure modes until you would be </span><em><span class=\"by_HoGziwmhpMGqGeWZy\">shocked</span></em><span><span class=\"by_HoGziwmhpMGqGeWZy\"> to see it fail. </span><span class=\"by_Lr69WZKzx8po47y3j\">Here’</span><span class=\"by_HoGziwmhpMGqGeWZy\">s the basic setup of Murphyjitsu:</span></span></blockquote><blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">1. Make a plan.</span></blockquote><blockquote><span><span class=\"by_HoGziwmhpMGqGeWZy\">2. Imagine that </span><span class=\"by_Lr69WZKzx8po47y3j\">you’</span><span class=\"by_HoGziwmhpMGqGeWZy\">ve passed the deadline and find out that the plan failed.</span></span></blockquote><blockquote><span><span class=\"by_HoGziwmhpMGqGeWZy\">3. If </span><span class=\"by_Lr69WZKzx8po47y3j\">you’</span><span class=\"by_HoGziwmhpMGqGeWZy\">re </span></span><em><span class=\"by_HoGziwmhpMGqGeWZy\">shocked</span></em><span><span class=\"by_HoGziwmhpMGqGeWZy\"> in this scenario, </span><span class=\"by_Lr69WZKzx8po47y3j\">you’</span><span class=\"by_HoGziwmhpMGqGeWZy\">re done.</span></span></blockquote><blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">4. Otherwise, simulate the most likely failure mode, defend against it, and repeat.</span></blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">-alkjash, </span><a href=\"https://www.lesswrong.com/posts/N47M3JiHveHfwdbFg/hammertime-day-10-murphyjitsu\"><span class=\"by_HoGziwmhpMGqGeWZy\">Hammertime Day 10: Murphyjitsu</span></a></p><h1 id=\"See_also\"><span class=\"by_Lr69WZKzx8po47y3j\">See also</span></h1><ul><li><a href=\"https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter\"><span class=\"by_Lr69WZKzx8po47y3j\">Inner Simulator / Surprise-o-meter</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 8,
    "description": {
      "markdown": "> In the course of making plans, Murphyjitsu is the practice of strengthening plans by repeatedly envisioning and defending against failure modes until you would be _shocked_ to see it fail. Here’s the basic setup of Murphyjitsu:\n\n> 1\\. Make a plan.\n\n> 2\\. Imagine that you’ve passed the deadline and find out that the plan failed.\n\n> 3\\. If you’re _shocked_ in this scenario, you’re done.\n\n> 4\\. Otherwise, simulate the most likely failure mode, defend against it, and repeat.\n\n-alkjash, [Hammertime Day 10: Murphyjitsu](https://www.lesswrong.com/posts/N47M3JiHveHfwdbFg/hammertime-day-10-murphyjitsu)\n\nSee also\n========\n\n*   [Inner Simulator / Surprise-o-meter](https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HNJiR8Jzafsv8cHrC",
    "name": "Pascal's Mugging",
    "core": false,
    "slug": "pascal-s-mugging",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal'</span><span class=\"by_mPipmBTniuABY5PQy\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">mugging</span></span></strong><span><span class=\"by_mPipmBTniuABY5PQy\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">refers to</span><span class=\"by_mPipmBTniuABY5PQy\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">a</span><span class=\"by_mPipmBTniuABY5PQy\"> </span></span><a href=\"https://wiki.lesswrong.com/wiki/thought_experiment\"><span class=\"by_qgdGA4ZEyW7zNdK84\">thought experiment</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\"> in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">decision theory, a finite analogue</span><span class=\"by_nmk3nLpQE89dMRzzN\"> of </span></span><a href=\"https://en.wikipedia.org/wiki/Pascal's_wager\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal'</span><span class=\"by_9c2mQkLQq6gQSksMs\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wager</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><blockquote><p><i><span><span class=\"by_HoGziwmhpMGqGeWZy\">Suppose someone comes to me and says, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Give</span><span class=\"by_HoGziwmhpMGqGeWZy\"> me five dollars, or </span><span class=\"by_qgdGA4ZEyW7zNdK84\">I'</span><span class=\"by_HoGziwmhpMGqGeWZy\">ll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people. </span></span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">– </span><a href=\"https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal's Mugging: Tiny Probabilities of Vast Utilities</span></a></p></blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/counterfactual-mugging\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Counterfactual Mugging</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Shut up and multiply</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Expected Utility</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utilitarianism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/scope-insensitivity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scope Insensitivity</span></a></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Unpacking</span><span class=\"by_HoGziwmhpMGqGeWZy\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">theory behind Pascal's Mugging:</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A rational agent chooses those actions with outcomes that, after being weighted by their probabilities, have a greater </span><a href=\"https://www.lesswrong.com/tag/utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utility</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - in other words, those actions with greater </span><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">expected utility</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. If an agent's utilities over outcomes can potentially grow much faster than the probability of those outcomes diminishes, then it will be dominated by tiny probabilities of hugely important outcomes; speculations about low-probability-high-stakes scenarios will come to dominate its moral decision making.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A common method an agent could use to assign </span><a href=\"https://wiki.lesswrong.com/wiki/prior\"><span class=\"by_qgdGA4ZEyW7zNdK84\">prior</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> probabilities to outcomes is </span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, which gives a prior inversely proportional to the length of the outcome's description. Some outcomes can have a very short description but correspond to an event with enormous utility (i.e.: saving </span><a href=\"https://wiki.lesswrong.com/wiki/3%5E%5E%5E%5E3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">3^^^^3</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> lives), hence they would have non-negligible prior probabilities but a huge utility. The agent would always have to take those kinds of actions with far-fetched results, that have low but non-negligible probabilities but extremely high returns.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">This is seen as an unreasonable result. Intuitively, one</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is not </span><span class=\"by_qgdGA4ZEyW7zNdK84\">inclined </span><span class=\"by_HoGziwmhpMGqGeWZy\">to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">acquiesce to the mugger's demands - or even pay all that much attention one way or another -</span><span class=\"by_HoGziwmhpMGqGeWZy\"> but </span><span class=\"by_qgdGA4ZEyW7zNdK84\">what kind of prior does this imply?</span></span></p><p><a href=\"https://www.lesswrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> has suggested penalizing the prior probability of hypotheses which argue that we are in a </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">surprisingly unique</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> position to affect large numbers of other people who cannot symmetrically affect us. Since only one in 3^^^^3 people can be in a unique position to ordain the existence of at least 3^^^^3 other people who can't have a symmetrical effect on this one person, the prior probability would be penalized by a factor on the same order as the utility.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Peter de Blanc has proven</span><a href=\"http://arxiv.org/abs/0712.4318\"><span class=\"by_qgdGA4ZEyW7zNdK84\"> [1]</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> that if an agent assigns a finite probability to all computable hypotheses and assigns unboundedly large finite utilities over certain environment inputs, then the expected utility of any outcome</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">undefined. Peter de Blanc's paper, and</span><span class=\"by_HoGziwmhpMGqGeWZy\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal'</span><span class=\"by_nmk3nLpQE89dMRzzN\">s </span><span class=\"by_HoGziwmhpMGqGeWZy\">Mugging</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> argument, are sometimes misinterpreted as showing that any agent with an </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">unbounded finite utility function</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> over outcomes is not consistent, but this has yet to be demonstrated. The unreasonable result can also be seen as an argument against the use of Solomonoff induction for weighting prior probabilities.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">If an outcome with infinite utility is presented, then it doesn't matter how small its probability is: all actions which lead to that outcome will have to dominate the agent'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">behavior. This infinite case was stated by 17th century philosopher</span><span class=\"by_HoGziwmhpMGqGeWZy\"> Blaise Pascal</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> and named </span></span><a href=\"https://en.wikipedia.org/wiki/Pascal's_wager\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wager</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Many other abnormalities arise when dealing with </span><a href=\"https://www.lesswrong.com/tag/infinities-in-ethics\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">infinities</span><span class=\"by_HoGziwmhpMGqGeWZy\"> in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">ethics</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal's Mugging: Tiny Probabilities of Vast Utilities</span></a></li><li><a href=\"https://www.lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal's Muggle: Infinitesimal Priors and Strong Evidence</span></a></li></ul><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Peter de Blanc (2007). </span><a href=\"http://arxiv.org/abs/0712.4318\"><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Convergence of Expected Utilities with Algorithmic Probability Distributions</span></u></i></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Nick Bostrom (2009). \"Pascal's Mugging\". </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Analysis</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">69</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> (3): 443-445. (</span><a href=\"http://www.nickbostrom.com/papers/pascal.pdf\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">PDF</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></li></ol>",
      "sections": [
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 28,
    "description": {
      "markdown": "**Pascal's mugging** refers to a [thought experiment](https://wiki.lesswrong.com/wiki/thought_experiment) in decision theory, a finite analogue of [Pascal's wager](https://en.wikipedia.org/wiki/Pascal's_wager).\n\n> *Suppose someone comes to me and says, \"Give me five dollars, or I'll use my magic powers from outside the Matrix to run a Turing machine that simulates and kills 3^^^^3 people.* – [Pascal's Mugging: Tiny Probabilities of Vast Utilities](https://www.lesswrong.com/posts/a5JAiTdytou3Jg749/pascal-s-mugging-tiny-probabilities-of-vast-utilities)\n\n*See also*: [Decision theory](https://www.lesswrong.com/tag/decision-theory), [Counterfactual Mugging](https://www.lesswrong.com/tag/counterfactual-mugging), [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), [Expected Utility](https://www.lesswrong.com/tag/expected-utility), [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Scope Insensitivity](https://www.lesswrong.com/tag/scope-insensitivity)\n\nUnpacking the theory behind Pascal's Mugging:\n\nA rational agent chooses those actions with outcomes that, after being weighted by their probabilities, have a greater [utility](https://www.lesswrong.com/tag/utility) \\- in other words, those actions with greater [expected utility](https://www.lesswrong.com/tag/expected-utility). If an agent's utilities over outcomes can potentially grow much faster than the probability of those outcomes diminishes, then it will be dominated by tiny probabilities of hugely important outcomes; speculations about low-probability-high-stakes scenarios will come to dominate its moral decision making.\n\nA common method an agent could use to assign [prior](https://wiki.lesswrong.com/wiki/prior) probabilities to outcomes is [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), which gives a prior inversely proportional to the length of the outcome's description. Some outcomes can have a very short description but correspond to an event with enormous utility (i.e.: saving [3^^^^3](https://wiki.lesswrong.com/wiki/3%5E%5E%5E%5E3) lives), hence they would have non-negligible prior probabilities but a huge utility. The agent would always have to take those kinds of actions with far-fetched results, that have low but non-negligible probabilities but extremely high returns.\n\nThis is seen as an unreasonable result. Intuitively, one is not inclined to acquiesce to the mugger's demands - or even pay all that much attention one way or another - but what kind of prior does this imply?\n\n[Robin Hanson](https://www.lesswrong.com/tag/robin-hanson) has suggested penalizing the prior probability of hypotheses which argue that we are in a *surprisingly unique* position to affect large numbers of other people who cannot symmetrically affect us. Since only one in 3^^^^3 people can be in a unique position to ordain the existence of at least 3^^^^3 other people who can't have a symmetrical effect on this one person, the prior probability would be penalized by a factor on the same order as the utility.\n\nPeter de Blanc has proven [\\[1\\]](http://arxiv.org/abs/0712.4318) that if an agent assigns a finite probability to all computable hypotheses and assigns unboundedly large finite utilities over certain environment inputs, then the expected utility of any outcome is undefined. Peter de Blanc's paper, and the Pascal's Mugging argument, are sometimes misinterpreted as showing that any agent with an *unbounded finite utility function* over outcomes is not consistent, but this has yet to be demonstrated. The unreasonable result can also be seen as an argument against the use of Solomonoff induction for weighting prior probabilities.\n\nIf an outcome with infinite utility is presented, then it doesn't matter how small its probability is: all actions which lead to that outcome will have to dominate the agent's behavior. This infinite case was stated by 17th century philosopher Blaise Pascal and named [Pascal's wager](https://en.wikipedia.org/wiki/Pascal's_wager). Many other abnormalities arise when dealing with [infinities in ethics](https://www.lesswrong.com/tag/infinities-in-ethics).\n\nNotable Posts\n-------------\n\n*   [Pascal's Mugging: Tiny Probabilities of Vast Utilities](https://www.lesswrong.com/lw/kd/pascals_mugging_tiny_probabilities_of_vast/)\n*   [Pascal's Muggle: Infinitesimal Priors and Strong Evidence](https://www.lesswrong.com/lw/h8k/pascals_muggle_infinitesimal_priors_and_strong/)\n\nReferences\n----------\n\n1.  Peter de Blanc (2007). [*Convergence of Expected Utilities with Algorithmic Probability Distributions*](http://arxiv.org/abs/0712.4318).\n2.  Nick Bostrom (2009). \"Pascal's Mugging\". *Analysis* **69** (3): 443-445. ([PDF](http://www.nickbostrom.com/papers/pascal.pdf))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JHYaBGQuuKHdwnrAK",
    "name": "Logical Uncertainty",
    "core": false,
    "slug": "logical-uncertainty",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Logical </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Uncertainty</span></span></strong><span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is probabilistic </span><span class=\"by_5yNJS8bxEYhgFD9XJ\">uncertainty </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">about</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">implications</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> of </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">beliefs. (Another way of thinking about it is: uncertainty about computations.) Probability theory typically assumes </span></span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">logical omniscience, </span></strong><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">IE, perfect knowledge of logic. The easiest way</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> to </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">see</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">importance</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> of this </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">assumption</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> is </span><span class=\"by_aqNJvZZDZc3QAamJD\">to</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> consider Bayesian reasoning:</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> to </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">evaluate</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the probability of </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">evidence</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> given </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">a hypothesis,&nbsp;</span></span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"P(e|h)\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">e</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">h</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">)</span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">, it's necessary to know what</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">implications</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> of the </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">hypothesis are. However, realistic agents cannot be logically omniscient.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also:</span><a href=\"https://www.lesswrong.com/tag/logical-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\"> Logical Induction</span></a></p><h2 id=\"Motivation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivation</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Is the googolth digit of pi odd? The probability that it is odd is, intuitively, 0.5. Yet we know that this is definitely true or false by the rules of logic, even though we don't know which. Formalizing this sort of probability is the primary goal of the field of logical uncertainty.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The problem with the 0.5 probability is that it gives non-zero probability to false statements. If I am asked to bet on whether the googolth digit of pi is odd, I can reason as follows: There is 0.5 chance that it is odd. Let P represent the actual, unknown, parity of the googolth digit (odd or even); and let Q represent the other parity. If Q, then anything follows. (By the Principle of Explosion, a false statement implies anything.) For example, Q implies that I will win $1 billion. Therefore the value of this bet is at least $500,000,000, which is 0.5 * $1,000,000, and I should be willing to pay that much to take the bet. This is an absurdity. Only expenditure of finite computational power stands between the uncertainty and 100% certainty.</span></p><h2 id=\"Logical_Uncertainty___Counterfactuals\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Logical Uncertainty &amp; Counterfactuals</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Logical uncertainty is closely related to the problem of </span><a href=\"/tag/counterfactuals\"><span class=\"by_qgdGA4ZEyW7zNdK84\">counterfactuals</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Ordinary probability theory relies on counterfactuals. For example, I see a coin that came up heads, and I say that the probability of tails was 0.5, even though clearly, given all air currents and muscular movements involved in throwing that coin, the probability of tails was 0.0. Yet we can imagine this possible impossible world where the coin came up tails. In the case of logical uncertainly, it is hard to imagine a world in which mathematical facts are different.</span></p><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ul><li><a href=\"https://intelligence.org/files/QuestionsLogicalUncertainty.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Questions of Reasoning Under Logical Uncertainty</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Nate Soares and Benja Fallenstein.</span></li></ul>",
      "sections": [
        {
          "title": "Motivation",
          "anchor": "Motivation",
          "level": 1
        },
        {
          "title": "Logical Uncertainty & Counterfactuals",
          "anchor": "Logical_Uncertainty___Counterfactuals",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 48,
    "description": {
      "markdown": "**Logical Uncertainty** is probabilistic uncertainty about the implications of beliefs. (Another way of thinking about it is: uncertainty about computations.) Probability theory typically assumes **logical omniscience,** IE, perfect knowledge of logic. The easiest way to see the importance of this assumption is to consider Bayesian reasoning: to evaluate the probability of evidence given a hypothesis, \\\\(P(e|h)\\\\), it's necessary to know what the implications of the hypothesis are. However, realistic agents cannot be logically omniscient.\n\nSee Also: [Logical Induction](https://www.lesswrong.com/tag/logical-induction)\n\nMotivation\n----------\n\nIs the googolth digit of pi odd? The probability that it is odd is, intuitively, 0.5. Yet we know that this is definitely true or false by the rules of logic, even though we don't know which. Formalizing this sort of probability is the primary goal of the field of logical uncertainty.\n\nThe problem with the 0.5 probability is that it gives non-zero probability to false statements. If I am asked to bet on whether the googolth digit of pi is odd, I can reason as follows: There is 0.5 chance that it is odd. Let P represent the actual, unknown, parity of the googolth digit (odd or even); and let Q represent the other parity. If Q, then anything follows. (By the Principle of Explosion, a false statement implies anything.) For example, Q implies that I will win $1 billion. Therefore the value of this bet is at least $500,000,000, which is 0.5 * $1,000,000, and I should be willing to pay that much to take the bet. This is an absurdity. Only expenditure of finite computational power stands between the uncertainty and 100% certainty.\n\nLogical Uncertainty & Counterfactuals\n-------------------------------------\n\nLogical uncertainty is closely related to the problem of [counterfactuals](/tag/counterfactuals). Ordinary probability theory relies on counterfactuals. For example, I see a coin that came up heads, and I say that the probability of tails was 0.5, even though clearly, given all air currents and muscular movements involved in throwing that coin, the probability of tails was 0.0. Yet we can imagine this possible impossible world where the coin came up tails. In the case of logical uncertainly, it is hard to imagine a world in which mathematical facts are different.\n\nReferences\n----------\n\n*   [Questions of Reasoning Under Logical Uncertainty](https://intelligence.org/files/QuestionsLogicalUncertainty.pdf) by Nate Soares and Benja Fallenstein."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Jzm2mYuuDBCNWq8hi",
    "name": "Happiness",
    "core": false,
    "slug": "happiness-1",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">Posts about </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Happiness</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">. one of the tricky things about Happiness is that sometimes directly pursuing it doesn't work or is even counterproductive. Happiness isn't (and can't be) </span><a href=\"https://www.lesswrong.com/posts/synsRtBKDeAFuo7e3/not-for-the-sake-of-happiness-alone\"><span class=\"by_sKAL2jzfkYkDbQmx9\">the only important thing</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, but is non the less important. Thus LessWrong dealt a lot with the questions about happiness and how to pursue it.</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">“Happiness is a choice. If you’re so smart, how come you aren’t happy? How come you haven’t figured that out? That’s my challenge to all the people who think they’re so smart and so capable.” - Naval Ravikant</span></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/gratitude\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gratitude</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/well-being\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Well-being</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 41,
    "description": {
      "markdown": "Posts about **Happiness**. one of the tricky things about Happiness is that sometimes directly pursuing it doesn't work or is even counterproductive. Happiness isn't (and can't be) [the only important thing](https://www.lesswrong.com/posts/synsRtBKDeAFuo7e3/not-for-the-sake-of-happiness-alone), but is non the less important. Thus LessWrong dealt a lot with the questions about happiness and how to pursue it.\n\n> “Happiness is a choice. If you’re so smart, how come you aren’t happy? How come you haven’t figured that out? That’s my challenge to all the people who think they’re so smart and so capable.” - Naval Ravikant\n\nSee also: [Gratitude](https://www.lesswrong.com/tag/gratitude), [Well-being](https://www.lesswrong.com/tag/well-being)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oraLTPkETL5xKmhx3",
    "name": "Moderation (topic)",
    "core": false,
    "slug": "moderation-topic",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Moderation</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">, in online communities especially, deals with what decision and limitation should be made in order for that community to thrive.&nbsp;</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">For posts regarding LW moderation policies and decisions (such as ban notices) see </span><a href=\"https://www.lesswrong.com/tag/lw-moderation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LW Moderation</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "**Moderation**, in online communities especially, deals with what decision and limitation should be made in order for that community to thrive. \n\nFor posts regarding LW moderation policies and decisions (such as ban notices) see [LW Moderation](https://www.lesswrong.com/tag/lw-moderation)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SW3euSNqpozcsxXaX",
    "name": "Litanies & Mantras",
    "core": false,
    "slug": "litanies-and-mantras",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></p><ul><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Litany of Tarski</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-gendlin\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Litany of Gendlin</span></a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-occam\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Litany of Occam</span></a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-jai\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Litany of Jai</span></a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-hodgell\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Litany of Hodgell</span></a></li><li><a href=\"https://www.lesswrong.com/tag/adding-up-to-normality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Adding Up to Normality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/reality-is-normal\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Reality Is Normal</span></a></li><li><a href=\"https://www.lesswrong.com/tag/virtues\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Virtues</span></a></li><li><a href=\"https://www.lesswrong.com/tag/principles\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Principles</span></a></li><li><a href=\"https://www.lesswrong.com/tag/taking-ideas-seriously\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Taking Ideas Seriously</span></a></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "**Related Pages:** \n\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski) \n*   [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)\n*   [Litany of Occam](https://www.lesswrong.com/tag/litany-of-occam)\n*   [Litany of Jai](https://www.lesswrong.com/tag/litany-of-jai)\n*   [Litany of Hodgell](https://www.lesswrong.com/tag/litany-of-hodgell)\n*   [Adding Up to Normality](https://www.lesswrong.com/tag/adding-up-to-normality)\n*   [Reality Is Normal](https://www.lesswrong.com/tag/reality-is-normal)\n*   [Virtues](https://www.lesswrong.com/tag/virtues)\n*   [Principles](https://www.lesswrong.com/tag/principles)\n*   [Taking Ideas Seriously](https://www.lesswrong.com/tag/taking-ideas-seriously)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RqNHf9ZpPW29uMKyA",
    "name": "Demon Threads",
    "core": false,
    "slug": "demon-threads",
    "tableOfContents": {
      "html": "<blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">A </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Demon Thread</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a discussion where everything is subtly warping towards aggression and confusion (i.e. as if people are under demonic influence), even if people are well intentioned and on the same 'side.' You can see a demon thread coming in advance, but it's still hard to do anything about. </span></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "> A **Demon Thread** is a discussion where everything is subtly warping towards aggression and confusion (i.e. as if people are under demonic influence), even if people are well intentioned and on the same 'side.' You can see a demon thread coming in advance, but it's still hard to do anything about."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hGzywXvWhSdJi5F2a",
    "name": "LW Moderation",
    "core": false,
    "slug": "lw-moderation",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">LessWrong Moderation</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> posts deal with how the site moderators comments and posts. It includes laying out policies, decisions about who's on the mod team, and concrete moderation decisions.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">For general posts on the </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">topic</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> of moderation, see </span><a href=\"https://www.lesswrong.com/tag/moderation-topic\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Moderation (topic)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": {
      "markdown": "**LessWrong Moderation** posts deal with how the site moderators comments and posts. It includes laying out policies, decisions about who's on the mod team, and concrete moderation decisions.\n\nFor general posts on the *topic* of moderation, see [Moderation (topic)](https://www.lesswrong.com/tag/moderation-topic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zCyW4sAD7gkmAT38f",
    "name": "Comfort Zone Expansion (CoZE)",
    "core": false,
    "slug": "comfort-zone-expansion-coze",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ntahi2tr7e9DjCYdu",
    "name": "Chesterton's Fence",
    "core": false,
    "slug": "chesterton-s-fence",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">Chesterton'</span><span class=\"by_qmJFRN7jitjPsuF3f\">s </span><span class=\"by_HoGziwmhpMGqGeWZy\">Fence</span></span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><span class=\"by_qmJFRN7jitjPsuF3f\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the</span><span class=\"by_qmJFRN7jitjPsuF3f\"> principle that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">reforms</span><span class=\"by_qmJFRN7jitjPsuF3f\"> should not </span><span class=\"by_qgdGA4ZEyW7zNdK84\">be made until the reasoning behind the existing state</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">affairs is understood. </span></span></p><p><em><span class=\"by_qgdGA4ZEyW7zNdK84\">Related</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/epistemic-modesty\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic Modesty</span></a></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">From Chesterton’s 1929 book, The Thing,</span><span class=\"by_qmJFRN7jitjPsuF3f\"> in the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">chapter entitled </span></span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">The Drift from Domesticity </span></em><span><span class=\"by_qgdGA4ZEyW7zNdK84\">[1]</span><span class=\"by_qmJFRN7jitjPsuF3f\">:</span></span></p><blockquote><span><span class=\"by_qmJFRN7jitjPsuF3f\">In the matter of reforming things, as distinct from deforming them, there is one plain and simple principle; a principle which will probably be called a paradox. There exists in such a case a certain institution or law; let us say, for the sake of simplicity, a fence or gate erected across a road. The more modern type of reformer goes gaily up to it and says, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“</span><span class=\"by_HoGziwmhpMGqGeWZy\">I don</span><span class=\"by_qgdGA4ZEyW7zNdK84\">’</span><span class=\"by_qmJFRN7jitjPsuF3f\">t see the use of this; let us clear it away.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span><span class=\"by_qmJFRN7jitjPsuF3f\"> To which the more intelligent type of reformer will do well to answer: </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“</span><span class=\"by_HoGziwmhpMGqGeWZy\">If</span><span class=\"by_qmJFRN7jitjPsuF3f\"> you </span><span class=\"by_HoGziwmhpMGqGeWZy\">don</span><span class=\"by_qgdGA4ZEyW7zNdK84\">’</span><span class=\"by_qmJFRN7jitjPsuF3f\">t see the use of it, I certainly </span><span class=\"by_HoGziwmhpMGqGeWZy\">won</span><span class=\"by_qgdGA4ZEyW7zNdK84\">’</span><span class=\"by_qmJFRN7jitjPsuF3f\">t let you clear it away. Go away and think. Then, when you can come back and tell me that you do see the use of it, I may allow you to destroy it. </span></span></blockquote><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://en.wikipedia.org/wiki/Wikipedia:Chesterton's_fence\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Wikipedia: Chesterton</span><span class=\"by_HoGziwmhpMGqGeWZy\">'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s Fence</span></span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 12,
    "description": {
      "markdown": "**Chesterton's Fence** is the principle that reforms should not be made until the reasoning behind the existing state of affairs is understood.\n\n_Related_: [Epistemic Modesty](https://www.lesswrong.com/tag/epistemic-modesty)\n\nFrom Chesterton’s 1929 book, The Thing, in the chapter entitled _The Drift from Domesticity_ \\[1\\]:\n\n> In the matter of reforming things, as distinct from deforming them, there is one plain and simple principle; a principle which will probably be called a paradox. There exists in such a case a certain institution or law; let us say, for the sake of simplicity, a fence or gate erected across a road. The more modern type of reformer goes gaily up to it and says, “I don’t see the use of this; let us clear it away.” To which the more intelligent type of reformer will do well to answer: “If you don’t see the use of it, I certainly won’t let you clear it away. Go away and think. Then, when you can come back and tell me that you do see the use of it, I may allow you to destroy it.\n\nSee also\n--------\n\n*   [Wikipedia: Chesterton's Fence](http://en.wikipedia.org/wiki/Wikipedia:Chesterton's_fence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cNLSTJzaY6Gi3iYQr",
    "name": "Five minute timers",
    "core": false,
    "slug": "five-minute-timers",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Five minute timers. </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">Sometimes, all you need to solve an impossible problem is to actually think for 5 minutes. (Sometimes it is actually 15 minutes, or two hours. But, the first five minutes is helpful for transforming it from an inpenetrable ugh field to a tractable problem)</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Sometimes called Yoda timers. (A timebox wherein you Actually Try)</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "**Five minute timers.** Sometimes, all you need to solve an impossible problem is to actually think for 5 minutes. (Sometimes it is actually 15 minutes, or two hours. But, the first five minutes is helpful for transforming it from an inpenetrable ugh field to a tractable problem)\n\nSometimes called Yoda timers. (A timebox wherein you Actually Try)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7dpiuzLNPMtdjAvNq",
    "name": "Summoning Sapience",
    "core": false,
    "slug": "summoning-sapience",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Summoning Sapience</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is the act of noticing that you are in a situation where it'd be beneficial to </span><i><span class=\"by_r38pkCm7wF4M44MDQ\">actually think</span></i><span class=\"by_r38pkCm7wF4M44MDQ\">, rather than running on autopilot.&nbsp;</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">What \"actually thinking\" means depends on context, but often includes paying extra attention to details of your environment, thinking through the possible ramifications of those details, making a plan, checking if that plan will work, etc.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Summon Sapience is a Rationality skill. Related to Murphijitsu.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 4,
    "description": {
      "markdown": "**Summoning Sapience** is the act of noticing that you are in a situation where it'd be beneficial to *actually think*, rather than running on autopilot. \n\nWhat \"actually thinking\" means depends on context, but often includes paying extra attention to details of your environment, thinking through the possible ramifications of those details, making a plan, checking if that plan will work, etc.\n\nSummon Sapience is a Rationality skill. Related to Murphijitsu."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DQHWBcKeiLnyh9za9",
    "name": "Events (Community)",
    "core": false,
    "slug": "events-community",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">Announcements of planned real-time gatherings, either online or in person.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 143,
    "description": {
      "markdown": "Announcements of planned real-time gatherings, either online or in person."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pnSXfWXbQihrFadeD",
    "name": "Case Study",
    "core": false,
    "slug": "case-study",
    "tableOfContents": {
      "html": "<p><span class=\"by_L4j57Ah7zd637c6c8\">A post looking at a particular idea through a real-world example or examples.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "A post looking at a particular idea through a real-world example or examples."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jZF2jwLnPKBv6m3Ag",
    "name": "Organization Updates",
    "core": false,
    "slug": "organization-updates",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pbREHuM5F5t5nyWqh\">Organization updates</span></strong><span><span class=\"by_pbREHuM5F5t5nyWqh\"> are what you might expect - updates</span><span class=\"by_L4j57Ah7zd637c6c8\"> relating to a specific group or organization.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": {
      "markdown": "**Organization updates** are what you might expect - updates relating to a specific group or organization."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MAp6Ft8b3s7kJdrQ9",
    "name": "Selection Effects",
    "core": false,
    "slug": "selection-effects",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Y3oHd7CQpy8aQFWD9",
    "name": "Behavior Change",
    "core": false,
    "slug": "behavior-change",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 3,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "32DdRimdM7sB5wmKu",
    "name": "Empiricism",
    "core": false,
    "slug": "empiricism",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">\"The sixth virtue is </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">empiricism</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes”, and one saying “No”, the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.” Do not be blinded by words. When words are subtracted, anticipation remains.\" - </span><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">(Twelve Virtues of Rationality)</span></a></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 32,
    "description": {
      "markdown": "> \"The sixth virtue is **empiricism**. The roots of knowledge are in observation and its fruit is prediction. What tree grows without roots? What tree nourishes us without fruit? If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.” Though they argue, one saying “Yes”, and one saying “No”, the two do not anticipate any different experience of the forest. Do not ask which beliefs to profess, but which experiences to anticipate. Always know which difference of experience you argue about. Do not let the argument wander and become about something else, such as someone’s virtue as a rationalist. Jerry Cleaver said: “What does you in is not failure to apply some high-level, intricate, complicated technique. It’s overlooking the basics. Not keeping your eye on the ball.” Do not be blinded by words. When words are subtracted, anticipation remains.\" - [(Twelve Virtues of Rationality)](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gsv9XWbZDcnZmKuqM",
    "name": "Psychiatry",
    "core": false,
    "slug": "psychiatry",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_me9AorpaKpuWPmz6m\">Psychiatry </span></strong><span class=\"by_me9AorpaKpuWPmz6m\">is the medical specialty devoted to the diagnosis, prevention, and treatment of mental disorders. These include various maladaptations related to mood, behavior, cognition, and perceptions.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 25,
    "description": {
      "markdown": "**Psychiatry** is the medical specialty devoted to the diagnosis, prevention, and treatment of mental disorders. These include various maladaptations related to mood, behavior, cognition, and perceptions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "a65Lgr7Q5jqRWHtM6",
    "name": "Gratitude",
    "core": false,
    "slug": "gratitude",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Gratitude </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is the the feeling or showing of appreciation. practices that aim to increase how much one is expressing gratitude (such as </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Gratitude Journaling</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">) seem to have one of the highest correlation with actually increasing happiness and life satisfaction.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "**Gratitude** is the the feeling or showing of appreciation. practices that aim to increase how much one is expressing gratitude (such as **Gratitude Journaling**) seem to have one of the highest correlation with actually increasing happiness and life satisfaction."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "A4kr45wS7fBW5PBpf",
    "name": "Ideological Turing Tests",
    "core": false,
    "slug": "ideological-turing-tests",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Ideological Turing Test </span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">is an exercise where you try to pretend to hold an opposing ideology convincingly enough that outside observers can't reliably distinguish you from a true believer.</span></p><p><a href=\"https://www.econlib.org/archives/2011/06/the_ideological.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">It was first described by economist Bryan Caplan:</span></a></p><blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">Put me and five random liberal social science Ph.D.s in a chat room. Let liberal readers ask questions for an hour, then vote on who isn't really a liberal. Then put [economist Paul] Krugman and five random libertarian social science Ph.D.s in a chat room. Let libertarian readers ask questions for an hour, then vote on who isn't really a libertarian. Simple as that. </span></blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Passing the ideological Turing test is a sign that you understand the opposing ideology on a deep level.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The ideological Turing test has a similar motivation to </span><a href=\"https://www.lesswrong.com/tag/steelmanning\"><span class=\"by_HoGziwmhpMGqGeWZy\">Steelmanning</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, but works in a different way.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The name comes from the </span><a href=\"https://en.wikipedia.org/wiki/Turing_test\"><span class=\"by_HoGziwmhpMGqGeWZy\">Turing Test</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> proposed by computer scientist Alan Turing, where a computer program must pretend to be a human while human judges try to tell it apart from real humans.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "The **Ideological Turing Test** is an exercise where you try to pretend to hold an opposing ideology convincingly enough that outside observers can't reliably distinguish you from a true believer.\n\n[It was first described by economist Bryan Caplan:](https://www.econlib.org/archives/2011/06/the_ideological.html)\n\n> Put me and five random liberal social science Ph.D.s in a chat room. Let liberal readers ask questions for an hour, then vote on who isn't really a liberal. Then put \\[economist Paul\\] Krugman and five random libertarian social science Ph.D.s in a chat room. Let libertarian readers ask questions for an hour, then vote on who isn't really a libertarian. Simple as that.\n\nPassing the ideological Turing test is a sign that you understand the opposing ideology on a deep level.\n\nThe ideological Turing test has a similar motivation to [Steelmanning](https://www.lesswrong.com/tag/steelmanning), but works in a different way.\n\nThe name comes from the [Turing Test](https://en.wikipedia.org/wiki/Turing_test) proposed by computer scientist Alan Turing, where a computer program must pretend to be a human while human judges try to tell it apart from real humans."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WqLn4pAWi5hn6McHQ",
    "name": "Self Improvement",
    "core": false,
    "slug": "self-improvement",
    "tableOfContents": {
      "html": "<p><span class=\"by_SsduPgHwY2zeZpmKT\">Personal Growth, etc</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/growth-stories\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Growth Stories</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 83,
    "description": {
      "markdown": "Personal Growth, etc\n\n**Related Pages:** [Growth Stories](https://www.lesswrong.com/tag/growth-stories)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2JdCpTrNgBMNpJiyB",
    "name": "Space Exploration & Colonization",
    "core": false,
    "slug": "space-exploration-and-colonization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Space Exploration &amp; Colonization</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> involves humans or human-built machines traveling to other planets, star systems, or galaxies.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Possible motivations include curiosity, reducing the chance of </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">existential risks</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, and expanding human civilization.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Possible techniques include both present or near-future technology and </span><a href=\"https://www.lesswrong.com/tag/futurism\"><span class=\"by_HoGziwmhpMGqGeWZy\">speculative future technology</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">For the question of why alien civilizations </span><span class=\"by_sKAL2jzfkYkDbQmx9\">haven'</span><span class=\"by_HoGziwmhpMGqGeWZy\">t space-colonized us, see </span></span><a href=\"https://www.lesswrong.com/tag/great-filter\"><span class=\"by_HoGziwmhpMGqGeWZy\">Great Filter</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/96XzQgTL2HBNkBwL4\"><span class=\"by_sKAL2jzfkYkDbQmx9\">So You Want To Colonize The Universe</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 43,
    "description": {
      "markdown": "**Space Exploration & Colonization** involves humans or human-built machines traveling to other planets, star systems, or galaxies.\n\nPossible motivations include curiosity, reducing the chance of [existential risks](https://www.lesswrong.com/tag/existential-risk), and expanding human civilization.\n\nPossible techniques include both present or near-future technology and [speculative future technology](https://www.lesswrong.com/tag/futurism).\n\nFor the question of why alien civilizations haven't space-colonized us, see [Great Filter](https://www.lesswrong.com/tag/great-filter).\n\n**Related Sequences:** [So You Want To Colonize The Universe](https://www.lesswrong.com/s/96XzQgTL2HBNkBwL4)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8sh6iLwYWDJ7z3fPo",
    "name": "Startups",
    "core": false,
    "slug": "startups",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Startups</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> are small, recently-founded companies that are looking</span><span class=\"by_6jLdWqegNefgaabhr\"> to </span><span class=\"by_HoGziwmhpMGqGeWZy\">grow much larger. They generally seek funding through venture capital investors.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 41,
    "description": {
      "markdown": "**Startups** are small, recently-founded companies that are looking to grow much larger. They generally seek funding through venture capital investors."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Lgy35Xh222bwgeGTL",
    "name": "Government",
    "core": false,
    "slug": "government",
    "tableOfContents": {
      "html": "<p><span class=\"by_p8SHJFHRgZeMuw7qk\">Governments are institutions set up by societies to create and enforce laws and rules of conduct.</span></p><p><span><span class=\"by_p8SHJFHRgZeMuw7qk\">Modern western philosophy usually analyzes governments through some variant of social contract theory, which was developed by John Locke and Thomas Hobbes among others and seeks to describe government as a contractual agreement among the people to help create a world that is better than the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"state</span><span class=\"by_p8SHJFHRgZeMuw7qk\"> of nature.</span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span></span></p><p><span><span class=\"by_p8SHJFHRgZeMuw7qk\">Since then, political philosophers have developed a myriad of different systems for thinking about government, from the Marxist viewpoint of government as a class construct of oppression, to the Keynesian view of government as an important economic </span><span class=\"by_sKAL2jzfkYkDbQmx9\">regulator, to Max Weber's view that the government is defined by its </span></span><a href=\"https://en.wikipedia.org/wiki/Monopoly_on_violence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Monopoly on Violence</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">Rationalists typically see government as a means to a greater utilitarian end. In a democracy, the government can act as the hand that directly enacts and restricts actions in accordance with popular opinion. Obviously, there are many ways this can go wrong.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/law-and-legal-systems\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Law and Legal systems</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/politics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Politics</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/mechanism-design\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mechanism Design</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/voting-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Voting Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/selectorate-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Selectorate Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/futarchy\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Futarchy</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/coordination-cooperation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Coordination / Cooperation</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 55,
    "description": {
      "markdown": "Governments are institutions set up by societies to create and enforce laws and rules of conduct.\n\nModern western philosophy usually analyzes governments through some variant of social contract theory, which was developed by John Locke and Thomas Hobbes among others and seeks to describe government as a contractual agreement among the people to help create a world that is better than the \"state of nature.\"\n\nSince then, political philosophers have developed a myriad of different systems for thinking about government, from the Marxist viewpoint of government as a class construct of oppression, to the Keynesian view of government as an important economic regulator, to Max Weber's view that the government is defined by its [Monopoly on Violence](https://en.wikipedia.org/wiki/Monopoly_on_violence).\n\nRationalists typically see government as a means to a greater utilitarian end. In a democracy, the government can act as the hand that directly enacts and restricts actions in accordance with popular opinion. Obviously, there are many ways this can go wrong.\n\n**Related Pages:** [Law and Legal systems](https://www.lesswrong.com/tag/law-and-legal-systems), [Politics](https://www.lesswrong.com/tag/politics), [Mechanism Design](https://www.lesswrong.com/tag/mechanism-design), [Voting Theory](https://www.lesswrong.com/tag/voting-theory), [Selectorate Theory](https://www.lesswrong.com/tag/selectorate-theory), [Futarchy](https://www.lesswrong.com/tag/futarchy), [Coordination / Cooperation](https://www.lesswrong.com/tag/coordination-cooperation)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dBPou4ihoQNY4cquv",
    "name": "Psychology",
    "core": false,
    "slug": "psychology",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_p8SHJFHRgZeMuw7qk\">Psychology</span></strong><span class=\"by_p8SHJFHRgZeMuw7qk\"> is the study of the mind. Typically, psychology focuses on the human mind, but animal minds are also sometimes studied.</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">Just like much of the research on animal psychology also applies to humans, research on human psychology may inform how we develop and use AI.</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">Modern psychology began with the Freudian model of psychoanalysis, which quickly supplanted the prevailing pseudosciences like phrenology. Psychoanalysis is now largely obsolete, and has been replaced by other schools. The most prominent among these include:</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">Behavioral Psychology, which focuses on observable learning and behavior.</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">Humanistic Psychology, which focuses on theories of self-actualization and self-help.</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">Cognitive Psychology, which uses modern research on brain anatomy and other subjects to create theories about how the brain learns, develops, and makes decisions.</span></p><p><span class=\"by_p8SHJFHRgZeMuw7qk\">From an academic standpoint, much of the content of Lesswrong can be described as trying to implement the results of Decision Theory to people's daily actions using knowledge of human psychology. Sometimes, this involves overriding our native tendencies. Other times, it involves using them to our advantage.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 117,
    "description": {
      "markdown": "**Psychology** is the study of the mind. Typically, psychology focuses on the human mind, but animal minds are also sometimes studied.\n\nJust like much of the research on animal psychology also applies to humans, research on human psychology may inform how we develop and use AI.\n\nModern psychology began with the Freudian model of psychoanalysis, which quickly supplanted the prevailing pseudosciences like phrenology. Psychoanalysis is now largely obsolete, and has been replaced by other schools. The most prominent among these include:\n\nBehavioral Psychology, which focuses on observable learning and behavior.\n\nHumanistic Psychology, which focuses on theories of self-actualization and self-help.\n\nCognitive Psychology, which uses modern research on brain anatomy and other subjects to create theories about how the brain learns, develops, and makes decisions.\n\nFrom an academic standpoint, much of the content of Lesswrong can be described as trying to implement the results of Decision Theory to people's daily actions using knowledge of human psychology. Sometimes, this involves overriding our native tendencies. Other times, it involves using them to our advantage."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "z95PGFXtPpwakqkTA",
    "name": "Intuition",
    "core": false,
    "slug": "intuition",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Intuition</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the ability to acquire knowledge without recourse to conscious reasoning. Also relevant are </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Intuition Pumps</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">, a thought experiment, a model, or anything else, that's structured to allow the thinker to use their intuition to develop an answer to a problem.&nbsp;</span></p><p><i><span><span class=\"by_XtphY3uYHwruKqDyG\">(Modified from</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> Wikipedia </span></span></i><a href=\"https://en.wikipedia.org/wiki/Intuition\"><i><span class=\"by_sKAL2jzfkYkDbQmx9\">1</span></i></a><i><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span></i><a href=\"https://en.wikipedia.org/wiki/Intuition_pump\"><i><span class=\"by_sKAL2jzfkYkDbQmx9\">2</span></i></a><i><span class=\"by_XtphY3uYHwruKqDyG\">, this section is licensed as </span></i><a href=\"https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\"><i><strong><span class=\"by_XtphY3uYHwruKqDyG\">CC BY-SA 3.0 Unported License</span></strong></i></a><i><strong><span class=\"by_XtphY3uYHwruKqDyG\"> </span></strong><span class=\"by_XtphY3uYHwruKqDyG\">to conform with the Wikipedia license requirements)</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 37,
    "description": {
      "markdown": "**Intuition** is the ability to acquire knowledge without recourse to conscious reasoning. Also relevant are **Intuition Pumps**, a thought experiment, a model, or anything else, that's structured to allow the thinker to use their intuition to develop an answer to a problem. \n\n*(Modified from Wikipedia* [*1*](https://en.wikipedia.org/wiki/Intuition)*,* [*2*](https://en.wikipedia.org/wiki/Intuition_pump)*, this section is licensed as* [***CC BY-SA 3.0 Unported License***](https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License) *to conform with the Wikipedia license requirements)*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SbsZrDB844cENk4DQ",
    "name": "Falsifiability",
    "core": false,
    "slug": "falsifiability",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Falsifiability</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> or </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Refutability</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the capacity for a statement, theory or hypothesis to be contradicted by evidence. For example, the statement \"All swans are white\" is falsifiable because one can observe that black swans exist. </span><a href=\"https://en.wikipedia.org/wiki/Falsifiability\"><span class=\"by_sKAL2jzfkYkDbQmx9\">(From Wikipedia)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "**Falsifiability** or **Refutability** is the capacity for a statement, theory or hypothesis to be contradicted by evidence. For example, the statement \"All swans are white\" is falsifiable because one can observe that black swans exist. [(From Wikipedia)](https://en.wikipedia.org/wiki/Falsifiability)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yrg267i4a8EsgYAXp",
    "name": "HPMOR (discussion & meta)",
    "core": false,
    "slug": "hpmor-discussion-and-meta",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">This tag is for posts </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">discussing </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">the fan-fiction </span><strong><a href=\"https://www.lesswrong.com/hpmor\"><span><span class=\"by_sBWszXPhPsNNemv4Q\">Harry Potter and the Methods of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (HPMOR) </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">or posts talking about events, fan-fan-fiction content, etc – just not actual chapters/posts</span><span class=\"by_sBWszXPhPsNNemv4Q\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the story that can be be found in the HPMOR </span><span class=\"by_HoGziwmhpMGqGeWZy\">sequences.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 111,
    "description": {
      "markdown": "This tag is for posts **discussing** the fan-fiction **[Harry Potter and the Methods of Rationality](https://www.lesswrong.com/hpmor) (HPMOR)** or posts talking about events, fan-fan-fiction content, etc – just not actual chapters/posts of the story that can be be found in the HPMOR sequences."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CYMR6p5iZG75QAT8a",
    "name": "Censorship",
    "core": false,
    "slug": "censorship",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Censorship</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the suppression of speech, public communication, or other information, on the basis that such material is considered objectionable, harmful, sensitive, or \"inconvenient.\" Censorship can be conducted by governments, private institutions, and other controlling bodies. </span><a href=\"https://en.wikipedia.org/wiki/Censorship\"><span class=\"by_sKAL2jzfkYkDbQmx9\">(From Wikipedia)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 24,
    "description": {
      "markdown": "**Censorship** is the suppression of speech, public communication, or other information, on the basis that such material is considered objectionable, harmful, sensitive, or \"inconvenient.\" Censorship can be conducted by governments, private institutions, and other controlling bodies. [(From Wikipedia)](https://en.wikipedia.org/wiki/Censorship)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FGfgzGpPTtKEqSrDm",
    "name": "More Dakka",
    "core": false,
    "slug": "more-dakka",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">More Dakka </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is the technique of throwing more resources at a problem to see if you get better results.&nbsp;</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Originally, More Dakka is the </span><a href=\"https://tvtropes.org/pmwiki/pmwiki.php/Main/MoreDakka\"><span class=\"by_sKAL2jzfkYkDbQmx9\">trope</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> of solving problems by unloading as many rounds of ammunition at them as possible. In the rationalist community it was popularized by </span><a href=\"https://www.lesswrong.com/posts/z8usYeKX7dtTWsEnk/more-dakka\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Zvi </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">to have the above meaning.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "**More Dakka** is the technique of throwing more resources at a problem to see if you get better results. \n\nOriginally, More Dakka is the [trope](https://tvtropes.org/pmwiki/pmwiki.php/Main/MoreDakka) of solving problems by unloading as many rounds of ammunition at them as possible. In the rationalist community it was popularized by [Zvi](https://www.lesswrong.com/posts/z8usYeKX7dtTWsEnk/more-dakka) to have the above meaning."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zz3HWyByyKF64Sfns",
    "name": "The SF Bay Area",
    "core": false,
    "slug": "the-sf-bay-area",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_j8TwwtYJusmkqvGfh\">The San Francisco Bay Area</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a region in the US state of California. Many members of </span><a href=\"http://lesswrong.com/tag/community\"><span class=\"by_HoGziwmhpMGqGeWZy\">the rationalist community</span></a><span><span class=\"by_j8TwwtYJusmkqvGfh\"> </span><span class=\"by_HoGziwmhpMGqGeWZy\">are located there,</span><span class=\"by_j8TwwtYJusmkqvGfh\"> as </span><span class=\"by_HoGziwmhpMGqGeWZy\">are the</span><span class=\"by_j8TwwtYJusmkqvGfh\"> </span></span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span class=\"by_j8TwwtYJusmkqvGfh\">Machine Intelligence Research Institute</span></a><span><span class=\"by_j8TwwtYJusmkqvGfh\"> </span><span class=\"by_HoGziwmhpMGqGeWZy\">and</span><span class=\"by_j8TwwtYJusmkqvGfh\"> the </span></span><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar\"><span><span class=\"by_j8TwwtYJusmkqvGfh\">Center </span><span class=\"by_HoGziwmhpMGqGeWZy\">For</span><span class=\"by_j8TwwtYJusmkqvGfh\"> Applied Rationality</span></span></a></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Note: Covid-19 resulted in many people moving and the state of living affairs described below may no longer be accurate.</span></i></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/rationalist-movement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalist movement</span></a></li><li><a href=\"https://www.lesswrong.com/tag/history-of-less-wrong\"><span class=\"by_qgdGA4ZEyW7zNdK84\">History of Less Wrong</span></a></li><li><a href=\"/community\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong Meetup Groups</span></a></li></ul><h2 id=\"History\"><span class=\"by_qgdGA4ZEyW7zNdK84\">History</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">How did it become a hub?</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Machine Intelligence Research Institute</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (then the Singularity Institute) moved to the Bay Area in February 2005.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn1\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></sup></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The charity evaluator </span><a href=\"https://en.wikipedia.org/wiki/GiveWell\"><span class=\"by_qgdGA4ZEyW7zNdK84\">GiveWell</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> completed its move from New York to San Francisco in February 2013.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn2\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></sup></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The effective careers research organization 80,000 Hours announced it was moving to the Bay Area in May 2016, with the move completed by October.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn3\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> It was also in the Bay Area for summer 2016.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn4\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">4</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> Its 2017 review also mentions moving to the Bay Area in 2017.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn5\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">5</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> In 2019, 80,000 Hours moved from the Bay Area to the UK.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><a href=\"https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Center for Applied Rationality</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">'s 2017 Impact Report found that \"moved to the Bay Area due to CFAR\" is one of the strongest predictors for a CFAR participant having an \"increase in expected impact\"</span><span class=\"by_j8TwwtYJusmkqvGfh\">.</span></span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn6\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">6</span></sup></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Ward Street stuff.</span></p><h2 id=\"Debate\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Debate</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Especially since 2016 or so (possibly earlier?), there has been a considerable amount of debate about whether moving to the Bay Area is good for individuals or the community as a whole.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn7\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">7</span></sup></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Some illustrative quotes for concern about people moving to the Bay Area:</span></p><blockquote><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">\"The reasons for this are not immediately apparent. From the outside, people full of energy and enthusiasm make the pilgrimage to Berkeley, go quiet on social media, and when you finally hear from them six months later they don't seem like the person you once knew. </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Something</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> is happening to them, although it isn't particularly clear what.\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn8\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">8</span></sup></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">\"The theme of the Bay Solstice turned out to be 'Hey guys, so people keep coming to the Bay, running on a dream and a promise of community, but that community is not actually there, there's a tiny number of well-connected people who everyone is trying to get time with, and everyone seems lonely and sad. And we don't even know what to do about this.' \"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn9\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">9</span></sup></a><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span></sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn10\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\"> 10</span></sup></a><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span></sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn11\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">11,</span></sup></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn12\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\"> 12</span></sup></a></li></ul></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">\"It's seemed to me for awhile now that the stuff that people are actually talking about in-person (e.g. at CFAR workshops) has far outstripped the pace of what's publicly available in blog post format and I'm really happy to see progress on that front.\" </span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn13\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">13</span></sup></a><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span></sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn14\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">14</span></sup></a><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span></sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn15\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\"> 15</span></sup></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn16\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">,16,</span></sup></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn17\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">17</span></sup></a></p></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">4. Ward Street is quickly becoming the center of the rationalist scene in Berkeley. We’re trying to encourage that so that as many people as possible can live near each other and it can feel like more of a community. I’ll be staying there temporarily when I first get to California, and I know a lot of other people on the street and they’re all pretty interesting. Anyway, there’s a house opening up there as the current residents leave, and we’d like to get rationalist-adjacent people to move in. It’s three bedrooms, one bathroom, and it costs $4100/month total. If interested (either in renting the whole house with friends/family, or in just renting one room and hoping two other people want the same), email jsalvatier[at]gmail[dot]com and he can tell you more / help connect interested parties together.\"\"\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn18\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">18</span></sup></a><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span></sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn19\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">19</span></sup></a><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span></sup><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn20\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">20,</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn21\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">21</span></sup></a></p></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Even if you know nothing else, you know to move to San Francisco or New York and hoping something good happens there, rather than sitting around in some dying small town where you know nothing will ever happen and being curious about anything beyond the town is a cultural transgression. This is a strategy open to all.\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn22\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">22</span></sup></a></p></blockquote><h2 id=\"Sending_people_to_the_Bay\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sending people to the Bay</span></h2><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">I know meetups causing people to move to the Bay is a controversial topic, but from my perspective, moving to the Bay is one of the best things a person can do in terms of expected impact on the existential risk landscape. It gives people the opportunity to work at aligned organizations, and to be around hundreds of like-minded people, which (in addition to its social benefits) allows people to find collaborators with whom to start new projects and organizations.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn23\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">23</span></sup></a></p></blockquote><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://www.bayrationality.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Informational site about Bay Area rationality</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (not sure who runs the site)</span></li><li><a href=\"http://www.rationalistgames.org/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalist House Games</span></a></li></ul><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ol><li><a href=\"https://web.archive.org/web/20060220211402/http://www.singinst.org:80/news/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"News of the Singularity Institute for Artificial Intelligence\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Archived from </span><a href=\"http://www.singinst.org/news/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">the original</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> on February 20, 2006. Retrieved February 12, 2018. \"SIAI has moved to Silicon Valley. Executive Director Tyler Emerson and Advocacy Director Michael Anissimov are both located in the Bay Area of California, along with SIAI Research Fellow Eliezer Yudkowsky (since February). This should enable us to stay in better contact with donors, and cultivate team members and additional collaborators.\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Holden Karnofsky. </span><a href=\"https://blog.givewell.org/2013/02/08/givewells-progress-in-2012/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"GiveWell's progress in 2012\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. February 8, 2013. </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">The GiveWell Blog</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. GiveWell. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref2\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Benjamin Todd. </span><a href=\"https://groups.google.com/forum/#!msg/80k_updates/jAZNlgEhSsM/YIztgo9kAAAJ\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"80k supporter update - moving to the Bay; cost\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. May 15, 2016. Google Groups. Retrieved February 12, 2018. \"We decided to move and our trustees have approved. We plan to be out in the Bay by August, and fully moved by Oct. The next step is to get visas, which is in progress.\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Benjamin Todd. </span><a href=\"https://groups.google.com/forum/#!msg/80k_updates/RxRGOUF0ii4/TTUsGDgyDAAJ\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"80k supporter update\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. August 23, 2016. Google Groups. Retrieved February 12, 2018. \"Moved to the Bay Area for the summer.\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref4\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Benjamin Todd. </span><a href=\"https://80000hours.org/2017/12/annual-review/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Annual review December 2017\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. December 24, 2017. 80,000 Hours. Retrieved February 12, 2018. \"We completed our move to the Bay Area, securing visas for everyone on the team by April 2017, setting up our office, and doing the administration needed (though we’re yet to have the pleasure of filing our first personal US tax returns…).\"</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref5\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Dan Keys. </span><a href=\"http://www.rationality.org/resources/updates/2017/cfar-2017-impact-report\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"CFAR 2017 Impact Report\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Center for Applied Rationality. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref6\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><a href=\"https://thingofthings.wordpress.com/2017/05/03/why-do-all-the-rationalists-live-in-the-bay-area/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Why Do All The Rationalists Live In The Bay Area?\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. May 4, 2017. Thing of Things. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref7\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">bendini. </span><a href=\"https://www.lesswrong.com/posts/wmEcNP3KFEGPZaFJk/the-craft-and-the-community-a-post-mortem-and-resurrection\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"The Craft &amp; The Community - A Post-Mortem &amp; Resurrection\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. November 1, 2017. LessWrong. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref8\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Raymond Arnold. </span><a href=\"https://www.lesswrong.com/lw/p1f/notes_from_the_hufflepuff_unconference_part_1/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Notes from the Hufflepuff Unconference (Part 1)\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. May 23, 2017. LessWrong. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref9\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Zvi Mowshowitz. </span><a href=\"https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"What Is Rationalist Berkeley’s Community Culture?\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. November 3, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref10\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Elizabeth Van Nostrand. </span><a href=\"https://www.facebook.com/li.van.nostrand/posts/10102872753725305\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"It looks pretty likely I'll move to the bay...\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Facebook. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref11\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Brent Dill. </span><a href=\"https://www.facebook.com/ialdabaoth/posts/10208221491793885\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Two years ago, to the day, I decided to move to the...\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Facebook. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref12\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><a href=\"https://www.lesswrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y\"><span class=\"by_qgdGA4ZEyW7zNdK84\">https://www.lesserwrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y</span></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref13\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Raymond Arnold. </span><a href=\"https://www.facebook.com/raymond.arnold.5/posts/10211225174359187\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"It took me a disturbing amount of time to realize...\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Facebook. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref14\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Alyssa Vance. </span><a href=\"https://www.facebook.com/groups/effective.altruists/permalink/1366133853442968/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Many Effective Altruists think about moving to the San Francisco Bay Area, or have already done so …\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. April 14, 2017. Facebook. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref15\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Scott Alexander. </span><a href=\"http://slatestarcodex.com/2017/07/03/to-the-great-city/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"To The Great City!\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. July 3, 2017. Slate Star Codex. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref16\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><a href=\"http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132</span></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref17\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Scott Alexander. </span><a href=\"http://slatestarcodex.com/2017/07/03/ot79-open-road/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"OT79: Open Road\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. July 5, 2017. Slate Star Codex. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref18\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Zvi Mowshowitz. </span><a href=\"https://thezvi.wordpress.com/2017/04/04/responses-to-tyler-cohen-on-rationality/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Responses to Tyler Cowen on Rationality\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. September 10, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref19\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Katja Grace. </span><a href=\"https://meteuphoric.wordpress.com/2017/07/20/be-my-neighbor/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Be my neighbor\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. July 20, 2017. Meteuphoric. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref20\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Alyssa Vance. </span><a href=\"https://www.facebook.com/alyssamvance/posts/10214122240439702\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"'Why is the best place to live in the world so much...\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. August 5, 2017. Facebook. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref21\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><a href=\"https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047\"><span class=\"by_qgdGA4ZEyW7zNdK84\">https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047</span></a><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref22\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">mingyuan. </span><a href=\"https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-do-we-mean-by-meetups\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"What Do We Mean By 'Meetups'?\"</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. February 7, 2018. LessWrong. Retrieved February 12, 2018.</span><a href=\"https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref23\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li></ol>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "Debate",
          "anchor": "Debate",
          "level": 1
        },
        {
          "title": "Sending people to the Bay",
          "anchor": "Sending_people_to_the_Bay",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 26,
    "description": {
      "markdown": "**The San Francisco Bay Area** is a region in the US state of California. Many members of [the rationalist community](http://lesswrong.com/tag/community) are located there, as are the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and the [Center For Applied Rationality](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar)\n\n*Note: Covid-19 resulted in many people moving and the state of living affairs described below may no longer be accurate.*\n\nSee also\n--------\n\n*   [Rationalist movement](https://www.lesswrong.com/tag/rationalist-movement)\n*   [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong)\n*   [LessWrong Meetup Groups](/community)\n\nHistory\n-------\n\nHow did it become a hub?\n\nThe [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) (then the Singularity Institute) moved to the Bay Area in February 2005.[^1^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn1)\n\nThe charity evaluator [GiveWell](https://en.wikipedia.org/wiki/GiveWell) completed its move from New York to San Francisco in February 2013.[^2^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn2)\n\nThe effective careers research organization 80,000 Hours announced it was moving to the Bay Area in May 2016, with the move completed by October.[^3^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn3) It was also in the Bay Area for summer 2016.[^4^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn4) Its 2017 review also mentions moving to the Bay Area in 2017.[^5^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn5) In 2019, 80,000 Hours moved from the Bay Area to the UK.\n\nThe [Center for Applied Rationality](https://wiki.lesswrong.com/wiki/Center_for_Applied_Rationality)'s 2017 Impact Report found that \"moved to the Bay Area due to CFAR\" is one of the strongest predictors for a CFAR participant having an \"increase in expected impact\".[^6^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn6)\n\nWard Street stuff.\n\nDebate\n------\n\nEspecially since 2016 or so (possibly earlier?), there has been a considerable amount of debate about whether moving to the Bay Area is good for individuals or the community as a whole.[^7^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn7)\n\nSome illustrative quotes for concern about people moving to the Bay Area:\n\n> *   \"The reasons for this are not immediately apparent. From the outside, people full of energy and enthusiasm make the pilgrimage to Berkeley, go quiet on social media, and when you finally hear from them six months later they don't seem like the person you once knew. *Something* is happening to them, although it isn't particularly clear what.\"[^8^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn8)\n> *   \"The theme of the Bay Solstice turned out to be 'Hey guys, so people keep coming to the Bay, running on a dream and a promise of community, but that community is not actually there, there's a tiny number of well-connected people who everyone is trying to get time with, and everyone seems lonely and sad. And we don't even know what to do about this.' \"[^9^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn9)^,^ [^10^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn10)^,^[^11,^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn11) [^12^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn12)\n\n> \"It's seemed to me for awhile now that the stuff that people are actually talking about in-person (e.g. at CFAR workshops) has far outstripped the pace of what's publicly available in blog post format and I'm really happy to see progress on that front.\" [^13^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn13)^,^ [^14^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn14)^,^ [^15^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn15)[^,16,^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn16)[^17^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn17)\n\n> 4\\. Ward Street is quickly becoming the center of the rationalist scene in Berkeley. We’re trying to encourage that so that as many people as possible can live near each other and it can feel like more of a community. I’ll be staying there temporarily when I first get to California, and I know a lot of other people on the street and they’re all pretty interesting. Anyway, there’s a house opening up there as the current residents leave, and we’d like to get rationalist-adjacent people to move in. It’s three bedrooms, one bathroom, and it costs $4100/month total. If interested (either in renting the whole house with friends/family, or in just renting one room and hoping two other people want the same), email jsalvatier\\[at\\]gmail\\[dot\\]com and he can tell you more / help connect interested parties together.\"\"\"[^18^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn18)^,^ [^19^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn19)^,^ [^20,^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn20) [^21^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn21)\n\n> \"Even if you know nothing else, you know to move to San Francisco or New York and hoping something good happens there, rather than sitting around in some dying small town where you know nothing will ever happen and being curious about anything beyond the town is a cultural transgression. This is a strategy open to all.\"[^22^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn22)\n\nSending people to the Bay\n-------------------------\n\n> I know meetups causing people to move to the Bay is a controversial topic, but from my perspective, moving to the Bay is one of the best things a person can do in terms of expected impact on the existential risk landscape. It gives people the opportunity to work at aligned organizations, and to be around hundreds of like-minded people, which (in addition to its social benefits) allows people to find collaborators with whom to start new projects and organizations.[^23^](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fn23)\n\nExternal links\n--------------\n\n*   [Informational site about Bay Area rationality](http://www.bayrationality.com/) (not sure who runs the site)\n*   [Rationalist House Games](http://www.rationalistgames.org/)\n\nReferences\n----------\n\n1.  [\"News of the Singularity Institute for Artificial Intelligence\"](https://web.archive.org/web/20060220211402/http://www.singinst.org:80/news/). Archived from [the original](http://www.singinst.org/news/) on February 20, 2006. Retrieved February 12, 2018. \"SIAI has moved to Silicon Valley. Executive Director Tyler Emerson and Advocacy Director Michael Anissimov are both located in the Bay Area of California, along with SIAI Research Fellow Eliezer Yudkowsky (since February). This should enable us to stay in better contact with donors, and cultivate team members and additional collaborators.\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref1)\n2.  Holden Karnofsky. [\"GiveWell's progress in 2012\"](https://blog.givewell.org/2013/02/08/givewells-progress-in-2012/). February 8, 2013. *The GiveWell Blog*. GiveWell. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref2)\n3.  Benjamin Todd. [\"80k supporter update - moving to the Bay; cost\"](https://groups.google.com/forum/#!msg/80k_updates/jAZNlgEhSsM/YIztgo9kAAAJ). May 15, 2016. Google Groups. Retrieved February 12, 2018. \"We decided to move and our trustees have approved. We plan to be out in the Bay by August, and fully moved by Oct. The next step is to get visas, which is in progress.\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref3)\n4.  Benjamin Todd. [\"80k supporter update\"](https://groups.google.com/forum/#!msg/80k_updates/RxRGOUF0ii4/TTUsGDgyDAAJ). August 23, 2016. Google Groups. Retrieved February 12, 2018. \"Moved to the Bay Area for the summer.\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref4)\n5.  Benjamin Todd. [\"Annual review December 2017\"](https://80000hours.org/2017/12/annual-review/). December 24, 2017. 80,000 Hours. Retrieved February 12, 2018. \"We completed our move to the Bay Area, securing visas for everyone on the team by April 2017, setting up our office, and doing the administration needed (though we’re yet to have the pleasure of filing our first personal US tax returns…).\"[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref5)\n6.  Dan Keys. [\"CFAR 2017 Impact Report\"](http://www.rationality.org/resources/updates/2017/cfar-2017-impact-report). Center for Applied Rationality. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref6)\n7.  [\"Why Do All The Rationalists Live In The Bay Area?\"](https://thingofthings.wordpress.com/2017/05/03/why-do-all-the-rationalists-live-in-the-bay-area/). May 4, 2017. Thing of Things. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref7)\n8.  bendini. [\"The Craft & The Community - A Post-Mortem & Resurrection\"](https://www.lesswrong.com/posts/wmEcNP3KFEGPZaFJk/the-craft-and-the-community-a-post-mortem-and-resurrection). November 1, 2017. LessWrong. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref8)\n9.  Raymond Arnold. [\"Notes from the Hufflepuff Unconference (Part 1)\"](https://www.lesswrong.com/lw/p1f/notes_from_the_hufflepuff_unconference_part_1/). May 23, 2017. LessWrong. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref9)\n10.  Zvi Mowshowitz. [\"What Is Rationalist Berkeley’s Community Culture?\"](https://thezvi.wordpress.com/2017/08/12/what-is-rationalist-berkleys-community-culture/). November 3, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref10)\n11.  Elizabeth Van Nostrand. [\"It looks pretty likely I'll move to the bay...\"](https://www.facebook.com/li.van.nostrand/posts/10102872753725305). Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref11)\n12.  Brent Dill. [\"Two years ago, to the day, I decided to move to the...\"](https://www.facebook.com/ialdabaoth/posts/10208221491793885). Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref12)\n13.  [https://www.lesserwrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y](https://www.lesswrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#vDST8wTjsiWKCjZ3y)[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref13)\n14.  Raymond Arnold. [\"It took me a disturbing amount of time to realize...\"](https://www.facebook.com/raymond.arnold.5/posts/10211225174359187). Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref14)\n15.  Alyssa Vance. [\"Many Effective Altruists think about moving to the San Francisco Bay Area, or have already done so …\"](https://www.facebook.com/groups/effective.altruists/permalink/1366133853442968/). April 14, 2017. Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref15)\n16.  Scott Alexander. [\"To The Great City!\"](http://slatestarcodex.com/2017/07/03/to-the-great-city/). July 3, 2017. Slate Star Codex. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref16)\n17.  [http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132](http://slatestarcodex.com/2017/07/03/to-the-great-city/#comment-518132)[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref17)\n18.  Scott Alexander. [\"OT79: Open Road\"](http://slatestarcodex.com/2017/07/03/ot79-open-road/). July 5, 2017. Slate Star Codex. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref18)\n19.  Zvi Mowshowitz. [\"Responses to Tyler Cowen on Rationality\"](https://thezvi.wordpress.com/2017/04/04/responses-to-tyler-cohen-on-rationality/). September 10, 2017. Don't Worry About the Vase. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref19)\n20.  Katja Grace. [\"Be my neighbor\"](https://meteuphoric.wordpress.com/2017/07/20/be-my-neighbor/). July 20, 2017. Meteuphoric. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref20)\n21.  Alyssa Vance. [\"'Why is the best place to live in the world so much...\"](https://www.facebook.com/alyssamvance/posts/10214122240439702). August 5, 2017. Facebook. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref21)\n22.  [https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047](https://www.ribbonfarm.com/2017/08/17/the-premium-mediocre-life-of-maya-millennial/#more-6047)[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref22)\n23.  mingyuan. [\"What Do We Mean By 'Meetups'?\"](https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-do-we-mean-by-meetups). February 7, 2018. LessWrong. Retrieved February 12, 2018.[↩](https://www.lesswrong.com/tag/the-sf-bay-area?revision=0.0.5#fnref23)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6Qic6PwwBycopJFNN",
    "name": "Contrarianism",
    "core": false,
    "slug": "contrarianism",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">A </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">contrarian</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a person who holds a contrary position, especially a position against the </span><a href=\"https://www.lesswrong.com/tag/consensus\"><span class=\"by_sKAL2jzfkYkDbQmx9\">majority</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://en.wikipedia.org/wiki/Contrarian\"><span class=\"by_sKAL2jzfkYkDbQmx9\">(from Wikipedia).</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": {
      "markdown": "A **contrarian** is a person who holds a contrary position, especially a position against the [majority](https://www.lesswrong.com/tag/consensus) [(from Wikipedia).](https://en.wikipedia.org/wiki/Contrarian)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WPkEd3et8f488w8LT",
    "name": "Good Explanations (Advice)",
    "core": false,
    "slug": "good-explanations-advice",
    "tableOfContents": {
      "html": "<p><span class=\"by_XtphY3uYHwruKqDyG\">This </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">Good Explanations (Advice) </span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">tag</span><strong><span class=\"by_HoGziwmhpMGqGeWZy\"> </span></strong><span class=\"by_XtphY3uYHwruKqDyG\">is for advice on how to write good explanations.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": {
      "markdown": "This **Good Explanations (Advice)** tag  is for advice on how to write good explanations."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bFi5fzkCzBWoQSeiB",
    "name": "Technological Unemployment",
    "core": false,
    "slug": "technological-unemployment",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Technological Unemployment</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> occurs when businesses replace human workers with automated systems, and the displaced workers are unable to find new jobs.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Related to </span><a href=\"https://www.lesswrong.com/tag/automation\"><span class=\"by_r38pkCm7wF4M44MDQ\">Automation</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 9,
    "description": {
      "markdown": "**Technological Unemployment** occurs when businesses replace human workers with automated systems, and the displaced workers are unable to find new jobs.\n\nRelated to [Automation](https://www.lesswrong.com/tag/automation)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LnEEs8xGooYmQ8iLA",
    "name": "Truth, Semantics, & Meaning",
    "core": false,
    "slug": "truth-semantics-and-meaning",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Truth, Semantics, and Meaning</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">What does it mean to assert that something</span><span class=\"by_sQdyMYniENgxfQsf2\"> is </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">true? A very popular answer is </span></span><a href=\"https://www.lesswrong.com/tag/map-and-territory\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">map-territory correspondence </span><span class=\"by_Xn6ACr6Cua8upALWQ\">theory</span></span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> But </span><span class=\"by_sQdyMYniENgxfQsf2\">the </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">details of this theory are not clear,</span><span class=\"by_nmk3nLpQE89dMRzzN\"> and </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">there are other contenders.</span></span></p><h2 id=\"Truth_as_Correspondence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Truth as Correspondence</span></h2><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Many consider truth as the correspondence between </span><span class=\"by_d7Nir2ShioHnEDk22\">reality </span><span class=\"by_qgdGA4ZEyW7zNdK84\">and one's beliefs about reality. Within this frame, truth itself is not necessarily limited to one's belief about something. For a statement/ideal/proposed fact to be considered \"true,\" you must take it as its definition. Truth doesn't imply that something has to be proven in order for it to be made true, but that the statement/ideal/proposed fact has to be true all of the time, regardless of one's belief.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Alfred Tarski defined truth in terms of an infinite family of sentences such as:</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The sentence 'snow is white' is </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">true</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> if and only if snow is white.</span></p></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">To understand whether a belief is true, we need (only) to understand what possible states of the world would make it true or false, and then ask directly about the world. Often, people assume that ideals and morals change with culture; as they tend to do. Unfortunately, many people struggle with their belief of \"truth\" based on their religion. Because of their belief, they object the currently accepted \"truth\" about the world, about life (how we all got here), and most importantly, what is considered \"right\" or \"wrong.\"</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Truth\" is not, however, a determination. Truth is not simply a belief. Truth is an ideal, concept, or fact that can be observed. Whether an individual has a belief derived from their religion on what is truth or not, unless they have observed it, they cannot prove whether their belief is truth or not. Reiterating from above: the lack of proof or justification, or even rationalization, does not change the status of truth. What's truth is truth, and what is false, is false. Humans simply decide to reject notions and proposed facts as truth if they are not observable, or are not able to show any proof.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">'Truth' is a very simple concept, understood perfectly well by three-year-olds, but often made unnecessarily complicated by adults.</span></p><h2 id=\"Other_Theories_of_Truth\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Other Theories of Truth</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&lt;needed&gt;</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/eqn/the_useful_idea_of_truth/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Useful Idea of Truth</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - A basic guide to what 'truth' means.</span></li><li><a href=\"https://www.lesswrong.com/lw/go/why_truth_and/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why truth? And...</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - You have an instrumental motive to care about the truth of your </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">beliefs about</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> anything you care about.</span></li><li><a href=\"https://www.lesswrong.com/lw/lz/guardians_of_the_truth/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Guardians of the Truth</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Endorsing a concept of truth is not the same as endorsing a particular belief as eternally, absolutely, knowably true.</span></li><li><a href=\"https://www.lesswrong.com/lw/hp/feeling_rational/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Feeling Rational</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Emotions cannot be true or false, but they can follow from true or false beliefs.</span></li><li><a href=\"https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Meditation on Curiosity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - In particular, the </span><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Litany of Tarski</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><a href=\"https://www.lesswrong.com/lw/sf/fake_norms_or_truth_vs_truth/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fake Norms, or \"Truth\" vs. Truth</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Our society has a moral norm for applauding \"truth\", but actual truths get much less applause (this is a bad thing).</span></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://yudkowsky.net/rational/the-simple-truth\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Simple Truth</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Epistemic_rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Litany of Tarski</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/litany-of-gendlin\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Litany of Gendlin</span></a></li><li><a href=\"https://www.lesswrong.com/tag/self-deception\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-deception</span></a></li><li><a href=\"https://www.lesswrong.com/tag/belief\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Belief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/highly-advanced-epistemology-101-for-beginners\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Highly Advanced Epistemology 101 for Beginners</span></a></li></ul>",
      "sections": [
        {
          "title": "Truth as Correspondence",
          "anchor": "Truth_as_Correspondence",
          "level": 1
        },
        {
          "title": "Other Theories of Truth",
          "anchor": "Other_Theories_of_Truth",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 78,
    "description": {
      "markdown": "**Truth, Semantics, and Meaning**: What does it mean to assert that something is true? A very popular answer is [map-territory correspondence theory](https://www.lesswrong.com/tag/map-and-territory). But the details of this theory are not clear, and there are other contenders.\n\nTruth as Correspondence\n-----------------------\n\nMany consider truth as the correspondence between reality and one's beliefs about reality. Within this frame, truth itself is not necessarily limited to one's belief about something. For a statement/ideal/proposed fact to be considered \"true,\" you must take it as its definition. Truth doesn't imply that something has to be proven in order for it to be made true, but that the statement/ideal/proposed fact has to be true all of the time, regardless of one's belief.\n\nAlfred Tarski defined truth in terms of an infinite family of sentences such as:\n\n> The sentence 'snow is white' is *true* if and only if snow is white.\n\nTo understand whether a belief is true, we need (only) to understand what possible states of the world would make it true or false, and then ask directly about the world. Often, people assume that ideals and morals change with culture; as they tend to do. Unfortunately, many people struggle with their belief of \"truth\" based on their religion. Because of their belief, they object the currently accepted \"truth\" about the world, about life (how we all got here), and most importantly, what is considered \"right\" or \"wrong.\"\n\n\"Truth\" is not, however, a determination. Truth is not simply a belief. Truth is an ideal, concept, or fact that can be observed. Whether an individual has a belief derived from their religion on what is truth or not, unless they have observed it, they cannot prove whether their belief is truth or not. Reiterating from above: the lack of proof or justification, or even rationalization, does not change the status of truth. What's truth is truth, and what is false, is false. Humans simply decide to reject notions and proposed facts as truth if they are not observable, or are not able to show any proof.\n\n'Truth' is a very simple concept, understood perfectly well by three-year-olds, but often made unnecessarily complicated by adults.\n\nOther Theories of Truth\n-----------------------\n\n<needed>\n\nNotable Posts\n-------------\n\n*   [The Useful Idea of Truth](https://www.lesswrong.com/lw/eqn/the_useful_idea_of_truth/) \\- A basic guide to what 'truth' means.\n*   [Why truth? And...](https://www.lesswrong.com/lw/go/why_truth_and/) \\- You have an instrumental motive to care about the truth of your *beliefs about* anything you care about.\n*   [Guardians of the Truth](https://www.lesswrong.com/lw/lz/guardians_of_the_truth/) \\- Endorsing a concept of truth is not the same as endorsing a particular belief as eternally, absolutely, knowably true.\n*   [Feeling Rational](https://www.lesswrong.com/lw/hp/feeling_rational/) \\- Emotions cannot be true or false, but they can follow from true or false beliefs.\n*   [The Meditation on Curiosity](https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/) \\- In particular, the [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski).\n*   [Fake Norms, or \"Truth\" vs. Truth](https://www.lesswrong.com/lw/sf/fake_norms_or_truth_vs_truth/) \\- Our society has a moral norm for applauding \"truth\", but actual truths get much less applause (this is a bad thing).\n\nExternal links\n--------------\n\n*   [The Simple Truth](http://yudkowsky.net/rational/the-simple-truth)\n\nSee also\n--------\n\n*   [Epistemic rationality](https://wiki.lesswrong.com/wiki/Epistemic_rationality)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski), [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)\n*   [Self-deception](https://www.lesswrong.com/tag/self-deception)\n*   [Belief](https://www.lesswrong.com/tag/belief)\n*   [Highly Advanced Epistemology 101 for Beginners](https://www.lesswrong.com/tag/highly-advanced-epistemology-101-for-beginners)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DigEmY3RrF3XL5cwe",
    "name": "Q&A (format)",
    "core": false,
    "slug": "q-and-a-format",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">Posts in the format of </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Question and Answers (Q&amp;A)</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">, usually on some specific topic.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">This includes both question and answer style</span><a href=\"interviews\"><span class=\"by_HoGziwmhpMGqGeWZy\"> interviews</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> between actual people, and essays formatted as question and answer sessions between fictional people.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 31,
    "description": {
      "markdown": "Posts in the format of **Question and Answers (Q&A)**, usually on some specific topic.\n\nThis includes both question and answer style [interviews](interviews) between actual people, and essays formatted as question and answer sessions between fictional people."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "irYLXtT9hkPXoZqhH",
    "name": "Growth Stories",
    "core": false,
    "slug": "growth-stories",
    "tableOfContents": {
      "html": "<p><span class=\"by_QBvPFLFyZyuHcBwFm\">Recollections of personal progress, lessons learned, memorable experiences, coming of age, in autobiographical form.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></p><ul><li><a href=\"https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Yudkowsky's Coming of Age</span></a></li></ul><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/postmortems-and-retrospectives\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Postmortems &amp; Retrospectives</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/updated-beliefs-examples-of\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Updated Beliefs (examples of)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/self-improvement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Self Improvement</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/progress-studies\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Progress Studies</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> (society level)</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 56,
    "description": {
      "markdown": "Recollections of personal progress, lessons learned, memorable experiences, coming of age, in autobiographical form.\n\n**Sequences:** \n\n*   [Yudkowsky's Coming of Age](https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG)\n\n**Related Pages:** [Postmortems & Retrospectives](https://www.lesswrong.com/tag/postmortems-and-retrospectives), [Updated Beliefs (examples of)](https://www.lesswrong.com/tag/updated-beliefs-examples-of), [Self Improvement](https://www.lesswrong.com/tag/self-improvement), [Progress Studies](https://www.lesswrong.com/tag/progress-studies) (society level)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8hPTCJbwJnLBmfpCX",
    "name": "Calibration",
    "core": false,
    "slug": "calibration",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">Someone is </span><strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\">well-</span><span class=\"by_sKAL2jzfkYkDbQmx9\">calibrated</span></span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> if</span><span class=\"by_HoGziwmhpMGqGeWZy\"> the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">things they predict with</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><span class=\"by_Xn6ACr6Cua8upALWQ\">X%</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><span class=\"by_sKAL2jzfkYkDbQmx9\">chance of happening</span><span class=\"by_HoGziwmhpMGqGeWZy\"> in </span><span class=\"by_sKAL2jzfkYkDbQmx9\">fact occur</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><span class=\"by_Xn6ACr6Cua8upALWQ\">X%</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">time. Importantly, calibration is </span></span><em><span class=\"by_sKAL2jzfkYkDbQmx9\">not the same as accuracy. </span></em><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Calibration is about accurately assessing how good your predictions are, not making good predictions. Person A, whose predictions are marginally better than </span><span class=\"by_Xn6ACr6Cua8upALWQ\">chance (60%</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> of them come </span><span class=\"by_Xn6ACr6Cua8upALWQ\">true when choosing from two options)</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> and who is </span><span class=\"by_Xn6ACr6Cua8upALWQ\">precisely 60% confident in their choices, is perfectly </span><span class=\"by_sKAL2jzfkYkDbQmx9\">calibrated. In contrast, Person B, </span><span class=\"by_Xn6ACr6Cua8upALWQ\">who is 99% confident in their predictions, and right</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> 90% </span><span class=\"by_Xn6ACr6Cua8upALWQ\">of the time,</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> is more </span></span><em><span class=\"by_sKAL2jzfkYkDbQmx9\">accurate</span></em><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> than Person </span><span class=\"by_Xn6ACr6Cua8upALWQ\">A, but</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> less </span></span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">well-calibrated</span></em><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span></p><p><em><span class=\"by_Xn6ACr6Cua8upALWQ\">See also: </span><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Betting</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Epistemic Modesty</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Forecasting &amp; Prediction</span></a></em></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Being well-calibrated has value for rationalists separately from accuracy.</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> Among other things, being </span><span class=\"by_Xn6ACr6Cua8upALWQ\">well-</span><span class=\"by_sKAL2jzfkYkDbQmx9\">calibrated lets you make good </span></span><a href=\"https://www.lesswrong.com/tag/betting\"><span class=\"by_sKAL2jzfkYkDbQmx9\">bets </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">/ </span><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making\"><span class=\"by_sKAL2jzfkYkDbQmx9\">make good decisions</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, communicate information helpfully to others if they know you to be well-calibrated (See </span><a href=\"https://www.lesswrong.com/tag/group-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Group Rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">), and helps prioritize </span><a href=\"https://www.lesswrong.com/tag/value-of-information\"><span class=\"by_sKAL2jzfkYkDbQmx9\">which information is worth acquiring</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">. </span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Note that all expressions of quantified confidence in </span><a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&amp;useTagName=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">beliefs</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> can be well- or poorly- calibrated.</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> For example, calibration applies to whether a </span><span class=\"by_Xn6ACr6Cua8upALWQ\">person'</span><span class=\"by_sKAL2jzfkYkDbQmx9\">s 95% confidence intervals </span><span class=\"by_Xn6ACr6Cua8upALWQ\">captures the true outcome</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> 95% of the time. </span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 46,
    "description": {
      "markdown": "Someone is **well-calibrated** if the things they predict with X% chance of happening in fact occur X% of the time. Importantly, calibration is _not the same as accuracy._ Calibration is about accurately assessing how good your predictions are, not making good predictions. Person A, whose predictions are marginally better than chance (60% of them come true when choosing from two options) and who is precisely 60% confident in their choices, is perfectly calibrated. In contrast, Person B, who is 99% confident in their predictions, and right 90% of the time, is more _accurate_ than Person A, but less _well-calibrated_.\n\n_See also: [Betting](https://www.lesswrong.com/tag/betting?showPostCount=true&useTagName=true), [Epistemic Modesty](https://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&useTagName=true), [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)_\n\nBeing well-calibrated has value for rationalists separately from accuracy. Among other things, being well-calibrated lets you make good [bets](https://www.lesswrong.com/tag/betting) / [make good decisions](https://www.lesswrong.com/tag/planning-and-decision-making), communicate information helpfully to others if they know you to be well-calibrated (See [Group Rationality](https://www.lesswrong.com/tag/group-rationality)), and helps prioritize [which information is worth acquiring](https://www.lesswrong.com/tag/value-of-information).\n\nNote that all expressions of quantified confidence in [beliefs](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&useTagName=false) can be well- or poorly- calibrated. For example, calibration applies to whether a person's 95% confidence intervals captures the true outcome 95% of the time."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b9FzogZE4pAGfo5bY",
    "name": "Urban Planning / Design",
    "core": false,
    "slug": "urban-planning-design",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Urban Planning / Design</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> refers to ideas and plans for how to better approach the design of urban environments (such as cities).</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "**Urban Planning / Design** refers to ideas and plans for how to better approach the design of urban environments (such as cities)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8SfkJYYMe75MwjHzN",
    "name": "Summaries",
    "core": false,
    "slug": "summaries",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Summaries</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> of papers, books, Sequences or anything else.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/book-reviews\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Book Reviews</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/literature-reviews\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Literature Reviews</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 84,
    "description": {
      "markdown": "**Summaries** of papers, books, Sequences or anything else.\n\nSee also: [Book Reviews](https://www.lesswrong.com/tag/book-reviews), [Literature Reviews](https://www.lesswrong.com/tag/literature-reviews)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NXn3MSft8kzmMJbeg",
    "name": "Category Theory",
    "core": false,
    "slug": "category-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Category Theory </span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">is a </span><span class=\"by_ypbkRWpFgPgzvNg3n\">subfield of Mathematics studying mathematical structures and their conservation through various transformation. It emerged</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> in the </span><span class=\"by_ypbkRWpFgPgzvNg3n\">study</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> of algebraic </span><span class=\"by_ypbkRWpFgPgzvNg3n\">topology, then went on to apply to most parts of mathematics.</span></span></p><p><span class=\"by_ypbkRWpFgPgzvNg3n\">Despite some work on </span><a href=\"https://www.appliedcategorytheory.org/\"><span class=\"by_ypbkRWpFgPgzvNg3n\">applied category theory</span></a><span class=\"by_ypbkRWpFgPgzvNg3n\">, it's usefulness for problem solving (especially in topics close to LW, such as </span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_ypbkRWpFgPgzvNg3n\">Rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> and </span><a href=\"https://www.alignmentforum.org/\"><span class=\"by_ypbkRWpFgPgzvNg3n\">AI Safety</span></a><span><span class=\"by_ypbkRWpFgPgzvNg3n\">) is still controversial. One possible explanation is that</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> category theory </span><span class=\"by_ypbkRWpFgPgzvNg3n\">usually comes after the results, to structure them </span><span class=\"by_sKAL2jzfkYkDbQmx9\">and </span><span class=\"by_ypbkRWpFgPgzvNg3n\">find the links with previous results in other fields.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": {
      "markdown": "**Category Theory** is a subfield of Mathematics studying mathematical structures and their conservation through various transformation. It emerged in the study of algebraic topology, then went on to apply to most parts of mathematics.\n\nDespite some work on [applied category theory](https://www.appliedcategorytheory.org/), it's usefulness for problem solving (especially in topics close to LW, such as [Rationality](https://www.lesswrong.com/tag/rationality) and [AI Safety](https://www.alignmentforum.org/)) is still controversial. One possible explanation is that category theory usually comes after the results, to structure them and find the links with previous results in other fields."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fR7QfYx4JA3BnptT9",
    "name": "Skill Building",
    "core": false,
    "slug": "skill-building",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Skill Building</span></strong><span><span class=\"by_nLbwLhBaQeG6tCNDN\"> is the meta-</span><span class=\"by_SsduPgHwY2zeZpmKT\">skill of getting good at things i.e. developing procedural knowledge.</span></span></p><p><span class=\"by_SsduPgHwY2zeZpmKT\">Subtopics may include: Fast feedback loops, Choosing the correct level of challenge, studying people better than you to pick up their techniques or assimilate their style or philosophy, synthesizing advice from many tutorials, instructions, or how-tos </span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 53,
    "description": {
      "markdown": "**Skill Building** is the meta-skill of getting good at things i.e. developing procedural knowledge.\n\nSubtopics may include: Fast feedback loops, Choosing the correct level of challenge, studying people better than you to pick up their techniques or assimilate their style or philosophy, synthesizing advice from many tutorials, instructions, or how-tos"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4Man2iP6ftuTPze9K",
    "name": "Modeling People",
    "core": false,
    "slug": "modeling-people",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Tg9aFPFCPBHxGABRr",
    "name": "Life Improvements",
    "core": false,
    "slug": "life-improvements",
    "tableOfContents": {
      "html": "<p><span class=\"by_SsduPgHwY2zeZpmKT\">Life-hacks, eliminating trivial inconveniences, process improvements, purchases that save you a minute a day, etc</span></p><p><span class=\"by_SsduPgHwY2zeZpmKT\">Found most often on posts under Practical</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 48,
    "description": {
      "markdown": "Life-hacks, eliminating trivial inconveniences, process improvements, purchases that save you a minute a day, etc\n\nFound most often on posts under Practical"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jzd84f2H95DeyHvxE",
    "name": "Prepping",
    "core": false,
    "slug": "prepping",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Prepping </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is the act of actively preparing for emergencies unexpected crisis.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "**Prepping** is the act of actively preparing for emergencies unexpected crisis."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "T57Qd9J3AfxmwhQtY",
    "name": "Meetups & Local Communities (topic)",
    "core": false,
    "slug": "meetups-and-local-communities-topic",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_sKAL2jzfkYkDbQmx9\">The </span><span class=\"by_rPsASnzcvaBdBEaSJ\">rationalist</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> community </span><span class=\"by_rPsASnzcvaBdBEaSJ\">has chapters all over the world, the oldest being the NYC community, which has been around since 2009. Many</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> of these</span><span class=\"by_rPsASnzcvaBdBEaSJ\"> groups are centered around regular meetups, or call themselves 'meetups'.</span></span></p><p><span><span class=\"by_rPsASnzcvaBdBEaSJ\">These</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> are posts about meetups </span><span class=\"by_rPsASnzcvaBdBEaSJ\">and local communities </span><span class=\"by_sKAL2jzfkYkDbQmx9\">in general, not about specific </span><span class=\"by_rPsASnzcvaBdBEaSJ\">communities</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> (unless they also provide general </span><span class=\"by_rPsASnzcvaBdBEaSJ\">insight).</span></span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">For example - </span><a href=\"https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-are-meetups-actually-trying-to-accomplish\"><span class=\"by_sKAL2jzfkYkDbQmx9\">What are meetups actually trying to accomplish?</span></a></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">For specific meetups see </span><a href=\"https://www.lesswrong.com/tag/events-community\"><span class=\"by_HoGziwmhpMGqGeWZy\">Events(community)</span></a></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also - </span><a href=\"https://www.lesswrong.com/tag/community\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Community</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 72,
    "description": {
      "markdown": "The rationalist community has chapters all over the world, the oldest being the NYC community, which has been around since 2009. Many of these groups are centered around regular meetups, or call themselves 'meetups'.\n\nThese are posts about meetups and local communities in general, not about specific communities (unless they also provide general insight).  \n  \nFor example - [What are meetups actually trying to accomplish?](https://www.lesswrong.com/posts/bDnFhJBcLQvCY3vJW/what-are-meetups-actually-trying-to-accomplish)\n\nFor specific meetups see [Events(community)](https://www.lesswrong.com/tag/events-community)\n\nSee also - [Community](https://www.lesswrong.com/tag/community)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aa3Qg7Qrp9LM7QMaz",
    "name": "Definitions",
    "core": false,
    "slug": "definitions",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">Posts that attempt to </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Define </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">or clarify the meaning of a concept, a word, phrase, or something else.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 33,
    "description": {
      "markdown": "Posts that attempt to **Define** or clarify the meaning of a concept, a word, phrase, or something else."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "aAXgXTG7SNwfC5mr5",
    "name": "Weirdness Points",
    "core": false,
    "slug": "weirdness-points",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Weirdness Points</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> posit that the ability to convince people to believe, do, or respect something that sounds weird is a limited resource. </span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Someone with dozens of unconventional beliefs and habits will seem too eccentric to be worth paying attention to, but someone who pushes one weird thing while otherwise being respectable has a shot at convincing the people around them to take it seriously too.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Weirdness Points** posit that the ability to convince people to believe, do, or respect something that sounds weird is a limited resource.\n\nSomeone with dozens of unconventional beliefs and habits will seem too eccentric to be worth paying attention to, but someone who pushes one weird thing while otherwise being respectable has a shot at convincing the people around them to take it seriously too."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "be2Mh2bddQ6ZaBcti",
    "name": "Prisoner's Dilemma",
    "core": false,
    "slug": "prisoner-s-dilemma",
    "tableOfContents": {
      "html": "<p><span class=\"by_RyiDJDCG6R7xyAXzp\">The</span><strong><span><span class=\"by_RyiDJDCG6R7xyAXzp\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner'</span><span class=\"by_RyiDJDCG6R7xyAXzp\">s </span><span class=\"by_HoGziwmhpMGqGeWZy\">Dilemma</span></span></strong><span><span class=\"by_RyiDJDCG6R7xyAXzp\"> is a </span><span class=\"by_HoGziwmhpMGqGeWZy\">well-studied game</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> in </span></span><a href=\"http://lesswrong.com/tag/game-theory\"><span><span class=\"by_RyiDJDCG6R7xyAXzp\">game </span><span class=\"by_cn4SiEmqWbu7K9em5\">theory</span></span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">, where supposedly rational incentive following leads to</span><span class=\"by_mPipmBTniuABY5PQy\"> both players </span><span class=\"by_HoGziwmhpMGqGeWZy\">stabbing</span><span class=\"by_mPipmBTniuABY5PQy\"> each </span><span class=\"by_HoGziwmhpMGqGeWZy\">other in the back</span><span class=\"by_mPipmBTniuABY5PQy\"> and </span><span class=\"by_HoGziwmhpMGqGeWZy\">being worse off</span><span class=\"by_mPipmBTniuABY5PQy\"> than if they </span><span class=\"by_HoGziwmhpMGqGeWZy\">had cooperated.</span></span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The original formulation, via </span><a href=\"https://en.wikipedia.org/wiki/Prisoner%27s_dilemma\"><span class=\"by_HoGziwmhpMGqGeWZy\">Wikipedia</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">:</span></p><blockquote><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">Two members of a criminal gang are arrested and imprisoned. Each prisoner is in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">solitary confinement</span><span class=\"by_HoGziwmhpMGqGeWZy\"> with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict </span><span class=\"by_mPipmBTniuABY5PQy\">both </span><span class=\"by_HoGziwmhpMGqGeWZy\">on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The possible outcomes are:</span></span></p></blockquote><blockquote><p><span><span class=\"by_mPipmBTniuABY5PQy\">If </span><span class=\"by_HoGziwmhpMGqGeWZy\">A and B each betray </span><span class=\"by_mPipmBTniuABY5PQy\">the </span><span class=\"by_HoGziwmhpMGqGeWZy\">other, each of them serves two years in prison</span></span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">If A betrays B but B remains silent, A will be set free and B will serve three years in prison</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">If A remains silent but B betrays A, A will serve three years in prison and B will be set free</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">If A and B both remain silent, both of them will serve only one year in prison (on the lesser charge).</span></p></blockquote><p><span><span class=\"by_RyiDJDCG6R7xyAXzp\">The </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"stay silent\"</span><span class=\"by_HoGziwmhpMGqGeWZy\"> option is generally called </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Cooperate</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_LoykQRMTxJFxwwdPy\"> and the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"betray\"</span><span class=\"by_HoGziwmhpMGqGeWZy\"> option is called </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Defect</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><span class=\"by_HoGziwmhpMGqGeWZy\"> The only Nash Equilibrium of the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner'</span><span class=\"by_LoykQRMTxJFxwwdPy\">s Dilemma </span><span class=\"by_HoGziwmhpMGqGeWZy\">is both players defecting, even though each would prefer the cooperate/cooperate outcome.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Notice that it's only if you treat the other player's decision as completely independent from yours, if the other player defects, then you score higher if you defect as well, whereas if the other player cooperates, you do better by defecting. Hence Nash Equilibrium to defect (at least if the game is to be played only once), and indeed, this is what classical causal decision theory says. And yet—and yet, if only somehow both players could agree to cooperate, they would both do better than if they both defected. If the players are </span><a href=\"https://wiki.lesswrong.com/wiki/timeless_decision_agent\"><span class=\"by_qgdGA4ZEyW7zNdK84\">timeless decision agents</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, or functional decision theory agents, &nbsp;they can.</span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">A popular variant is the Iterated </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s Dilemma, where two agents play the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s Dilemma against each other a number</span><span class=\"by_LoykQRMTxJFxwwdPy\"> of </span><span class=\"by_HoGziwmhpMGqGeWZy\">times in a row. A simple and successful strategy is called Tit for Tat - cooperate on </span><span class=\"by_LoykQRMTxJFxwwdPy\">the </span><span class=\"by_HoGziwmhpMGqGeWZy\">first round, then on subsequent rounds do whatever your opponent did on the last round.</span></span></p><h2 id=\"External_links\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">External links</span></h2><ul><li><a href=\"http://plato.stanford.edu/entries/prisoner-dilemma/\"><u><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner'</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">s dilemma</span></span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (Stanford Encyclopedia of Philosophy)</span></li></ul><h2 id=\"See_also\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">See </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">also</span></span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Game_theory\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Game theory</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Decision_theory\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Decision theory</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Newcomb%27s_problem\"><u><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb'</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">s problem</span></span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Counterfactual_mugging\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Counterfactual mugging</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker\"><u><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Parfit'</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">s hitchhiker</span></span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoking_lesion\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Smoking lesion</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Absentminded driver</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Pascal%27s_mugging\"><u><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal'</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">s mugging</span></span></u></a></li><li><a href=\"https://www.lesswrong.com/tag/coordination-cooperation\"><span class=\"by_HoGziwmhpMGqGeWZy\">Coordination/Cooperation</span></a></li></ul><h2 id=\"References\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">References</span></h2><ul><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Drescher, Gary (2006). </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Good and Real</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Cambridge: The MIT Press. ISBN 0262042339.</span></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 53,
    "description": {
      "markdown": "The **Prisoner's Dilemma** is a well-studied game in [game theory](http://lesswrong.com/tag/game-theory), where supposedly rational incentive following leads to both players stabbing each other in the back and being worse off than if they had cooperated.\n\nThe original formulation, via [Wikipedia](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma):\n\n> Two members of a criminal gang are arrested and imprisoned. Each prisoner is in solitary confinement with no means of communicating with the other. The prosecutors lack sufficient evidence to convict the pair on the principal charge, but they have enough to convict both on a lesser charge. Simultaneously, the prosecutors offer each prisoner a bargain. Each prisoner is given the opportunity either to betray the other by testifying that the other committed the crime, or to cooperate with the other by remaining silent. The possible outcomes are:\n\n> If A and B each betray the other, each of them serves two years in prison\n\n> If A betrays B but B remains silent, A will be set free and B will serve three years in prison\n\n> If A remains silent but B betrays A, A will serve three years in prison and B will be set free\n\n> If A and B both remain silent, both of them will serve only one year in prison (on the lesser charge).\n\nThe \"stay silent\" option is generally called **Cooperate**, and the \"betray\" option is called **Defect**. The only Nash Equilibrium of the Prisoner's Dilemma is both players defecting, even though each would prefer the cooperate/cooperate outcome.\n\nNotice that it's only if you treat the other player's decision as completely independent from yours, if the other player defects, then you score higher if you defect as well, whereas if the other player cooperates, you do better by defecting. Hence Nash Equilibrium to defect (at least if the game is to be played only once), and indeed, this is what classical causal decision theory says. And yet—and yet, if only somehow both players could agree to cooperate, they would both do better than if they both defected. If the players are [timeless decision agents](https://wiki.lesswrong.com/wiki/timeless_decision_agent), or functional decision theory agents,  they can.\n\nA popular variant is the Iterated Prisoner's Dilemma, where two agents play the Prisoner's Dilemma against each other a number of times in a row. A simple and successful strategy is called Tit for Tat - cooperate on the first round, then on subsequent rounds do whatever your opponent did on the last round.\n\nExternal links\n--------------\n\n*   [Prisoner's dilemma](http://plato.stanford.edu/entries/prisoner-dilemma/) (Stanford Encyclopedia of Philosophy)\n\nSee also\n--------\n\n*   [Game theory](https://wiki.lesswrong.com/wiki/Game_theory)\n*   [Decision theory](https://wiki.lesswrong.com/wiki/Decision_theory)\n*   [Newcomb's problem](https://wiki.lesswrong.com/wiki/Newcomb%27s_problem)\n*   [Counterfactual mugging](https://wiki.lesswrong.com/wiki/Counterfactual_mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit%27s_hitchhiker)\n*   [Smoking lesion](https://wiki.lesswrong.com/wiki/Smoking_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Pascal's mugging](https://wiki.lesswrong.com/wiki/Pascal%27s_mugging)\n*   [Coordination/Cooperation](https://www.lesswrong.com/tag/coordination-cooperation)\n\nReferences\n----------\n\n*   Drescher, Gary (2006). *Good and Real*. Cambridge: The MIT Press. ISBN 0262042339."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GY5kPPpCoyt9fnTMn",
    "name": "Computer Science",
    "core": false,
    "slug": "computer-science",
    "tableOfContents": {
      "html": "<p><span class=\"by_p8SHJFHRgZeMuw7qk\">The study of computers and algorithms from both practical and theoretical standpoints. Often considered to be a subset of mathematics, particularly its more theoretical considerations. A thorough understanding of computer science is necessary to understand AI, and indeed, the modern world.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 61,
    "description": {
      "markdown": "The study of computers and algorithms from both practical and theoretical standpoints. Often considered to be a subset of mathematics, particularly its more theoretical considerations. A thorough understanding of computer science is necessary to understand AI, and indeed, the modern world."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "56yXXrcxRjrQs6z9R",
    "name": "Transparency / Interpretability (ML & AI)",
    "core": false,
    "slug": "transparency-interpretability-ml-and-ai",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Transparency and interpretability</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.</span></p><p><span><span class=\"by_KhxbR4grt5KvqvFpq\">Present-</span><span class=\"by_HoGziwmhpMGqGeWZy\">day machine learning systems are typically not very transparent or interpretable. You can use a </span><span class=\"by_KhxbR4grt5KvqvFpq\">model'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s output, but the model </span><span class=\"by_KhxbR4grt5KvqvFpq\">can'</span><span class=\"by_HoGziwmhpMGqGeWZy\">t tell you why it made that output.  This makes it hard to determine the cause of biases in ML models.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 92,
    "description": {
      "markdown": "**Transparency and interpretability** is the ability for the decision processes and inner workings of AI and machine learning systems to be understood by humans or other outside observers.\n\nPresent-day machine learning systems are typically not very transparent or interpretable. You can use a model's output, but the model can't tell you why it made that output. This makes it hard to determine the cause of biases in ML models."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4kQXps8dYsKJgaayN",
    "name": "Careers",
    "core": false,
    "slug": "careers",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">Posts relating to jobs, career development, etc.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 96,
    "description": {
      "markdown": "Posts relating to jobs, career development, etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2eG84YuMhatBpBXPJ",
    "name": "Indexical Information",
    "core": false,
    "slug": "indexical-information",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 1,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wMPYFGmhcFg4bSb4Z",
    "name": "Map and Territory",
    "core": false,
    "slug": "map-and-territory",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">Models of reality are often mistaken for reality itself, and clarifying the distinction is an important rationalist technique.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 50,
    "description": {
      "markdown": "Models of reality are often mistaken for reality itself, and clarifying the distinction is an important rationalist technique."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "N5JGtFnhex2DbyPvy",
    "name": "Privacy",
    "core": false,
    "slug": "privacy",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5oDii8KKW53n4HSx4",
    "name": "Hansonian Pre-Rationality",
    "core": false,
    "slug": "hansonian-pre-rationality",
    "tableOfContents": {
      "html": "<p><span class=\"by_XtphY3uYHwruKqDyG\">In defining </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">Hansonian Pre-Rationality </span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Robin Hanson offers an intriguing&nbsp;argument that, upon learning that our beliefs were created by an irrational process (be it a religious upbringing or a genetic predisposition to paranoid depression), we should update to agree with the alternate version of ourselves who could have had different beliefs. Agents who agree with alternate selves in this way are \"pre-rational\". (NOTE: not to be confused with \"pre-rational\" meaning \"not yet rational\" or \"less than rational\".)</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Suppose you are an AI who was designed by a drunk programmer. Your prior contains an \"optimism\" parameter which broadly skews how you see the world -- set it to -100 and you'd expect world-ending danger around every corner, while +100 would make you expect heaven around every corner. Although your powerful learning algorithm allows you to accurately predict the world, the optimism/pessimism bias never fully goes away: it skews your views about anything you </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">don't</span></i><span class=\"by_Q7NW4XaWQmfPfdcFj\"> know.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Unfortunately for you, your programmer set the parameter randomly, rather than attempting to figure out which setting was most accurate or useful. You know for a fact they just mashed the num pad randomly.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">How should you think about this?</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "In defining **Hansonian Pre-Rationality** Robin Hanson offers an intriguing argument that, upon learning that our beliefs were created by an irrational process (be it a religious upbringing or a genetic predisposition to paranoid depression), we should update to agree with the alternate version of ourselves who could have had different beliefs. Agents who agree with alternate selves in this way are \"pre-rational\". (NOTE: not to be confused with \"pre-rational\" meaning \"not yet rational\" or \"less than rational\".)\n\nSuppose you are an AI who was designed by a drunk programmer. Your prior contains an \"optimism\" parameter which broadly skews how you see the world -- set it to -100 and you'd expect world-ending danger around every corner, while +100 would make you expect heaven around every corner. Although your powerful learning algorithm allows you to accurately predict the world, the optimism/pessimism bias never fully goes away: it skews your views about anything you *don't* know.\n\nUnfortunately for you, your programmer set the parameter randomly, rather than attempting to figure out which setting was most accurate or useful. You know for a fact they just mashed the num pad randomly.\n\nHow should you think about this?"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "frcrRgCk9PDbEScua",
    "name": "Climate Change",
    "core": false,
    "slug": "climate-change",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 34,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "22z6XpWKqw3bNv4oR",
    "name": "Simulation Hypothesis",
    "core": false,
    "slug": "simulation-hypothesis",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">The </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Simulation Hypothesis</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> proposes that conscious beings could be immersed within an artificial Universe embedded within a higher order of reality. The roots of this argument can be found throughout the history of philosophy in such works as Plato's \"</span><a href=\"https://en.wikipedia.org/wiki/The_Allegory_of_the_Cave\"><span class=\"by_HoGziwmhpMGqGeWZy\">Allegory of the Cave</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">\" and Descartes \"</span><a href=\"https://en.wikipedia.org/wiki/Evil_demon\"><span class=\"by_HoGziwmhpMGqGeWZy\">evil demon</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">\".</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The important distinction between these and modern </span><a href=\"https://www.lesswrong.com/tag/simulation-argument\"><span class=\"by_HoGziwmhpMGqGeWZy\">Simulation Arguments</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> has been the addition of proposed methods of engineering Simulated Reality through the use of computers. The modern </span><a href=\"https://www.lesswrong.com/tag/simulation-argument\"><span class=\"by_HoGziwmhpMGqGeWZy\">Simulation Argument</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\"> makes the case that since a civilization will be able to simulate many more ancient civilizations than there were ancient civilizations, it </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is </span><span class=\"by_HoGziwmhpMGqGeWZy\">more likely that we are a been simulated than not. It shows that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the </span><span class=\"by_HoGziwmhpMGqGeWZy\">belief</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> that </span><span class=\"by_HoGziwmhpMGqGeWZy\">there</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is </span><span class=\"by_HoGziwmhpMGqGeWZy\">a significant chance that we will one day become posthumans who run ancestor simulations is false, unless we are currently living in a simulation.</span></span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">John Barrow </span><a href=\"http://www.simulation-argument.com/barrowsim.pdf\"><span class=\"by_HoGziwmhpMGqGeWZy\">has suggested</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\"> that if we are living in</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> a computer </span><span class=\"by_HoGziwmhpMGqGeWZy\">simulation we may observe \"glitches\" in the our programmed environment due to the level of detail being compromised to save computing power. Alternatively, the Simulators may not have a full understanding of the Laws of Nature which would mean over time the simulated environment would drift away from its stable state. These \"glitches\" could be identified by scientists scrutinizing nature using unusual methods of observation. However, </span></span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><a href=\"http://www.simulation-argument.com/simulation.pdf\"><span class=\"by_HoGziwmhpMGqGeWZy\">argues</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> that it is extremely likely that a civilization will have far surpassing computational powers than the ones needed to simulate an ancient civilization in great detail. Moreover, one can argue that due to exponential grow, it's extremely unlikely that the simulators are in the region of progress where they already can simulate an artificial reality but can't simulate it with finer detail. They either can't simulate at all, or have computational powers that far exceed the needed amount.</span></p><h2 id=\"External_links\"><span class=\"by_HoGziwmhpMGqGeWZy\">External links</span></h2><ul><li><span class=\"by_HoGziwmhpMGqGeWZy\">Barrow, John (2008) </span><a href=\"http://www.simulation-argument.com/barrowsim.pdf\"><span class=\"by_HoGziwmhpMGqGeWZy\">Living in a Simulated Universe</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> Universe or Multiverse? ed. Bernard Carr (Cambridge University Press): pp. 481-486.</span></li><li><a href=\"http://hplusmagazine.com/2011/01/18/is-god-an-alien-mathematician/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Is God an Alien Mathematician?</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> — A discussion between Ben Goertzel and Hugo de Garis on Simulated Universes and their Creators</span></li><li><a href=\"http://www.kurzweilai.net/from-cosmism-to-deism\"><span class=\"by_HoGziwmhpMGqGeWZy\">From cosmism to deism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> — Hugo de Garis's essay on Simulated Universes</span></li><li><a href=\"https://en.wikipedia.org/wiki/Allegory_of_the_Cave\"><span class=\"by_HoGziwmhpMGqGeWZy\">The Allegory of the Cave</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> on Wikipedia</span></li><li><a href=\"https://en.wikipedia.org/wiki/Evil_demon\"><span class=\"by_HoGziwmhpMGqGeWZy\">Evil demon</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> on Wikipedia</span></li></ul><h2 id=\"See_also\"><span class=\"by_HoGziwmhpMGqGeWZy\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/simulation-argument\"><span class=\"by_HoGziwmhpMGqGeWZy\">Simulation Argument</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 37,
    "description": {
      "markdown": "The **Simulation Hypothesis** proposes that conscious beings could be immersed within an artificial Universe embedded within a higher order of reality. The roots of this argument can be found throughout the history of philosophy in such works as Plato's \"[Allegory of the Cave](https://en.wikipedia.org/wiki/The_Allegory_of_the_Cave)\" and Descartes \"[evil demon](https://en.wikipedia.org/wiki/Evil_demon)\".\n\nThe important distinction between these and modern [Simulation Arguments](https://www.lesswrong.com/tag/simulation-argument) has been the addition of proposed methods of engineering Simulated Reality through the use of computers. The modern [Simulation Argument](https://www.lesswrong.com/tag/simulation-argument) makes the case that since a civilization will be able to simulate many more ancient civilizations than there were ancient civilizations, it is more likely that we are a been simulated than not. It shows that the belief that there is a significant chance that we will one day become posthumans who run ancestor simulations is false, unless we are currently living in a simulation.\n\nJohn Barrow [has suggested](http://www.simulation-argument.com/barrowsim.pdf) that if we are living in a computer simulation we may observe \"glitches\" in the our programmed environment due to the level of detail being compromised to save computing power. Alternatively, the Simulators may not have a full understanding of the Laws of Nature which would mean over time the simulated environment would drift away from its stable state. These \"glitches\" could be identified by scientists scrutinizing nature using unusual methods of observation. However, [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) [argues](http://www.simulation-argument.com/simulation.pdf) that it is extremely likely that a civilization will have far surpassing computational powers than the ones needed to simulate an ancient civilization in great detail. Moreover, one can argue that due to exponential grow, it's extremely unlikely that the simulators are in the region of progress where they already can simulate an artificial reality but can't simulate it with finer detail. They either can't simulate at all, or have computational powers that far exceed the needed amount.\n\nExternal links\n--------------\n\n*   Barrow, John (2008) [Living in a Simulated Universe](http://www.simulation-argument.com/barrowsim.pdf) Universe or Multiverse? ed. Bernard Carr (Cambridge University Press): pp. 481-486.\n*   [Is God an Alien Mathematician?](http://hplusmagazine.com/2011/01/18/is-god-an-alien-mathematician/) — A discussion between Ben Goertzel and Hugo de Garis on Simulated Universes and their Creators\n*   [From cosmism to deism](http://www.kurzweilai.net/from-cosmism-to-deism) — Hugo de Garis's essay on Simulated Universes\n*   [The Allegory of the Cave](https://en.wikipedia.org/wiki/Allegory_of_the_Cave) on Wikipedia\n*   [Evil demon](https://en.wikipedia.org/wiki/Evil_demon) on Wikipedia\n\nSee also\n--------\n\n*   [Simulation Argument](https://www.lesswrong.com/tag/simulation-argument)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5Whwix4cZ3p5otshm",
    "name": "Habits",
    "core": false,
    "slug": "habits",
    "tableOfContents": {
      "html": "<p><span class=\"by_qmJFRN7jitjPsuF3f\">A </span><strong><span class=\"by_qmJFRN7jitjPsuF3f\">habit</span></strong><span class=\"by_qmJFRN7jitjPsuF3f\"> is a routine of behavior that is repeated regularly and tends to occur subconsciously. Creating and maintaining useful habits is a core rationality practice as is getting rid of unhelpful habits.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 38,
    "description": {
      "markdown": "A **habit** is a routine of behavior that is repeated regularly and tends to occur subconsciously. Creating and maintaining useful habits is a core rationality practice as is getting rid of unhelpful habits."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cPFuhAE7PwoKF7yTj",
    "name": "Inverse Reinforcement Learning",
    "core": false,
    "slug": "inverse-reinforcement-learning",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xXX3n22DQZuKqXEdT",
    "name": "War",
    "core": false,
    "slug": "war",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 77,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EdDGrAxYcrXnKkDca",
    "name": "Distillation & Pedagogy",
    "core": false,
    "slug": "distillation-and-pedagogy",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Distillation</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is the process of taking a complex subject, and making it easier to understand. </span><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Pedagogy </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is the method and practice of teaching. A good intellectual pipeline requires not just discovering new ideas, but making it easier for newcomers to learn them, stand on the shoulders of giants, and discover even more ideas.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Chris Olah, founder of </span><a href=\"https://distill.pub/\"><span class=\"by_r38pkCm7wF4M44MDQ\">distill.pub</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">, writes in his essay </span><a href=\"https://distill.pub/2017/research-debt/\"><span class=\"by_r38pkCm7wF4M44MDQ\">Research Debt</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">:</span></p><blockquote><p><span class=\"by_r38pkCm7wF4M44MDQ\">Programmers talk about technical debt: there are ways to write software that are faster in the short run but problematic in the long run. Managers talk about institutional debt: institutions can grow quickly at the cost of bad practices creeping in. Both are easy to accumulate but hard to get rid of.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Research can also have debt. It comes in several forms:</span></p><ul><li><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Poor Exposition</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> – Often, there is no good explanation of important ideas and one has to struggle to understand them. This problem is so pervasive that we take it for granted and don’t appreciate how much better things could be.</span></li><li><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Undigested Ideas</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> – Most ideas start off rough and hard to understand. They become radically easier as we polish them, developing the right analogies, language, and ways of thinking.</span></li><li><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Bad abstractions and notation</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> – Abstractions and notation are the user interface of research, shaping how we think and communicate. Unfortunately, we often get stuck with the first formalisms to develop even when they’re bad. For example, an object with extra electrons is negative, and pi is wrong.</span></li><li><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Noise</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> – Being a researcher is like standing in the middle of a construction site. Countless papers scream for your attention and there’s no easy way to filter or summarize them.Because most work is explained poorly, it takes a lot of energy to understand each piece of work. For many papers, one wants a simple one sentence explanation of it, but needs to fight with it to get that sentence. Because the simplest way to get the attention of interested parties is to get everyone’s attention, we get flooded with work. Because we incentivize people being “prolific,” we get flooded with a lot of work… We think noise is the main way experts experience research debt.</span></li></ul><p><span class=\"by_r38pkCm7wF4M44MDQ\">The insidious thing about research debt is that it’s normal. Everyone takes it for granted, and doesn’t realize that things could be different. For example, it’s normal to give very mediocre explanations of research, and people perceive that to be the ceiling of explanation quality. On the rare occasions that truly excellent explanations come along, people see them as one-off miracles rather than a sign that we could systematically be doing better.</span></p></blockquote><p><span class=\"by_r38pkCm7wF4M44MDQ\">See also </span><a href=\"lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_r38pkCm7wF4M44MDQ\">Scholarship and Learning</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">, and </span><a href=\"https://www.lesswrong.com/tag/good-explanations-advice\"><span class=\"by_r38pkCm7wF4M44MDQ\">Good Explanations</span></a><span class=\"by_r38pkCm7wF4M44MDQ\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 81,
    "description": {
      "markdown": "**Distillation** is the process of taking a complex subject, and making it easier to understand. **Pedagogy** is the method and practice of teaching. A good intellectual pipeline requires not just discovering new ideas, but making it easier for newcomers to learn them, stand on the shoulders of giants, and discover even more ideas.\n\nChris Olah, founder of [distill.pub](https://distill.pub/), writes in his essay [Research Debt](https://distill.pub/2017/research-debt/):\n\n> Programmers talk about technical debt: there are ways to write software that are faster in the short run but problematic in the long run. Managers talk about institutional debt: institutions can grow quickly at the cost of bad practices creeping in. Both are easy to accumulate but hard to get rid of.\n> \n> Research can also have debt. It comes in several forms:\n> \n> *   **Poor Exposition** – Often, there is no good explanation of important ideas and one has to struggle to understand them. This problem is so pervasive that we take it for granted and don’t appreciate how much better things could be.\n> *   **Undigested Ideas** – Most ideas start off rough and hard to understand. They become radically easier as we polish them, developing the right analogies, language, and ways of thinking.\n> *   **Bad abstractions and notation** – Abstractions and notation are the user interface of research, shaping how we think and communicate. Unfortunately, we often get stuck with the first formalisms to develop even when they’re bad. For example, an object with extra electrons is negative, and pi is wrong.\n> *   **Noise** – Being a researcher is like standing in the middle of a construction site. Countless papers scream for your attention and there’s no easy way to filter or summarize them.Because most work is explained poorly, it takes a lot of energy to understand each piece of work. For many papers, one wants a simple one sentence explanation of it, but needs to fight with it to get that sentence. Because the simplest way to get the attention of interested parties is to get everyone’s attention, we get flooded with work. Because we incentivize people being “prolific,” we get flooded with a lot of work… We think noise is the main way experts experience research debt.\n> \n> The insidious thing about research debt is that it’s normal. Everyone takes it for granted, and doesn’t realize that things could be different. For example, it’s normal to give very mediocre explanations of research, and people perceive that to be the ceiling of explanation quality. On the rare occasions that truly excellent explanations come along, people see them as one-off miracles rather than a sign that we could systematically be doing better.\n\nSee also [Scholarship and Learning](lesswrong.com/tag/scholarship-and-learning), and [Good Explanations](https://www.lesswrong.com/tag/good-explanations-advice)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bmfs4jiLaF6HiiYkC",
    "name": "Reductionism",
    "core": false,
    "slug": "reductionism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Reductionism</span></strong><span class=\"by_qf77EiaoMw7tH3GSr\"> is a disbelief that the higher levels of simplified multilevel models are out there in the </span><a href=\"https://wiki.lesswrong.com/wiki/territory\"><span class=\"by_qf77EiaoMw7tH3GSr\">territory</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, that concepts constructed by mind in themselves play a role in the behavior of reality. This doesn't contradict the notion that the concepts used in simplified multilevel models refer to the actual clusters of configurations of reality.</span></p><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/reductionism-sequence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Reductionism (sequence)</span></a></li><li><a href=\"https://www.lesswrong.com/tag/universal-law\"><span class=\"by_qf77EiaoMw7tH3GSr\">Universal law</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">,</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span></span><a href=\"https://www.lesswrong.com/tag/magic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Magic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\"><span class=\"by_qf77EiaoMw7tH3GSr\">Mind projection fallacy</span></a></li><li><a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\"><span class=\"by_qf77EiaoMw7tH3GSr\">How an algorithm feels</span></a></li><li><a href=\"https://www.lesswrong.com/tag/free-will\"><span class=\"by_qf77EiaoMw7tH3GSr\">Free will</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 32,
    "description": {
      "markdown": "**Reductionism** is a disbelief that the higher levels of simplified multilevel models are out there in the [territory](https://wiki.lesswrong.com/wiki/territory), that concepts constructed by mind in themselves play a role in the behavior of reality. This doesn't contradict the notion that the concepts used in simplified multilevel models refer to the actual clusters of configurations of reality.\n\nSee also\n--------\n\n*   [Reductionism (sequence)](https://www.lesswrong.com/tag/reductionism-sequence)\n*   [Universal law](https://www.lesswrong.com/tag/universal-law), [Magic](https://www.lesswrong.com/tag/magic)\n*   [Mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [How an algorithm feels](https://www.lesswrong.com/tag/how-an-algorithm-feels)\n*   [Free will](https://www.lesswrong.com/tag/free-will)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BAhM42jvzuWMzTDxR",
    "name": "Depression",
    "core": false,
    "slug": "depression",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Depression</span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\"> is a</span><span class=\"by_p8SHJFHRgZeMuw7qk\"> psychological disorder characterized by low mood, loss of interest in life, and poor self-esteem. Both cognitive behavioral therapy (CBT) and antidepressants such as SSRIs have proven to be effective treatments.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "**Depression** is a psychological disorder characterized by low mood, loss of interest in life, and poor self-esteem. Both cognitive behavioral therapy (CBT) and antidepressants such as SSRIs have proven to be effective treatments."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "io2ExA7GEeTgHFTFW",
    "name": "Center on Long-Term Risk (CLR)",
    "core": false,
    "slug": "center-on-long-term-risk-clr",
    "tableOfContents": {
      "html": "<p><span class=\"by_Co2dGXQxHAf92LHea\">The </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">Center on Long-Term Risk</span></strong><span class=\"by_qxJ28GN72aiJu96iF\">, formerly </span><em><span><span class=\"by_Co2dGXQxHAf92LHea\">Foundational Research </span><span class=\"by_qxJ28GN72aiJu96iF\">Institute, </span></span></em><span><span class=\"by_Co2dGXQxHAf92LHea\">is </span><span class=\"by_qxJ28GN72aiJu96iF\">an</span><span class=\"by_Co2dGXQxHAf92LHea\"> </span></span><a href=\"https://www.lesswrong.com/tag/effective-altruism\"><span class=\"by_qxJ28GN72aiJu96iF\">effective altruist</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a research group affiliated with the Swiss/German </span></span><a href=\"https://ea-foundation.org/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Effective Altruism Foundation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. It investigates cooperative strategies to reduce </span><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_qgdGA4ZEyW7zNdK84\">risks of astronomical suffering</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> in humanity's future (s-risks). This includes not only (post-)human suffering, but also the suffering of non-human animals and potential digital sentience. Their research is interdisciplinary, drawing</span><span class=\"by_qxJ28GN72aiJu96iF\"> on </span><span class=\"by_qgdGA4ZEyW7zNdK84\">insights from</span><span class=\"by_Co2dGXQxHAf92LHea\"> </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">artificial intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/anthropic_reasoning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">anthropic reasoning</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, international relations, sociology, philosophy, and other fields.</span></p><h2 id=\"See_also\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span><span class=\"by_qxJ28GN72aiJu96iF\">Suffering </span><span class=\"by_qgdGA4ZEyW7zNdK84\">risk</span></span></a></li><li><a href=\"https://www.lesswrong.com/tag/abolitionism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Abolitionism</span></a></li></ul><h2 id=\"External_links\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></strong></h2><ul><li><a href=\"https://longtermrisk.org/\"><span class=\"by_qxJ28GN72aiJu96iF\">CLR website</span></a></li><li><a href=\"http://archive.is/aZjiv\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Effective Altruism Wiki article on FRI</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 13,
    "description": {
      "markdown": "The **Center on Long-Term Risk**, formerly _Foundational Research Institute,_ is an [effective altruist](https://www.lesswrong.com/tag/effective-altruism) is a research group affiliated with the Swiss/German [Effective Altruism Foundation](https://ea-foundation.org/). It investigates cooperative strategies to reduce [risks of astronomical suffering](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks) in humanity's future (s-risks). This includes not only (post-)human suffering, but also the suffering of non-human animals and potential digital sentience. Their research is interdisciplinary, drawing on insights from [artificial intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence), [anthropic reasoning](https://wiki.lesswrong.com/wiki/anthropic_reasoning), international relations, sociology, philosophy, and other fields.\n\n**See also**\n------------\n\n*   [Suffering risk](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n\n**External links**\n------------------\n\n*   [CLR website](https://longtermrisk.org/)\n*   [Effective Altruism Wiki article on FRI](http://archive.is/aZjiv)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EewHHv3ewvQ3mqbyb",
    "name": "Law-Thinking",
    "core": false,
    "slug": "law-thinking",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">Law-thinking</span></strong><span class=\"by_qxJ28GN72aiJu96iF\"> is an approach in which action and reasoning are thought to have theoretical criteria (laws) specifying the optimal actions and belief adjustments in any given situation. These criteria may be impossible to apply to a situation directly, and one may be forced to use only rough approximations. But one can still evaluate the approximations based on how well they match the optimal criteria.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">The relationship between laws and approximations resembles that of between physics and engineering. Physics specify the laws by which the world works, while engineering tries to find practical solutions as constrained by those laws.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">Some concepts which have been used as theoretical criteria in law-thinking:</span></p><ul><li><a href=\"https://www.lesswrong.com/tag/bayes-theorem?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">Bayes Theorem</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate.</span></li><li><a href=\"https://www.lesswrong.com/tag/decision-theory?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">Decision Theory</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> studies the general laws for choosing between actions in any given situation.</span></li><li><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">Solomonoff Induction</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> is a theoretically optimal way of arriving at true beliefs, though impossible to use directly. </span><a href=\"https://www.lesswrong.com/tag/aixi?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">AIXI</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> is an AI design based on Solomonoff Induction; it is also impossible to build directly, but some approximations exist.</span></li></ul><p><span class=\"by_qxJ28GN72aiJu96iF\">Note that one can make use of e.g. Bayes Theorem or decision theory without being a law-thinker. Thus, articles covering the above topics do not automatically fall under this tag. A \"toolbox-thinker\" may use such tools if that seems warranted, </span><em><span class=\"by_qxJ28GN72aiJu96iF\">without</span></em><span class=\"by_qxJ28GN72aiJu96iF\"> considering them normative standards to compare things against. This difference is discussed in </span><a href=\"https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking\"><span class=\"by_qxJ28GN72aiJu96iF\">Toolbox-thinking and Law-thinking</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Law-thinking** is an approach in which action and reasoning are thought to have theoretical criteria (laws) specifying the optimal actions and belief adjustments in any given situation. These criteria may be impossible to apply to a situation directly, and one may be forced to use only rough approximations. But one can still evaluate the approximations based on how well they match the optimal criteria.\n\nThe relationship between laws and approximations resembles that of between physics and engineering. Physics specify the laws by which the world works, while engineering tries to find practical solutions as constrained by those laws.\n\nSome concepts which have been used as theoretical criteria in law-thinking:\n\n*   [Bayes Theorem](https://www.lesswrong.com/tag/bayes-theorem?useTagName=true) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate.\n*   [Decision Theory](https://www.lesswrong.com/tag/decision-theory?useTagName=true) studies the general laws for choosing between actions in any given situation.\n*   [Solomonoff Induction](https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true) is a theoretically optimal way of arriving at true beliefs, though impossible to use directly. [AIXI](https://www.lesswrong.com/tag/aixi?useTagName=true) is an AI design based on Solomonoff Induction; it is also impossible to build directly, but some approximations exist.\n\nNote that one can make use of e.g. Bayes Theorem or decision theory without being a law-thinker. Thus, articles covering the above topics do not automatically fall under this tag. A \"toolbox-thinker\" may use such tools if that seems warranted, _without_ considering them normative standards to compare things against. This difference is discussed in [Toolbox-thinking and Law-thinking](https://www.lesswrong.com/posts/CPP2uLcaywEokFKQG/toolbox-thinking-and-law-thinking)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jiuackr7B5JAetbF6",
    "name": "Transhumanism",
    "core": false,
    "slug": "transhumanism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HriC2mR9onYvGtwWr\">Transhumanism</span></strong><span><span class=\"by_HriC2mR9onYvGtwWr\"> is the </span><span class=\"by_qxJ28GN72aiJu96iF\">belief or movement in favour of</span><span class=\"by_HriC2mR9onYvGtwWr\"> human </span><span class=\"by_qxJ28GN72aiJu96iF\">enhancement, especially beyond current human limitations and with advanced technology such</span><span class=\"by_HriC2mR9onYvGtwWr\"> as </span><span class=\"by_qxJ28GN72aiJu96iF\">AI, cognitive enhancement, and life extension.</span></span></p><h2 id=\"References\"><span class=\"by_HoGziwmhpMGqGeWZy\">References</span></h2><ul><li><a href=\"http://yudkowsky.net/singularity/simplified\"><span class=\"by_HoGziwmhpMGqGeWZy\">Transhumanism as Simplified Humanism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_HoGziwmhpMGqGeWZy\">Eliezer Yudkowsky</span></a></li><li><span class=\"by_HoGziwmhpMGqGeWZy\">A </span><a href=\"http://www.ted.com/talks/nick_bostrom_on_our_biggest_problems.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">TED talk</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by transhumanist </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\"> on </span><span class=\"by_qgdGA4ZEyW7zNdK84\">humanity'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s biggest problems</span></span></li><li><a href=\"http://www.nickbostrom.com/views/transhumanist.pdf\"><span class=\"by_HoGziwmhpMGqGeWZy\">The Transhumanist FAQ</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (PDF) by </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (</span><a href=\"http://whatistranshumanism.org/\"><span class=\"by_HoGziwmhpMGqGeWZy\">HTML version</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">)</span></li><li><a href=\"http://www.nickbostrom.com/ethics/values.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">Transhumanist Values</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a></li><li><a href=\"http://www.nickbostrom.com/papers/history.pdf\"><span class=\"by_HoGziwmhpMGqGeWZy\">A History of Transhumanist Thought</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (PDF) by </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a></li><li><a href=\"https://web.archive.org/web/20130115205756/http://www.acceleratingfuture.com/michael/blog/2007/09/seven-definitions-of-transhumanism/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Seven Definitions of Transhumanism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by </span><a href=\"https://www.lesswrong.com/tag/michael-anissimov\"><span class=\"by_HoGziwmhpMGqGeWZy\">Michael Anissimov</span></a></li><li><a href=\"https://www.youtube.com/watch?v=bTMS9y8OVuY\"><span class=\"by_HoGziwmhpMGqGeWZy\">PostHuman: An Introduction to Transhumanism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (video)</span></li></ul><h2 id=\"See_Also\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/H+Pedia\"><span class=\"by_HoGziwmhpMGqGeWZy\">H+Pedia</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fun-theory\"><span class=\"by_HoGziwmhpMGqGeWZy\">Fun theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_HoGziwmhpMGqGeWZy\">Metaethics sequence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/mind-uploading\"><span class=\"by_HoGziwmhpMGqGeWZy\">Mind uploading</span></a></li><li><a href=\"https://www.lesswrong.com/tag/cryonics\"><span class=\"by_HoGziwmhpMGqGeWZy\">Cryonics</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_HoGziwmhpMGqGeWZy\">Friendly AI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/abolitionism\"><span class=\"by_HoGziwmhpMGqGeWZy\">Abolitionism</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_HoGziwmhpMGqGeWZy\">External links</span></h2><ul><li><a href=\"https://hpluspedia.org/\"><span class=\"by_HoGziwmhpMGqGeWZy\">H+Pedia, the transhumanist wiki</span></a></li></ul>",
      "sections": [
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 53,
    "description": {
      "markdown": "**Transhumanism** is the belief or movement in favour of human enhancement, especially beyond current human limitations and with advanced technology such as AI, cognitive enhancement, and life extension.\n\nReferences\n----------\n\n*   [Transhumanism as Simplified Humanism](http://yudkowsky.net/singularity/simplified) by [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)\n*   A [TED talk](http://www.ted.com/talks/nick_bostrom_on_our_biggest_problems.html) by transhumanist [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) on humanity's biggest problems\n*   [The Transhumanist FAQ](http://www.nickbostrom.com/views/transhumanist.pdf) (PDF) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) ([HTML version](http://whatistranshumanism.org/))\n*   [Transhumanist Values](http://www.nickbostrom.com/ethics/values.html) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [A History of Transhumanist Thought](http://www.nickbostrom.com/papers/history.pdf) (PDF) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [Seven Definitions of Transhumanism](https://web.archive.org/web/20130115205756/http://www.acceleratingfuture.com/michael/blog/2007/09/seven-definitions-of-transhumanism/) by [Michael Anissimov](https://www.lesswrong.com/tag/michael-anissimov)\n*   [PostHuman: An Introduction to Transhumanism](https://www.youtube.com/watch?v=bTMS9y8OVuY) (video)\n\nSee Also\n--------\n\n*   [H+Pedia](https://wiki.lesswrong.com/wiki/H+Pedia)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Mind uploading](https://www.lesswrong.com/tag/mind-uploading)\n*   [Cryonics](https://www.lesswrong.com/tag/cryonics)\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n\nExternal links\n--------------\n\n*   [H+Pedia, the transhumanist wiki](https://hpluspedia.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rWzGNdjuep56W5u2d",
    "name": "Inside/Outside View",
    "core": false,
    "slug": "inside-outside-view",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">An </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">Inside View </span></strong><span><span class=\"by_qxJ28GN72aiJu96iF\">on a topic involves making predictions based on your understanding of</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">details of the process. An </span></span><strong><span class=\"by_qxJ28GN72aiJu96iF\">Outside View </span></strong><span><span class=\"by_qxJ28GN72aiJu96iF\">involves ignoring these details and</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><span class=\"by_nmk3nLpQE89dMRzzN\">using an estimate based on a class of roughly similar previous </span><span class=\"by_qgdGA4ZEyW7zNdK84\">cases (alternatively, this is called </span></span><a href=\"http://en.wikipedia.org/wiki/Reference_class_forecasting\"><span class=\"by_qgdGA4ZEyW7zNdK84\">reference class forecasting</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span><span class=\"by_sKAL2jzfkYkDbQmx9\">, though it has been </span></span><a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\"><span class=\"by_sKAL2jzfkYkDbQmx9\">pointed out</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> that the possible meaning has expanded beyond that.</span></p><p><span><span class=\"by_nmk3nLpQE89dMRzzN\">For example, </span><span class=\"by_qxJ28GN72aiJu96iF\">someone working on a project may estimate that they can reasonably get 20% of it done per day, so they will get it done in five days (inside view). Or they might consider that all of their previous projects were completed just before</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">deadline, so since the deadline for this</span><span class=\"by_nmk3nLpQE89dMRzzN\"> project </span><span class=\"by_qxJ28GN72aiJu96iF\">is</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> in </span><span class=\"by_qxJ28GN72aiJu96iF\">30 days, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">that'</span><span class=\"by_qxJ28GN72aiJu96iF\">s when it will get done (outside view).</span></span></p><p><span><span class=\"by_rordgt937kcgXxfKP\">The terms were originally developed by Daniel Kahneman and Amos Tversky. An early use</span><span class=\"by_nmk3nLpQE89dMRzzN\"> is </span><span class=\"by_rordgt937kcgXxfKP\">in </span></span><a href=\"http://doi.org/10.1287/mnsc.39.1.17\"><span class=\"by_rordgt937kcgXxfKP\">Timid Choices and Bold Forecasts: A Cognitive Perspective on Risk Taking (Kahneman &amp; Lovallo, 1993)</span></a><span class=\"by_rordgt937kcgXxfKP\"> and the terms were popularised in </span><i><span class=\"by_rordgt937kcgXxfKP\">Thinking, Fast and Slow</span></i><span class=\"by_rordgt937kcgXxfKP\"> (Kahneman, 2011; </span><a href=\"https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/daniel-kahneman-beware-the-inside-view\"><span class=\"by_rordgt937kcgXxfKP\">relevant excerpt</span></a><span><span class=\"by_rordgt937kcgXxfKP\">). </span><span class=\"by_qxJ28GN72aiJu96iF\">The planning example is discussed in </span></span><a href=\"https://www.lesswrong.com/posts/CPm5LTwHrvBJCa9h5/planning-fallacy\"><span class=\"by_qxJ28GN72aiJu96iF\">The Planning Fallacy</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.&nbsp;</span></p><h3 id=\"Examples_of_outside_view\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Examples of outside view</span></h3><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">1.</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> From </span><a href=\"https://www.overcomingbias.com/2007/07/beware-the-insi.html\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Beware</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Inside View</span></span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, by Robin Hanson:</span></p><blockquote><p><span><span class=\"by_qxJ28GN72aiJu96iF\">I did 1500 piece jigsaw puzzle of fireworks, my first jigsaw in at least ten years.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> Several times I had the strong impression that I had carefully eliminated every possible place a piece could go, or every possible piece that could go in a place.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> I was very tempted to conclude that many pieces were missing, or that the box had extra pieces from another puzzle.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> This </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wasn’</span><span class=\"by_qxJ28GN72aiJu96iF\">t impossible </span><span class=\"by_qgdGA4ZEyW7zNdK84\">–</span><span class=\"by_qxJ28GN72aiJu96iF\"> the puzzle was an open box a relative had done before.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> And the alternative seemed humiliating.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></span></p></blockquote><blockquote><p><span><span class=\"by_qxJ28GN72aiJu96iF\">But I allowed a very different part of my mind, using different considerations, to overrule this judgment; so many extra or missing pieces seemed unlikely.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> And in the end there was only one missing and no extra pieces.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> I recall a similar experience when I was learning to program. I would carefully check my program and find no errors, and then when my program </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wouldn’</span><span class=\"by_qxJ28GN72aiJu96iF\">t run I was tempted to suspect compiler or hardware errors.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\"> Of course the problem was almost always my fault.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp;</span></span></p></blockquote><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">2.</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Japanese students expected to finish their essays an average of 10 days before deadline. The average completion time was actually 1 day before deadline. When asked when they'd completed similar, previous tasks, the average reply was 1 day before </span><span class=\"by_sKAL2jzfkYkDbQmx9\">deadline[1].</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">3.</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Students instructed to visualize how, where, and when they would perform their Christmas shopping, expected to finish shopping more than a week before Christmas. A control group asked when they expected their Christmas shopping to be finished, expected it to be done 4 days before Christmas. Both groups finished 3 days before </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Christmas[2].</span></span></p><h3 id=\"Problems_with_the_outside_view\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Problems with the outside view</span></h3><p><span class=\"by_qgdGA4ZEyW7zNdK84\">It is controversial how far the lesson of these experiments can be extended. Robin Hanson argues that this implies that, in futurism, forecasts should be made by trying to find a reference class of similar cases, rather than by trying to visualize outcomes. Eliezer Yudkowsky responds that this leads to \"reference class tennis\" wherein people feel that the same event 'obviously' belongs to two different reference classes, and that the above experiments were performed in cases where the new example was highly similar to past examples. I.e., this year's Christmas shopping optimism and last year's Christmas shopping optimism are much more similar to one another, than the invention of the Internet is to the invention of agriculture. If someone else then feels that the invention of the Internet is more like the category 'recent communications innovations' and should be forecast by reference to television instead of agriculture, both sides pleading the outside view has no resolution except \"I'm taking my reference class and going home!\"</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">More possible limitations and problems with using the outside view are discussed in </span><a href=\"https://www.lesswrong.com/posts/pqoxE3AGMbse68dvb/the-outside-view-s-domain\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">The </span><span class=\"by_ezbRa3dntKWQ5995r\">Outside </span><span class=\"by_sKAL2jzfkYkDbQmx9\">View's Domain</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> and</span><span class=\"by_ezbRa3dntKWQ5995r\"> </span></span><a href=\"https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"Outside View\"</span><span class=\"by_ezbRa3dntKWQ5995r\"> as </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Conversation-Halter</span></span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">. </span><a href=\"https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Model Combination and Adjustment</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> discusses the implications of there usually existing multiple </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">different</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> outside views. </span><a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Taboo \"Outside View\"</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> argues that the meaning of \"Outside View\" have expanded</span><span class=\"by_ezbRa3dntKWQ5995r\"> too </span><span class=\"by_sKAL2jzfkYkDbQmx9\">much, and that it should be </span></span><a href=\"https://www.lesswrong.com/tag/rationalist-taboo\"><span class=\"by_sKAL2jzfkYkDbQmx9\">tabooed</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> and replaced with more precise terminology.</span><span class=\"by_ezbRa3dntKWQ5995r\"> An alternative </span><span class=\"by_sKAL2jzfkYkDbQmx9\">to \"inside/outside view\"</span><span class=\"by_ezbRa3dntKWQ5995r\"> has been proposed in </span></span><a href=\"https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/vKbAWFZRDBhyD6K6A\"><span class=\"by_ezbRa3dntKWQ5995r\">Gears Level &amp; Policy Level</span></a><span class=\"by_ezbRa3dntKWQ5995r\">.</span></p><h2 id=\"External_Posts\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">External</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Posts</span></span></h2><ul><li><a href=\"http://www.overcomingbias.com/2007/07/beware-the-insi.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Beware the Inside View</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/planning-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Planning fallacy</span></a></li><li><a href=\"https://www.lesswrong.com/tag/modest-epistemology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Modest Epistemology</span></a></li><li><a href=\"https://lessestwrong.com/tag/near-far-thinking\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Near/far thinking</span></a></li><li><a href=\"https://lessestwrong.com/tag/connotation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Connotation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/absurdity-heuristic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Absurdity heuristic</span></a></li><li><a href=\"https://lessestwrong.com/tag/arguing-by-analogy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Arguing by analogy</span></a></li><li><a href=\"https://lessestwrong.com/tag/intelligence-explosion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intelligence explosion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/the-hanson-yudkowsky-ai-foom-debate\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Hanson-Yudkowsky AI-Foom Debate</span></a></li></ul><p><span class=\"by_sKAL2jzfkYkDbQmx9\">[1] Buehler, R., Griffin, D., &amp; Ross, M. 2002. Inside the planning fallacy: The causes and consequences of optimistic time predictions. Heuristics and biases: The psychology of intuitive judgment, 250-270. Cambridge, UK: Cambridge University Press.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">[2] Buehler, R., Griffin, D. and Ross, M. 1995. It's about time: Optimistic predictions in work and love. European Review of Social Psychology, Volume 6, eds. W. Stroebe and M. Hewstone. Chichester: John Wiley &amp; Sons.</span></p>",
      "sections": [
        {
          "title": "Examples of outside view",
          "anchor": "Examples_of_outside_view",
          "level": 2
        },
        {
          "title": "Problems with the outside view",
          "anchor": "Problems_with_the_outside_view",
          "level": 2
        },
        {
          "title": "External Posts",
          "anchor": "External_Posts",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 46,
    "description": {
      "markdown": "An **Inside View** on a topic involves making predictions based on your understanding of the details of the process. An **Outside View** involves ignoring these details and using an estimate based on a class of roughly similar previous cases (alternatively, this is called [reference class forecasting](http://en.wikipedia.org/wiki/Reference_class_forecasting)), though it has been [pointed out](https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view) that the possible meaning has expanded beyond that.\n\nFor example, someone working on a project may estimate that they can reasonably get 20% of it done per day, so they will get it done in five days (inside view). Or they might consider that all of their previous projects were completed just before the deadline, so since the deadline for this project is in 30 days, that's when it will get done (outside view).\n\nThe terms were originally developed by Daniel Kahneman and Amos Tversky. An early use is in [Timid Choices and Bold Forecasts: A Cognitive Perspective on Risk Taking (Kahneman & Lovallo, 1993)](http://doi.org/10.1287/mnsc.39.1.17) and the terms were popularised in *Thinking, Fast and Slow* (Kahneman, 2011; [relevant excerpt](https://www.mckinsey.com/business-functions/strategy-and-corporate-finance/our-insights/daniel-kahneman-beware-the-inside-view)). The planning example is discussed in [The Planning Fallacy](https://www.lesswrong.com/posts/CPm5LTwHrvBJCa9h5/planning-fallacy). \n\n### Examples of outside view\n\n**1.** From [Beware the Inside View](https://www.overcomingbias.com/2007/07/beware-the-insi.html), by Robin Hanson:\n\n> I did 1500 piece jigsaw puzzle of fireworks, my first jigsaw in at least ten years.  Several times I had the strong impression that I had carefully eliminated every possible place a piece could go, or every possible piece that could go in a place.  I was very tempted to conclude that many pieces were missing, or that the box had extra pieces from another puzzle.  This wasn’t impossible – the puzzle was an open box a relative had done before.  And the alternative seemed humiliating. \n\n> But I allowed a very different part of my mind, using different considerations, to overrule this judgment; so many extra or missing pieces seemed unlikely.  And in the end there was only one missing and no extra pieces.  I recall a similar experience when I was learning to program. I would carefully check my program and find no errors, and then when my program wouldn’t run I was tempted to suspect compiler or hardware errors.  Of course the problem was almost always my fault.   \n\n**2.** Japanese students expected to finish their essays an average of 10 days before deadline. The average completion time was actually 1 day before deadline. When asked when they'd completed similar, previous tasks, the average reply was 1 day before deadline\\[1\\].\n\n**3.** Students instructed to visualize how, where, and when they would perform their Christmas shopping, expected to finish shopping more than a week before Christmas. A control group asked when they expected their Christmas shopping to be finished, expected it to be done 4 days before Christmas. Both groups finished 3 days before Christmas\\[2\\].\n\n### Problems with the outside view\n\nIt is controversial how far the lesson of these experiments can be extended. Robin Hanson argues that this implies that, in futurism, forecasts should be made by trying to find a reference class of similar cases, rather than by trying to visualize outcomes. Eliezer Yudkowsky responds that this leads to \"reference class tennis\" wherein people feel that the same event 'obviously' belongs to two different reference classes, and that the above experiments were performed in cases where the new example was highly similar to past examples. I.e., this year's Christmas shopping optimism and last year's Christmas shopping optimism are much more similar to one another, than the invention of the Internet is to the invention of agriculture. If someone else then feels that the invention of the Internet is more like the category 'recent communications innovations' and should be forecast by reference to television instead of agriculture, both sides pleading the outside view has no resolution except \"I'm taking my reference class and going home!\"\n\nMore possible limitations and problems with using the outside view are discussed in [The Outside View's Domain](https://www.lesswrong.com/posts/pqoxE3AGMbse68dvb/the-outside-view-s-domain) and [\"Outside View\" as Conversation-Halter](https://www.lesswrong.com/posts/FsfnDfADftGDYeG4c/outside-view-as-conversation-halter). [Model Combination and Adjustment](https://www.lesswrong.com/posts/iyRpsScBa6y4rduEt/model-combination-and-adjustment) discusses the implications of there usually existing multiple *different* outside views. [Taboo \"Outside View\"](https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view) argues that the meaning of \"Outside View\" have expanded too much, and that it should be [tabooed](https://www.lesswrong.com/tag/rationalist-taboo) and replaced with more precise terminology. An alternative to \"inside/outside view\" has been proposed in [Gears Level & Policy Level](https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/vKbAWFZRDBhyD6K6A).\n\nExternal Posts\n--------------\n\n*   [Beware the Inside View](http://www.overcomingbias.com/2007/07/beware-the-insi.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n\nSee Also\n--------\n\n*   [Planning fallacy](https://lessestwrong.com/tag/planning-fallacy)\n*   [Modest Epistemology](https://www.lesswrong.com/tag/modest-epistemology)\n*   [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Connotation](https://lessestwrong.com/tag/connotation), [Absurdity heuristic](https://lessestwrong.com/tag/absurdity-heuristic)\n*   [Arguing by analogy](https://lessestwrong.com/tag/arguing-by-analogy)\n*   [Intelligence explosion](https://lessestwrong.com/tag/intelligence-explosion), [The Hanson-Yudkowsky AI-Foom Debate](https://lessestwrong.com/tag/the-hanson-yudkowsky-ai-foom-debate)\n\n\\[1\\] Buehler, R., Griffin, D., & Ross, M. 2002. Inside the planning fallacy: The causes and consequences of optimistic time predictions. Heuristics and biases: The psychology of intuitive judgment, 250-270. Cambridge, UK: Cambridge University Press.\n\n\\[2\\] Buehler, R., Griffin, D. and Ross, M. 1995. It's about time: Optimistic predictions in work and love. European Review of Social Psychology, Volume 6, eds. W. Stroebe and M. Hewstone. Chichester: John Wiley & Sons."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hrezrpGqXXdSe76ks",
    "name": "Ambition",
    "core": false,
    "slug": "ambition",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Ambition. </span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Because they don't think they could have an impact. Because they were always told ambition was dangerous. To get to the other side. </span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Questions to all these answers and more in the ambition tag!</span></p><blockquote><em><span class=\"by_Xn6ACr6Cua8upALWQ\">Never confess to me that you are just as flawed as I am unless you can tell me what you plan to do about it. Afterward you will still have plenty of flaws left, but that’s not the point; the important thing is to do better, to keep moving ahead, to take one more step forward. Tsuyoku naritai!</span></em></blockquote><p><em><span class=\"by_Xn6ACr6Cua8upALWQ\">-- </span></em><a href=\"https://www.lesswrong.com/posts/DoLQN5ryZ9XkZjq5h/tsuyoku-naritai-i-want-to-become-stronger\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Eliezer Yudkowsky </span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 38,
    "description": {
      "markdown": "**Ambition.** Because they don't think they could have an impact. Because they were always told ambition was dangerous. To get to the other side.\n\nQuestions to all these answers and more in the ambition tag!\n\n> _Never confess to me that you are just as flawed as I am unless you can tell me what you plan to do about it. Afterward you will still have plenty of flaws left, but that’s not the point; the important thing is to do better, to keep moving ahead, to take one more step forward. Tsuyoku naritai!_\n\n_\\-\\-_ [Eliezer Yudkowsky](https://www.lesswrong.com/posts/DoLQN5ryZ9XkZjq5h/tsuyoku-naritai-i-want-to-become-stronger)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Pa2SdZsLFmqhs42Do",
    "name": "Open Problems",
    "core": false,
    "slug": "open-problems",
    "tableOfContents": {
      "html": "<p><a href=\"http://en.wikipedia.org/wiki/Open_problems\"><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Open problems</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">(or open questions) are the things in a field that haven't yet been figured out. </span><span class=\"by_Sp5wM4aRAhNERd4oY\">Gathering</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> them all in the same place </span><span class=\"by_Sp5wM4aRAhNERd4oY\">can help to see</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> on what's the frontier of that </span><span class=\"by_Sp5wM4aRAhNERd4oY\">field or find things to work on.</span></span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also </span><a href=\"https://www.lesswrong.com/tag/hamming-questions\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Hamming Questions</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 22,
    "description": {
      "markdown": "[**Open problems**](http://en.wikipedia.org/wiki/Open_problems) (or open questions) are the things in a field that haven't yet been figured out. Gathering them all in the same place can help to see on what's the frontier of that field or find things to work on.\n\nSee also [Hamming Questions](https://www.lesswrong.com/tag/hamming-questions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zv7v2ziqexSn5iS9v",
    "name": "Group Rationality",
    "core": false,
    "slug": "group-rationality",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">In almost anything, individuals are inferior</span><span class=\"by_LoykQRMTxJFxwwdPy\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">groups. Several articles address this concern regarding rationality, i.e., the topic of </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Group Rationality</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h2 id=\"External_links\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External links</span></h2><ul><li><a href=\"https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Open Problems in</span><span class=\"by_LoykQRMTxJFxwwdPy\"> Group Rationality</span></span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\"> </span><span class=\"by_sKAL2jzfkYkDbQmx9\">is one</span><span class=\"by_LoykQRMTxJFxwwdPy\"> of the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">main articles about</span><span class=\"by_LoykQRMTxJFxwwdPy\"> the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">subject.</span></span></li><li><a href=\"http://ratio.huji.ac.il/dp/dp154.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Individual and Group Behavior in the Ultimatum Game</span></a></li><li><a href=\"http://www.andrew.cmu.edu/user/kzollman/research/Presentations/LRR%20-%20IndividualVsSocial.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Individual vs. Group Rationality in Inquiry</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/rationality-as-martial-art\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality as martial art</span></a></li><li><a href=\"https://www.lesswrong.com/tag/problem-of-verifying-rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Problem of verifying rationality</span></a></li><li><a href=\"/community\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Less Wrong meetup groups</span></a></li><li><a href=\"https://www.lesswrong.com/tag/the-craft-and-the-community\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Craft and the Community</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 74,
    "description": {
      "markdown": "In almost anything, individuals are inferior to groups. Several articles address this concern regarding rationality, i.e., the topic of **Group Rationality**.\n\nExternal links\n--------------\n\n*   [Open Problems in Group Rationality](https://medium.com/@ThingMaker/open-problems-in-group-rationality-5636440a2cd1) is one of the main articles about the subject.\n*   [Individual and Group Behavior in the Ultimatum Game](http://ratio.huji.ac.il/dp/dp154.pdf)\n*   [Individual vs. Group Rationality in Inquiry](http://www.andrew.cmu.edu/user/kzollman/research/Presentations/LRR%20-%20IndividualVsSocial.pdf)\n\nSee also\n--------\n\n*   [Rationality as martial art](https://www.lesswrong.com/tag/rationality-as-martial-art)\n*   [Problem of verifying rationality](https://www.lesswrong.com/tag/problem-of-verifying-rationality)\n*   [Less Wrong meetup groups](/community)\n*   [The Craft and the Community](https://www.lesswrong.com/tag/the-craft-and-the-community)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZWmB62xB6uLyRuAtX",
    "name": "Wiki/Tagging",
    "core": false,
    "slug": "wiki-tagging",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">Posts about Lesswrong's </span><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Wiki/</span><span class=\"by_sKAL2jzfkYkDbQmx9\">Tagging</span></span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> feature</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> or wiki-tag features in general.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "Posts about Lesswrong's **Wiki/Tagging** feature or wiki-tag features in general."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "rfZ6DY88ApBDXFpyW",
    "name": "Human Bodies",
    "core": false,
    "slug": "human-bodies",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 32,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2oWPnnnzMbiAxWfbs",
    "name": "Checklists",
    "core": false,
    "slug": "checklists",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nxwND2hTjBeCy58hi",
    "name": "Developmental Psychology",
    "core": false,
    "slug": "developmental-psychology",
    "tableOfContents": {
      "html": "<p><span class=\"by_gjoi5eBQob27Lww62\">The field of </span><strong><span class=\"by_gjoi5eBQob27Lww62\">developmental psychology</span></strong><span class=\"by_gjoi5eBQob27Lww62\"> focuses on analyzing how human minds change as they grow. Originally this focused on development from birth through childhood and ended at adulthood, but by the 1960s this had expanded to include the study of ways adult continue to develop psychologically.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages: </span></strong><a href=\"https://www.lesswrong.com/tag/parenting\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Parenting</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/education\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Education</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Psychology</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "The field of **developmental psychology** focuses on analyzing how human minds change as they grow. Originally this focused on development from birth through childhood and ended at adulthood, but by the 1960s this had expanded to include the study of ways adult continue to develop psychologically.\n\n**Related pages:** [Parenting](https://www.lesswrong.com/tag/parenting), [Education](https://www.lesswrong.com/tag/education), [Psychology](https://www.lesswrong.com/tag/psychology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "m4zvJHAiGBTjc5ZFt",
    "name": "Babble and Prune",
    "core": false,
    "slug": "babble-and-prune",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zQw5d37qwzdpgQs5P",
    "name": "Cognitive Science",
    "core": false,
    "slug": "cognitive-science",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Cognitive science</span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> draws upon a variety of different disciplines to try to describe and explain the way humans </span><span class=\"by_hEhNLnuatH4rxkpML\">think.</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> It heavily involves </span></span><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">neuroscience</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_Xn6ACr6Cua8upALWQ\">psychology</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, and philosophy. It differs from neuroscience in that it focuses less on relating structure to function, and more on using many approaches to form higher-level models to predict behaviour.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 24,
    "description": {
      "markdown": "**Cognitive science** draws upon a variety of different disciplines to try to describe and explain the way humans think. It heavily involves [neuroscience](https://www.lesswrong.com/tag/neuroscience?showPostCount=true&useTagName=true), [psychology](https://www.lesswrong.com/tag/psychology), and philosophy. It differs from neuroscience in that it focuses less on relating structure to function, and more on using many approaches to form higher-level models to predict behaviour."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6bdeSR7aKmdSQLw6P",
    "name": "Cooking",
    "core": false,
    "slug": "cooking",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 24,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "R6uagTfhhBeejGrrf",
    "name": "Complexity of Value",
    "core": false,
    "slug": "complexity-of-value",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_2YpRin5m5vBJu8Tg9\">Complexity of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">value</span></span></strong><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is the thesis that human values have high </span></span><a href=\"https://wiki.lesswrong.com/wiki/Kolmogorov_complexity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Kolmogorov complexity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">; that our </span><a href=\"https://wiki.lesswrong.com/wiki/preferences\"><span class=\"by_qgdGA4ZEyW7zNdK84\">preferences</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, the things we care about, cannot be summed by a few simple rules, or compressed. </span><strong><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fragility of value</span></a></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the thesis that losing even a small part of the rules that make up our values could lead</span><span class=\"by_HoGziwmhpMGqGeWZy\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">results that most</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">us would now consider as unacceptable (just like dialing nine out of ten phone digits correctly does not connect you to a person 90% similar to your friend). For example, all of our values </span></span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">except</span></em><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> novelty might yield</span><span class=\"by_woC2b5rav5sGrAo3E\"> a future </span><span class=\"by_qgdGA4ZEyW7zNdK84\">full of individuals replaying only one optimal experience through all eternity.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Related: </span><a href=\"https://www.lesswrong.com/tag/metaethics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ethics &amp; Metaethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/fun-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fun Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/preference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Preference</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/wireheading\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wireheading</span></a></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Many human choices can be compressed, by representing them by simple rules - the desire to survive produces innumerable actions and subgoals as we fulfill </span><span class=\"by_HoGziwmhpMGqGeWZy\">that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">desire. But people don't </span></span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">just</span></em><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> want to survive - although you can compress many human activities to that desire, you cannot compress all of human existence into it. The human equivalents of a utility function, our terminal values, contain many different elements that are not strictly reducible to</span><span class=\"by_HoGziwmhpMGqGeWZy\"> one</span><span class=\"by_woC2b5rav5sGrAo3E\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">another.</span><span class=\"by_HoGziwmhpMGqGeWZy\"> William </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Frankena offered </span></span><a href=\"http://plato.stanford.edu/entries/value-intrinsic-extrinsic/#WhaHasIntVal\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">this</span><span class=\"by_nmk3nLpQE89dMRzzN\"> list</span></span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\"> of things </span><span class=\"by_qgdGA4ZEyW7zNdK84\">which many cultures and people seem to</span><span class=\"by_nmk3nLpQE89dMRzzN\"> value </span><span class=\"by_qgdGA4ZEyW7zNdK84\">(for their own sake rather than strictly for their external consequences)</span><span class=\"by_nmk3nLpQE89dMRzzN\">:</span></span></p><blockquote><span><span class=\"by_nmk3nLpQE89dMRzzN\">Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in </span><span class=\"by_HoGziwmhpMGqGeWZy\">one'</span><span class=\"by_nmk3nLpQE89dMRzzN\">s own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc.</span></span></blockquote><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">The \"etc.\" at</span><span class=\"by_woC2b5rav5sGrAo3E\"> the </span><span class=\"by_HoGziwmhpMGqGeWZy\">end is</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_HoGziwmhpMGqGeWZy\">tricky part, because there may be a great many</span><span class=\"by_nmk3nLpQE89dMRzzN\"> values </span><span class=\"by_HoGziwmhpMGqGeWZy\">not included on</span><span class=\"by_KTefAYDBNT64CLRau\"> this </span><span class=\"by_HoGziwmhpMGqGeWZy\">list.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Since natural selection reifies selection pressures as </span><a href=\"https://www.lesswrong.com/tag/adaptation-executors\"><span class=\"by_qgdGA4ZEyW7zNdK84\">psychological drives which then continue to execute</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/lw/yi/the_evolutionarycognitive_boundary/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">independently of any consequentialist reasoning in the organism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> or that organism explicitly representing, let alone caring about, the original evolutionary context, we have no reason to expect these terminal values to be reducible to any one thing, or each other.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Taken in conjunction with another LessWrong claim, that all values are morally relevant, this would suggest that those philosophers who seek to do so are mistaken in trying to find cognitively tractable overarching principles of ethics. However, it is coherent to suppose that not all values are morally relevant, and that the morally relevant ones form a tractable subset.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of value also runs into underappreciation in the presence of bad </span><a href=\"https://www.lesswrong.com/tag/metaethics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">metaethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. The local flavor of metaethics could be characterized as cognitivist, without implying \"thick\" notions of instrumental rationality; in other words, moral discourse can be about a coherent subject matter, without all possible minds and agents necessarily finding truths about that subject matter to be psychologically compelling. An </span><a href=\"https://www.lesswrong.com/tag/paperclip-maximizer\"><span class=\"by_qgdGA4ZEyW7zNdK84\">expected paperclip maximizer</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> doesn't disagree with you about morality any more than you disagree with it about \"which action leads to the greatest number of expected paperclips\", it is just constructed to find the latter subject matter psychologically compelling but not the former. Failure to appreciate that \"But it's just paperclips! What a dumb goal! No sufficiently intelligent agent would pick such a dumb goal!\" is a judgment carried out on a local brain that evaluates paperclips as inherently low-in-the-preference-ordering means that someone will expect all moral judgments to be automatically reproduced in a sufficiently intelligent agent, since, after all, they would not lack the intelligence to see that paperclips are so obviously inherently-low-in-the-preference-ordering. This is a particularly subtle species of </span><a href=\"https://www.lesswrong.com/tag/anthropomorphism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">anthropomorphism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">mind projection fallacy</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Because the human brain very often fails to grasp all these difficulties involving our values, we tend to think building an awesome future is much less problematic than it really is. Fragility of value is relevant for building </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, because an </span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AGI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> which does not respect human values is likely to create a world that we would consider devoid of value - not necessarily full of explicit attempts to be evil, but perhaps just a dull, boring loss.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">As values are orthogonal with intelligence, they can freely vary no matter how intelligent and efficient an AGI is [</span><a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]. Since human / humane values have high Kolmogorov complexity, a random AGI is highly unlikely to maximize human / humane values. The fragility of value thesis implies that a poorly constructed AGI might e.g. turn us into blobs of perpetual orgasm. Because of this relevance the complexity and fragility of value is a major theme of </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">'s writings.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Wrongly designing the future because we wrongly encoded human values is a serious and difficult to assess type of </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Existential risk</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. \"Touch too hard in the wrong dimension, and the physical representation of those values will shatter - </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">and not come back, for there will be nothing left to want to bring it back</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\">. And the referent of those values - a worthwhile universe - would no longer have any physical reason to come into being. Let go of the steering wheel, and the Future crashes.\" [</span><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span></p><h2 id=\"Complexity_of_Value_and_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of Value and AI</span></h2><p><span><span class=\"by_nmk3nLpQE89dMRzzN\">Complexity of value </span><span class=\"by_HoGziwmhpMGqGeWZy\">poses a problem for </span></span><a href=\"http://lesswrong.com/tag/ai\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI alignment</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">. If you can't easily compress what humans </span><span class=\"by_zGEp95GCa3Ew8GJnP\">want</span><span class=\"by_nmk3nLpQE89dMRzzN\"> into </span><span class=\"by_HoGziwmhpMGqGeWZy\">a simple function that</span><span class=\"by_nmk3nLpQE89dMRzzN\"> can be </span><span class=\"by_HoGziwmhpMGqGeWZy\">fed into</span><span class=\"by_nmk3nLpQE89dMRzzN\"> a </span><span class=\"by_HoGziwmhpMGqGeWZy\">computer, it isn't easy to make a powerful AI that does things humans want</span><span class=\"by_nmk3nLpQE89dMRzzN\"> and </span><span class=\"by_HoGziwmhpMGqGeWZy\">doesn't do things humans don't want.</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span></span><a href=\"https://www.lesswrong.com/tag/value-learning\"><span class=\"by_HoGziwmhpMGqGeWZy\">Value Learning</span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\"> attempts to </span><span class=\"by_HoGziwmhpMGqGeWZy\">address</span><span class=\"by_woC2b5rav5sGrAo3E\"> this </span><span class=\"by_HoGziwmhpMGqGeWZy\">problem.</span></span></p><h2 id=\"Major_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Major posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/xy/the_fun_theory_sequence/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Fun Theory Sequence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> describes some of the many complex considerations that determine </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">what sort of happiness</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> we most prefer to have - given that many of us would decline to just have an electrode planted in our pleasure centers.</span></li><li><a href=\"https://www.lesswrong.com/lw/l3/thou_art_godshatter/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Thou Art Godshatter</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> describes the </span><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">evolutionary psychology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> behind the complexity of human values - how they got to be complex, and why, given that origin, there is no reason in hindsight to expect them to be simple. We certainly are not built to </span><a href=\"https://wiki.lesswrong.com/wiki/adaptation_executers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">maximize genetic fitness</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><a href=\"https://www.lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Not for the Sake of Happiness (Alone)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tackles the </span><a href=\"https://www.lesswrong.com/tag/hollywood-rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hollywood Rationality</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> trope that \"rational\" preferences must reduce to selfish hedonism - caring strictly about personally experienced pleasure. An ideal Bayesian agent - implementing strict Bayesian decision theory - can have a utility function that </span><a href=\"https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">ranges over anything, not just internal subjective experiences</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><a href=\"https://www.lesswrong.com/lw/lq/fake_utility_functions/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fake Utility Functions</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> describes the seeming fascination that many have with trying to compress morality down to a single principle. The </span><a href=\"https://www.lesswrong.com/lw/lp/fake_fake_utility_functions/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">sequence leading up</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> to this post tries to explain the cognitive twists whereby people </span><a href=\"https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">smuggle</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> all of their complicated </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">other</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> preferences into their choice of </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">exactly</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> which acts they try to </span><em><a href=\"https://www.lesswrong.com/lw/kq/fake_justification/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">justify using</span></a></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> their single principle; but if they were </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">really</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> following </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">only</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> that single principle, they would </span><a href=\"https://www.lesswrong.com/lw/kz/fake_optimization_criteria/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">choose other acts to justify</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li></ul><h2 id=\"Other_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Other posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/y3/value_is_fragile/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Value is Fragile</span></a></li><li><a href=\"https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Hidden Complexity of Wishes</span></a></li><li><a href=\"https://www.lesswrong.com/lw/ky/fake_morality/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fake Morality</span></a></li><li><a href=\"https://www.lesswrong.com/lw/1o9/welcome_to_heaven/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Welcome to Heaven</span></a></li><li><a href=\"https://www.lesswrong.com/lw/1oj/complexity_of_value_complexity_of_outcome/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of Value ≠ Complexity of Outcome</span></a></li><li><a href=\"https://www.lesswrong.com/lw/65w/not_for_the_sake_of_pleasure_alone/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Not for the Sake of Pleasure Alone</span></a></li><li><a href=\"https://casparoesterheld.com/2017/02/10/a-non-comprehensive-list-of-human-values/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A Non-Comprehensive List of Human Values</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"http://intelligence.org/files/ComplexValues.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complex Value Systems are Required to Realize Valuable Futures</span></a></li><li><a href=\"https://www.lesswrong.com/tag/human-universal\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Human universal</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fake-simplicity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fake simplicity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics sequence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fun-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fun theory</span></a></li><li><a href=\"https://www.lesswrong.com/tag/magical-categories\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Magical categories</span></a></li><li><a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly Artificial Intelligence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/preference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Preference</span></a></li><li><a href=\"https://www.lesswrong.com/tag/wireheading\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wireheading</span></a></li><li><a href=\"https://www.lesswrong.com/tag/the-utility-function-is-not-up-for-grabs\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The utility function is not up for grabs</span></a></li></ul>",
      "sections": [
        {
          "title": "Complexity of Value and AI",
          "anchor": "Complexity_of_Value_and_AI",
          "level": 1
        },
        {
          "title": "Major posts",
          "anchor": "Major_posts",
          "level": 1
        },
        {
          "title": "Other posts",
          "anchor": "Other_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 58,
    "description": {
      "markdown": "**Complexity of value** is the thesis that human values have high [Kolmogorov complexity](https://wiki.lesswrong.com/wiki/Kolmogorov_complexity); that our [preferences](https://wiki.lesswrong.com/wiki/preferences), the things we care about, cannot be summed by a few simple rules, or compressed. **[Fragility of value](https://www.lesswrong.com/lw/y3/value_is_fragile/)** is the thesis that losing even a small part of the rules that make up our values could lead to results that most of us would now consider as unacceptable (just like dialing nine out of ten phone digits correctly does not connect you to a person 90% similar to your friend). For example, all of our values _except_ novelty might yield a future full of individuals replaying only one optimal experience through all eternity.\n\nRelated: [Ethics & Metaethics](https://www.lesswrong.com/tag/metaethics), [Fun Theory](https://www.lesswrong.com/tag/fun-theory), [Preference](https://www.lesswrong.com/tag/preference), [Wireheading](https://www.lesswrong.com/tag/wireheading)\n\nMany human choices can be compressed, by representing them by simple rules - the desire to survive produces innumerable actions and subgoals as we fulfill that desire. But people don't _just_ want to survive - although you can compress many human activities to that desire, you cannot compress all of human existence into it. The human equivalents of a utility function, our terminal values, contain many different elements that are not strictly reducible to one another. William Frankena offered [this list](http://plato.stanford.edu/entries/value-intrinsic-extrinsic/#WhaHasIntVal) of things which many cultures and people seem to value (for their own sake rather than strictly for their external consequences):\n\n> Life, consciousness, and activity; health and strength; pleasures and satisfactions of all or certain kinds; happiness, beatitude, contentment, etc.; truth; knowledge and true opinions of various kinds, understanding, wisdom; beauty, harmony, proportion in objects contemplated; aesthetic experience; morally good dispositions or virtues; mutual affection, love, friendship, cooperation; just distribution of goods and evils; harmony and proportion in one's own life; power and experiences of achievement; self-expression; freedom; peace, security; adventure and novelty; and good reputation, honor, esteem, etc.\n\nThe \"etc.\" at the end is the tricky part, because there may be a great many values not included on this list.\n\nSince natural selection reifies selection pressures as [psychological drives which then continue to execute](https://www.lesswrong.com/tag/adaptation-executors) [independently of any consequentialist reasoning in the organism](https://www.lesswrong.com/lw/yi/the_evolutionarycognitive_boundary/) or that organism explicitly representing, let alone caring about, the original evolutionary context, we have no reason to expect these terminal values to be reducible to any one thing, or each other.\n\nTaken in conjunction with another LessWrong claim, that all values are morally relevant, this would suggest that those philosophers who seek to do so are mistaken in trying to find cognitively tractable overarching principles of ethics. However, it is coherent to suppose that not all values are morally relevant, and that the morally relevant ones form a tractable subset.\n\nComplexity of value also runs into underappreciation in the presence of bad [metaethics](https://www.lesswrong.com/tag/metaethics). The local flavor of metaethics could be characterized as cognitivist, without implying \"thick\" notions of instrumental rationality; in other words, moral discourse can be about a coherent subject matter, without all possible minds and agents necessarily finding truths about that subject matter to be psychologically compelling. An [expected paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) doesn't disagree with you about morality any more than you disagree with it about \"which action leads to the greatest number of expected paperclips\", it is just constructed to find the latter subject matter psychologically compelling but not the former. Failure to appreciate that \"But it's just paperclips! What a dumb goal! No sufficiently intelligent agent would pick such a dumb goal!\" is a judgment carried out on a local brain that evaluates paperclips as inherently low-in-the-preference-ordering means that someone will expect all moral judgments to be automatically reproduced in a sufficiently intelligent agent, since, after all, they would not lack the intelligence to see that paperclips are so obviously inherently-low-in-the-preference-ordering. This is a particularly subtle species of [anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism) and [mind projection fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy).\n\nBecause the human brain very often fails to grasp all these difficulties involving our values, we tend to think building an awesome future is much less problematic than it really is. Fragility of value is relevant for building [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), because an [AGI](https://wiki.lesswrong.com/wiki/AGI) which does not respect human values is likely to create a world that we would consider devoid of value - not necessarily full of explicit attempts to be evil, but perhaps just a dull, boring loss.\n\nAs values are orthogonal with intelligence, they can freely vary no matter how intelligent and efficient an AGI is \\[[1](http://www.nickbostrom.com/superintelligentwill.pdf)\\]. Since human / humane values have high Kolmogorov complexity, a random AGI is highly unlikely to maximize human / humane values. The fragility of value thesis implies that a poorly constructed AGI might e.g. turn us into blobs of perpetual orgasm. Because of this relevance the complexity and fragility of value is a major theme of [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)'s writings.\n\nWrongly designing the future because we wrongly encoded human values is a serious and difficult to assess type of [Existential risk](https://www.lesswrong.com/tag/existential-risk). \"Touch too hard in the wrong dimension, and the physical representation of those values will shatter - _and not come back, for there will be nothing left to want to bring it back_. And the referent of those values - a worthwhile universe - would no longer have any physical reason to come into being. Let go of the steering wheel, and the Future crashes.\" \\[[2](https://www.lesswrong.com/lw/y3/value_is_fragile/)\\]\n\nComplexity of Value and AI\n--------------------------\n\nComplexity of value poses a problem for [AI alignment](http://lesswrong.com/tag/ai). If you can't easily compress what humans want into a simple function that can be fed into a computer, it isn't easy to make a powerful AI that does things humans want and doesn't do things humans don't want. [Value Learning](https://www.lesswrong.com/tag/value-learning) attempts to address this problem.\n\nMajor posts\n-----------\n\n*   [The Fun Theory Sequence](https://www.lesswrong.com/lw/xy/the_fun_theory_sequence/) describes some of the many complex considerations that determine _what sort of happiness_ we most prefer to have - given that many of us would decline to just have an electrode planted in our pleasure centers.\n*   [Thou Art Godshatter](https://www.lesswrong.com/lw/l3/thou_art_godshatter/) describes the [evolutionary psychology](https://www.lesswrong.com/tag/evolutionary-psychology) behind the complexity of human values - how they got to be complex, and why, given that origin, there is no reason in hindsight to expect them to be simple. We certainly are not built to [maximize genetic fitness](https://wiki.lesswrong.com/wiki/adaptation_executers).\n*   [Not for the Sake of Happiness (Alone)](https://www.lesswrong.com/lw/lb/not_for_the_sake_of_happiness_alone/) tackles the [Hollywood Rationality](https://www.lesswrong.com/tag/hollywood-rationality) trope that \"rational\" preferences must reduce to selfish hedonism - caring strictly about personally experienced pleasure. An ideal Bayesian agent - implementing strict Bayesian decision theory - can have a utility function that [ranges over anything, not just internal subjective experiences](https://www.lesswrong.com/lw/l4/terminal_values_and_instrumental_values/).\n*   [Fake Utility Functions](https://www.lesswrong.com/lw/lq/fake_utility_functions/) describes the seeming fascination that many have with trying to compress morality down to a single principle. The [sequence leading up](https://www.lesswrong.com/lw/lp/fake_fake_utility_functions/) to this post tries to explain the cognitive twists whereby people [smuggle](https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/) all of their complicated _other_ preferences into their choice of _exactly_ which acts they try to _[justify using](https://www.lesswrong.com/lw/kq/fake_justification/)_ their single principle; but if they were _really_ following _only_ that single principle, they would [choose other acts to justify](https://www.lesswrong.com/lw/kz/fake_optimization_criteria/).\n\nOther posts\n-----------\n\n*   [Value is Fragile](https://www.lesswrong.com/lw/y3/value_is_fragile/)\n*   [The Hidden Complexity of Wishes](https://www.lesswrong.com/lw/ld/the_hidden_complexity_of_wishes/)\n*   [Fake Morality](https://www.lesswrong.com/lw/ky/fake_morality/)\n*   [Welcome to Heaven](https://www.lesswrong.com/lw/1o9/welcome_to_heaven/)\n*   [Complexity of Value ≠ Complexity of Outcome](https://www.lesswrong.com/lw/1oj/complexity_of_value_complexity_of_outcome/)\n*   [Not for the Sake of Pleasure Alone](https://www.lesswrong.com/lw/65w/not_for_the_sake_of_pleasure_alone/)\n*   [A Non-Comprehensive List of Human Values](https://casparoesterheld.com/2017/02/10/a-non-comprehensive-list-of-human-values/)\n\nSee also\n--------\n\n*   [Complex Value Systems are Required to Realize Valuable Futures](http://intelligence.org/files/ComplexValues.pdf)\n*   [Human universal](https://www.lesswrong.com/tag/human-universal)\n*   [Fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Fun theory](https://www.lesswrong.com/tag/fun-theory)\n*   [Magical categories](https://www.lesswrong.com/tag/magical-categories)\n*   [Friendly Artificial Intelligence](https://www.lesswrong.com/tag/friendly-artificial-intelligence)\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Wireheading](https://www.lesswrong.com/tag/wireheading)\n*   [The utility function is not up for grabs](https://www.lesswrong.com/tag/the-utility-function-is-not-up-for-grabs)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KQP7fNjin8Zqg4N2x",
    "name": "Double-Crux",
    "core": false,
    "slug": "double-crux",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Double-</span><span class=\"by_r38pkCm7wF4M44MDQ\">Crux</span></span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\"> is a technique for </span><span class=\"by_qgdGA4ZEyW7zNdK84\">addressing complex disagreements</span><span class=\"by_r38pkCm7wF4M44MDQ\"> by </span><span class=\"by_qgdGA4ZEyW7zNdK84\">systematically uncovering </span><span class=\"by_r38pkCm7wF4M44MDQ\">the </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">cruxes</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> upon which the disagreement hinges. A crux for an individual is any fact that if they believed differently about it, they would change their conclusion in the overall disagreement. A </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">double-crux </span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a crux for both parties. Perhaps we disagree on whether swimming </span><span class=\"by_HHiJSvTEQkMx8ej62\">in</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> a lake is safe. A crux for each of us is the presence of crocodiles in water: I believe there aren't, you believe there are. Either of us would change our mind about the safety if we were persuaded about this crux.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Double-Crux differs from typical debates which are usually adversarial (your opinion vs mine), and instead attempt to be a collaborative attempt to uncover the true structure of the disagreement and </span><span class=\"by_Nin2paRhwLcHw7tDr\">what</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> would change the disputants minds.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Related: </span><a href=\"https://www.lesswrong.com/tag/disagreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/conversation-topic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Conversation</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A version of the technique is described in </span><a href=\"https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Double Crux – A Strategy for Resolving Disagreement</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> written by (then) CFAR instructor, Duncan_Sabien. The </span><span class=\"by_r38pkCm7wF4M44MDQ\">Center for Applied </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality (CFAR) originated the technique. Eli Tyre, another CFAR instructor who has spent a lot of time developing the technique, more recently shared </span></span><a href=\"https://www.lesswrong.com/posts/hNztRARB52Riy36Kz/the-basic-double-crux-pattern\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Basic Double Crux pattern</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://srconstantin.wordpress.com/2017/08/30/gleanings-from-double-crux-on-the-craft-is-not-the-community/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Gleanings from Double Crux on “The Craft is Not The Community”</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - a writeup of Double-Crux being used in practice.</span></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 23,
    "description": {
      "markdown": "**Double-Crux** is a technique for addressing complex disagreements by systematically uncovering the *cruxes* upon which the disagreement hinges. A crux for an individual is any fact that if they believed differently about it, they would change their conclusion in the overall disagreement. A *double-crux* is a crux for both parties. Perhaps we disagree on whether swimming in a lake is safe. A crux for each of us is the presence of crocodiles in water: I believe there aren't, you believe there are. Either of us would change our mind about the safety if we were persuaded about this crux.\n\nDouble-Crux differs from typical debates which are usually adversarial (your opinion vs mine), and instead attempt to be a collaborative attempt to uncover the true structure of the disagreement and what would change the disputants minds.\n\nRelated: [Disagreement](https://www.lesswrong.com/tag/disagreement), [Conversation](https://www.lesswrong.com/tag/conversation-topic)\n\nA version of the technique is described in [Double Crux – A Strategy for Resolving Disagreement](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement) written by (then) CFAR instructor, Duncan_Sabien. The Center for Applied Rationality (CFAR) originated the technique. Eli Tyre, another CFAR instructor who has spent a lot of time developing the technique, more recently shared [The Basic Double Crux pattern](https://www.lesswrong.com/posts/hNztRARB52Riy36Kz/the-basic-double-crux-pattern).\n\nSee Also\n--------\n\n*   [Gleanings from Double Crux on “The Craft is Not The Community”](https://srconstantin.wordpress.com/2017/08/30/gleanings-from-double-crux-on-the-craft-is-not-the-community/) \\- a writeup of Double-Crux being used in practice."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3RnEKrsNgNEDxuNnw",
    "name": "Updated Beliefs (examples of)",
    "core": false,
    "slug": "updated-beliefs-examples-of",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">Updated Beliefs </span></strong><span class=\"by_qxJ28GN72aiJu96iF\">is a report of general beliefs or perspectives that one has substantially changed their mind about. A post on this may share what one used to think, what the new views are, and what caused the change.  Sharing such accounts publicly is prosocial and allows others to learn from one's experience too. </span></p><p><em><span class=\"by_Xn6ACr6Cua8upALWQ\">See also: </span></em><a href=\"https://www.lesswrong.com/tag/bayes-theorem-bayesianism\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Bayesianism</span></a><em><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span></em><a href=\"https://www.lesswrong.com/tag/changing-your-mind\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Changing Your Mind</span></a></p><p><span class=\"by_qxJ28GN72aiJu96iF\">This tag is specifically for changes in general beliefs about the world – what you believed before, what you believe now, and causes of the change. If a post focuses reporting actions and outcomes together with an evaluation of the choices/thinking patterns used, then it is a good fit for&nbsp;</span><a href=\"https://www.lesswrong.com/tag/postmortems-and-retrospectives\"><span class=\"by_qxJ28GN72aiJu96iF\">Postmortems &amp; Retrospectives</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. Central examples of Updated Beliefs posts are “</span><a href=\"https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right\"><span class=\"by_qxJ28GN72aiJu96iF\">'Other people are wrong' vs 'I am right'</span></a><span class=\"by_qxJ28GN72aiJu96iF\">” and “</span><a href=\"https://www.lesswrong.com/posts/MgFDzAfCku9MSDLuw/six-economics-misconceptions-of-mine-which-i-ve-resolved\"><span class=\"by_qxJ28GN72aiJu96iF\">Six Economics Misconceptions</span></a><span class=\"by_qxJ28GN72aiJu96iF\">”; in contrast, a central example of a Postmortems &amp; Retrospectives post is “</span><a href=\"https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem\"><span class=\"by_qxJ28GN72aiJu96iF\">Arbital Postmortem</span></a><span class=\"by_qxJ28GN72aiJu96iF\">”. </span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 36,
    "description": {
      "markdown": "**Updated Beliefs** is a report of general beliefs or perspectives that one has substantially changed their mind about. A post on this may share what one used to think, what the new views are, and what caused the change. Sharing such accounts publicly is prosocial and allows others to learn from one's experience too.\n\n_See also:_ [Bayesianism](https://www.lesswrong.com/tag/bayes-theorem-bayesianism)_,_ [Changing Your Mind](https://www.lesswrong.com/tag/changing-your-mind)\n\nThis tag is specifically for changes in general beliefs about the world – what you believed before, what you believe now, and causes of the change. If a post focuses reporting actions and outcomes together with an evaluation of the choices/thinking patterns used, then it is a good fit for [Postmortems & Retrospectives](https://www.lesswrong.com/tag/postmortems-and-retrospectives). Central examples of Updated Beliefs posts are “['Other people are wrong' vs 'I am right'](https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right)” and “[Six Economics Misconceptions](https://www.lesswrong.com/posts/MgFDzAfCku9MSDLuw/six-economics-misconceptions-of-mine-which-i-ve-resolved)”; in contrast, a central example of a Postmortems & Retrospectives post is “[Arbital Postmortem](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem)”."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RseFyq6FqAhTycBEY",
    "name": "Ought",
    "core": false,
    "slug": "ought",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Ought</span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\"> is an AI alignment research non-profit focused on the problem of </span><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Factored Cognition</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 16,
    "description": {
      "markdown": "**Ought** is an AI alignment research non-profit focused on the problem of [Factored Cognition](https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&useTagName=true)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "WafcndjLNe4x3uXPL",
    "name": "Center for Human-Compatible AI (CHAI)",
    "core": false,
    "slug": "center-for-human-compatible-ai-chai",
    "tableOfContents": {
      "html": "<p><span class=\"by_EQNTWXLKMeWMp2FQS\">The </span><strong><span><span class=\"by_DgsGzjyBXN8XSK22q\">Center</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> for Human-Compatible AI</span></span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> is a research institute at UC Berkeley lead and founded by Stuart Russell.</span><span class=\"by_MFXaevNCv4GTWRne6\"> Its stated objective is to prevent building </span></span><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_MFXaevNCv4GTWRne6\">unfriendly AI</span></a><span class=\"by_MFXaevNCv4GTWRne6\"> by focusing research on provably beneficial behaviour.</span></p><p><span class=\"by_MFXaevNCv4GTWRne6\">External links:</span></p><ul><li><a href=\"https://humancompatible.ai/\"><span class=\"by_MFXaevNCv4GTWRne6\">Homepage of the Center for Human-Compatible AI</span></a></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "The **Center for Human-Compatible AI** is a research institute at UC Berkeley lead and founded by Stuart Russell. Its stated objective is to prevent building [unfriendly AI](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) by focusing research on provably beneficial behaviour.\n\nExternal links:\n\n*   [Homepage of the Center for Human-Compatible AI](https://humancompatible.ai/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "K6oowPZC6kds6LDTg",
    "name": "Future of Humanity Institute (FHI)",
    "core": false,
    "slug": "future-of-humanity-institute-fhi",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">The </span><strong><span class=\"by_5wu9jG4pm9q6xjZ9R\">Future of Humanity Institute</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is </span><span class=\"by_HoGziwmhpMGqGeWZy\">part of the Faculty of Philosophy and the Oxford Martin School at the University of Oxford. Founded in 2005, its director is </span></span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">. The mission of FHI is described on their website:</span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">FHI puts together </span><span class=\"by_EQNTWXLKMeWMp2FQS\">a </span><span class=\"by_HoGziwmhpMGqGeWZy\">wide range of researches, prominent on their original fields, which decided to focus on global questions about the progress</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><span class=\"by_HoGziwmhpMGqGeWZy\">future of humanity, e.g.:</span></span></p><ul><li><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Nick </span><span class=\"by_HoGziwmhpMGqGeWZy\">Bostrom</span></span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> : Director, philosopher, has more than 200 publications on subjects such as: </span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_HoGziwmhpMGqGeWZy\">Artificial General Intelligence</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> (AGI) Risks, </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">Existential risk</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement\"><span class=\"by_HoGziwmhpMGqGeWZy\">Biological Cognitive Enhancement</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> and </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_HoGziwmhpMGqGeWZy\">Whole brain emulation</span></a></li><li><a href=\"http://en.wikipedia.org/wiki/Anders_Sandberg\"><span class=\"by_HoGziwmhpMGqGeWZy\">Anders Sandberg</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">: Research Fellow, computational neuroscientist, researches human enhancement and ethics of new technologies</span></li><li><a href=\"http://en.wikipedia.org/wiki/Robin_Hanson\"><span class=\"by_HoGziwmhpMGqGeWZy\">Robin Hanson</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">: Research Associate, economist, interested in prediction market given the future of technology and many other questions involving </span><a href=\"http://wiki.lesswrong.com/wiki/Bayesian\"><span class=\"by_HoGziwmhpMGqGeWZy\">bayesianism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"http://wiki.lesswrong.com/wiki/Bias\"><span class=\"by_HoGziwmhpMGqGeWZy\">cognitive biases</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, technology, policies and the </span><a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\"><span class=\"by_HoGziwmhpMGqGeWZy\">Fermi Paradox</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></li><li><a href=\"http://en.wikipedia.org/wiki/Toby_Ord\"><span class=\"by_HoGziwmhpMGqGeWZy\">Toby Ord</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">: Research Associate, philosopher, researches decision making and theoretical and pratical ethics. Founder of </span><a href=\"http://www.givingwhatwecan.org/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Giving What We Can</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, an international society dedicated to the elimination of poverty;</span></li><li><a href=\"http://mcirkovic.aob.rs/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Milan Cirkovic</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">: Research Associate, astrophysicist, interested in the anthropic principle and the </span><a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\"><span class=\"by_HoGziwmhpMGqGeWZy\">Fermi Paradox Fermi Paradox</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></li></ul><p><span class=\"by_HoGziwmhpMGqGeWZy\">The FHI is an affiliate to LessWrong and </span><a href=\"http://www.overcomingbias.com/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Overcoming Bias</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">. Their past activities include holding a conference in 2008 titled </span><a href=\"http://www.global-catastrophic-risks.com/aboutconf.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">Global Catastrophic Risks Conference</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> and publishing a book, also titled </span><a href=\"http://www.global-catastrophic-risks.com/book.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">Global Catastrophic Risks</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><h2 id=\"See_also\"><span class=\"by_HoGziwmhpMGqGeWZy\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_HoGziwmhpMGqGeWZy\">Nick Bostrom</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">Existential risk</span></a></li><li><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span class=\"by_HoGziwmhpMGqGeWZy\">Machine Intelligence Research Institute</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_HoGziwmhpMGqGeWZy\">External links</span></h2><ul><li><a href=\"http://www.fhi.ox.ac.uk/\"><span class=\"by_HoGziwmhpMGqGeWZy\">FHI website</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 28,
    "description": {
      "markdown": "The **Future of Humanity Institute** is part of the Faculty of Philosophy and the Oxford Martin School at the University of Oxford. Founded in 2005, its director is [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom). The mission of FHI is described on their website:\n\nFHI puts together a wide range of researches, prominent on their original fields, which decided to focus on global questions about the progress and future of humanity, e.g.:\n\n*   [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) : Director, philosopher, has more than 200 publications on subjects such as: [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) (AGI) Risks, [Existential risk](https://www.lesswrong.com/tag/existential-risk), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement) and [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation)\n*   [Anders Sandberg](http://en.wikipedia.org/wiki/Anders_Sandberg): Research Fellow, computational neuroscientist, researches human enhancement and ethics of new technologies\n*   [Robin Hanson](http://en.wikipedia.org/wiki/Robin_Hanson): Research Associate, economist, interested in prediction market given the future of technology and many other questions involving [bayesianism](http://wiki.lesswrong.com/wiki/Bayesian), [cognitive biases](http://wiki.lesswrong.com/wiki/Bias), technology, policies and the [Fermi Paradox](http://en.wikipedia.org/wiki/Fermi_paradox).\n*   [Toby Ord](http://en.wikipedia.org/wiki/Toby_Ord): Research Associate, philosopher, researches decision making and theoretical and pratical ethics. Founder of [Giving What We Can](http://www.givingwhatwecan.org/), an international society dedicated to the elimination of poverty;\n*   [Milan Cirkovic](http://mcirkovic.aob.rs/): Research Associate, astrophysicist, interested in the anthropic principle and the [Fermi Paradox Fermi Paradox](http://en.wikipedia.org/wiki/Fermi_paradox).\n\nThe FHI is an affiliate to LessWrong and [Overcoming Bias](http://www.overcomingbias.com/). Their past activities include holding a conference in 2008 titled [Global Catastrophic Risks Conference](http://www.global-catastrophic-risks.com/aboutconf.html) and publishing a book, also titled [Global Catastrophic Risks](http://www.global-catastrophic-risks.com/book.html).\n\nSee also\n--------\n\n*   [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri)\n\nExternal links\n--------------\n\n*   [FHI website](http://www.fhi.ox.ac.uk/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "chuP2QqQycjD8qakL",
    "name": "Coordination / Cooperation",
    "core": false,
    "slug": "coordination-cooperation",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Coordination</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the challenge of distinct actors being able to jointly choose their actions to achieve a favorable outcome. An example of easy coordination is coordinating to drive on the same side of the road. A hard example is coordinating everyone to move a superior yet different social media platform. Many failures of civilization are failures of coordination. A closely related concept is that of </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">cooperation </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">– multiple actors choosing their actions in ways that maximize collective value despite the temptation of greater short-term individual gain by acting to the detriment of the group/other actors. The Prisoner's Dilemma is the canonical cooperation/defection game, but the term is used in other games too, e.g. Stag Hunt.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Coordination and cooperation are fundamental concepts in </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. LessWrong discussion has long been interested in overcoming the gnarly coordination and cooperation challenges that prevent many improvements to </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">optimizing the world</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. There is also interest because failures to coordinate/cooperate are likely causes of </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">existential risks</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> such as </span><a href=\"https://www.lesswrong.com/tag/ai-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Risk</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> or nuclear war.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See also: </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/moloch\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Another related term in this space is </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">collective action problem.&nbsp;</span></i></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Kickstarter for Coordinated Action</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 142,
    "description": {
      "markdown": "**Coordination** is the challenge of distinct actors being able to jointly choose their actions to achieve a favorable outcome. An example of easy coordination is coordinating to drive on the same side of the road. A hard example is coordinating everyone to move a superior yet different social media platform. Many failures of civilization are failures of coordination. A closely related concept is that of **cooperation** – multiple actors choosing their actions in ways that maximize collective value despite the temptation of greater short-term individual gain by acting to the detriment of the group/other actors. The Prisoner's Dilemma is the canonical cooperation/defection game, but the term is used in other games too, e.g. Stag Hunt.\n\nCoordination and cooperation are fundamental concepts in [Game Theory](https://www.lesswrong.com/tag/game-theory). LessWrong discussion has long been interested in overcoming the gnarly coordination and cooperation challenges that prevent many improvements to [optimizing the world](https://www.lesswrong.com/tag/world-optimization). There is also interest because failures to coordinate/cooperate are likely causes of [existential risks](https://www.lesswrong.com/tag/existential-risk) such as [AI Risk](https://www.lesswrong.com/tag/ai-risk) or nuclear war.\n\nSee also: [Game Theory](https://www.lesswrong.com/tag/game-theory), [Moloch](https://www.lesswrong.com/tag/moloch)\n\nAnother related term in this space is *collective action problem. *\n\n**Related Sequences:** [Kickstarter for Coordinated Action](https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EXgFbrqoRRkCRgnDy",
    "name": "Online Socialization",
    "core": false,
    "slug": "online-socialization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Online Socialization</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is, among other things, something you might have to do a lot of if there's a worldwide pandemic.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 30,
    "description": {
      "markdown": "**Online Socialization** is, among other things, something you might have to do a lot of if there's a worldwide pandemic."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GBpwq8cWvaeRoE9X5",
    "name": "Fiction (Topic)",
    "core": false,
    "slug": "fiction-topic",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">This tag collects post which discuss </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">fiction</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> at the meta-level. For a list of actual fiction content, see the </span><a href=\"https://www.lesswrong.com/tag/fiction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fiction tag</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> For posts about HPMOR, see </span></span><a href=\"https://www.lesswrong.com/tag/hpmor-discussion-and-meta\"><span class=\"by_sKAL2jzfkYkDbQmx9\">HPMOR (discussion &amp; meta)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">. For post specifically about writing, see </span><a href=\"https://www.lesswrong.com/tag/writing-communication-method\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Writing (communication method)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">“Nonfiction conveys </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">knowledge,</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> fiction conveys </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">experience.</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">” - Eliezer Yudkowsky</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 121,
    "description": {
      "markdown": "This tag collects post which discuss **fiction** at the meta-level. For a list of actual fiction content, see the [Fiction tag](https://www.lesswrong.com/tag/fiction). For posts about HPMOR, see [HPMOR (discussion & meta)](https://www.lesswrong.com/tag/hpmor-discussion-and-meta). For post specifically about writing, see [Writing (communication method)](https://www.lesswrong.com/tag/writing-communication-method).\n\n> “Nonfiction conveys *knowledge,* fiction conveys *experience.*” \\- Eliezer Yudkowsky"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "moeYqrcakMgXnQNyF",
    "name": "Curiosity",
    "core": false,
    "slug": "curiosity",
    "tableOfContents": {
      "html": "<blockquote><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">The first virtue is </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">curiosity</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span><span class=\"by_LoykQRMTxJFxwwdPy\">A </span><span class=\"by_Xn6ACr6Cua8upALWQ\">burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch</span><span class=\"by_LoykQRMTxJFxwwdPy\"> of </span><span class=\"by_Xn6ACr6Cua8upALWQ\">curiosity requires both that you be ignorant,</span><span class=\"by_LoykQRMTxJFxwwdPy\"> and </span><span class=\"by_Xn6ACr6Cua8upALWQ\">that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance. There is a time to confess your ignorance and a time to relinquish your ignorance.</span></span></i></p></blockquote><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">-- </span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">Eliezer Yudkowsky, </span><a href=\"https://yudkowsky.net/rational/virtues/#:~:text=These%20then%20are%20twelve%20virtues,%2C%20scholarship%2C%20and%20the%20void.\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">The </span><span class=\"by_Xn6ACr6Cua8upALWQ\">Twelve Virtues</span><span class=\"by_LoykQRMTxJFxwwdPy\"> of </span><span class=\"by_Xn6ACr6Cua8upALWQ\">Rationality</span></span></a></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/seeing-with-fresh-eyes\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Seeing with Fresh Eyes</span></a></li><li><a href=\"https://www.lesswrong.com/tag/semantic-stopsign\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Semantic stopsign</span></a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Litany of Tarski</span></a></li><li><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship</span></a></li><li><a href=\"https://www.lesswrong.com/tag/doubt\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Doubt</span></a></li></ul><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A Fable of Science and Politics</span></a></li><li><a href=\"https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Meditation on Curiosity</span></a></li><li><a href=\"https://www.lesswrong.com/lw/4ku/use_curiosity/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Use Curiosity</span></a></li><li><a href=\"https://www.lesswrong.com/lw/96j/what_curiosity_looks_like/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">What Curiosity Looks Like</span></a></li><li><a href=\"https://www.lesswrong.com/lw/9m2/the_neglected_virtue_of_curiosity/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Neglected Virtue of Curiosity</span></a></li><li><a href=\"https://www.lesswrong.com/lw/aa7/get_curious/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Get Curious</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 31,
    "description": {
      "markdown": "> *The first virtue is **curiosity**. A burning itch to know is higher than a solemn vow to pursue truth. To feel the burning itch of curiosity requires both that you be ignorant, and that you desire to relinquish your ignorance. If in your heart you believe you already know, or if in your heart you do not wish to know, then your questioning will be purposeless and your skills without direction. Curiosity seeks to annihilate itself; there is no curiosity that does not want an answer. The glory of glorious mystery is to be solved, after which it ceases to be mystery. Be wary of those who speak of being open-minded and modestly confess their ignorance. There is a time to confess your ignorance and a time to relinquish your ignorance.*\n\n*\\-\\-* Eliezer Yudkowsky, [The Twelve Virtues of Rationality](https://yudkowsky.net/rational/virtues/#:~:text=These%20then%20are%20twelve%20virtues,%2C%20scholarship%2C%20and%20the%20void.)\n\nSee Also\n--------\n\n*   [Seeing with Fresh Eyes](https://www.lesswrong.com/tag/seeing-with-fresh-eyes)\n*   [Semantic stopsign](https://www.lesswrong.com/tag/semantic-stopsign)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski)\n*   [Scholarship](https://www.lesswrong.com/tag/scholarship-and-learning)\n*   [Doubt](https://www.lesswrong.com/tag/doubt)\n\nNotable Posts\n-------------\n\n*   [A Fable of Science and Politics](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/)\n*   [The Meditation on Curiosity](https://www.lesswrong.com/lw/jz/the_meditation_on_curiosity/)\n*   [Use Curiosity](https://www.lesswrong.com/lw/4ku/use_curiosity/)\n*   [What Curiosity Looks Like](https://www.lesswrong.com/lw/96j/what_curiosity_looks_like/)\n*   [The Neglected Virtue of Curiosity](https://www.lesswrong.com/lw/9m2/the_neglected_virtue_of_curiosity/)\n*   [Get Curious](https://www.lesswrong.com/lw/aa7/get_curious/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KoXbd2HmbdRfqLngk",
    "name": "Planning & Decision-Making",
    "core": false,
    "slug": "planning-and-decision-making",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 79,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tk4R4LrX88gmFeMmY",
    "name": "Experiments",
    "core": false,
    "slug": "experiments",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b8nerwC3Dp2Q9MvbG",
    "name": "Corrigibility",
    "core": false,
    "slug": "corrigibility",
    "tableOfContents": {
      "html": "<p><span class=\"by_EQNTWXLKMeWMp2FQS\">A </span><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">corrigible</span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\"> agent is one that doesn't interfere with what we would intuitively see as attempts to 'correct' the agent, or 'correct' our mistakes in building it; and permits these 'corrections' despite the apparent instrumentally convergent reasoning saying otherwise.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 57,
    "description": {
      "markdown": "A **corrigible** agent is one that doesn't interfere with what we would intuitively see as attempts to 'correct' the agent, or 'correct' our mistakes in building it; and permits these 'corrections' despite the apparent instrumentally convergent reasoning saying otherwise."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "d2s5yH2MNwX4pveW7",
    "name": "Humans Consulting HCH",
    "core": false,
    "slug": "humans-consulting-hch",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Humans Consulting HCH (HCH) </span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\">is a recursive acronym describing a setup where humans can consult simulations of themselves to help answer questions. It is a concept used in discussion of the </span><a href=\"https://www.lesswrong.com/tag/iterated-amplification\"><span class=\"by_EQNTWXLKMeWMp2FQS\">iterated amplification</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\"> proposal to solve the alignment problem.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">It was first described by Paul Christiano in his post </span><a href=\"https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Humans Consulting HCH</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">:</span></p><blockquote><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">if</span></i><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">Hugh had access to the question-answering machine</span></i><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh…</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Let’s call this process HCH, for “Humans Consulting HCH.”</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "**Humans Consulting HCH (HCH)** is a recursive acronym describing a setup where humans can consult simulations of themselves to help answer questions. It is a concept used in discussion of the [iterated amplification](https://www.lesswrong.com/tag/iterated-amplification) proposal to solve the alignment problem.\n\nIt was first described by Paul Christiano in his post [Humans Consulting HCH](https://www.lesswrong.com/posts/NXqs4nYXaq8q6dTTx/humans-consulting-hch):\n\n> Consider a human Hugh who has access to a question-answering machine. Suppose the machine answers question Q by perfectly imitating how Hugh would answer question Q, *if* *Hugh had access to the question-answering machine*.\n> \n> That is, Hugh is able to consult a copy of Hugh, who is able to consult a copy of Hugh, who is able to consult a copy of Hugh…\n> \n> Let’s call this process HCH, for “Humans Consulting HCH.”"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BisjoDrd3oNatDu7X",
    "name": "Outer Alignment",
    "core": false,
    "slug": "outer-alignment",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Outer Alignment</span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> in the context of machine learning is the property where the specified loss function is aligned with the intended goal of its designers. This is an intuitive notion, in part because human intentions are themselves not well-understood. This is what is typically discussed as the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">'value alignment'</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> problem. It is contrasted with </span></span><a href=\"https://www.lesswrong.com/tag/inner-alignment\"><span class=\"by_EQNTWXLKMeWMp2FQS\">inner alignment</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, which discusses if an optimizer is the production of an outer aligned system, then whether that optimizer is itself aligned.</span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">See also: </span></em></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 99,
    "description": {
      "markdown": "**Outer Alignment** in the context of machine learning is the property where the specified loss function is aligned with the intended goal of its designers. This is an intuitive notion, in part because human intentions are themselves not well-understood. This is what is typically discussed as the 'value alignment' problem. It is contrasted with [inner alignment](https://www.lesswrong.com/tag/inner-alignment), which discusses if an optimizer is the production of an outer aligned system, then whether that optimizer is itself aligned._See also:_"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Dw5Z6wtTgk4Fikz9f",
    "name": "Inner Alignment",
    "core": false,
    "slug": "inner-alignment",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Inner Alignment </span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\">is the problem of ensuring </span><a href=\"https://www.lesswrong.com/tag/mesa-optimization\"><span class=\"by_EQNTWXLKMeWMp2FQS\">mesa-optimizers</span></a><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> (i.e. when a trained ML system is itself an </span><span class=\"by_iBcH2a3HdWGS2JEZA\">optimizer)</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><span class=\"by_2HL96yNHSLfzYbncR\">are</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> aligned with the objective </span><span class=\"by_iBcH2a3HdWGS2JEZA\">function</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> of the training process. As an example, evolution is an optimization force that itself </span><span class=\"by_3AAZ8hRwoJvfkozKD\">'designed'</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> optimizers (humans) to achieve its goals. However, humans do not primarily maximise reproductive success, they instead use birth control and then go out and have fun. This is a failure of inner alignment.&nbsp;</span></span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">The term was first given a definition in the Hubinger et al paper </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">Risk from Learned Optimization</span></i><span class=\"by_EQNTWXLKMeWMp2FQS\">:</span></p><blockquote><p><span class=\"by_EQNTWXLKMeWMp2FQS\">We refer to this problem of aligning mesa-optimizers with the base objective as the inner alignment problem. This is distinct from the outer alignment problem, which is the traditional problem of ensuring that the base objective captures the intended goal of the programmers.</span></p></blockquote><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/mesa-optimization\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mesa-Optimization</span></a></p><h2 id=\"External_Links_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></h2><p><a href=\"https://www.youtube.com/watch?v=bJLcIBixGj8\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Video by Robert Miles</span></a></p>",
      "sections": [
        {
          "title": "External Links:",
          "anchor": "External_Links_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 114,
    "description": {
      "markdown": "**Inner Alignment** is the problem of ensuring [mesa-optimizers](https://www.lesswrong.com/tag/mesa-optimization) (i.e. when a trained ML system is itself an optimizer) are aligned with the objective function of the training process. As an example, evolution is an optimization force that itself 'designed' optimizers (humans) to achieve its goals. However, humans do not primarily maximise reproductive success, they instead use birth control and then go out and have fun. This is a failure of inner alignment. \n\nThe term was first given a definition in the Hubinger et al paper *Risk from Learned Optimization*:\n\n> We refer to this problem of aligning mesa-optimizers with the base objective as the inner alignment problem. This is distinct from the outer alignment problem, which is the traditional problem of ensuring that the base objective captures the intended goal of the programmers.\n\n**Related Pages:** [Mesa-Optimization](https://www.lesswrong.com/tag/mesa-optimization)\n\nExternal Links:\n---------------\n\n[Video by Robert Miles](https://www.youtube.com/watch?v=bJLcIBixGj8)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8ckoduMw3gvCMJGSB",
    "name": "Logical Induction",
    "core": false,
    "slug": "logical-induction",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Logical Induction </span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\">is </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">a formal theory of reasoning under </span></span><a href=\"https://www.lesswrong.com/tag/logical-uncertainty\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">logical</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> uncertainty</span></span></a><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">, developed by</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> Scott Garrabrant and other </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">researchers. Rationality is defined through a prediction-market analogy. High-quality beliefs are those which are computationally difficult</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> to </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">win bets against. The</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> writeup can be found </span></span><a href=\"https://intelligence.org/2016/09/12/new-paper-logical-induction/\"><span class=\"by_EQNTWXLKMeWMp2FQS\">here</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 28,
    "description": {
      "markdown": "**Logical Induction** is a formal theory of reasoning under [logical uncertainty](https://www.lesswrong.com/tag/logical-uncertainty), developed by Scott Garrabrant and other researchers. Rationality is defined through a prediction-market analogy. High-quality beliefs are those which are computationally difficult to win bets against. The writeup can be found [here](https://intelligence.org/2016/09/12/new-paper-logical-induction/)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nvKzwpiranwy29HFJ",
    "name": "Optimization",
    "core": false,
    "slug": "optimization",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">An </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">optimization process</span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">any kind of process that systematically comes up with solutions that are better than the solution used before. More technically, this kind of process moves</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> the world into</span><span class=\"by_mPipmBTniuABY5PQy\"> a </span><span class=\"by_EQNTWXLKMeWMp2FQS\">specific and unexpected set</span><span class=\"by_mPipmBTniuABY5PQy\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">states by searching through a large search space, hitting small and low probability targets. When this process is gradually guided by some agent into some specific state, through searching specific targets, we can say it </span></span><a href=\"https://www.lesswrong.com/tag/preference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">prefers</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> that state.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The best way to exemplify an optimization process is through a simple example: </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> suggests natural selection is such a process. Through an implicit preference – better replicators – natural selection searches all the genetic landscape space and hit small targets: efficient mutations.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Consider the human being. We are a highly complex object with a low probability to have been created by chance - natural selection, however, over millions of years, built up the infrastructure needed to build such a functioning body. This body, as well as other organisms, had the chance (was </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">selected</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">) to develop because it is in itself a rather efficient replicator suitable for the environment where it came up.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Or consider the famous chessplaying computer, </span><a href=\"https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Deep Blue</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Outside of the narrow domain of selecting moves for chess games, it can't do anything impressive: but </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">as</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> a chessplayer, it was massively more effective than virtually all humans. It has a high optimization power in the chess domain but almost none in any other field. Humans or evolution, on the other hand, are more domain-general optimization processes than Deep Blue, but that doesn't mean they're more effective at chess specifically. (Although note in what contexts this </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">optimization process</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> abstraction is useful and where it fails to be useful: it's not obvious what it would mean for \"evolution\" to play chess, and yet it is useful to talk about the optimization power of natural selection, or of Deep Blue.)</span></p><h2 id=\"Measuring_Optimization_Power\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Measuring Optimization Power</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">One way to think mathematically about optimization, like </span><a href=\"https://www.lesswrong.com/tag/amount-of-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">evidence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, is in information-theoretic bits. The optimization power is the amount of </span><a href=\"http://en.wikipedia.org/wiki/Self-information\"><span class=\"by_qgdGA4ZEyW7zNdK84\">surprise</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> we would have in the result if there were no optimization process present. Therefore we take the base-two logarithm of the reciprocal of the probability of the result. A one-in-a-million solution (a solution so good relative to your preference ordering that it would take a million random tries to find something that good or better) can be said to have log_2(1,000,000) = 19.9 bits of optimization. Compared to a random configuration of matter, any artifact you see is going to be much more optimized than this. The math describes only laws and general principles for reasoning about optimization; as with </span><a href=\"https://www.lesswrong.com/tag/bayesian-probability\"><span class=\"by_qgdGA4ZEyW7zNdK84\">probability theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, you oftentimes can't apply the math directly.</span></p><h2 id=\"Further_Reading___References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Further Reading &amp; References</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/rk/optimization_and_the_singularity/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimization and the Singularity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Eliezer Yudkowsky</span></li><li><a href=\"https://www.lesswrong.com/lw/va/measuring_optimization_power/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Measuring Optimization Power</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Eliezer Yudkowsky</span></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/preference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Preference</span></a></li><li><a href=\"https://www.lesswrong.com/tag/really-powerful-optimization-process\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Really powerful optimization process</span></a></li><li><a href=\"https://www.lesswrong.com/tag/control-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Control theory</span></a></li></ul>",
      "sections": [
        {
          "title": "Measuring Optimization Power",
          "anchor": "Measuring_Optimization_Power",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 79,
    "description": {
      "markdown": "An **optimization process** is any kind of process that systematically comes up with solutions that are better than the solution used before. More technically, this kind of process moves the world into a specific and unexpected set of states by searching through a large search space, hitting small and low probability targets. When this process is gradually guided by some agent into some specific state, through searching specific targets, we can say it [prefers](https://www.lesswrong.com/tag/preference) that state.\n\nThe best way to exemplify an optimization process is through a simple example: [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky) suggests natural selection is such a process. Through an implicit preference – better replicators – natural selection searches all the genetic landscape space and hit small targets: efficient mutations.\n\nConsider the human being. We are a highly complex object with a low probability to have been created by chance - natural selection, however, over millions of years, built up the infrastructure needed to build such a functioning body. This body, as well as other organisms, had the chance (was *selected*) to develop because it is in itself a rather efficient replicator suitable for the environment where it came up.\n\nOr consider the famous chessplaying computer, [Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)). Outside of the narrow domain of selecting moves for chess games, it can't do anything impressive: but *as* a chessplayer, it was massively more effective than virtually all humans. It has a high optimization power in the chess domain but almost none in any other field. Humans or evolution, on the other hand, are more domain-general optimization processes than Deep Blue, but that doesn't mean they're more effective at chess specifically. (Although note in what contexts this *optimization process* abstraction is useful and where it fails to be useful: it's not obvious what it would mean for \"evolution\" to play chess, and yet it is useful to talk about the optimization power of natural selection, or of Deep Blue.)\n\nMeasuring Optimization Power\n----------------------------\n\nOne way to think mathematically about optimization, like [evidence](https://www.lesswrong.com/tag/amount-of-evidence), is in information-theoretic bits. The optimization power is the amount of [surprise](http://en.wikipedia.org/wiki/Self-information) we would have in the result if there were no optimization process present. Therefore we take the base-two logarithm of the reciprocal of the probability of the result. A one-in-a-million solution (a solution so good relative to your preference ordering that it would take a million random tries to find something that good or better) can be said to have log_2(1,000,000) = 19.9 bits of optimization. Compared to a random configuration of matter, any artifact you see is going to be much more optimized than this. The math describes only laws and general principles for reasoning about optimization; as with [probability theory](https://www.lesswrong.com/tag/bayesian-probability), you oftentimes can't apply the math directly.\n\nFurther Reading & References\n----------------------------\n\n*   [Optimization and the Singularity](https://www.lesswrong.com/lw/rk/optimization_and_the_singularity/) by Eliezer Yudkowsky\n*   [Measuring Optimization Power](https://www.lesswrong.com/lw/va/measuring_optimization_power/) by Eliezer Yudkowsky\n\nSee also\n--------\n\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Really powerful optimization process](https://www.lesswrong.com/tag/really-powerful-optimization-process)\n*   [Control theory](https://www.lesswrong.com/tag/control-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HJZZxyYXWzB74M4FT",
    "name": "Fixed Point Theorems",
    "core": false,
    "slug": "fixed-point-theorems",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Fixed Point Theorems</span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\"> are very general theorems in mathematics that show for a given function&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">f</span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span><span class=\"by_EQNTWXLKMeWMp2FQS\">&nbsp;and input&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">x</span></span></span></span></span></span></span></span><span class=\"by_EQNTWXLKMeWMp2FQS\">&nbsp;that&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f(x) = x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">f</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">x</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">=</span></span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">x</span></span></span></span></span></span></span></span><span class=\"by_EQNTWXLKMeWMp2FQS\">. We say that the input&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"x\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.225em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">x</span></span></span></span></span></span></span></span><span class=\"by_EQNTWXLKMeWMp2FQS\">&nbsp;is a fixed point for the function&nbsp;</span><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"f\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.519em; padding-right: 0.06em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">f</span></span></span></span></span></span></span></span><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">These come up most commonly on LessWrong in work around </span><a href=\"https://www.lesswrong.com/tag/embedded-agency\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Embedded Agency</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\"> research, as well as in discussion of game theory.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "**Fixed Point Theorems** are very general theorems in mathematics that show for a given function \\\\(f\\\\) and input \\\\(x\\\\) that \\\\(f(x) = x\\\\). We say that the input \\\\(x\\\\) is a fixed point for the function \\\\(f\\\\).\n\nThese come up most commonly on LessWrong in work around [Embedded Agency](https://www.lesswrong.com/tag/embedded-agency) research, as well as in discussion of game theory."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MP8NqPNATMqPrij4n",
    "name": "Embedded Agency",
    "core": false,
    "slug": "embedded-agency",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Embedded Agency</span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> is </span><span class=\"by_nRknKQuPzoG2Wuyyi\">the problem</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> that an understanding of the theory of rational agents must account for the fact that the agents we create (and we ourselves) are </span><span class=\"by_nRknKQuPzoG2Wuyyi\">inside</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> the </span><span class=\"by_nRknKQuPzoG2Wuyyi\">world or universe we are trying to affect,</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> and not separated from it. This is in contrast with much current basic theory of AI </span><span class=\"by_nRknKQuPzoG2Wuyyi\">or Rationality </span><span class=\"by_EQNTWXLKMeWMp2FQS\">(such as </span><span class=\"by_nRknKQuPzoG2Wuyyi\">Solomonoff induction or Bayesianism)</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> which implicitly supposes a separation between the agent and the-things-the-agent-has-beliefs about.</span><span class=\"by_nRknKQuPzoG2Wuyyi\"> In other words, agents in this universe do not have Cartesian or dualistic boundaries like much of philosophy thinks, and are instead reductionist, that is agents are made up of non-agent parts like bits and atoms.</span></span></p><span class=\"by_nRknKQuPzoG2Wuyyi\">\n</span><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Embedded Agency is not a fully formalised research agenda, but Scott Garrabrant and Abram Demski have written the canonical explanation of the idea in their sequence </span><a href=\"https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh\"><em><span class=\"by_EQNTWXLKMeWMp2FQS\">Embedded Agency</span></em></a><span class=\"by_EQNTWXLKMeWMp2FQS\">. This points to many of the core confusions we have about rational agency and attempts to tie them into a single picture.</span></p><span class=\"by_EQNTWXLKMeWMp2FQS\">\n</span>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 58,
    "description": {
      "markdown": "**Embedded Agency** is the problem that an understanding of the theory of rational agents must account for the fact that the agents we create (and we ourselves) are inside the world or universe we are trying to affect, and not separated from it. This is in contrast with much current basic theory of AI or Rationality (such as Solomonoff induction or Bayesianism) which implicitly supposes a separation between the agent and the-things-the-agent-has-beliefs about. In other words, agents in this universe do not have Cartesian or dualistic boundaries like much of philosophy thinks, and are instead reductionist, that is agents are made up of non-agent parts like bits and atoms.\n\nEmbedded Agency is not a fully formalised research agenda, but Scott Garrabrant and Abram Demski have written the canonical explanation of the idea in their sequence [*Embedded Agency*](https://www.lesswrong.com/s/Rm6oQRJJmhGCcLvxh). This points to many of the core confusions we have about rational agency and attempts to tie them into a single picture."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vmiSKxPpzMuNB5ZmJ",
    "name": "LessWrong Event Transcripts",
    "core": false,
    "slug": "lesswrong-event-transcripts",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_gXeEWGjTWyqgrQTzR\">LessWrong </span><span class=\"by_XtphY3uYHwruKqDyG\">Event Transcripts</span><span class=\"by_gXeEWGjTWyqgrQTzR\"> </span></span></strong><span><span class=\"by_gXeEWGjTWyqgrQTzR\">are </span><span class=\"by_XtphY3uYHwruKqDyG\">transcripts from </span><span class=\"by_gXeEWGjTWyqgrQTzR\">events that </span><span class=\"by_XtphY3uYHwruKqDyG\">were</span><span class=\"by_gXeEWGjTWyqgrQTzR\"> organised by the LessWrong team.&nbsp;</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "**LessWrong Event Transcripts** are transcripts from events that were organised by the LessWrong team."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZFrgTgzwEfStg26JL",
    "name": "AI Risk",
    "core": false,
    "slug": "ai-risk",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">AI Risk</span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\"> is analysis of the risks associated with building powerful AI systems.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 398,
    "description": {
      "markdown": "**AI Risk** is analysis of the risks associated with building powerful AI systems."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "H4n4rzs33JfEgkf8b",
    "name": "OpenAI",
    "core": false,
    "slug": "openai",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">OpenAI </span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\">is an organisation that performs AI research, and houses a substantial amount of AI alignment research. Its stated mission is \"Discovering and enacting the path to safe artificial general intelligence.\"</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">This tag is for explicit discussion of the organisation, not for all work published by researchers at that organisation.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 43,
    "description": {
      "markdown": "**OpenAI** is an organisation that performs AI research, and houses a substantial amount of AI alignment research. Its stated mission is \"Discovering and enacting the path to safe artificial general intelligence.\"\n\nThis tag is for explicit discussion of the organisation, not for all work published by researchers at that organisation."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zHjC29kkPmsdo7WTr",
    "name": "AI Timelines",
    "core": false,
    "slug": "ai-timelines",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">AI Timelines</span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\"> is the discussion of how long until various major milestones in AI progress are achieved, whether it's the timeline until a human-level AI is developed, the timeline until certain benchmarks are defeated, the timeline until we can simulate a mouse-level intelligence, or something else.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">This is to be distinguished from the closely related question of </span><a href=\"https://www.lesswrong.com/tag/ai-takeoff\"><span class=\"by_EQNTWXLKMeWMp2FQS\">AI takeoff</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\"> speeds, which is about the dynamics of AI progress after human-level AI is developed (e.g. will it be a single project or the whole economy that sees growth, how fast will that growth be, etc).</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 168,
    "description": {
      "markdown": "**AI Timelines** is the discussion of how long until various major milestones in AI progress are achieved, whether it's the timeline until a human-level AI is developed, the timeline until certain benchmarks are defeated, the timeline until we can simulate a mouse-level intelligence, or something else.\n\nThis is to be distinguished from the closely related question of [AI takeoff](https://www.lesswrong.com/tag/ai-takeoff) speeds, which is about the dynamics of AI progress after human-level AI is developed (e.g. will it be a single project or the whole economy that sees growth, how fast will that growth be, etc)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pszEEb3ctztv3rozd",
    "name": "Narratives (stories)",
    "core": false,
    "slug": "narratives-stories",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/public-discourse\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Public discourse</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/fiction\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Fiction</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/writing\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Writing</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/social-reality\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Social reality</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 33,
    "description": {
      "markdown": "See also: [Public discourse](https://www.lesswrong.com/tag/public-discourse), [Fiction](https://www.lesswrong.com/tag/fiction), [Writing](https://www.lesswrong.com/tag/writing), [Social reality](https://www.lesswrong.com/tag/social-reality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "p8nXWqwPH7mPSZf6p",
    "name": "Terminology / Jargon (meta)",
    "core": false,
    "slug": "terminology-jargon-meta",
    "tableOfContents": {
      "html": "<p><br><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/conversation-topic\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Conversation (topic)</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/public-discourse\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Public discourse</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "  \nSee also: [Conversation (topic)](https://www.lesswrong.com/tag/conversation-topic), [Public discourse](https://www.lesswrong.com/tag/public-discourse)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5gcpKG2XEAZGj5DEf",
    "name": "Noticing",
    "core": false,
    "slug": "noticing",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/introspection\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Introspection</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 31,
    "description": {
      "markdown": "See also: [Introspection](https://www.lesswrong.com/tag/introspection)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "6nS8oYmSMuFMaiowF",
    "name": "Logic & Mathematics ",
    "core": false,
    "slug": "logic-and-mathematics",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">Logic</span><span class=\"by_NdGxkt87FHaZZxmc5\"> and </span><span class=\"by_HoGziwmhpMGqGeWZy\">Mathematics</span></span></strong><span class=\"by_NdGxkt87FHaZZxmc5\"> are deductive systems, where the conclusion of a successful argument follows necessarily from its premises, given the axioms of the system you’re using: number theory, geometry, predicate logic, etc.</span></p><h2 id=\"See_also\"><span class=\"by_NdGxkt87FHaZZxmc5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/valid-argument\"><span class=\"by_NdGxkt87FHaZZxmc5\">Valid argument</span></a><span class=\"by_NdGxkt87FHaZZxmc5\"> - An argument is valid when it contains no logical fallacies</span></li><li><a href=\"https://www.lesswrong.com/tag/sound-argument\"><span class=\"by_NdGxkt87FHaZZxmc5\">Sound argument</span></a><span class=\"by_NdGxkt87FHaZZxmc5\"> - an argument that is valid and whose premises are all true. In other words, the premises are true and the conclusion necessarily follows from them, making the conclusion true as well.</span></li><li><a href=\"https://www.lesswrong.com/tag/formal-proof\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Formal proof</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\"> - A set of steps from axiom(s) and previous proof(s) which follows the rules of induction of a mathematical system.</span></li><li><a href=\"https://www.lesswrong.com/tag/logical-uncertainty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Logical Uncertainty</span></a></li><li><a href=\"https://www.lesswrong.com/tag/logical-induction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Logical Induction</span></a></li><li><a href=\"https://www.lesswrong.com/tag/probability-and-statistics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Probability &amp; Statistics</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 262,
    "description": {
      "markdown": "**Logic and Mathematics** are deductive systems, where the conclusion of a successful argument follows necessarily from its premises, given the axioms of the system you’re using: number theory, geometry, predicate logic, etc.\n\nSee also\n--------\n\n*   [Valid argument](https://www.lesswrong.com/tag/valid-argument) \\- An argument is valid when it contains no logical fallacies\n*   [Sound argument](https://www.lesswrong.com/tag/sound-argument) \\- an argument that is valid and whose premises are all true. In other words, the premises are true and the conclusion necessarily follows from them, making the conclusion true as well.\n*   [Formal proof](https://www.lesswrong.com/tag/formal-proof) \\- A set of steps from axiom(s) and previous proof(s) which follows the rules of induction of a mathematical system.\n*   [Logical Uncertainty](https://www.lesswrong.com/tag/logical-uncertainty)\n*   [Logical Induction](https://www.lesswrong.com/tag/logical-induction)\n*   [Probability & Statistics](https://www.lesswrong.com/tag/probability-and-statistics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "exZi6Bing5AiM4ZQB",
    "name": "Evolutionary Psychology",
    "core": false,
    "slug": "evolutionary-psychology",
    "tableOfContents": {
      "html": "<p><a href=\"https://wiki.lesswrong.com/wiki/Evolution\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Evolution</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, the cause of the diversity of biological life on Earth, </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">does not work like humans do</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">, and does not design things the way a human engineer would. This </span><a href=\"https://wiki.lesswrong.com/wiki/Alienness_of_evolution\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">blind idiot god</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> is also the source and patterner of human beings. \"Nothing in biology makes sense except in the light of evolution,\" said Theodosius Dobzhansky. Humans brains are also biology, and nothing about our thinking makes sense except in the light of evolution.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">Consider, for example, the following tale:</span></p><blockquote><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">A man and a woman meet in a bar. The man is attracted to her clear complexion, which would have been fertility cues in the ancestral environment, but which in this case result from makeup and a bra. This does not bother the man; he just likes the way she looks. His clear-complexion-detecting neural circuitry does not know that its purpose is to detect fertility, any more than the atoms in his hand contain tiny little XML tags reading \"&lt;purpose&gt;pick things up&lt;/purpose&gt;\". The woman is attracted to his confident smile and firm manner, cues to high status, which in the ancestral environment would have signified the ability to provide resources for children. She plans to use birth control, but her confident-smile-detectors don't know this any more than a toaster knows its designer intended it to make toast. She's not concerned philosophically with the meaning of this rebellion, because her brain is a creationist and denies vehemently that evolution exists. He's not concerned philosophically with the meaning of this rebellion, because he just wants to get laid. They go to a hotel, and undress. He puts on a condom, because he doesn't want kids, just the dopamine-noradrenaline rush of sex, which reliably produced offspring 50,000 years ago when it was an invariant feature of the ancestral environment that condoms did not exist. They have sex, and shower, and go their separate ways. The main objective consequence is to keep the bar and the hotel and condom-manufacturer in business; which was not the cognitive purpose in their minds, and has virtually nothing to do with the key statistical regularities of reproduction 50,000 years ago which explain how they got the genes that built their brains that executed all this behavior.</span></p></blockquote><p><span><span class=\"by_qxJ28GN72aiJu96iF\">This </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">only makes sense in the light of evolution as a designer - that we are </span></span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">poorly</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> optimized to reproduce by a blind and unforesightful god.</span></p><p><span class=\"by_6Fx2vQtkYSZkaCvAg\">The idea of evolution as the idiot designer of humans - that our brains are </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">not</span></i><span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> consistently well-designed -</span><span class=\"by_qxJ28GN72aiJu96iF\"> is </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">a key element of many of the </span></span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">explanations of human errors</span></i><span><span class=\"by_qxJ28GN72aiJu96iF\"> that </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">appear on this website.</span></span></p><p><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Some of</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">key ideas of</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_qxJ28GN72aiJu96iF\">evolutionary psychology</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">are these:</span></span></p><ul><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">People's brains do not explicitly represent evolutionary reasons, </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">consciously or unconsciously</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">.</span></li><li><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">We are optimized for an \"ancestral environment\" (often referred to as EEA, for \"environment of evolutionary adaptedness\") that differs significantly from the environments </span><span class=\"by_nmk3nLpQE89dMRzzN\">in </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">which most of us live. In the ancestral environment, calories were the limiting resource, so our tastebuds are built to like sugar and fat.</span></span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">The brain is not built the way a human engineer would build it. A human engineer would have built our bodies to measure what it needed, so that if you already had enough calories but were lacking micronutrients, your taste buds would start liking lettuce instead of cheeseburgers.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">The brain is a giant hack that starts to break down when you try to do things with it that hunter-gatherers weren't doing. Like computer programming, say.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Evolution's purposes also differ from our own purposes. We are built to deceive ourselves because self-deceivers were more effective liars in ancestral political disputes; and this fact about our underlying brain design doesn't change when we try to make a moral commitment to truth and rationality.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Although human beings do absorb significant additional complexity in the form of culture, we don't absorb it in a fully general way, but rather, in the way that we </span><a href=\"https://wiki.lesswrong.com/wiki/Detached_lever_fallacy\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">evolved to absorb it</span></u></a><span><span class=\"by_6Fx2vQtkYSZkaCvAg\">. That's why the Soviets couldn't raise perfect communist children. Children are programmed to absorb </span><span class=\"by_nmk3nLpQE89dMRzzN\">their </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">parents' language, say, but there is no environment which evokes the response of perfect altruism in human children.</span></span></li></ul><h2 id=\"External_links\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">External links</span></h2><ul><li><a href=\"http://www.cep.ucsb.edu/papers/pfc92.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Psychological Foundations of Culture</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> by Leda Cosmides and John Tooby.</span></li><li><a href=\"http://www.cep.ucsb.edu/primer.html\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Evolutionary Psychology: A Primer</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> by Leda Cosmides and John Tooby.</span></li><li><a href=\"http://hplusmagazine.com/2009/11/23/darwinian-psychologist-straw-mans-ass-kicked/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">“Darwinian Psychologist” Straw Man’s Ass Kicked</span></u></a></li><li><a href=\"http://www.amazon.com/Moral-Animal-Science-Evolutionary-Psychology/dp/0679763996\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Moral Animal: Why We Are the Way We Are: The New Science of Evolutionary Psychology</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> - popular book by Robert Wright.</span></li></ul><h2 id=\"See_also\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">See also</span></h2><ul><li><a href=\"lesswrong.com/tag/evolution\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Evolution</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Stupidity_of_evolution\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Stupidity of evolution</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Evolution_as_alien_god\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">evolution as alien god</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Human_universal\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Human universal</span></u></a></li><li><a href=\"https://www.lesswrong.com/tag/adaptation-executors\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Adaptation executers</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Corrupted_hardware\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Corrupted hardware</span></u></a></li><li><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Psychology</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 43,
    "description": {
      "markdown": "[Evolution](https://wiki.lesswrong.com/wiki/Evolution), the cause of the diversity of biological life on Earth, *does not work like humans do*, and does not design things the way a human engineer would. This [blind idiot god](https://wiki.lesswrong.com/wiki/Alienness_of_evolution) is also the source and patterner of human beings. \"Nothing in biology makes sense except in the light of evolution,\" said Theodosius Dobzhansky. Humans brains are also biology, and nothing about our thinking makes sense except in the light of evolution.\n\nConsider, for example, the following tale:\n\n> A man and a woman meet in a bar. The man is attracted to her clear complexion, which would have been fertility cues in the ancestral environment, but which in this case result from makeup and a bra. This does not bother the man; he just likes the way she looks. His clear-complexion-detecting neural circuitry does not know that its purpose is to detect fertility, any more than the atoms in his hand contain tiny little XML tags reading \"<purpose>pick things up</purpose>\". The woman is attracted to his confident smile and firm manner, cues to high status, which in the ancestral environment would have signified the ability to provide resources for children. She plans to use birth control, but her confident-smile-detectors don't know this any more than a toaster knows its designer intended it to make toast. She's not concerned philosophically with the meaning of this rebellion, because her brain is a creationist and denies vehemently that evolution exists. He's not concerned philosophically with the meaning of this rebellion, because he just wants to get laid. They go to a hotel, and undress. He puts on a condom, because he doesn't want kids, just the dopamine-noradrenaline rush of sex, which reliably produced offspring 50,000 years ago when it was an invariant feature of the ancestral environment that condoms did not exist. They have sex, and shower, and go their separate ways. The main objective consequence is to keep the bar and the hotel and condom-manufacturer in business; which was not the cognitive purpose in their minds, and has virtually nothing to do with the key statistical regularities of reproduction 50,000 years ago which explain how they got the genes that built their brains that executed all this behavior.\n\nThis only makes sense in the light of evolution as a designer - that we are *poorly* optimized to reproduce by a blind and unforesightful god.\n\nThe idea of evolution as the idiot designer of humans - that our brains are *not* consistently well-designed - is a key element of many of the *explanations of human errors* that appear on this website.\n\nSome of the key ideas of evolutionary psychology are these:\n\n*   People's brains do not explicitly represent evolutionary reasons, *consciously or unconsciously*.\n*   We are optimized for an \"ancestral environment\" (often referred to as EEA, for \"environment of evolutionary adaptedness\") that differs significantly from the environments in which most of us live. In the ancestral environment, calories were the limiting resource, so our tastebuds are built to like sugar and fat.\n*   The brain is not built the way a human engineer would build it. A human engineer would have built our bodies to measure what it needed, so that if you already had enough calories but were lacking micronutrients, your taste buds would start liking lettuce instead of cheeseburgers.\n*   The brain is a giant hack that starts to break down when you try to do things with it that hunter-gatherers weren't doing. Like computer programming, say.\n*   Evolution's purposes also differ from our own purposes. We are built to deceive ourselves because self-deceivers were more effective liars in ancestral political disputes; and this fact about our underlying brain design doesn't change when we try to make a moral commitment to truth and rationality.\n*   Although human beings do absorb significant additional complexity in the form of culture, we don't absorb it in a fully general way, but rather, in the way that we [evolved to absorb it](https://wiki.lesswrong.com/wiki/Detached_lever_fallacy). That's why the Soviets couldn't raise perfect communist children. Children are programmed to absorb their parents' language, say, but there is no environment which evokes the response of perfect altruism in human children.\n\nExternal links\n--------------\n\n*   [The Psychological Foundations of Culture](http://www.cep.ucsb.edu/papers/pfc92.pdf) by Leda Cosmides and John Tooby.\n*   [Evolutionary Psychology: A Primer](http://www.cep.ucsb.edu/primer.html) by Leda Cosmides and John Tooby.\n*   [“Darwinian Psychologist” Straw Man’s Ass Kicked](http://hplusmagazine.com/2009/11/23/darwinian-psychologist-straw-mans-ass-kicked/)\n*   [The Moral Animal: Why We Are the Way We Are: The New Science of Evolutionary Psychology](http://www.amazon.com/Moral-Animal-Science-Evolutionary-Psychology/dp/0679763996) \\- popular book by Robert Wright.\n\nSee also\n--------\n\n*   [Evolution](lesswrong.com/tag/evolution)\n*   [Stupidity of evolution](https://wiki.lesswrong.com/wiki/Stupidity_of_evolution), [evolution as alien god](https://wiki.lesswrong.com/wiki/Evolution_as_alien_god)\n*   [Human universal](https://wiki.lesswrong.com/wiki/Human_universal)\n*   [Adaptation executers](https://www.lesswrong.com/tag/adaptation-executors)\n*   [Corrupted hardware](https://wiki.lesswrong.com/wiki/Corrupted_hardware)\n*   [Psychology](https://www.lesswrong.com/tag/psychology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x3zyEPFaJANB2BHmP",
    "name": "Expertise (topic)",
    "core": false,
    "slug": "expertise-topic",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/social-status\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Social Status</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Practice &amp; Philosophy of Science</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 48,
    "description": {
      "markdown": "**Related Pages:** [Social Status](https://www.lesswrong.com/tag/social-status), [Practice & Philosophy of Science](https://www.lesswrong.com/tag/practice-and-philosophy-of-science)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x5TtBDjRg9egvg9gm",
    "name": "Cultural knowledge",
    "core": false,
    "slug": "cultural-knowledge",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">Cultural knowledge </span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">(or </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">metis</span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">) refers to knowledge that's codified in traditions, norms, institutions and intuitions, without necessarily being fully understood or legible to people in those cultures.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "**Cultural knowledge** (or **metis**) refers to knowledge that's codified in traditions, norms, institutions and intuitions, without necessarily being fully understood or legible to people in those cultures."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MXcpQvaPGtXpB6vkM",
    "name": "Public Discourse",
    "core": false,
    "slug": "public-discourse",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">Public discourse </span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">refers to our ability to have conversations </span><i><span class=\"by_gXeEWGjTWyqgrQTzR\">in large groups</span></i><span class=\"by_gXeEWGjTWyqgrQTzR\">, both as a society, and in smaller communities; as well as conversations between a few well-defined participants (such as presidential debates) that take place publicly.&nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">This tag is for understanding the nature of public discourse (How good is it? What makes it succeed or fail?), and ways of improving it using technology or novel institutions.&nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/conversation-topic\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Conversation (topic)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 104,
    "description": {
      "markdown": "**Public discourse** refers to our ability to have conversations *in large groups*, both as a society, and in smaller communities; as well as conversations between a few well-defined participants (such as presidential debates) that take place publicly. \n\nThis tag is for understanding the nature of public discourse (How good is it? What makes it succeed or fail?), and ways of improving it using technology or novel institutions. \n\nSee also: [Conversation (topic)](https://www.lesswrong.com/tag/conversation-topic)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xCXkjecsjwm8uSW3y",
    "name": "Cryptocurrency & Blockchain",
    "core": false,
    "slug": "cryptocurrency-and-blockchain",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/institution-design\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Institution Design</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/financial-investing\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Financial Investing</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 48,
    "description": {
      "markdown": "See also: [Institution Design](https://www.lesswrong.com/tag/institution-design), [Financial Investing](https://www.lesswrong.com/tag/financial-investing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z6DgiCrMtpSNxwuYW",
    "name": "Grants & Fundraising Opportunities",
    "core": false,
    "slug": "grants-and-fundraising-opportunities",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">Many LessWrong readers actively rely on </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">grants or fundraising opportunities</span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\"> to support their work (for example by running non-profits or startups, working as independent researchers, or being supported by academic grants).&nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">This tag lists concrete opportunities for fundraising of interest to the LessWrong community. This typically means projects working on improving the long-term future, refining the art of rationality, and related missions. &nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">This tag should not be used for meta-discussion, e.g. of fundraising strategies, coordination between funders, or cost-benefit analyses of particular funding opportunities.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 57,
    "description": {
      "markdown": "Many LessWrong readers actively rely on **grants or fundraising opportunities** to support their work (for example by running non-profits or startups, working as independent researchers, or being supported by academic grants). \n\nThis tag lists concrete opportunities for fundraising of interest to the LessWrong community. This typically means projects working on improving the long-term future, refining the art of rationality, and related missions.  \n\nThis tag should not be used for meta-discussion, e.g. of fundraising strategies, coordination between funders, or cost-benefit analyses of particular funding opportunities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cCK3fDdH9vHjrf2yP",
    "name": "Bounties & Prizes (active)",
    "core": false,
    "slug": "bounties-and-prizes-active",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">A </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">bounty </span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">is a monetary payment for accomplishing some task. On LessWrong, bounties have historically been paid out for things like providing useful information, doing a novel piece of research, or changing someone's mind about a topic.&nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">This tag is for bounties that are actively accepting submissions. (It has a sister for bounties that have closed.)</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">Bounties might be listed in the post itself or in its comments.&nbsp;</span></p><p><i><span class=\"by_gXeEWGjTWyqgrQTzR\">If you're hosting a bounty, please make sure to change the tags to indicate the status of the bounty.&nbsp;</span></i></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/bounties-closed\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Bounties (closed)</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Grants and fundraising opportunities</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, </span><a href=\"https://www.facebook.com/groups/1781724435404945/\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Bountied Rationality Facebook group</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 39,
    "description": {
      "markdown": "A **bounty** is a monetary payment for accomplishing some task. On LessWrong, bounties have historically been paid out for things like providing useful information, doing a novel piece of research, or changing someone's mind about a topic. \n\nThis tag is for bounties that are actively accepting submissions. (It has a sister for bounties that have closed.)\n\nBounties might be listed in the post itself or in its comments. \n\n*If you're hosting a bounty, please make sure to change the tags to indicate the status of the bounty. *\n\nSee also: [Bounties (closed)](https://www.lesswrong.com/tag/bounties-closed), [Grants and fundraising opportunities](https://www.lesswrong.com/tag/grants-and-fundraising-opportunities), [Bountied Rationality Facebook group](https://www.facebook.com/groups/1781724435404945/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hQiuNkBhn6xxcedTD",
    "name": "Occam's Razor",
    "core": false,
    "slug": "occam-s-razor",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">razor</span></span></strong><span><span class=\"by_gjoi5eBQob27Lww62\"> (more formally referred to as the principle of parsimony)</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a principle commonly stated as \"Entities must not be multiplied beyond necessity\". When</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> several theories are able to explain the same observations, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam's razor suggests </span><span class=\"by_nLbwLhBaQeG6tCNDN\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">simpler</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><span class=\"by_RyiDJDCG6R7xyAXzp\">one is preferable.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> It must be noted that Occam's razor is a requirement for the simplicity of </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">theories</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, not for the size of the systems described by those theories. For example, the immensity of the Universe isn't at odds with the principle of Occam's razor.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam's razor is necessitated by the conjunction rule of probability theory: the conjunction A and B is necessarily less (or equally, in the case of logical equivalence) probable than the A alone; </span><a href=\"https://www.lesswrong.com/tag/burdensome-details\"><span class=\"by_qgdGA4ZEyW7zNdK84\">every detail you tack onto your story drives the probability down</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam's razor has been formalized as Minimum Description Length or Minimum Message Length, in which the total size of the theory is the length of the message required to describe the theory, plus the length of the message required to describe the evidence </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">using</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> the theory. </span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the ultimate case of </span><a href=\"https://wiki.lesswrong.com/wiki/minimum_message_length\"><span class=\"by_qgdGA4ZEyW7zNdK84\">minimum message length</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> in which the code for messages can describe all computable hypotheses. This has jokingly been referred to as \"</span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff's lightsaber</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">\".</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/jp/occams_razor/\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s Razor</span></span></a></li><li><a href=\"https://www.lesswrong.com/lw/k2/a_priori/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A Priori</span></a></li><li><a href=\"https://www.lesswrong.com/lw/q3/decoherence_is_simple/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decoherence is Simple</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">Solomonoff </span><span class=\"by_qgdGA4ZEyW7zNdK84\">induction</span></span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Occam's_imaginary_razor\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam's imaginary razor</span></a></li><li><a href=\"https://www.lesswrong.com/tag/priors\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Priors</span></a></li><li><a href=\"https://www.lesswrong.com/tag/burdensome-details\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Burdensome details</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Egan's_law\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Egan's law</span></a></li></ul><h2 id=\"External_Links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Links</span></h2><ul><li><a href=\"http://www.andrew.cmu.edu/user/kk3n/ockham/Ockham.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ockham’s Razor: A New Justification</span></a></li></ul>",
      "sections": [
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 32,
    "description": {
      "markdown": "**Occam's razor** (more formally referred to as the principle of parsimony) is a principle commonly stated as \"Entities must not be multiplied beyond necessity\". When several theories are able to explain the same observations, Occam's razor suggests the simpler one is preferable. It must be noted that Occam's razor is a requirement for the simplicity of *theories*, not for the size of the systems described by those theories. For example, the immensity of the Universe isn't at odds with the principle of Occam's razor.\n\nOccam's razor is necessitated by the conjunction rule of probability theory: the conjunction A and B is necessarily less (or equally, in the case of logical equivalence) probable than the A alone; [every detail you tack onto your story drives the probability down](https://www.lesswrong.com/tag/burdensome-details).\n\nOccam's razor has been formalized as Minimum Description Length or Minimum Message Length, in which the total size of the theory is the length of the message required to describe the theory, plus the length of the message required to describe the evidence *using* the theory. [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction) is the ultimate case of [minimum message length](https://wiki.lesswrong.com/wiki/minimum_message_length) in which the code for messages can describe all computable hypotheses. This has jokingly been referred to as \"[Solomonoff's lightsaber](https://www.lesswrong.com/tag/solomonoff-induction)\".\n\nNotable Posts\n-------------\n\n*   [Occam's Razor](https://www.lesswrong.com/lw/jp/occams_razor/)\n*   [A Priori](https://www.lesswrong.com/lw/k2/a_priori/)\n*   [Decoherence is Simple](https://www.lesswrong.com/lw/q3/decoherence_is_simple/)\n\nSee Also\n--------\n\n*   [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction)\n*   [Occam's imaginary razor](https://wiki.lesswrong.com/wiki/Occam's_imaginary_razor)\n*   [Priors](https://www.lesswrong.com/tag/priors)\n*   [Burdensome details](https://www.lesswrong.com/tag/burdensome-details)\n*   [Egan's law](https://wiki.lesswrong.com/wiki/Egan's_law)\n\nExternal Links\n--------------\n\n*   [Ockham’s Razor: A New Justification](http://www.andrew.cmu.edu/user/kk3n/ockham/Ockham.htm)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7mTviCYysGmLqiHai",
    "name": "Writing (communication method)",
    "core": false,
    "slug": "writing-communication-method",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">About the art and science of using </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">writing</span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\"> to think and communicate.</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">Including posts on: the history of writing, how to write better, comparing and recommending writing tools, and more.&nbsp;</span></p><p><strong id=\"External_Links_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong></p><p><a href=\"https://slatestarcodex.com/2016/02/20/writing-advice/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Nonfiction Writing Advice</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/communication-cultures\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Communication Cultures</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/philosophy-of-language\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Philosophy of Language</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/inferential-distance\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Inferential Distance</span></a></p>",
      "sections": [
        {
          "title": "External Links:",
          "anchor": "External_Links_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 116,
    "description": {
      "markdown": "About the art and science of using **writing** to think and communicate.\n\nIncluding posts on: the history of writing, how to write better, comparing and recommending writing tools, and more. \n\n**External Links:**\n\n[Nonfiction Writing Advice](https://slatestarcodex.com/2016/02/20/writing-advice/) by Scott Alexander\n\n**Related Pages:** [Communication Cultures](https://www.lesswrong.com/tag/communication-cultures), [Philosophy of Language](https://www.lesswrong.com/tag/philosophy-of-language), [Inferential Distance](https://www.lesswrong.com/tag/inferential-distance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HkiwLtMRLxpBa6zs5",
    "name": "Industrial Revolution",
    "core": false,
    "slug": "industrial-revolution",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">The </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">Industrial Revolution </span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">was a set of economic and social changes that occurred in Europe and the United States in the 18th and 19th centuries, characterised by a transition from an \"agrarian and handicraft economy to one dominated by industry and machine manufacturing\" [</span><a href=\"https://www.britannica.com/event/Industrial-Revolution\"><span class=\"by_gXeEWGjTWyqgrQTzR\">1</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">].&nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/history\"><span class=\"by_gXeEWGjTWyqgrQTzR\">History</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 29,
    "description": {
      "markdown": "The **Industrial Revolution** was a set of economic and social changes that occurred in Europe and the United States in the 18th and 19th centuries, characterised by a transition from an \"agrarian and handicraft economy to one dominated by industry and machine manufacturing\" \\[[1](https://www.britannica.com/event/Industrial-Revolution)\\]. \n\nSee also: [History](https://www.lesswrong.com/tag/history)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HLoxy2feb2PYqooom",
    "name": "Sleep",
    "core": false,
    "slug": "sleep",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 30,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8nAXyYLu8eT72Hwuh",
    "name": "Exercise (Physical)",
    "core": false,
    "slug": "exercise-physical",
    "tableOfContents": {
      "html": "<p><span class=\"by_Xn6ACr6Cua8upALWQ\">Can one single intervention make you live around 5 minutes longer for every minute invested, change how you look, and allow you to perform physical feats that you would have thought impossible? Yes, and it's high-intensity strength training. </span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The current most comprehensive overview in this topic is </span><a href=\"https://www.lesswrong.com/posts/bZ2w99pEAeAbKnKqo/optimal-exercise\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">. </span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"http://lesswrong.com/tag/sports\"><span class=\"by_HoGziwmhpMGqGeWZy\">Sports</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"http://lesswrong.com/tag/well-being\"><span class=\"by_HoGziwmhpMGqGeWZy\">Well-being</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 28,
    "description": {
      "markdown": "Can one single intervention make you live around 5 minutes longer for every minute invested, change how you look, and allow you to perform physical feats that you would have thought impossible? Yes, and it's high-intensity strength training.\n\nThe current most comprehensive overview in this topic is [here](https://www.lesswrong.com/posts/bZ2w99pEAeAbKnKqo/optimal-exercise).\n\nSee also: [Sports](http://lesswrong.com/tag/sports), [Well-being](http://lesswrong.com/tag/well-being)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DWWZwkxTJs4d5WrcX",
    "name": "Exercises / Problem-Sets",
    "core": false,
    "slug": "exercises-problem-sets",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">This tag collects posts with concrete </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">exercises</span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">. Problems that have solutions (or least some clear feedback loop). Things that you can attempt yourself in order to learn and grow.&nbsp;</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/games-posts-describing\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Games (posts describing)</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 142,
    "description": {
      "markdown": "This tag collects posts with concrete **exercises**. Problems that have solutions (or least some clear feedback loop). Things that you can attempt yourself in order to learn and grow. \n\n**Related Pages:** [Games (posts describing)](https://www.lesswrong.com/tag/games-posts-describing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kCuRQE5Tkv9zeKyzK",
    "name": "Common Knowledge",
    "core": false,
    "slug": "common-knowledge",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Common knowledge</span></strong><span><span class=\"by_ApLdjhKjuYkMYSboq\"> is </span><span class=\"by_EQNTWXLKMeWMp2FQS\">information that everyone knows </span><span class=\"by_qgdGA4ZEyW7zNdK84\">and, importantly, that</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> everyone knows that everyone knows, and so on, ad infinitum. If information is </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">common knowledge</span></i><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> in</span><span class=\"by_ApLdjhKjuYkMYSboq\"> a group of </span><span class=\"by_EQNTWXLKMeWMp2FQS\">people,</span><span class=\"by_ApLdjhKjuYkMYSboq\"> that </span><span class=\"by_EQNTWXLKMeWMp2FQS\">information that can be relied</span><span class=\"by_ApLdjhKjuYkMYSboq\"> and </span><span class=\"by_EQNTWXLKMeWMp2FQS\">acted upon with the trust that everyone else is also coordinating around that information.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> This stands, in contrast, to merely publicly known information where one person cannot be sure that another person knows the information, or that another person knows that they know the information. Establishing true common knowledge is, in fact, rather hard.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\"> </span><a href=\"https://www.lesswrong.com/tag/public-discourse\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Public discourse</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/consensus\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Consensus</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/inferential-distance\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Inferential Distance</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External posts:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span><br><a href=\"https://www.scottaaronson.com/blog/?p=3376\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Kolmogorov option</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Aaronson</span><br><a href=\"https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">kolmogorov complicity and-the parable of lightning</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 20,
    "description": {
      "markdown": "**Common knowledge** is information that everyone knows and, importantly, that everyone knows that everyone knows, and so on, ad infinitum. If information is *common knowledge* in a group of people, that information that can be relied and acted upon with the trust that everyone else is also coordinating around that information. This stands, in contrast, to merely publicly known information where one person cannot be sure that another person knows the information, or that another person knows that they know the information. Establishing true common knowledge is, in fact, rather hard.\n\n**Related Pages:** [Public discourse](https://www.lesswrong.com/tag/public-discourse), [Consensus](https://www.lesswrong.com/tag/consensus), [Inferential Distance](https://www.lesswrong.com/tag/inferential-distance)\n\n**External posts:**   \n[The Kolmogorov option](https://www.scottaaronson.com/blog/?p=3376) by Scott Aaronson  \n[kolmogorov complicity and-the parable of lightning](https://slatestarcodex.com/2017/10/23/kolmogorov-complicity-and-the-parable-of-lightning/) by Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4mRJmYxNDnn7r2gNu",
    "name": "Iterated Amplification ",
    "core": false,
    "slug": "iterated-amplification",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">Iterated Amplification </span></strong><span><span class=\"by_gXeEWGjTWyqgrQTzR\">is an approach to AI </span><span class=\"by_EQNTWXLKMeWMp2FQS\">alignment, spearheaded by Paul Christiano. In this setup, we build powerful, aligned ML systems through a process of initially building weak aligned AIs, and recursively using each new AI to build a slightly smarter and still aligned AI.</span><span class=\"by_gXeEWGjTWyqgrQTzR\">&nbsp;</span></span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/factored-cognition\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Factored cognition</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">.&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 58,
    "description": {
      "markdown": "**Iterated Amplification** is an approach to AI alignment, spearheaded by Paul Christiano. In this setup, we build powerful, aligned ML systems through a process of initially building weak aligned AIs, and recursively using each new AI to build a slightly smarter and still aligned AI. \n\nSee also: [Factored cognition](https://www.lesswrong.com/tag/factored-cognition)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nKtyrL5u4Y5kmMWT5",
    "name": "DeepMind",
    "core": false,
    "slug": "deepmind",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">DeepMind</span><span class=\"by_gXeEWGjTWyqgrQTzR\"> </span></span></strong><span><span class=\"by_gXeEWGjTWyqgrQTzR\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">an</span><span class=\"by_gXeEWGjTWyqgrQTzR\"> AI research laboratory </span><span class=\"by_qgdGA4ZEyW7zNdK84\">that was acquired by Google </span><span class=\"by_HoGziwmhpMGqGeWZy\">in 2014.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> It is known for several record-breaking AI algorithms, often named with the prefix \"Alpha\", e.g.</span><span class=\"by_gXeEWGjTWyqgrQTzR\"> including AlphaGo, AlphaGo Zero, AlphaStar and AlphaFold.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 36,
    "description": {
      "markdown": "**DeepMind** is an AI research laboratory that was acquired by Google in 2014. It is known for several record-breaking AI algorithms, often named with the prefix \"Alpha\", e.g. including AlphaGo, AlphaGo Zero, AlphaStar and AlphaFold."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BzghQYM9GnkMHxZKb",
    "name": "Problem-solving (skills and techniques)",
    "core": false,
    "slug": "problem-solving-skills-and-techniques",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 15,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GLykb6NukBeBQtDvQ",
    "name": "Philosophy",
    "core": false,
    "slug": "philosophy",
    "tableOfContents": {
      "html": "<p><span class=\"by_SsduPgHwY2zeZpmKT\">(From Wikipedia,) </span><em><span class=\"by_SsduPgHwY2zeZpmKT\">Philosophy</span></em><span class=\"by_SsduPgHwY2zeZpmKT\"> is the study of general and fundamental questions about existence, knowledge, values, reason, mind, and language.</span></p><p><span class=\"by_SsduPgHwY2zeZpmKT\">This tag probably implies non-Less Wrong philosophy in particular. This could be mainstream or academic philosophy, Eastern philosophy, or something else</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 128,
    "description": {
      "markdown": "(From Wikipedia,) _Philosophy_ is the study of general and fundamental questions about existence, knowledge, values, reason, mind, and language.\n\nThis tag probably implies non-Less Wrong philosophy in particular. This could be mainstream or academic philosophy, Eastern philosophy, or something else"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3Y4y9Kr8e24YWAEmD",
    "name": "Myopia",
    "core": false,
    "slug": "myopia",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">Myopia</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> means short-sighted, particularly with respect to planning -- neglecting long-term consequences in favor of the short term. The extreme case, in which </span><i><span class=\"by_Q7NW4XaWQmfPfdcFj\">only</span></i><span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> immediate rewards are considered, is of particular interest. </span><span class=\"by_XtphY3uYHwruKqDyG\">We can think of a myopic agent as one that only considers how best to answer the single question that you give to it rather than considering any sort of long-term consequences.</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> Such an agent might have a number of desirable safety properties, such as a lack of </span></span><a href=\"https://arbital.com/p/convergent_strategies/\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">instrumental incentives</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "**Myopia** means short-sighted, particularly with respect to planning -- neglecting long-term consequences in favor of the short term. The extreme case, in which *only* immediate rewards are considered, is of particular interest. We can think of a myopic agent as one that only considers how best to answer the single question that you give to it rather than considering any sort of long-term consequences. Such an agent might have a number of desirable safety properties, such as a lack of [instrumental incentives](https://arbital.com/p/convergent_strategies/)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "SJFsFfFhE6m2ThAYJ",
    "name": "Anticipated Experiences",
    "core": false,
    "slug": "anticipated-experiences",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">One principle of rationality is that \"</span><a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A\"><span class=\"by_qxJ28GN72aiJu96iF\">beliefs should pay rent in </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">anticipated experiences</span></strong></a><span class=\"by_qxJ28GN72aiJu96iF\">.\" If you believe in something, what do you expect to be different as a result? What does the belief say should happen, and what does it say should </span><em><span class=\"by_qxJ28GN72aiJu96iF\">not</span></em><span class=\"by_qxJ28GN72aiJu96iF\"> happen? If you have a verbal disagreement with someone, how does your disagreement cash out in differing expectations?</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">If two people try to get specific about the anticipated experiences driving their disagreement, one method for doing so is the </span><a href=\"https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement\"><span class=\"by_qxJ28GN72aiJu96iF\">double crux</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> technique. The notion that beliefs are models of what we expect to experience is also one of the basic premises of </span><a href=\"https://www.lesswrong.com/tag/predictive-processing\"><span class=\"by_qxJ28GN72aiJu96iF\">predictive processing</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> theories of how the brain works. Beliefs that do not pay rent may be related to </span><a href=\"https://www.lesswrong.com/posts/4xKeNKFXFB458f5N8/ethnic-tension-and-meaningless-arguments\"><span class=\"by_qxJ28GN72aiJu96iF\">meaningless arguments</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> driven by </span><a href=\"https://www.lesswrong.com/tag/coalitional-instincts\"><span class=\"by_qxJ28GN72aiJu96iF\">coalitional instincts</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. </span></p><blockquote><span class=\"by_qxJ28GN72aiJu96iF\"> </span><em><span class=\"by_qxJ28GN72aiJu96iF\">If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”</span></em><span class=\"by_qxJ28GN72aiJu96iF\">  [...]</span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">Suppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?</span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">Though the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will </span><em><span class=\"by_qxJ28GN72aiJu96iF\">happen to</span></em><span class=\"by_qxJ28GN72aiJu96iF\"> them; their maps of the world do not diverge in any sensory detail.</span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">-- Eliezer Yudkowsky, </span><a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A\"><span class=\"by_qxJ28GN72aiJu96iF\">Making Beliefs Pay Rent (In Anticipated Experiences)</span></a></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 39,
    "description": {
      "markdown": "One principle of rationality is that \"[beliefs should pay rent in **anticipated experiences**](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A).\" If you believe in something, what do you expect to be different as a result? What does the belief say should happen, and what does it say should _not_ happen? If you have a verbal disagreement with someone, how does your disagreement cash out in differing expectations?\n\nIf two people try to get specific about the anticipated experiences driving their disagreement, one method for doing so is the [double crux](https://www.lesswrong.com/posts/exa5kmvopeRyfJgCy/double-crux-a-strategy-for-resolving-disagreement) technique. The notion that beliefs are models of what we expect to experience is also one of the basic premises of [predictive processing](https://www.lesswrong.com/tag/predictive-processing) theories of how the brain works. Beliefs that do not pay rent may be related to [meaningless arguments](https://www.lesswrong.com/posts/4xKeNKFXFB458f5N8/ethnic-tension-and-meaningless-arguments) driven by [coalitional instincts](https://www.lesswrong.com/tag/coalitional-instincts).\n\n> _If a tree falls in a forest and no one hears it, does it make a sound? One says, “Yes it does, for it makes vibrations in the air.” Another says, “No it does not, for there is no auditory processing in any brain.”_ \\[...\\]\n\n> Suppose that, after a tree falls, the two arguers walk into the forest together. Will one expect to see the tree fallen to the right, and the other expect to see the tree fallen to the left? Suppose that before the tree falls, the two leave a sound recorder next to the tree. Would one, playing back the recorder, expect to hear something different from the other? Suppose they attach an electroencephalograph to any brain in the world; would one expect to see a different trace than the other?\n\n> Though the two argue, one saying “No,” and the other saying “Yes,” they do not anticipate any different experiences. The two think they have different models of the world, but they have no difference with respect to what they expect will _happen to_ them; their maps of the world do not diverge in any sensory detail.\n\n> \\-\\- Eliezer Yudkowsky, [Making Beliefs Pay Rent (In Anticipated Experiences)](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/a7n8GdKiAZRX86T5A)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DdgSyQoZXjj3KnF4N",
    "name": "Tribalism",
    "core": false,
    "slug": "tribalism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Tribalism</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Coalitional Instincts </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is</span><span class=\"by_qxJ28GN72aiJu96iF\"> closely connected to the concept of in/out-groups. Coalitional </span><span class=\"by_qgdGA4ZEyW7zNdK84\">instincts </span><span class=\"by_qxJ28GN72aiJu96iF\">drive humans to act in ways which cause them join, support, defend, and maintain their membership in various coalitions that are defined by sharing a common identity. An illustrative example can be found in </span></span><a href=\"https://www.lesswrong.com/posts/6hfGNLf4Hg5DXqJCF/a-fable-of-science-and-politics\"><span class=\"by_qxJ28GN72aiJu96iF\">A Fable of Science and Politics</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">See also: </span><a href=\"https://www.lesswrong.com/tag/blues-and-greens\"><span class=\"by_qxJ28GN72aiJu96iF\">Blues and Greens</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_qxJ28GN72aiJu96iF\">Groupthink</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/tag/motivated-reasoning\"><span class=\"by_qxJ28GN72aiJu96iF\">Motivated Reasoning</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics\"><span class=\"by_qxJ28GN72aiJu96iF\">Social and Cultural Dynamics</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/tag/social-reality\"><span class=\"by_qxJ28GN72aiJu96iF\">Social Reality</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">The primary function that drove the evolution of coalitions is the amplification of the power of its members in conflicts with non-members. This function explains a number of otherwise puzzling phenomena. For example,  ancestrally, if you had no coalition you were nakedly at the mercy of everyone else, so the instinct to belong to a coalition has urgency, preexisting and superseding any policy-driven basis for membership. This is why group beliefs are free to be so weird. [...] </span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">... to earn membership in a group you must send signals that clearly indicate that you differentially support it, compared to rival groups. Hence, optimal weighting of beliefs and communications in the individual mind will make it feel good to think and express content conforming to and flattering to one’s group’s shared beliefs and to attack and misrepresent rival groups. The more biased away from neutral truth, the better the communication functions to affirm coalitional identity, generating polarization in excess of actual policy disagreements. Communications of practical and functional truths are generally useless as differential signals, because any honest person might say them regardless of coalitional loyalty. In contrast, unusual, exaggerated beliefs [...] are unlikely to be said except as expressive of identity, because there is no external reality to motivate nonmembers to speak absurdities. </span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">-- John Tooby, \"</span><a href=\"https://www.edge.org/conversation/john_tooby-coalitional-instincts\"><span class=\"by_qxJ28GN72aiJu96iF\">Coalitional Instincts</span></a><span class=\"by_qxJ28GN72aiJu96iF\">\"</span></blockquote><br><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">Humans interact in dense social networks, and this poses a problem for bystanders when conflicts arise: which side, if any, to support. Choosing sides is a difficult strategic problem because the outcome of a conflict critically depends on which side other bystanders support. One strategy is siding with the higher status disputant, which can allow bystanders to coordinate with one another to take the same side, reducing fighting costs. However, this strategy carries the cost of empowering high-status individuals to exploit others. A second possible strategy is choosing sides based on preexisting relationships. This strategy balances power but carries another cost: Bystanders choose different sides, and this discoordination causes escalated conflicts and high fighting costs. We propose that moral cognition is designed to manage both of these problems by implementing a dynamic coordination strategy in which bystanders coordinate side-taking based on a public signal derived from disputants’ actions rather than their identities. By focusing on disputants’ actions, bystanders can dynamically change which individuals they support across different disputes, simultaneously solving the problems of coordination and exploitation.</span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">-- Peter DeScioli &amp; Robert Kurzban, \" </span><a href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.840.3768&amp;rep=rep1&amp;type=pdf\"><span class=\"by_qxJ28GN72aiJu96iF\">A Solution to the Mysteries of Morality</span></a><span class=\"by_qxJ28GN72aiJu96iF\">\"</span></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 49,
    "description": {
      "markdown": "**Tribalism** or **Coalitional Instincts** is closely connected to the concept of in/out-groups. Coalitional instincts drive humans to act in ways which cause them join, support, defend, and maintain their membership in various coalitions that are defined by sharing a common identity. An illustrative example can be found in [A Fable of Science and Politics](https://www.lesswrong.com/posts/6hfGNLf4Hg5DXqJCF/a-fable-of-science-and-politics).\n\nSee also: [Blues and Greens](https://www.lesswrong.com/tag/blues-and-greens), [Groupthink](https://www.lesswrong.com/tag/groupthink), [Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning), [Social and Cultural Dynamics](https://www.lesswrong.com/tag/social-and-cultural-dynamics), [Social Reality](https://www.lesswrong.com/tag/social-reality).\n\n> The primary function that drove the evolution of coalitions is the amplification of the power of its members in conflicts with non-members. This function explains a number of otherwise puzzling phenomena. For example, ancestrally, if you had no coalition you were nakedly at the mercy of everyone else, so the instinct to belong to a coalition has urgency, preexisting and superseding any policy-driven basis for membership. This is why group beliefs are free to be so weird. \\[...\\]\n\n> ... to earn membership in a group you must send signals that clearly indicate that you differentially support it, compared to rival groups. Hence, optimal weighting of beliefs and communications in the individual mind will make it feel good to think and express content conforming to and flattering to one’s group’s shared beliefs and to attack and misrepresent rival groups. The more biased away from neutral truth, the better the communication functions to affirm coalitional identity, generating polarization in excess of actual policy disagreements. Communications of practical and functional truths are generally useless as differential signals, because any honest person might say them regardless of coalitional loyalty. In contrast, unusual, exaggerated beliefs \\[...\\] are unlikely to be said except as expressive of identity, because there is no external reality to motivate nonmembers to speak absurdities.\n\n> \\-\\- John Tooby, \"[Coalitional Instincts](https://www.edge.org/conversation/john_tooby-coalitional-instincts)\"\n\n  \n\n> Humans interact in dense social networks, and this poses a problem for bystanders when conflicts arise: which side, if any, to support. Choosing sides is a difficult strategic problem because the outcome of a conflict critically depends on which side other bystanders support. One strategy is siding with the higher status disputant, which can allow bystanders to coordinate with one another to take the same side, reducing fighting costs. However, this strategy carries the cost of empowering high-status individuals to exploit others. A second possible strategy is choosing sides based on preexisting relationships. This strategy balances power but carries another cost: Bystanders choose different sides, and this discoordination causes escalated conflicts and high fighting costs. We propose that moral cognition is designed to manage both of these problems by implementing a dynamic coordination strategy in which bystanders coordinate side-taking based on a public signal derived from disputants’ actions rather than their identities. By focusing on disputants’ actions, bystanders can dynamically change which individuals they support across different disputes, simultaneously solving the problems of coordination and exploitation.\n\n> \\-\\- Peter DeScioli & Robert Kurzban, \" [A Solution to the Mysteries of Morality](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.840.3768&rep=rep1&type=pdf)\""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HXA9WxPpzZCCEwXHT",
    "name": "Alief",
    "core": false,
    "slug": "alief",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">An </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">alief</span></strong><span><span class=\"by_DsNyKCEFecm4EHPNv\"> is a </span><span class=\"by_qxJ28GN72aiJu96iF\">belief-like attitude, behavior, or </span><span class=\"by_qgdGA4ZEyW7zNdK84\">expectation</span><span class=\"by_qxJ28GN72aiJu96iF\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">can coexist with a contradictory belief. For example,</span><span class=\"by_DsNyKCEFecm4EHPNv\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">fear felt when a monster jumps out of the darkness in a scary movie is based</span><span class=\"by_qxJ28GN72aiJu96iF\"> on</span><span class=\"by_DsNyKCEFecm4EHPNv\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the alief</span><span class=\"by_DsNyKCEFecm4EHPNv\"> that the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">monster is about</span><span class=\"by_DsNyKCEFecm4EHPNv\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">attack you, even though you </span><span class=\"by_DsNyKCEFecm4EHPNv\">believe that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">it cannot.</span></span></p><p><span><span class=\"by_PdzQ73mN7S4SvRMhu\">Philospoher Tamar Gendler introduced the</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> word </span><span class=\"by_PdzQ73mN7S4SvRMhu\">in her 2008 paper </span></span><i><span class=\"by_PdzQ73mN7S4SvRMhu\">Alief and Belief</span></i><span><span class=\"by_PdzQ73mN7S4SvRMhu\"> as</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> a sort of pun on </span></span><a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">dual process theory</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">; what beliefs (\"B-liefs\") are to system 2, aliefs (\"A-liefs\") are to system 1. Thus, beliefs are explicitly held beliefs which inform slow reasoning, while aliefs are implicit attitudes which guide fast reactions. However, dual process theory is not totally necessary to make sense of the term alief.</span></p><p><span><span class=\"by_PdzQ73mN7S4SvRMhu\">Gendler (2008) also introduced a</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> related pun </span><span class=\"by_PdzQ73mN7S4SvRMhu\">of</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> \"cesire vs desire\"; a desire (\"D-zire\") is an explicit want which enters into explicit planning, while a cesire (\"C-zire\") is an implicit one which guides reactions.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related tags: </span></i><a href=\"https://www.lesswrong.com/tag/belief\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Belief</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/emotions\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Emotion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">,&nbsp;</span></p><h2 id=\"Blog_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/1l/the_mystery_of_the_haunted_rationalist/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Mystery of the Haunted Rationalist</span></a></li><li><a href=\"https://www.lesswrong.com/lw/1xh/living_luminously/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Living Luminously</span></a></li></ul><h2 id=\"See_also\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">See</span><span class=\"by_qxJ28GN72aiJu96iF\"> also</span></span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/hollywood-rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hollywood rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/corrupted-hardware\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Corrupted hardware</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Separate_magisteria\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Separate magisteria</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Living_Luminously_(sequence)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Living Luminously (sequence)</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 20,
    "description": {
      "markdown": "An **alief** is a belief-like attitude, behavior, or expectation that can coexist with a contradictory belief. For example, the fear felt when a monster jumps out of the darkness in a scary movie is based on the alief that the monster is about to attack you, even though you believe that it cannot.\n\nPhilospoher Tamar Gendler introduced the word in her 2008 paper *Alief and Belief* as a sort of pun on [dual process theory](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2); what beliefs (\"B-liefs\") are to system 2, aliefs (\"A-liefs\") are to system 1. Thus, beliefs are explicitly held beliefs which inform slow reasoning, while aliefs are implicit attitudes which guide fast reactions. However, dual process theory is not totally necessary to make sense of the term alief.\n\nGendler (2008) also introduced a related pun of \"cesire vs desire\"; a desire (\"D-zire\") is an explicit want which enters into explicit planning, while a cesire (\"C-zire\") is an implicit one which guides reactions.\n\n*Related tags:* [Belief](https://www.lesswrong.com/tag/belief), [Emotion](https://www.lesswrong.com/tag/emotions), \n\nBlog posts\n----------\n\n*   [The Mystery of the Haunted Rationalist](https://www.lesswrong.com/lw/1l/the_mystery_of_the_haunted_rationalist/)\n*   [Living Luminously](https://www.lesswrong.com/lw/1xh/living_luminously/)\n\nSee also\n--------\n\n*   [Hollywood rationality](https://www.lesswrong.com/tag/hollywood-rationality)\n*   [Corrupted hardware](https://www.lesswrong.com/tag/corrupted-hardware)\n*   [Separate magisteria](https://wiki.lesswrong.com/wiki/Separate_magisteria)\n*   [Living Luminously (sequence)](https://wiki.lesswrong.com/wiki/Living_Luminously_(sequence))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nzvHaqwdXtvWkbonG",
    "name": "Risks of Astronomical Suffering (S-risks)",
    "core": false,
    "slug": "risks-of-astronomical-suffering-s-risks",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_2aoRX3ookcCozcb3m\">(Astronomical) suffering</span><span class=\"by_Co2dGXQxHAf92LHea\"> risks</span></span></strong><span><span class=\"by_2aoRX3ookcCozcb3m\">, also</span><span class=\"by_Co2dGXQxHAf92LHea\"> known as </span></span><strong><span class=\"by_Co2dGXQxHAf92LHea\">s-risks</span></strong><span><span class=\"by_2aoRX3ookcCozcb3m\">,</span><span class=\"by_Co2dGXQxHAf92LHea\"> are risks of the creation of</span><span class=\"by_Yw87LwbkMwGpB6hFh\"> intense</span><span class=\"by_Co2dGXQxHAf92LHea\"> suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.</span></span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">S-</span><span class=\"by_Co2dGXQxHAf92LHea\">risks </span><span class=\"by_2aoRX3ookcCozcb3m\">are an example</span><span class=\"by_Co2dGXQxHAf92LHea\"> of </span></span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_Co2dGXQxHAf92LHea\">existential risk</span></a><span><span class=\"by_Co2dGXQxHAf92LHea\"> </span><span class=\"by_2aoRX3ookcCozcb3m\">(also known as </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">x-risks</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\">) </span><span class=\"by_Co2dGXQxHAf92LHea\">according to</span><span class=\"by_2aoRX3ookcCozcb3m\"> Nick</span><span class=\"by_Co2dGXQxHAf92LHea\"> Bostrom's original definition, as they threaten to </span><span class=\"by_2aoRX3ookcCozcb3m\">\"permanently and drastically curtail [Earth-originating intelligent life'</span><span class=\"by_Co2dGXQxHAf92LHea\">s] potential\". </span><span class=\"by_2aoRX3ookcCozcb3m\">Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a</span><span class=\"by_Co2dGXQxHAf92LHea\"> useful </span><span class=\"by_2aoRX3ookcCozcb3m\">reminder that some x-risks are scary because they cause </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">bad</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> experiences, and not just because they prevent good ones.</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">Within the space of x-risks, we can</span><span class=\"by_Co2dGXQxHAf92LHea\"> distinguish </span><span class=\"by_2aoRX3ookcCozcb3m\">x-</span><span class=\"by_Co2dGXQxHAf92LHea\">risks that </span><span class=\"by_2aoRX3ookcCozcb3m\">are s-risks, x-risks involving human extinction, x-risks that involve immense suffering </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">and</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\"> human extinction,</span><span class=\"by_Co2dGXQxHAf92LHea\"> and </span><span class=\"by_2aoRX3ookcCozcb3m\">x-risks that involve neither. For example:</span></span></p><figure class=\"table\"><table><tbody><tr><td><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></td><td><strong><span class=\"by_2aoRX3ookcCozcb3m\">extinction risk</span></strong></td><td><strong><span class=\"by_2aoRX3ookcCozcb3m\">non-extinction risk</span></strong></td></tr><tr><td><strong><span class=\"by_2aoRX3ookcCozcb3m\">suffering risk</span></strong></td><td><span class=\"by_2aoRX3ookcCozcb3m\">Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</span></td><td><span><span class=\"by_2aoRX3ookcCozcb3m\">Misaligned AGI tiles the universe with </span><span class=\"by_Yw87LwbkMwGpB6hFh\">experiences of severe suffering.</span></span></td></tr><tr><td><strong><span class=\"by_2aoRX3ookcCozcb3m\">non-suffering risk</span></strong></td><td><span class=\"by_2aoRX3ookcCozcb3m\">Misaligned AGI wipes out humans.</span></td><td><span class=\"by_2aoRX3ookcCozcb3m\">Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</span></td></tr></tbody></table></figure><p><span class=\"by_2aoRX3ookcCozcb3m\">A related concept is </span><a href=\"https://arbital.com/p/hyperexistential_separation/\"><strong><span class=\"by_2aoRX3ookcCozcb3m\">hyperexistential risk</span></strong></a><span><span class=\"by_2aoRX3ookcCozcb3m\">, the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. </span><span class=\"by_Yw87LwbkMwGpB6hFh\">But arguably</span><span class=\"by_2aoRX3ookcCozcb3m\"> all s-risks are hyperexistential, since \"tiling the universe with </span><span class=\"by_Yw87LwbkMwGpB6hFh\">experiences of severe suffering\"</span><span class=\"by_Co2dGXQxHAf92LHea\"> would </span><span class=\"by_Yw87LwbkMwGpB6hFh\">likely </span><span class=\"by_2aoRX3ookcCozcb3m\">be worse than death.</span></span></p><p><span class=\"by_Yw87LwbkMwGpB6hFh\">There are two </span><a href=\"https://wiki.lesswrong.com/wiki/EA\"><span class=\"by_Yw87LwbkMwGpB6hFh\">EA</span></a><span class=\"by_Yw87LwbkMwGpB6hFh\"> organizations with s-risk prevention research as their primary focus: the </span><a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\"><span class=\"by_Yw87LwbkMwGpB6hFh\">Center on Long-Term Risk</span></a><span class=\"by_Yw87LwbkMwGpB6hFh\"> (CLR) and the </span><a href=\"https://centerforreducingsuffering.org/\"><span class=\"by_Yw87LwbkMwGpB6hFh\">Center for Reducing Suffering</span></a><span class=\"by_Yw87LwbkMwGpB6hFh\">. Much of CLR's work is on suffering-focused </span><a href=\"https://wiki.lesswrong.com/wiki/AI_safety\"><span class=\"by_Yw87LwbkMwGpB6hFh\">AI safety</span></a><span class=\"by_Yw87LwbkMwGpB6hFh\"> and </span><a href=\"https://www.lesswrong.com/tag/crucial-considerations\"><span class=\"by_Yw87LwbkMwGpB6hFh\">crucial considerations</span></a><span><span class=\"by_Yw87LwbkMwGpB6hFh\">. </span><span class=\"by_Co2dGXQxHAf92LHea\">Although</span><span class=\"by_Yw87LwbkMwGpB6hFh\"> to a much lesser extent,</span><span class=\"by_Co2dGXQxHAf92LHea\"> the </span></span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span class=\"by_Co2dGXQxHAf92LHea\">Machine Intelligence Research Institute</span></a><span class=\"by_Co2dGXQxHAf92LHea\"> and </span><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute-fhi\"><span class=\"by_Co2dGXQxHAf92LHea\">Future of Humanity Institute</span></a><span><span class=\"by_Co2dGXQxHAf92LHea\"> have investigated strategies to prevent s-</span><span class=\"by_Yw87LwbkMwGpB6hFh\">risks too.&nbsp;</span></span></p><p><span class=\"by_Co2dGXQxHAf92LHea\">Another approach to reducing s-risk is to \"expand the moral circle\" </span><a href=\"https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/\"><i><span class=\"by_Yw87LwbkMwGpB6hFh\">together</span></i></a><span><span class=\"by_Yw87LwbkMwGpB6hFh\"> with raising concern for suffering, </span><span class=\"by_Co2dGXQxHAf92LHea\">so that future (post)human civilizations and AI are less likely to </span></span><a href=\"https://www.lesswrong.com/tag/instrumental-value\"><span class=\"by_Co2dGXQxHAf92LHea\">instrumentally</span></a><span class=\"by_Co2dGXQxHAf92LHea\"> cause suffering to non-human minds such as animals or digital sentience. </span><a href=\"http://www.sentienceinstitute.org/\"><span class=\"by_Co2dGXQxHAf92LHea\">Sentience Institute</span></a><span class=\"by_Co2dGXQxHAf92LHea\"> works on this value-spreading problem.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"See_also\"><span class=\"by_Co2dGXQxHAf92LHea\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\"><span class=\"by_Yw87LwbkMwGpB6hFh\">Center on Long-Term Risk</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_Co2dGXQxHAf92LHea\">Existential risk</span></a></li><li><a href=\"https://www.lesswrong.com/tag/abolitionism\"><span class=\"by_Co2dGXQxHAf92LHea\">Abolitionism</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Mind_crime\"><span class=\"by_Co2dGXQxHAf92LHea\">Mind crime</span></a></li><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_Co2dGXQxHAf92LHea\">Utilitarianism</span></a><span class=\"by_Co2dGXQxHAf92LHea\">, </span><a href=\"https://www.lesswrong.com/tag/hedonism\"><span class=\"by_Co2dGXQxHAf92LHea\">Hedonism</span></a></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"External_links\"><span class=\"by_Co2dGXQxHAf92LHea\">External links</span></h2><ul><li><a href=\"https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/\"><span class=\"by_Co2dGXQxHAf92LHea\">Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)</span></a></li><li><a href=\"https://foundational-research.org/s-risks-talk-eag-boston-2017/\"><span class=\"by_Co2dGXQxHAf92LHea\">Introductory talk on s-risks (FRI)</span></a></li><li><a href=\"https://foundational-research.org/risks-of-astronomical-future-suffering/\"><span class=\"by_Co2dGXQxHAf92LHea\">Risks of Astronomical Future Suffering (FRI)</span></a></li><li><a href=\"https://foundational-research.org/files/suffering-focused-ai-safety.pdf\"><span class=\"by_Co2dGXQxHAf92LHea\">Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)</span></a></li><li><a href=\"https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering\"><span class=\"by_Co2dGXQxHAf92LHea\">Artificial Intelligence and Its Implications for Future Suffering (FRI)</span></a></li><li><a href=\"https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/\"><span class=\"by_Co2dGXQxHAf92LHea\">Expanding our moral circle to reduce suffering in the far future (Sentience Politics)</span></a></li><li><a href=\"https://sentience-politics.org/philosophy/the-importance-of-the-future/\"><span class=\"by_Co2dGXQxHAf92LHea\">The Importance of the Far Future (Sentience Politics)</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 34,
    "description": {
      "markdown": "**(Astronomical) suffering risks**, also known as **s-risks**, are risks of the creation of intense suffering in the far future on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far.\n\nS-risks are an example of [existential risk](https://www.lesswrong.com/tag/existential-risk) (also known as *x-risks*) according to Nick Bostrom's original definition, as they threaten to \"permanently and drastically curtail \\[Earth-originating intelligent life's\\] potential\". Most existential risks are of the form \"event E happens which drastically reduces the number of conscious experiences in the future\". S-risks therefore serve as a useful reminder that some x-risks are scary because they cause *bad* experiences, and not just because they prevent good ones.\n\nWithin the space of x-risks, we can distinguish x-risks that are s-risks, x-risks involving human extinction, x-risks that involve immense suffering *and* human extinction, and x-risks that involve neither. For example:\n\n<table><tbody><tr><td>&nbsp;</td><td><strong>extinction risk</strong></td><td><strong>non-extinction risk</strong></td></tr><tr><td><strong>suffering risk</strong></td><td>Misaligned AGI wipes out humans, simulates many suffering alien civilizations.</td><td>Misaligned AGI tiles the universe with experiences of severe suffering.</td></tr><tr><td><strong>non-suffering risk</strong></td><td>Misaligned AGI wipes out humans.</td><td>Misaligned AGI keeps humans as \"pets,\" limiting growth but not causing immense suffering.</td></tr></tbody></table>\n\nA related concept is [**hyperexistential risk**](https://arbital.com/p/hyperexistential_separation/), the risk of \"fates worse than death\" on an astronomical scale. It is not clear whether all hyperexistential risks are s-risks per se. But arguably all s-risks are hyperexistential, since \"tiling the universe with experiences of severe suffering\" would likely be worse than death.\n\nThere are two [EA](https://wiki.lesswrong.com/wiki/EA) organizations with s-risk prevention research as their primary focus: the [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr) (CLR) and the [Center for Reducing Suffering](https://centerforreducingsuffering.org/). Much of CLR's work is on suffering-focused [AI safety](https://wiki.lesswrong.com/wiki/AI_safety) and [crucial considerations](https://www.lesswrong.com/tag/crucial-considerations). Although to a much lesser extent, the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri) and [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) have investigated strategies to prevent s-risks too. \n\nAnother approach to reducing s-risk is to \"expand the moral circle\" [*together*](https://magnusvinding.com/2018/09/04/moral-circle-expansion-might-increase-future-suffering/) with raising concern for suffering, so that future (post)human civilizations and AI are less likely to [instrumentally](https://www.lesswrong.com/tag/instrumental-value) cause suffering to non-human minds such as animals or digital sentience. [Sentience Institute](http://www.sentienceinstitute.org/) works on this value-spreading problem.\n\nSee also\n--------\n\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n*   [Existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   [Abolitionism](https://www.lesswrong.com/tag/abolitionism)\n*   [Mind crime](https://wiki.lesswrong.com/wiki/Mind_crime)\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism), [Hedonism](https://www.lesswrong.com/tag/hedonism)\n\nExternal links\n--------------\n\n*   [Reducing Risks of Astronomical Suffering: A Neglected Global Priority (FRI)](https://foundational-research.org/reducing-risks-of-astronomical-suffering-a-neglected-global-priority/)\n*   [Introductory talk on s-risks (FRI)](https://foundational-research.org/s-risks-talk-eag-boston-2017/)\n*   [Risks of Astronomical Future Suffering (FRI)](https://foundational-research.org/risks-of-astronomical-future-suffering/)\n*   [Suffering-focused AI safety: Why \"fail-safe\" measures might be our top intervention PDF (FRI)](https://foundational-research.org/files/suffering-focused-ai-safety.pdf)\n*   [Artificial Intelligence and Its Implications for Future Suffering (FRI)](https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering)\n*   [Expanding our moral circle to reduce suffering in the far future (Sentience Politics)](https://sentience-politics.org/expanding-moral-circle-reduce-suffering-far-future/)\n*   [The Importance of the Far Future (Sentience Politics)](https://sentience-politics.org/philosophy/the-importance-of-the-future/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LaDu5bKDpe8LxaR7C",
    "name": "Suffering",
    "core": false,
    "slug": "suffering",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 72,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TkZ7MFwCi4D63LJ5n",
    "name": "Software Tools",
    "core": false,
    "slug": "software-tools",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">Specific pieces of software (downloadable or cloud/browser-based) that may be of interest to people on this site. The focus is on software with a practical application: for games, see </span><a href=\"https://www.lesswrong.com/tag/gaming-videogames-tabletop\"><span class=\"by_qxJ28GN72aiJu96iF\">Gaming (videogames/tabletop)</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Kickstarter for Coordinated Action</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 110,
    "description": {
      "markdown": "Specific pieces of software (downloadable or cloud/browser-based) that may be of interest to people on this site. The focus is on software with a practical application: for games, see [Gaming (videogames/tabletop)](https://www.lesswrong.com/tag/gaming-videogames-tabletop).\n\n**Related Sequences:** [Kickstarter for Coordinated Action](https://www.lesswrong.com/s/vz9Zrj3oBGsttG3Jh)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DLskYNGdAGDFpxBF8",
    "name": "Dual Process Theory (System 1 & System 2)",
    "core": false,
    "slug": "dual-process-theory-system-1-and-system-2",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Dual Process Theory</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> posits two </span><span class=\"by_qxJ28GN72aiJu96iF\">types of processes</span><span class=\"by_HoGziwmhpMGqGeWZy\"> in the human brain. </span><span class=\"by_qxJ28GN72aiJu96iF\">According to one characterization, </span></span><strong><span class=\"by_qxJ28GN72aiJu96iF\">Type 2 </span></strong><span class=\"by_qxJ28GN72aiJu96iF\">(also known as </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">System 2</span></strong><span class=\"by_qxJ28GN72aiJu96iF\">) processes are those which require working memory, and </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">Type 1</span></strong><span class=\"by_qxJ28GN72aiJu96iF\"> (also known as </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">System 1</span></strong><span class=\"by_qxJ28GN72aiJu96iF\">) are those which not.</span></p><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">The terms System 1 and System 2 were originally coined by the psychologist Keith Stanovich and then popularized by Daniel Kahneman in his book </span><em><span class=\"by_qxJ28GN72aiJu96iF\">Thinking, Fast and Slow.</span></em><span class=\"by_qxJ28GN72aiJu96iF\"> Stanovich noted that a number of fields within psychology had been developing various kinds of theories distinguishing between fast/intuitive on the one hand and slow/deliberative thinking on the other. Often these fields were not aware of each other. The S1/S2 model was offered as a general version of these specific theories, highlighting features of the two modes of thought that tended to appear in all the theories.</span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">Since then, academics have continued to discuss the models. Among other developments, </span><em><span class=\"by_qxJ28GN72aiJu96iF\">Stanovich and other authors have discontinued the use of the System 1/System 2 terminology as misleading</span></em><span class=\"by_qxJ28GN72aiJu96iF\">, choosing to instead talk about Type 1 and Type 2 processing. [...] there’s no single “System 1”: rather, a wide variety of different processes and systems are lumped together under this term. It is also unclear whether there is any single System 2, either. [...]</span></blockquote><blockquote><span><span class=\"by_qxJ28GN72aiJu96iF\">People sometimes refer to Type 1 reasoning as biased, and to Type 2 reasoning as unbiased. But [...] there is nothing that</span><span class=\"by_HoGziwmhpMGqGeWZy\"> makes </span><span class=\"by_qxJ28GN72aiJu96iF\">one of the two types intrinsically more or less biased than the other. The bias-correction power of Type 2 processing emerges from the fact that </span></span><em><span class=\"by_qxJ28GN72aiJu96iF\">if</span></em><span class=\"by_qxJ28GN72aiJu96iF\"> Type 1 operations are known to be erroneous </span><em><span class=\"by_qxJ28GN72aiJu96iF\">and</span></em><span><span class=\"by_qxJ28GN72aiJu96iF\"> a rule-based procedure for correcting them exists, a Type 2 operation can be learned</span><span class=\"by_HoGziwmhpMGqGeWZy\"> which </span><span class=\"by_qxJ28GN72aiJu96iF\">implements that rule. </span></span></blockquote><blockquote><span class=\"by_qxJ28GN72aiJu96iF\">-- Kaj Sotala, </span><a href=\"https://www.lesswrong.com/posts/HbXXd2givHBBLxr3d/against-system-1-and-system-2-subagent-sequence\"><span><span class=\"by_qxJ28GN72aiJu96iF\">Against \"System 1\" and \"</span><span class=\"by_HoGziwmhpMGqGeWZy\">System 2</span><span class=\"by_qxJ28GN72aiJu96iF\">\"</span></span></a></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": {
      "markdown": "**Dual Process Theory** posits two types of processes in the human brain. According to one characterization, **Type 2** (also known as **System 2**) processes are those which require working memory, and **Type 1** (also known as **System 1**) are those which not.\n\n> The terms System 1 and System 2 were originally coined by the psychologist Keith Stanovich and then popularized by Daniel Kahneman in his book _Thinking, Fast and Slow._ Stanovich noted that a number of fields within psychology had been developing various kinds of theories distinguishing between fast/intuitive on the one hand and slow/deliberative thinking on the other. Often these fields were not aware of each other. The S1/S2 model was offered as a general version of these specific theories, highlighting features of the two modes of thought that tended to appear in all the theories.\n\n> Since then, academics have continued to discuss the models. Among other developments, _Stanovich and other authors have discontinued the use of the System 1/System 2 terminology as misleading_, choosing to instead talk about Type 1 and Type 2 processing. \\[...\\] there’s no single “System 1”: rather, a wide variety of different processes and systems are lumped together under this term. It is also unclear whether there is any single System 2, either. \\[...\\]\n\n> People sometimes refer to Type 1 reasoning as biased, and to Type 2 reasoning as unbiased. But \\[...\\] there is nothing that makes one of the two types intrinsically more or less biased than the other. The bias-correction power of Type 2 processing emerges from the fact that _if_ Type 1 operations are known to be erroneous _and_ a rule-based procedure for correcting them exists, a Type 2 operation can be learned which implements that rule.\n\n> \\-\\- Kaj Sotala, [Against \"System 1\" and \"System 2\"](https://www.lesswrong.com/posts/HbXXd2givHBBLxr3d/against-system-1-and-system-2-subagent-sequence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fH8jPjHF2R27sRTTG",
    "name": "Education",
    "core": false,
    "slug": "education",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 153,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dHNS6r6LD6s2hEvZz",
    "name": "AI Services (CAIS)",
    "core": false,
    "slug": "ai-services-cais",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">An </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">AI service </span></strong><span class=\"by_qxJ28GN72aiJu96iF\">as used in the context of Eric Drexler's technical report </span><a href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa\"><span class=\"by_qxJ28GN72aiJu96iF\">Reframing Superintelligence: Comprehensive AI Services as General Intelligence</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> (CAIS), is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentive AGI, which carries out open-ended goals over an unbounded period of time. </span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">A gradual accumulation of increasingly competent services is one model of how AI might develop. For a summary, see </span><a href=\"https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as\"><span class=\"by_qxJ28GN72aiJu96iF\">this post</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": {
      "markdown": "An **AI service** as used in the context of Eric Drexler's technical report [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf?asd=sa) (CAIS), is an AI system that delivers bounded results for some task using bounded resources in bounded time. It is contrasted with agentive AGI, which carries out open-ended goals over an unbounded period of time.\n\nA gradual accumulation of increasingly competent services is one model of how AI might develop. For a summary, see [this post](https://www.lesswrong.com/posts/x3fNwSe5aWZb5yXEG/reframing-superintelligence-comprehensive-ai-services-as)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ac84EpK6mZbPLzmqj",
    "name": "General Intelligence",
    "core": false,
    "slug": "general-intelligence",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qxJ28GN72aiJu96iF\">General</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Intelligence</span></span></strong><span class=\"by_woC2b5rav5sGrAo3E\"> or </span><strong><span class=\"by_woC2b5rav5sGrAo3E\">Universal Intelligence</span></strong><span class=\"by_woC2b5rav5sGrAo3E\"> is the ability to efficiently achieve goals in a wide range of domains.&nbsp;</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">This tag is specifically for discussing intelligence</span><span class=\"by_woC2b5rav5sGrAo3E\"> in the </span><span class=\"by_qxJ28GN72aiJu96iF\">broad sense: for discussion of IQ testing and psychometric intelligence, see </span></span><a href=\"https://www.lesswrong.com/tag/iq-g-factor\"><span class=\"by_qxJ28GN72aiJu96iF\">IQ / g-factor</span></a><span class=\"by_qxJ28GN72aiJu96iF\">; for discussion about e.g. specific results in artificial intelligence, see </span><a href=\"https://www.lesswrong.com/tag/ai\"><span class=\"by_qxJ28GN72aiJu96iF\">AI</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">. These tags may overlap with this one</span><span class=\"by_woC2b5rav5sGrAo3E\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">the extent that they discuss the nature of general intelligence.</span></span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">Examples of posts that fall under this tag include </span><a href=\"https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence\"><span class=\"by_qxJ28GN72aiJu96iF\">The Power of Intelligence</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power\"><span class=\"by_qxJ28GN72aiJu96iF\">Measuring Optimization Power</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers\"><span class=\"by_qxJ28GN72aiJu96iF\">Adaption-Executers not Fitness Maximizers</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/posts/FbQ9Y9pBif5xZ7w2f/distinctions-in-types-of-thought\"><span><span class=\"by_qxJ28GN72aiJu96iF\">Distinctions</span><span class=\"by_woC2b5rav5sGrAo3E\"> in </span><span class=\"by_qxJ28GN72aiJu96iF\">Types of Thought</span></span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale\"><span><span class=\"by_qxJ28GN72aiJu96iF\">The Octopus, the Dolphin and Us: </span><span class=\"by_woC2b5rav5sGrAo3E\">a </span><span class=\"by_qxJ28GN72aiJu96iF\">Great Filter tale</span></span></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">On the difference between psychometric intelligence (IQ)</span><span class=\"by_woC2b5rav5sGrAo3E\"> and </span><span class=\"by_qxJ28GN72aiJu96iF\">general intelligence:</span></span></p><blockquote><p><span><span class=\"by_qxJ28GN72aiJu96iF\">But</span><span class=\"by_woC2b5rav5sGrAo3E\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">word </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“intelligence”</span><span class=\"by_qxJ28GN72aiJu96iF\"> commonly evokes pictures</span><span class=\"by_woC2b5rav5sGrAo3E\"> of the </span><span class=\"by_qxJ28GN72aiJu96iF\">starving professor with an IQ of 160 and the billionaire CEO with an IQ of merely 120. Indeed there are differences of individual ability apart from </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“book smarts”</span><span class=\"by_qxJ28GN72aiJu96iF\"> which contribute</span><span class=\"by_woC2b5rav5sGrAo3E\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">relative success</span><span class=\"by_woC2b5rav5sGrAo3E\"> in the </span><span class=\"by_qxJ28GN72aiJu96iF\">human world: enthusiasm, social skills, education, musical talent, rationality. Note that each factor I listed is cognitive. Social skills reside in the brain, not the liver. And jokes aside, you will not find many CEOs, nor yet professors</span><span class=\"by_woC2b5rav5sGrAo3E\"> of </span><span class=\"by_qxJ28GN72aiJu96iF\">academia, who are chimpanzees. You will not find</span><span class=\"by_woC2b5rav5sGrAo3E\"> many </span><span class=\"by_qxJ28GN72aiJu96iF\">acclaimed rationalists, nor artists, nor poets, nor leaders, nor engineers, nor skilled networkers, nor martial artists, nor musical composers who are mice. Intelligence</span><span class=\"by_woC2b5rav5sGrAo3E\"> is the </span><span class=\"by_qxJ28GN72aiJu96iF\">foundation</span><span class=\"by_woC2b5rav5sGrAo3E\"> of </span><span class=\"by_qxJ28GN72aiJu96iF\">human power,</span><span class=\"by_woC2b5rav5sGrAo3E\"> the </span><span class=\"by_qxJ28GN72aiJu96iF\">strength that fuels our other arts.</span></span></p></blockquote><blockquote><p><span><span class=\"by_qxJ28GN72aiJu96iF\">-- Eliezer Yudkowsky,</span><span class=\"by_woC2b5rav5sGrAo3E\"> </span></span><a href=\"https://intelligence.org/files/AIPosNegFactor.pdf\"><span><span class=\"by_qxJ28GN72aiJu96iF\">Artificial </span><span class=\"by_woC2b5rav5sGrAo3E\">Intelligence as </span><span class=\"by_qxJ28GN72aiJu96iF\">a Positive</span><span class=\"by_woC2b5rav5sGrAo3E\"> and </span><span class=\"by_qxJ28GN72aiJu96iF\">Negative Factor in Global Risk</span></span></a></p></blockquote><h2 id=\"Definitions_of_General_Intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Definitions of General Intelligence</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">After reviewing extensive literature on the subject, Legg and Hutter</span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefosnb04qur8\"><sup><a href=\"#fnosnb04qur8\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">[</span><span class=\"by_Sp5wM4aRAhNERd4oY\">1]</span></span></a></sup></span><span><span class=\"by_Sp5wM4aRAhNERd4oY\">&nbsp;</span><span class=\"by_qgdGA4ZEyW7zNdK84\">summarizes the many possible valuable definitions in the informal statement “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use </span></span><a href=\"https://lessestwrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - a formalization of </span><a href=\"https://lessestwrong.com/tag/occam-s-razor\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam's razor</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - to construct an </span><a href=\"https://lessestwrong.com/tag/aixi\"><span class=\"by_qgdGA4ZEyW7zNdK84\">universal artificial intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> with a embedded </span><a href=\"https://lessestwrong.com/tag/utility-functions\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utility function</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> which assigns less </span><a href=\"https://lessestwrong.com/tag/expected-utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utility</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> to those actions based on theories with higher </span><a href=\"https://wiki.lesswrong.com/wiki/Kolmogorov_complexity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">complexity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">We can relate Legg and Hutter's definition with the concept of </span><a href=\"https://lessestwrong.com/tag/optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">optimization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. According to </span><a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> intelligence is </span><a href=\"https://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">efficient cross-domain optimization</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent’s </span><span class=\"by_Sp5wM4aRAhNERd4oY\">preferences.</span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7hbpdfpe6x3\"><sup><a href=\"#fn7hbpdfpe6x3\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">[</span><span class=\"by_Sp5wM4aRAhNERd4oY\">2]</span></span></a></sup></span><span><span class=\"by_Sp5wM4aRAhNERd4oY\">&nbsp;</span><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It’s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov’s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">One could cast this definition in a possible world vocabulary, intelligence is:</span></p><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">using fewer resources than the other alternatives paths for getting there; and in the</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">most diverse domains as possible.</span></li></ol><p><span class=\"by_qgdGA4ZEyW7zNdK84\">How many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(a) the numbers of worlds with lower preference than the one realized,</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(b) how much smaller is the set of paths more efficient than the one taken by the agent and</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(c) how more wider are the domains where the agent can effectively realize his preferences;</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">and it is, accordingly, inversely proportional to:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(d) the numbers of world with higher preference than the one realized,</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(e) how much bigger is the set of paths more efficient than the one taken by the agent and</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">(f) how much more narrow are the domains where the agent can efficiently realize his preferences.</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">This definition avoids several problems common in many others definitions, especially it avoids </span><a href=\"https://lessestwrong.com/tag/anthropomorphism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">anthropomorphizing</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> intelligence.</span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimization process</span></a></li><li><a href=\"https://lessestwrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory</span></a></li><li><a href=\"https://lessestwrong.com/tag/rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></a></li><li><a href=\"http://arxiv.org/pdf/0712.3329.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Legg and Hutter paper “Universal Intelligence: A Deﬁnition of Machine Intelligence”</span></a></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnosnb04qur8\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefosnb04qur8\"><span class=\"by_Sp5wM4aRAhNERd4oY\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span class=\"by_Sp5wM4aRAhNERd4oY\">http://arxiv.org/pdf/0712.3329.pdf</span></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7hbpdfpe6x3\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7hbpdfpe6x3\"><span class=\"by_Sp5wM4aRAhNERd4oY\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span class=\"by_Sp5wM4aRAhNERd4oY\">http://intelligence.org/files/IE-EI.pdf</span></p></div></li></ol>",
      "sections": [
        {
          "title": "Definitions of General Intelligence",
          "anchor": "Definitions_of_General_Intelligence",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 76,
    "description": {
      "markdown": "**General Intelligence** or **Universal Intelligence** is the ability to efficiently achieve goals in a wide range of domains. \n\nThis tag is specifically for discussing intelligence in the broad sense: for discussion of IQ testing and psychometric intelligence, see [IQ / g-factor](https://www.lesswrong.com/tag/iq-g-factor); for discussion about e.g. specific results in artificial intelligence, see [AI](https://www.lesswrong.com/tag/ai). These tags may overlap with this one to the extent that they discuss the nature of general intelligence.\n\nExamples of posts that fall under this tag include [The Power of Intelligence](https://www.lesswrong.com/posts/aiQabnugDhcrFtr9n/the-power-of-intelligence), [Measuring Optimization Power](https://www.lesswrong.com/posts/Q4hLMDrFd8fbteeZ8/measuring-optimization-power), [Adaption-Executers not Fitness Maximizers](https://www.lesswrong.com/posts/XPErvb8m9FapXCjhA/adaptation-executers-not-fitness-maximizers), [Distinctions in Types of Thought](https://www.lesswrong.com/posts/FbQ9Y9pBif5xZ7w2f/distinctions-in-types-of-thought), [The Octopus, the Dolphin and Us: a Great Filter tale](https://www.lesswrong.com/posts/GMqZ2ofMnxwhoa7fD/the-octopus-the-dolphin-and-us-a-great-filter-tale).\n\nOn the difference between psychometric intelligence (IQ) and general intelligence:\n\n> But the word “intelligence” commonly evokes pictures of the starving professor with an IQ of 160 and the billionaire CEO with an IQ of merely 120. Indeed there are differences of individual ability apart from “book smarts” which contribute to relative success in the human world: enthusiasm, social skills, education, musical talent, rationality. Note that each factor I listed is cognitive. Social skills reside in the brain, not the liver. And jokes aside, you will not find many CEOs, nor yet professors of academia, who are chimpanzees. You will not find many acclaimed rationalists, nor artists, nor poets, nor leaders, nor engineers, nor skilled networkers, nor martial artists, nor musical composers who are mice. Intelligence is the foundation of human power, the strength that fuels our other arts.\n\n> \\-\\- Eliezer Yudkowsky, [Artificial Intelligence as a Positive and Negative Factor in Global Risk](https://intelligence.org/files/AIPosNegFactor.pdf)\n\nDefinitions of General Intelligence\n-----------------------------------\n\nAfter reviewing extensive literature on the subject, Legg and Hutter^[\\[1\\]](#fnosnb04qur8)^ summarizes the many possible valuable definitions in the informal statement “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” They then show this definition can be mathematically formalized given reasonable mathematical definitions of its terms. They use [Solomonoff induction](https://lessestwrong.com/tag/solomonoff-induction) \\- a formalization of [Occam's razor](https://lessestwrong.com/tag/occam-s-razor) \\- to construct an [universal artificial intelligence](https://lessestwrong.com/tag/aixi) with a embedded [utility function](https://lessestwrong.com/tag/utility-functions) which assigns less [utility](https://lessestwrong.com/tag/expected-utility) to those actions based on theories with higher [complexity](https://wiki.lesswrong.com/wiki/Kolmogorov_complexity). They argue this final formalization is a valid, meaningful, informative, general, unbiased, fundamental, objective, universal and practical definition of intelligence.\n\nWe can relate Legg and Hutter's definition with the concept of [optimization](https://lessestwrong.com/tag/optimization). According to [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) intelligence is [efficient cross-domain optimization](https://lessestwrong.com/lw/vb/efficient_crossdomain_optimization/). It measures an agent's capacity for efficient cross-domain optimization of the world according to the agent’s preferences.^[\\[2\\]](#fn7hbpdfpe6x3)^ Optimization measures not only the capacity to achieve the desired goal but also is inversely proportional to the amount of resources used. It’s the ability to steer the future so it hits that small target of desired outcomes in the large space of all possible outcomes, using fewer resources as possible. For example, when Deep Blue defeated Kasparov, it was able to hit that small possible outcome where it made the right order of moves given Kasparov’s moves from the very large set of all possible moves. In that domain, it was more optimal than Kasparov. However, Kasparov would have defeated Deep Blue in almost any other relevant domain, and hence, he is considered more intelligent.\n\nOne could cast this definition in a possible world vocabulary, intelligence is:\n\n1.  the ability to precisely realize one of the members of a small set of possible future worlds that have a higher preference over the vast set of all other possible worlds with lower preference; while\n2.  using fewer resources than the other alternatives paths for getting there; and in the\n3.  most diverse domains as possible.\n\nHow many more worlds have a higher preference then the one realized by the agent, less intelligent he is. How many more worlds have a lower preference than the one realized by the agent, more intelligent he is. (Or: How much smaller is the set of worlds at least as preferable as the one realized, more intelligent the agent is). How much less paths for realizing the desired world using fewer resources than those spent by the agent, more intelligent he is. And finally, in how many more domains the agent can be more efficiently optimal, more intelligent he is. Restating it, the intelligence of an agent is directly proportional to:\n\n*   (a) the numbers of worlds with lower preference than the one realized,\n*   (b) how much smaller is the set of paths more efficient than the one taken by the agent and\n*   (c) how more wider are the domains where the agent can effectively realize his preferences;\n\nand it is, accordingly, inversely proportional to:\n\n*   (d) the numbers of world with higher preference than the one realized,\n*   (e) how much bigger is the set of paths more efficient than the one taken by the agent and\n*   (f) how much more narrow are the domains where the agent can efficiently realize his preferences.\n\nThis definition avoids several problems common in many others definitions, especially it avoids [anthropomorphizing](https://lessestwrong.com/tag/anthropomorphism) intelligence.\n\nSee Also\n--------\n\n*   [Optimization process](https://lessestwrong.com/tag/optimization)\n*   [Decision theory](https://lessestwrong.com/tag/decision-theory)\n*   [Rationality](https://lessestwrong.com/tag/rationality)\n*   [Legg and Hutter paper “Universal Intelligence: A Deﬁnition of Machine Intelligence”](http://arxiv.org/pdf/0712.3329.pdf)\n\n1.  ^**[^](#fnrefosnb04qur8)**^\n    \n    http://arxiv.org/pdf/0712.3329.pdf\n    \n2.  ^**[^](#fnref7hbpdfpe6x3)**^\n    \n    http://intelligence.org/files/IE-EI.pdf"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ouT6wKhACJRouGokM",
    "name": "Moral Uncertainty",
    "core": false,
    "slug": "moral-uncertainty",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_cn4SiEmqWbu7K9em5\">Moral uncertainty</span></strong><span class=\"by_cn4SiEmqWbu7K9em5\"> (or </span><strong><span class=\"by_cn4SiEmqWbu7K9em5\">normative uncertainty</span></strong><span><span class=\"by_cn4SiEmqWbu7K9em5\">) is uncertainty about </span><span class=\"by_bNzw3cSaA6NZd65tB\">what we ought, morally,</span><span class=\"by_woC2b5rav5sGrAo3E\"> to </span><span class=\"by_bNzw3cSaA6NZd65tB\">do</span><span class=\"by_woC2b5rav5sGrAo3E\"> given the diversity of moral doctrines. </span><span class=\"by_qxJ28GN72aiJu96iF\">For example, suppose that we knew for certain that new technology would enable more humans to live on another planet with slightly less well-being than on Earth</span></span><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn1\"><sup><span class=\"by_qxJ28GN72aiJu96iF\">1</span></sup></a><span class=\"by_qxJ28GN72aiJu96iF\">. An average </span><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_qxJ28GN72aiJu96iF\">utilitarian</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> would consider these consequences bad, while a total utilitarian would endorse such technology. If we are uncertain about which of these two theories are right, what should we do?</span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">Moral uncertainty</span><span class=\"by_woC2b5rav5sGrAo3E\"> includes a level of uncertainty above the more usual uncertainty of </span></span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span><span class=\"by_cn4SiEmqWbu7K9em5\">what </span><span class=\"by_woC2b5rav5sGrAo3E\">to</span><span class=\"by_cn4SiEmqWbu7K9em5\"> do </span><span class=\"by_woC2b5rav5sGrAo3E\">given incomplete information</span></span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> since it deals also with</span><span class=\"by_cn4SiEmqWbu7K9em5\"> uncertainty about </span><span class=\"by_woC2b5rav5sGrAo3E\">which moral theory is right. Even with complete information</span><span class=\"by_cn4SiEmqWbu7K9em5\"> about the </span><span class=\"by_bNzw3cSaA6NZd65tB\">world,</span><span class=\"by_woC2b5rav5sGrAo3E\"> this kind</span><span class=\"by_cn4SiEmqWbu7K9em5\"> of </span><span class=\"by_woC2b5rav5sGrAo3E\">uncertainty would still remain </span></span><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn1\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">1</span></sup></a><span><span class=\"by_woC2b5rav5sGrAo3E\">. In one level of uncertainty, one can have doubts on how</span><span class=\"by_cn4SiEmqWbu7K9em5\"> to </span><span class=\"by_woC2b5rav5sGrAo3E\">act because all</span><span class=\"by_cn4SiEmqWbu7K9em5\"> the </span><span class=\"by_woC2b5rav5sGrAo3E\">relevant empirical information isn’t available, for </span><span class=\"by_bNzw3cSaA6NZd65tB\">example,</span><span class=\"by_woC2b5rav5sGrAo3E\"> choosing</span><span class=\"by_cn4SiEmqWbu7K9em5\"> whether to </span><span class=\"by_woC2b5rav5sGrAo3E\">implement</span><span class=\"by_cn4SiEmqWbu7K9em5\"> or </span><span class=\"by_woC2b5rav5sGrAo3E\">not a new technology (e.g.: </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_woC2b5rav5sGrAo3E\">AGI</span></a><span class=\"by_woC2b5rav5sGrAo3E\">, </span><a href=\"https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement\"><span class=\"by_woC2b5rav5sGrAo3E\">Biological Cognitive Enhancement</span></a><span class=\"by_woC2b5rav5sGrAo3E\">, </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_woC2b5rav5sGrAo3E\">Mind Uploading</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\">) not fully knowing about its consequences and nature. But even if we ideally get to know each and every </span><span class=\"by_bNzw3cSaA6NZd65tB\">consequence</span><span class=\"by_woC2b5rav5sGrAo3E\"> of new technology, we would still need to know which</span><span class=\"by_cn4SiEmqWbu7K9em5\"> is the right </span><span class=\"by_woC2b5rav5sGrAo3E\">ethical perspective for analyzing these consequences.</span></span></p><p><span><span class=\"by_woC2b5rav5sGrAo3E\">One</span><span class=\"by_cn4SiEmqWbu7K9em5\"> approach is to follow only the most probable theory. This has its own problems. For example, what if the most probable theory points only weakly in one way, and other theories point strongly the other way?</span><span class=\"by_woC2b5rav5sGrAo3E\"> A better approach is to “perform the action with the highest expected moral value. We get the expected moral value of an action by multiplying the subjective probability that some theory is true by the value of that action if it is true, doing the same for all of the other theories, and adding up the results.” </span></span><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn2\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">2</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\"> However, we would still need a method of comparing value intertheories, an </span><a href=\"https://www.lesswrong.com/tag/utility\"><span class=\"by_woC2b5rav5sGrAo3E\">utilon</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> in one theory may not be the same with an utilon in another theory. Outside </span><a href=\"https://www.lesswrong.com/tag/consequentialism\"><span class=\"by_woC2b5rav5sGrAo3E\">consequentialism</span></a><span class=\"by_woC2b5rav5sGrAo3E\">, many ethical theories don’t use utilions or even any quantifiable values. This is still an open problem.</span></p><p><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_cn4SiEmqWbu7K9em5\">Nick Bostrom</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> and </span><a href=\"https://en.wikipedia.org/wiki/Toby_Ord\"><span class=\"by_cn4SiEmqWbu7K9em5\">Toby Ord</span></a><span class=\"by_cn4SiEmqWbu7K9em5\"> have proposed a </span><a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">parliamentary model</span></a><span class=\"by_cn4SiEmqWbu7K9em5\">. In this model, each theory sends a number of delegates to a parliament in proportion to its probability. The theories then bargain for support as if the probability of each action were proportional to its votes. However, the actual output is always the action with the most votes. Bostrom and Ord's proposal lets probable theories determine most actions, but still gives less probable theories influence on issues they consider unusually important.</span></p><p><span><span class=\"by_woC2b5rav5sGrAo3E\">Even with a high degree of moral uncertainty and a wide range of possible moral theories, there are still certain actions </span><span class=\"by_bNzw3cSaA6NZd65tB\">that seem</span><span class=\"by_woC2b5rav5sGrAo3E\"> highly valuable in any theory. Bostrom argues that </span></span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_woC2b5rav5sGrAo3E\">Existential risk</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> reduction is among </span><span class=\"by_bNzw3cSaA6NZd65tB\">them,</span><span class=\"by_woC2b5rav5sGrAo3E\"> showing that </span><span class=\"by_bNzw3cSaA6NZd65tB\">it is </span><span class=\"by_woC2b5rav5sGrAo3E\">not only the most important task given most versions of </span><span class=\"by_bNzw3cSaA6NZd65tB\">consequentialism</span><span class=\"by_woC2b5rav5sGrAo3E\"> but highly recommended by many of the other widely acceptable moral theories</span></span><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fn3\"><sup><span class=\"by_woC2b5rav5sGrAo3E\">3</span></sup></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></p><h2 id=\"External_links\"><span class=\"by_HoGziwmhpMGqGeWZy\">External links</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\"><span class=\"by_cn4SiEmqWbu7K9em5\">Moral uncertainty — towards a solution?</span></a></li></ul><h2 id=\"Sequences\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Sequences</span></h2><ul><li><a href=\"https://www.lesswrong.com/s/4NFwxwzLzpiikfkk3\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Moral uncertainty</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_cn4SiEmqWbu7K9em5\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_cn4SiEmqWbu7K9em5\">Expected utility</span></a></li><li><a href=\"https://www.lesswrong.com/tag/value-learning\"><span class=\"by_cn4SiEmqWbu7K9em5\">Value learning</span></a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics\"><span class=\"by_cn4SiEmqWbu7K9em5\">Metaethics</span></a></li></ul><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Crouch, William. (2010) “Moral Uncertainty and Intertheoretic Comparisons of Value” BPhil Thesis, 2010. p. 6. Available at: </span><a href=\"http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral_Uncertainty_and_Intertheoretic_Comparisons_of_Value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral_Uncertainty_and_Intertheoretic_Comparisons_of_Value</span></a><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fnref1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Sepielli, Andrew. (2008) “Moral Uncertainty and the Principle of Equity among Moral Theories\". ISUS-X, Tenth Conference of the International Society for Utilitarian Studies, Kadish Center for Morality, Law and Public Affairs, UC Berkeley. Available at: </span><a href=\"http://escholarship.org/uc/item/7h5852rr.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://escholarship.org/uc/item/7h5852rr.pdf</span></a><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fnref2\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom, Nick. (2012) \"Existential Risk Reduction as the Most Important Task for Humanity\" Global Policy, forthcoming, 2012. p. 22. Available at: </span><a href=\"http://www.existential-risk.org/concept.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://www.existential-risk.org/concept.pdf</span></a><a href=\"https://www.lesswrong.com/tag/moral-uncertainty#fnref3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li></ol>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "Sequences",
          "anchor": "Sequences",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 56,
    "description": {
      "markdown": "**Moral uncertainty** (or **normative uncertainty**) is uncertainty about what we ought, morally, to do given the diversity of moral doctrines. For example, suppose that we knew for certain that new technology would enable more humans to live on another planet with slightly less well-being than on Earth[^1^](https://www.lesswrong.com/tag/moral-uncertainty#fn1). An average [utilitarian](https://www.lesswrong.com/tag/utilitarianism) would consider these consequences bad, while a total utilitarian would endorse such technology. If we are uncertain about which of these two theories are right, what should we do?\n\nMoral uncertainty includes a level of uncertainty above the more usual uncertainty of [what to do given incomplete information](https://www.lesswrong.com/tag/decision-theory) since it deals also with uncertainty about which moral theory is right. Even with complete information about the world, this kind of uncertainty would still remain [^1^](https://www.lesswrong.com/tag/moral-uncertainty#fn1). In one level of uncertainty, one can have doubts on how to act because all the relevant empirical information isn’t available, for example, choosing whether to implement or not a new technology (e.g.: [AGI](https://www.lesswrong.com/tag/artificial-general-intelligence), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement), [Mind Uploading](https://www.lesswrong.com/tag/whole-brain-emulation)) not fully knowing about its consequences and nature. But even if we ideally get to know each and every consequence of new technology, we would still need to know which is the right ethical perspective for analyzing these consequences.\n\nOne approach is to follow only the most probable theory. This has its own problems. For example, what if the most probable theory points only weakly in one way, and other theories point strongly the other way? A better approach is to “perform the action with the highest expected moral value. We get the expected moral value of an action by multiplying the subjective probability that some theory is true by the value of that action if it is true, doing the same for all of the other theories, and adding up the results.” [^2^](https://www.lesswrong.com/tag/moral-uncertainty#fn2) However, we would still need a method of comparing value intertheories, an [utilon](https://www.lesswrong.com/tag/utility) in one theory may not be the same with an utilon in another theory. Outside [consequentialism](https://www.lesswrong.com/tag/consequentialism), many ethical theories don’t use utilions or even any quantifiable values. This is still an open problem.\n\n[Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom) and [Toby Ord](https://en.wikipedia.org/wiki/Toby_Ord) have proposed a [parliamentary model](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html). In this model, each theory sends a number of delegates to a parliament in proportion to its probability. The theories then bargain for support as if the probability of each action were proportional to its votes. However, the actual output is always the action with the most votes. Bostrom and Ord's proposal lets probable theories determine most actions, but still gives less probable theories influence on issues they consider unusually important.\n\nEven with a high degree of moral uncertainty and a wide range of possible moral theories, there are still certain actions that seem highly valuable in any theory. Bostrom argues that [Existential risk](https://www.lesswrong.com/tag/existential-risk) reduction is among them, showing that it is not only the most important task given most versions of consequentialism but highly recommended by many of the other widely acceptable moral theories[^3^](https://www.lesswrong.com/tag/moral-uncertainty#fn3).\n\nExternal links\n--------------\n\n*   [Moral uncertainty — towards a solution?](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html)\n\nSequences\n---------\n\n*   [Moral uncertainty](https://www.lesswrong.com/s/4NFwxwzLzpiikfkk3)\n\nSee also\n--------\n\n*   [Expected utility](https://www.lesswrong.com/tag/expected-utility)\n*   [Value learning](https://www.lesswrong.com/tag/value-learning)\n*   [Metaethics](https://www.lesswrong.com/tag/metaethics)\n\nReferences\n----------\n\n1.  Crouch, William. (2010) “Moral Uncertainty and Intertheoretic Comparisons of Value” BPhil Thesis, 2010. p. 6. Available at: [http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral\\_Uncertainty\\_and\\_Intertheoretic\\_Comparisons\\_of\\_Value](http://oxford.academia.edu/WilliamCrouch/Papers/873903/Moral_Uncertainty_and_Intertheoretic_Comparisons_of_Value)[↩](https://www.lesswrong.com/tag/moral-uncertainty#fnref1)\n2.  Sepielli, Andrew. (2008) “Moral Uncertainty and the Principle of Equity among Moral Theories\". ISUS-X, Tenth Conference of the International Society for Utilitarian Studies, Kadish Center for Morality, Law and Public Affairs, UC Berkeley. Available at: [http://escholarship.org/uc/item/7h5852rr.pdf](http://escholarship.org/uc/item/7h5852rr.pdf)[↩](https://www.lesswrong.com/tag/moral-uncertainty#fnref2)\n3.  Bostrom, Nick. (2012) \"Existential Risk Reduction as the Most Important Task for Humanity\" Global Policy, forthcoming, 2012. p. 22. Available at: [http://www.existential-risk.org/concept.pdf](http://www.existential-risk.org/concept.pdf)[↩](https://www.lesswrong.com/tag/moral-uncertainty#fnref3)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nSHiKwWyMZFdZg5qt",
    "name": "Ethics & Morality",
    "core": false,
    "slug": "ethics-and-morality",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">For general discussion about </span><strong><span><span class=\"by_qxJ28GN72aiJu96iF\">ethics and </span><span class=\"by_XtphY3uYHwruKqDyG\">morality</span></span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\">.</span><span class=\"by_qxJ28GN72aiJu96iF\"> Example posts: </span></span><a href=\"https://www.lesswrong.com/posts/fATPBv4pnHC33EmJ2/fake-morality\"><span class=\"by_qxJ28GN72aiJu96iF\">Fake Morality</span></a><span class=\"by_qxJ28GN72aiJu96iF\">; </span><a href=\"https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality\"><span class=\"by_qxJ28GN72aiJu96iF\">What Would You Do Without Morality?</span></a><span class=\"by_qxJ28GN72aiJu96iF\">; </span><a href=\"https://www.lesswrong.com/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality\"><span class=\"by_qxJ28GN72aiJu96iF\">The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It</span></a><span class=\"by_qxJ28GN72aiJu96iF\">; </span><a href=\"https://www.lesswrong.com/posts/faHbrHuPziFH7Ef7p/why-are-individual-iq-differences-ok\"><span class=\"by_qxJ28GN72aiJu96iF\">Why Are Individual IQ Differences OK?</span></a><span class=\"by_qxJ28GN72aiJu96iF\">; </span><a href=\"https://www.lesswrong.com/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome\"><span class=\"by_qxJ28GN72aiJu96iF\">Morality is Awesome</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">See also </span><a href=\"https://www.lesswrong.com/tag/consequentialism\"><span class=\"by_qxJ28GN72aiJu96iF\">Consequentialism</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/tag/deontology\"><span class=\"by_qxJ28GN72aiJu96iF\">Deontology</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, </span><a href=\"https://www.lesswrong.com/tag/metaethics\"><span class=\"by_qxJ28GN72aiJu96iF\">Metaethics</span></a><span class=\"by_qxJ28GN72aiJu96iF\">, and </span><a href=\"https://www.lesswrong.com/tag/moral-uncertainty\"><span class=\"by_qxJ28GN72aiJu96iF\">Moral Uncertainty</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 288,
    "description": {
      "markdown": "For general discussion about **ethics and morality**. Example posts: [Fake Morality](https://www.lesswrong.com/posts/fATPBv4pnHC33EmJ2/fake-morality); [What Would You Do Without Morality?](https://www.lesswrong.com/posts/iGH7FSrdoCXa5AHGs/what-would-you-do-without-morality); [The Terrible, Horrible, No Good, Very Bad Truth About Morality and What To Do About It](https://www.lesswrong.com/posts/B5K3hg8FgrMDHuXjH/the-terrible-horrible-no-good-very-bad-truth-about-morality); [Why Are Individual IQ Differences OK?](https://www.lesswrong.com/posts/faHbrHuPziFH7Ef7p/why-are-individual-iq-differences-ok); [Morality is Awesome](https://www.lesswrong.com/posts/Aq8BQMXRZX3BoFd4c/morality-is-awesome).\n\nSee also [Consequentialism](https://www.lesswrong.com/tag/consequentialism), [Deontology](https://www.lesswrong.com/tag/deontology), [Metaethics](https://www.lesswrong.com/tag/metaethics), and [Moral Uncertainty](https://www.lesswrong.com/tag/moral-uncertainty)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4cKQgA4S7xfNeeWXg",
    "name": "IQ and g-factor",
    "core": false,
    "slug": "iq-and-g-factor",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">IQ </span></strong><span class=\"by_qxJ28GN72aiJu96iF\">is a score derived from a set of standardized tests designed to assess human intelligence. The </span><strong><span class=\"by_qxJ28GN72aiJu96iF\">g-factor </span></strong><span class=\"by_qxJ28GN72aiJu96iF\">(general intelligence factor) is the underlying psychometric construct that the IQ tests are trying to measure.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">This tag is specifically for discussions about these formal constructs. For discussions about artificial intelligence, see </span><a href=\"https://www.lesswrong.com/tag/ai\"><span class=\"by_qxJ28GN72aiJu96iF\">AI</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">. For discussions about </span><span class=\"by_EQNTWXLKMeWMp2FQS\">human-level</span><span class=\"by_qxJ28GN72aiJu96iF\"> intelligence in a broader sense, see </span></span><a href=\"https://www.lesswrong.com/tag/general-intelligence\"><span class=\"by_qxJ28GN72aiJu96iF\">General Intelligence</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 53,
    "description": {
      "markdown": "**IQ** is a score derived from a set of standardized tests designed to assess human intelligence. The **g-factor** (general intelligence factor) is the underlying psychometric construct that the IQ tests are trying to measure.\n\nThis tag is specifically for discussions about these formal constructs. For discussions about artificial intelligence, see [AI](https://www.lesswrong.com/tag/ai). For discussions about human-level intelligence in a broader sense, see [General Intelligence](https://www.lesswrong.com/tag/general-intelligence)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BhfefamXXee6c2CH8",
    "name": "Transcripts",
    "core": false,
    "slug": "transcripts",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 35,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8e9e8fzXuW5gGBS3F",
    "name": "Qualia",
    "core": false,
    "slug": "qualia",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_4fh2AAe3n7oBviyxx\">Subjective conscious experience. The discussion of qualia tends to come up on LessWrong in two contexts: as an argument against reductionism (with a claim that qualia cannot be a mere matter of, uh, matter), and as a key factor in how seriously we should weight the suffering of animals.</span><span class=\"by_nRknKQuPzoG2Wuyyi\"> There is also a third context for Qualia arguments: as an argument against Whole Brain Emulation being you, or of emulations of minds not being conscious and instead P-zonbies.</span></span></p><p><span class=\"by_4fh2AAe3n7oBviyxx\">There are several philosophical discussions of how to reduce qualia into functions of the brain (most famously by Daniel Dennett), but the discussion continues among philosophers.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 33,
    "description": {
      "markdown": "Subjective conscious experience. The discussion of qualia tends to come up on LessWrong in two contexts: as an argument against reductionism (with a claim that qualia cannot be a mere matter of, uh, matter), and as a key factor in how seriously we should weight the suffering of animals. There is also a third context for Qualia arguments: as an argument against Whole Brain Emulation being you, or of emulations of minds not being conscious and instead P-zonbies.\n\nThere are several philosophical discussions of how to reduce qualia into functions of the brain (most famously by Daniel Dennett), but the discussion continues among philosophers."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "25oxqHiadqM6Hf7Gn",
    "name": "Great Filter",
    "core": false,
    "slug": "great-filter",
    "tableOfContents": {
      "html": "<p><span class=\"by_qQqgj5ScvgQsJk7Ti\">The </span><strong><span class=\"by_qQqgj5ScvgQsJk7Ti\">Great Filter</span></strong><span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a proposed explanation for the </span></span><a href=\"http://en.wikipedia.org/wiki/Fermi_paradox\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fermi Paradox</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. The development of intelligent</span><span class=\"by_qxJ28GN72aiJu96iF\"> life </span><span class=\"by_qgdGA4ZEyW7zNdK84\">requires many steps, such as the emergence of single-celled life and the transition from unicellular to multicellular life forms. Since we have not observed intelligent life beyond our planet, there seems to be a developmental step that is </span><span class=\"by_qxJ28GN72aiJu96iF\">so </span><span class=\"by_qgdGA4ZEyW7zNdK84\">difficult and unlikely that it \"filters out\" nearly all civilizations before they can reach a space-faring stage. Robin Hanson coined</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">term in his 1998 essay </span></span><a href=\"http://hanson.gmu.edu/greatfilter.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Great Filter - Are We Almost Past It?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;From that essay:</span></p><blockquote><p><span><span class=\"by_qxJ28GN72aiJu96iF\">Humanity seems to have a bright future, i.e., a non-trivial chance of expanding to fill the universe with lasting life. But the fact that space near us seems dead now tells us that any given piece of dead matter faces an astronomically low chance of begating such a future. There thus exists a great filter between death and expanding lasting life, and humanity faces the ominous question: how far along this filter are we?</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></span></p></blockquote><h2 id=\"Should_we_worry_\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Should we worry?</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The Great Filter might be a step in our evolutionary past, in which case our civilization has already passed it. But the hard step might also be ahead of us: surviving the creation of nuclear bombs, </span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AGI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, biotechnology, </span><a href=\"https://lessestwrong.com/tag/nanotechnology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">nanotechnology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> or an asteroid impact </span><a href=\"http://www.global-catastrophic-risks.com/docs/Chap01.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. In that case, we should be worried, as the Great Filter seems to have been successful in stopping the development of every other civilization so far. Estimating the location of the Great Filter is thus important for helping estimate the magnitude of </span><a href=\"https://lessestwrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">existential risk</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. </span><a href=\"http://hanson.gmu.edu/greatfilter.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Many</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://hanson.gmu.edu/hardstep.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">efforts</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://www.stat.berkeley.edu/~aldous/Papers/GF.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">have</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://www.nickbostrom.com/papers/fermi.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">been</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://www.global-catastrophic-risks.com/docs/Chap01.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">made</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">in</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> that direction, but much remains uncertain.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Traces of life on other planets are evidence for a later Great Filter</span><a href=\"http://www.youtube.com/watch?v=_W8zu7lFmhY\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. If we were to find that complex life had evolved independently both on Earth and some other planet, it would suggest that getting to such a developmental stage was relatively easy. Thus the Great Filter would have to be at a later stage.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The study of </span><a href=\"http://en.wikipedia.org/wiki/Extinction_event#Major_extinction_events\"><span class=\"by_qgdGA4ZEyW7zNdK84\">past mass extinctions</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> and astrobiology can provide ideas for estimating the location of the Great Filter. However, there are many difficulties involved. For instance, the time that it takes to pass a step doesn't reveal much about how easy or hard that step was. </span><span class=\"by_qQqgj5ScvgQsJk7Ti\">Robin </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Hanson gives the following example in his </span></span><a href=\"http://hanson.gmu.edu/greatfilter.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">seminal paper</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">\"…say you have one hour to pick five locks by trial and error, locks with 1,2,3,4, and 5 dials of ten numbers, so that the expected time to pick each lock is .01,.1, 1, 10, and 100 hours respectively. Then just looking at those rare cases when you do pick all five locks in the hour, the average time to pick the first two locks would be .0096 and .075 hours respectively, close to the usual expected times of .01 and .1 hours. The average time to pick the third lock, however, would be .20 hours, and the average time for the other two locks, and the average time left over at the end, would be .24 hours. That is, conditional on success, all the hard steps, no matter how hard, take about the same time, while easy steps take about their usual time.\"</span></p><h2 id=\"Consequences\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consequences</span></h2><p><a href=\"http://hanson.gmu.edu/hardstep.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">In a subsequent paper</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Hanson constructs a simulation of the distribution of the hard steps, which suggests that there should be about four to seven hard steps, uniformly distributed in our past. It also suggests that there has been at least one hard step since the evolution of hominids, and that the best extinction model that fits all these requirements is </span><a href=\"http://www.pnas.org/content/91/15/6735.full.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">William Schopf's model</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Taking evolutionary arguments for </span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AGI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"https://lessestwrong.com/tag/observation-selection-effect\"><span class=\"by_qgdGA4ZEyW7zNdK84\">observation selection effects</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> together, </span><a href=\"http://www.nickbostrom.com/aievolution.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom and Shulman argue</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> that Hanson’s results can help estimate the difficulty of creating AGI.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2010/03/very-bad-news.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Very Bad News</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"https://lessestwrong.com/lw/1z8/an_empirical_test_of_anthropic_principle_great/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">An empirical test of anthropic principle / great filter reasoning</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by James Miller</span></li><li><a href=\"https://lessestwrong.com/lw/1zj/sia_wont_doom_you/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">SIA won't doom you</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Stuart Armstrong</span></li><li><a href=\"https://lessestwrong.com/lw/214/late_great_filter_is_not_bad_news/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Late Great Filter is not bad news</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Wei Dai</span></li><li><a href=\"https://lessestwrong.com/lw/7w8/planets_in_the_habitable_zone_the_drake_equation/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Planets in the habitable zone, the Drake Equation, and the Great Filter</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by JoshuaZ</span></li><li><a href=\"http://www.overcomingbias.com/2010/11/beware-future-filters.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Beware Future Filters</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson’s Great Filter original paper:</span><span class=\"by_qQqgj5ScvgQsJk7Ti\"> </span></span><a href=\"http://hanson.gmu.edu/greatfilter.html\"><span><span class=\"by_qQqgj5ScvgQsJk7Ti\">The Great Filter</span><span class=\"by_woC2b5rav5sGrAo3E\"> - Are We Almost Past It?</span></span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">A simulation of the hard steps distribution: </span><a href=\"http://hanson.gmu.edu/hardstep.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Must Early Life Be Easy? The Rhythm of Major Evolutionary Transitions</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Robin Hanson</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Strong candidates for present Great Filters: </span><a href=\"http://www.global-catastrophic-risks.com/docs/Chap01.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Introduction of the book “Global Catastrophic Risks”, summarizing it</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Nick Bostrom</span></li><li><a href=\"http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">SIA Doomsday: The filter is ahead</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Katja Grace</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">An audio with Bostrom talking about how finding traces of life on mars is terrible bad news: </span><a href=\"http://www.youtube.com/watch?v=_W8zu7lFmhY\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Nick Bostrom on life on Mars</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Existential risk</span></a></li><li><a href=\"https://lessestwrong.com/tag/doomsday-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Doomsday argument</span></a></li><li><a href=\"https://lessestwrong.com/tag/self-indication-assumption\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Self Indication Assumption</span></a></li></ul>",
      "sections": [
        {
          "title": "Should we worry?",
          "anchor": "Should_we_worry_",
          "level": 1
        },
        {
          "title": "Consequences",
          "anchor": "Consequences",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 33,
    "description": {
      "markdown": "The **Great Filter** is a proposed explanation for the [Fermi Paradox](http://en.wikipedia.org/wiki/Fermi_paradox). The development of intelligent life requires many steps, such as the emergence of single-celled life and the transition from unicellular to multicellular life forms. Since we have not observed intelligent life beyond our planet, there seems to be a developmental step that is so difficult and unlikely that it \"filters out\" nearly all civilizations before they can reach a space-faring stage. Robin Hanson coined the term in his 1998 essay [The Great Filter - Are We Almost Past It?](http://hanson.gmu.edu/greatfilter.html).\n\n From that essay:\n\n> Humanity seems to have a bright future, i.e., a non-trivial chance of expanding to fill the universe with lasting life. But the fact that space near us seems dead now tells us that any given piece of dead matter faces an astronomically low chance of begating such a future. There thus exists a great filter between death and expanding lasting life, and humanity faces the ominous question: how far along this filter are we? \n\nShould we worry?\n----------------\n\nThe Great Filter might be a step in our evolutionary past, in which case our civilization has already passed it. But the hard step might also be ahead of us: surviving the creation of nuclear bombs, [AGI](https://wiki.lesswrong.com/wiki/AGI), biotechnology, [nanotechnology](https://lessestwrong.com/tag/nanotechnology) or an asteroid impact [1](http://www.global-catastrophic-risks.com/docs/Chap01.pdf). In that case, we should be worried, as the Great Filter seems to have been successful in stopping the development of every other civilization so far. Estimating the location of the Great Filter is thus important for helping estimate the magnitude of [existential risk](https://lessestwrong.com/tag/existential-risk). [Many](http://hanson.gmu.edu/greatfilter.html) [efforts](http://hanson.gmu.edu/hardstep.pdf) [have](http://www.stat.berkeley.edu/~aldous/Papers/GF.pdf) [been](http://www.nickbostrom.com/papers/fermi.pdf) [made](http://www.global-catastrophic-risks.com/docs/Chap01.pdf) [in](http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/) that direction, but much remains uncertain.\n\nTraces of life on other planets are evidence for a later Great Filter[2](http://www.youtube.com/watch?v=_W8zu7lFmhY). If we were to find that complex life had evolved independently both on Earth and some other planet, it would suggest that getting to such a developmental stage was relatively easy. Thus the Great Filter would have to be at a later stage.\n\nThe study of [past mass extinctions](http://en.wikipedia.org/wiki/Extinction_event#Major_extinction_events) and astrobiology can provide ideas for estimating the location of the Great Filter. However, there are many difficulties involved. For instance, the time that it takes to pass a step doesn't reveal much about how easy or hard that step was. Robin Hanson gives the following example in his [seminal paper](http://hanson.gmu.edu/greatfilter.html):\n\n\"…say you have one hour to pick five locks by trial and error, locks with 1,2,3,4, and 5 dials of ten numbers, so that the expected time to pick each lock is .01,.1, 1, 10, and 100 hours respectively. Then just looking at those rare cases when you do pick all five locks in the hour, the average time to pick the first two locks would be .0096 and .075 hours respectively, close to the usual expected times of .01 and .1 hours. The average time to pick the third lock, however, would be .20 hours, and the average time for the other two locks, and the average time left over at the end, would be .24 hours. That is, conditional on success, all the hard steps, no matter how hard, take about the same time, while easy steps take about their usual time.\"\n\nConsequences\n------------\n\n[In a subsequent paper](http://hanson.gmu.edu/hardstep.pdf), Hanson constructs a simulation of the distribution of the hard steps, which suggests that there should be about four to seven hard steps, uniformly distributed in our past. It also suggests that there has been at least one hard step since the evolution of hominids, and that the best extinction model that fits all these requirements is [William Schopf's model](http://www.pnas.org/content/91/15/6735.full.pdf). Taking evolutionary arguments for [AGI](https://wiki.lesswrong.com/wiki/AGI) and [observation selection effects](https://lessestwrong.com/tag/observation-selection-effect) together, [Bostrom and Shulman argue](http://www.nickbostrom.com/aievolution.pdf) that Hanson’s results can help estimate the difficulty of creating AGI.\n\nBlog posts\n----------\n\n*   [Very Bad News](http://www.overcomingbias.com/2010/03/very-bad-news.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [An empirical test of anthropic principle / great filter reasoning](https://lessestwrong.com/lw/1z8/an_empirical_test_of_anthropic_principle_great/) by James Miller\n*   [SIA won't doom you](https://lessestwrong.com/lw/1zj/sia_wont_doom_you/) by Stuart Armstrong\n*   [Late Great Filter is not bad news](https://lessestwrong.com/lw/214/late_great_filter_is_not_bad_news/) by Wei Dai\n*   [Planets in the habitable zone, the Drake Equation, and the Great Filter](https://lessestwrong.com/lw/7w8/planets_in_the_habitable_zone_the_drake_equation/) by JoshuaZ\n*   [Beware Future Filters](http://www.overcomingbias.com/2010/11/beware-future-filters.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n\nExternal links\n--------------\n\n*   Robin Hanson’s Great Filter original paper: [The Great Filter - Are We Almost Past It?](http://hanson.gmu.edu/greatfilter.html)\n*   A simulation of the hard steps distribution: [Must Early Life Be Easy? The Rhythm of Major Evolutionary Transitions](http://hanson.gmu.edu/hardstep.pdf) by Robin Hanson\n*   Strong candidates for present Great Filters: [Introduction of the book “Global Catastrophic Risks”, summarizing it](http://www.global-catastrophic-risks.com/docs/Chap01.pdf) by Nick Bostrom\n*   [SIA Doomsday: The filter is ahead](http://meteuphoric.wordpress.com/2010/03/23/sia-doomsday-the-filter-is-ahead/) by Katja Grace\n*   An audio with Bostrom talking about how finding traces of life on mars is terrible bad news: [Nick Bostrom on life on Mars](http://www.youtube.com/watch?v=_W8zu7lFmhY)\n\nSee also\n--------\n\n*   [Existential risk](https://lessestwrong.com/tag/existential-risk)\n*   [Doomsday argument](https://lessestwrong.com/tag/doomsday-argument)\n*   [Self Indication Assumption](https://lessestwrong.com/tag/self-indication-assumption)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XJjvxWB68GYpts93N",
    "name": "Nanotechnology",
    "core": false,
    "slug": "nanotechnology",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XzXbiS2zWYNdZdLW8\">Nanotechnology</span></strong><span><span class=\"by_XzXbiS2zWYNdZdLW8\"> is </span><span class=\"by_LedhurJxi3baDAKDZ\">the field of study concerned with the manipulation of matter at an atomic and molecular scale. Tipically, this involves structures with sizes ranging from 1 to 100 nanometres. It is currently one of the most well-funded areas worlwide. The term was first coined in 1974 by Norio Taniguchi.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">The emergence of nanotechnology as </span><span class=\"by_XzXbiS2zWYNdZdLW8\">a </span><span class=\"by_LedhurJxi3baDAKDZ\">field by itself was the result</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">the convergence of several lines of work. These include the development of the </span></span><a href=\"http://en.wikipedia.org/wiki/Scanning_tunneling_microscope\"><span class=\"by_LedhurJxi3baDAKDZ\">scanning tunneling microscope</span></a><span class=\"by_LedhurJxi3baDAKDZ\"> by Gerd Binnigg and Einrich Rohrer in 1980s, Richard Feynman's talk \"There's plenty of room at the Bottom\" in 1959 and Eric Drexler suggestions of molecular manipulation in the 70s.</span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">The field of nanotechonology has led to the development of a huge amount of new technologies and the improvement of old methods. From drug-delivering systems to electronic chips development, there are nowadays hundreds of avaliable of functional applications stemming from this area. Besides the size and mobility advantages of such</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> devices </span><span class=\"by_LedhurJxi3baDAKDZ\">and technologies, the fundamental quantum properties that emerge at nano scales continue to defy researchers to speculate of further developments.</span></span></p><p><span><span class=\"by_LedhurJxi3baDAKDZ\">Drexler has proposed, </span><span class=\"by_XzXbiS2zWYNdZdLW8\">with </span><span class=\"by_LedhurJxi3baDAKDZ\">his </span></span><i><span class=\"by_LedhurJxi3baDAKDZ\">molecular nanotechnology</span></i><span><span class=\"by_LedhurJxi3baDAKDZ\">,</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> that </span><span class=\"by_LedhurJxi3baDAKDZ\">the field could evolve to exploit more than just this scale properties, this pure nanomaterials research. His suggestions, highly speculative, include research</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> on the </span><span class=\"by_LedhurJxi3baDAKDZ\">ability</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> of </span><span class=\"by_LedhurJxi3baDAKDZ\">developing means of mechanosynthesis - such as having miniature production lines using machines to build structures. This would allow, for example, the precise control of chemical reactions, eliminating the imprecision existing in conventional chemistry.</span></span></p><p><span class=\"by_LedhurJxi3baDAKDZ\">When </span><a href=\"http://intelligence.org/files/AIPosNegFactor.pdf\"><span class=\"by_LedhurJxi3baDAKDZ\">discussing</span></a><span class=\"by_LedhurJxi3baDAKDZ\"> the development of </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_LedhurJxi3baDAKDZ\">Friendly AI</span></a><span class=\"by_LedhurJxi3baDAKDZ\">, Yudkowsky proposes that the unrestricted access to nanotechnology by an </span><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_LedhurJxi3baDAKDZ\">Unfriendly artificial intelligence</span></a><span class=\"by_LedhurJxi3baDAKDZ\"> could have catastrophic results for mankind.</span></p><h2 id=\"Further_Reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further Reading &amp; References</span></h2><ul><li><span class=\"by_LedhurJxi3baDAKDZ\">Binnig, G.; Rohrer, H. (1986). \"Scanning tunneling microscopy\". IBM Journal of Research and Development 30: 4.</span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">\"Nanoscience and nanotechnologies: opportunities and uncertainties\". Royal Society and Royal Academy of Engineering. July 2004. Retrieved 13 May 2011.</span></li><li><span class=\"by_LedhurJxi3baDAKDZ\">Allhoff, Fritz; Lin, Patrick; Moore, Daniel (2010). What is nanotechnology and why does it matter?: from science to ethics. John Wiley and Sons. pp. 3–5. ISBN 1-4051-7545-1.</span></li><li><a href=\"http://www.zyvex.com/nanotech/feynman.html\"><span class=\"by_LedhurJxi3baDAKDZ\">There's Plenty of Room at the Bottom</span></a><span class=\"by_XzXbiS2zWYNdZdLW8\"> by Richard Feynman</span></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/exploratory-engineering\"><span class=\"by_9c2mQkLQq6gQSksMs\">Exploratory engineering</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rational-evidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Rational evidence</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/science\"><span class=\"by_qf77EiaoMw7tH3GSr\">Science</span></a></li></ul>",
      "sections": [
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 18,
    "description": {
      "markdown": "**Nanotechnology** is the field of study concerned with the manipulation of matter at an atomic and molecular scale. Tipically, this involves structures with sizes ranging from 1 to 100 nanometres. It is currently one of the most well-funded areas worlwide. The term was first coined in 1974 by Norio Taniguchi.\n\nThe emergence of nanotechnology as a field by itself was the result of the convergence of several lines of work. These include the development of the [scanning tunneling microscope](http://en.wikipedia.org/wiki/Scanning_tunneling_microscope) by Gerd Binnigg and Einrich Rohrer in 1980s, Richard Feynman's talk \"There's plenty of room at the Bottom\" in 1959 and Eric Drexler suggestions of molecular manipulation in the 70s.\n\nThe field of nanotechonology has led to the development of a huge amount of new technologies and the improvement of old methods. From drug-delivering systems to electronic chips development, there are nowadays hundreds of avaliable of functional applications stemming from this area. Besides the size and mobility advantages of such devices and technologies, the fundamental quantum properties that emerge at nano scales continue to defy researchers to speculate of further developments.\n\nDrexler has proposed, with his *molecular nanotechnology*, that the field could evolve to exploit more than just this scale properties, this pure nanomaterials research. His suggestions, highly speculative, include research on the ability of developing means of mechanosynthesis - such as having miniature production lines using machines to build structures. This would allow, for example, the precise control of chemical reactions, eliminating the imprecision existing in conventional chemistry.\n\nWhen [discussing](http://intelligence.org/files/AIPosNegFactor.pdf) the development of [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), Yudkowsky proposes that the unrestricted access to nanotechnology by an [Unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) could have catastrophic results for mankind.\n\nFurther Reading & References\n----------------------------\n\n*   Binnig, G.; Rohrer, H. (1986). \"Scanning tunneling microscopy\". IBM Journal of Research and Development 30: 4.\n*   \"Nanoscience and nanotechnologies: opportunities and uncertainties\". Royal Society and Royal Academy of Engineering. July 2004. Retrieved 13 May 2011.\n*   Allhoff, Fritz; Lin, Patrick; Moore, Daniel (2010). What is nanotechnology and why does it matter?: from science to ethics. John Wiley and Sons. pp. 3–5. ISBN 1-4051-7545-1.\n*   [There's Plenty of Room at the Bottom](http://www.zyvex.com/nanotech/feynman.html) by Richard Feynman\n\nSee also\n--------\n\n*   [Exploratory engineering](https://www.lesswrong.com/tag/exploratory-engineering)\n*   [Rational evidence](https://www.lesswrong.com/tag/rational-evidence), [Science](https://www.lesswrong.com/tag/science)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xHjy88N2uJvGdgzfw",
    "name": "Health / Medicine / Disease",
    "core": false,
    "slug": "health-medicine-disease",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Health. </span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Note that, for convenience, posts relating to the 2019 coronavirus outbreak are instead found </span><span class=\"by_6tLz8Q6yaejBENSzC\">in </span></span><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_6tLz8Q6yaejBENSzC\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 144,
    "description": {
      "markdown": "**Health.** Note that, for convenience, posts relating to the 2019 coronavirus outbreak are instead found in [here](https://www.lesswrong.com/tag/coronavirus?showPostCount=true&useTagName=true)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZpG9rheyAkgCoEQea",
    "name": "Practice & Philosophy of Science",
    "core": false,
    "slug": "practice-and-philosophy-of-science",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">Practice and Philosophy of Science</span></strong><span class=\"by_qxJ28GN72aiJu96iF\"> is for posts that discuss how science is done or should be done; examples include </span><a href=\"https://www.lesswrong.com/posts/tSemJckYr29Gnxod2/building-intuitions-on-non-empirical-arguments-in-science\"><span class=\"by_qxJ28GN72aiJu96iF\">Building Intuitions on Non-Empirical Arguments in Science</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> and the </span><a href=\"https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy\"><span class=\"by_qxJ28GN72aiJu96iF\">Science and Rationality sequence</span></a><span class=\"by_qxJ28GN72aiJu96iF\">. (It is not for posts that simply report on a new scientific result.)</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 166,
    "description": {
      "markdown": "**Practice and Philosophy of Science** is for posts that discuss how science is done or should be done; examples include [Building Intuitions on Non-Empirical Arguments in Science](https://www.lesswrong.com/posts/tSemJckYr29Gnxod2/building-intuitions-on-non-empirical-arguments-in-science) and the [Science and Rationality sequence](https://www.lesswrong.com/s/fxynfGCSHpY4FmBZy). (It is not for posts that simply report on a new scientific result.)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "qAvbtzdG2A2RBn7in",
    "name": "Effective Altruism",
    "core": false,
    "slug": "effective-altruism",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_Co2dGXQxHAf92LHea\">Effective </span><span class=\"by_HoGziwmhpMGqGeWZy\">Altruism</span></span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> (EA)</span><span class=\"by_Co2dGXQxHAf92LHea\"> </span><span class=\"by_cn4SiEmqWbu7K9em5\">is </span><span class=\"by_HoGziwmhpMGqGeWZy\">a movement trying</span><span class=\"by_cn4SiEmqWbu7K9em5\"> to </span><span class=\"by_HoGziwmhpMGqGeWZy\">invest</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> time and money</span><span class=\"by_cn4SiEmqWbu7K9em5\"> in </span><span class=\"by_HoGziwmhpMGqGeWZy\">causes that</span><span class=\"by_cn4SiEmqWbu7K9em5\"> do the most </span><span class=\"by_HoGziwmhpMGqGeWZy\">possible good per unit investment.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> EA was at one point called </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">optimal philanthropy</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">The basic concept behind EA is that you would really struggle to donate 100 times more money</span><span class=\"by_cJnvyeYrotgZgfG8W\"> or time</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> to charity than you currently do--but (assuming you are approximately utilitarian), spending a little time researching who to donate to </span></span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">could</span></i><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> have an impact on roughly this order of magnitude.</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> </span><span class=\"by_Xn6ACr6Cua8upALWQ\">The</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> same argument works for doing good with your career or volunteer hours.</span></span></p><h2 id=\"Concepts\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Concepts</span></h2><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Despite a broad diversity of ideas within </span><span class=\"by_HoGziwmhpMGqGeWZy\">the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">EA community on which areas are </span><span class=\"by_HoGziwmhpMGqGeWZy\">most </span><span class=\"by_Xn6ACr6Cua8upALWQ\">pressing, there are a handful of criteria that are generally agreed make an area potentially impactful to work on (either directly or through donation). These are:</span></span></p><ul><li><span class=\"by_Xn6ACr6Cua8upALWQ\">The area is generally </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">neglected, </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">that</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> is, it has capacity for more support either financially or in terms of skills</span></span></li><li><span class=\"by_Xn6ACr6Cua8upALWQ\">The area has the potential for </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">large impact, </span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\">either in human lives saved, animal or human suffering alleviated,</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\">catastrophic crises averted, etc.</span><span class=\"by_cJnvyeYrotgZgfG8W\"> Sometimes this is called \"scale\"</span></span></li><li><span class=\"by_Xn6ACr6Cua8upALWQ\">The area is </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">tractable</span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\">-- it</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is a </span><span class=\"by_Xn6ACr6Cua8upALWQ\">solvable problem, or is solvable with minimal resource investment (relative to other</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></span><br><span class=\"by_Xn6ACr6Cua8upALWQ\">problem areas)</span></li></ul><p><span class=\"by_cJnvyeYrotgZgfG8W\">A fourth semi-area is:</span></p><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Does the individual have good </span><strong><span class=\"by_cJnvyeYrotgZgfG8W\">personal fit</span></strong><span class=\"by_cJnvyeYrotgZgfG8W\">?</span><strong><span class=\"by_cJnvyeYrotgZgfG8W\"> </span></strong><span class=\"by_cJnvyeYrotgZgfG8W\">Do they have unique skills which will make them more effective in an area.</span></li></ul><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">From this, we can see a vast number</span><span class=\"by_cn4SiEmqWbu7K9em5\"> of </span><span class=\"by_Xn6ACr6Cua8upALWQ\">charities do not meet all or indeed any</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of </span><span class=\"by_Xn6ACr6Cua8upALWQ\">these criteria. A major issue with EA is that some areas are much easier to track progress in than others (think tracking the cost per life saved of malaria nets vs existential </span></span><a href=\"https://www.lesswrong.com/tag/ai?showPostCount=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">AI</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> risk, for instance). What is clear, however, is that some of the more </span><span class=\"by_HoGziwmhpMGqGeWZy\">effective </span><span class=\"by_Xn6ACr6Cua8upALWQ\">charities (of those which </span></span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">are</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\"> easy to track) have </span><a href=\"https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">far more benefit over the average charity than people think</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">-- perhaps as much as 10,000% as effective.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">A large portion of the</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> EA</span><span class=\"by_HoGziwmhpMGqGeWZy\"> community are</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> by and large, </span></span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">longtermist</span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\">. This refers to the idea that, if there are many future generations (100s, 1000s or more), and their lives are as valuable as ours, then even very small impacts on all of their lives-- or things like moving good changes forwards</span><span class=\"by_cn4SiEmqWbu7K9em5\"> in </span><span class=\"by_Xn6ACr6Cua8upALWQ\">time or bad ones back-- far outweigh impacts on people who are currently alive. Because this concept is less broadly-accepted than charity for currently-alive people, longtermist solutions are also generally considered to be neglected. Longtermist interventions generally focus on </span></span><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_Xn6ACr6Cua8upALWQ\">S-risks</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> or </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_Xn6ACr6Cua8upALWQ\">X-risks</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Examples of longtermist interventions include </span><a href=\"https://www.lesswrong.com/tag/ai\"><span class=\"by_Xn6ACr6Cua8upALWQ\">AI</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> safety, pandemic preparedness, and </span><a href=\"https://www.lesswrong.com/tag/nanotechnology\"><span class=\"by_Xn6ACr6Cua8upALWQ\">nanotechnology</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> security. Examples of other popular EA interventions include global poverty alleviation, malaria treatments, and vitamin supplementation in sub-saharan Africa.</span><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The</span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> Effective Altruism movement </span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">also has </span><span class=\"by_qgdGA4ZEyW7zNdK84\">its</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> own </span><span class=\"by_qgdGA4ZEyW7zNdK84\">forum,&nbsp;</span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></strong><a href=\"https://forum.effectivealtruism.org/\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">The EA Forum</span></strong></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. It runs on</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">same software as LessWrong.</span></span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Efficient charity: do unto others...</span></a></li><li><a href=\"https://lessestwrong.com/lw/6py/optimal_philanthropy_for_human_beings/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimal philanthropy for human beings</span></a></li><li><a href=\"https://lessestwrong.com/lw/745/why_we_cant_take_expected_value_estimates/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why we can't take expected value estimates literally (even when they're unbiased)</span></a></li></ul><h2 id=\"Total_resources_and_how_they_are_split\"><span class=\"by_cJnvyeYrotgZgfG8W\">Total resources and how they are split</span></h2><p><span class=\"by_cJnvyeYrotgZgfG8W\">[effectivealtruismdata.com]</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">[How much money 80k has an article on this]</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">[include graph]</span></p><h2 id=\"Impact\"><span class=\"by_cJnvyeYrotgZgfG8W\">Impact</span></h2><h3 id=\"Global_health_and_economic_development\"><span class=\"by_cJnvyeYrotgZgfG8W\">Global health and economic development</span></h3><p><span class=\"by_cJnvyeYrotgZgfG8W\">[estimated total amount of lives saved]</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">The </span><a href=\"https://www.againstmalaria.com/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Against Malaria Foundation</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> has distributed more than 70 million bednets to protect people (mostly children) from a debilitating parasite. (</span><a href=\"https://www.againstmalaria.com/Distributions.aspx\"><span class=\"by_cJnvyeYrotgZgfG8W\">Source</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">) [number of lives saved]</span></p><p><a href=\"https://givedirectly.org/\"><span class=\"by_cJnvyeYrotgZgfG8W\">GiveDirectly</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> has facilitated more than $100 million in direct cash transfers to families living in extreme poverty, who determine for themselves how best to spend the money. (</span><a href=\"https://www.givedirectly.org/financials/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Source</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">) [number of lives saved]</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">The </span><a href=\"https://www.imperial.ac.uk/schistosomiasis-control-initiative\"><span class=\"by_cJnvyeYrotgZgfG8W\">Schistosomiasis Control Initiative</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> and </span><a href=\"http://www.evidenceaction.org/dewormtheworld/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Deworm the World Initiative</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> invests in people's health and future well-being by treating preventable diseases that often get little attention. They have given out hundreds of millions of deworming treatments to fight intestinal parasites, which may help people earn higher incomes later in life. (Sources for </span><a href=\"https://schistosomiasiscontrolinitiative.org/reach\"><span class=\"by_cJnvyeYrotgZgfG8W\">SCI</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> and </span><a href=\"https://www.evidenceaction.org/dewormtheworld-2/\"><span class=\"by_cJnvyeYrotgZgfG8W\">DWI</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">)</span></p><h3 id=\"Animal_welfare\"><span class=\"by_cJnvyeYrotgZgfG8W\">Animal welfare</span></h3><p><span class=\"by_cJnvyeYrotgZgfG8W\">[how much animal welfare in some reasonable metric]</span></p><p><a href=\"https://thehumaneleague.org/\"><span class=\"by_cJnvyeYrotgZgfG8W\">The Humane League</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> and </span><a href=\"https://mercyforanimals.org/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Mercy for Animals</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, alongside many other organizations, have orchestrated corporate campaigns and legal reforms to fight the use of battery cages. Because of this work, more than 100 million hens that would have been caged instead live cage-free. (This includes all cage-free reform work, of which a sizable fraction was funded by EA-aligned donors.)</span></p><p><a href=\"https://gfi.org/\"><span class=\"by_cJnvyeYrotgZgfG8W\">The Good Food Institute</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> works with scientists, entrepreneurs, and investors to develop and promote meat alternatives that don't require the suffering of farmed animals.</span></p><h3 id=\"Existential_risk_and_the_long_term_future\"><span class=\"by_cJnvyeYrotgZgfG8W\">Existential risk and the long-term future</span></h3><p><span class=\"by_cJnvyeYrotgZgfG8W\">[how much lower higher? risk of existential catastrphe as a result]</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Organizations like the </span><a href=\"https://www.fhi.ox.ac.uk/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Future of Humanity Institute</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> and the </span><a href=\"https://www.cser.ac.uk/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Centre for the Study of Existential Risk</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> work on research and policy related to some of the biggest threats facing humanity, from pandemics and climate change to nuclear war and superintelligent AI systems.</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Some organizations in this space, like the </span><a href=\"https://humancompatible.ai/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Center for Human-Compatible AI</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> and the </span><a href=\"https://intelligence.org/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Machine Intelligence Research Institute</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, focus entirely on solving issues posed by advances in artificial intelligence. AI systems of the future could be very powerful and difficult to control --- a dangerous combination.</span></p><p><a href=\"https://sherlock.bio/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Sherlock Biosciences</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> is developing a diagnostic platform that could reduce threats from viral pandemics. (They are a private company, but much of their capital comes from a </span><a href=\"https://www.openphilanthropy.org/focus/scientific-research/sherlock-biosciences-research-viral-diagnostics\"><span class=\"by_cJnvyeYrotgZgfG8W\">grant</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> made by Open Philanthropy, an EA-aligned grantmaker.)</span></p><h2 id=\"Key_ideas\"><span class=\"by_cJnvyeYrotgZgfG8W\">Key ideas</span></h2><h2 id=\"Criticisms_\"><span class=\"by_cJnvyeYrotgZgfG8W\">Criticisms&nbsp;</span></h2><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">EA is incoherent. Consequentialism applies to one's whole life, but many EAs don’t take it this seriously</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">This argument applies to virtue ethics too, but no one criticises it - “why aren’t you constantly seeking to always do the virtuous action”. People in practice seem to take statements from consequentialist philosophies more seriously than they do from others</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">It is more intellectually honest to surface incoherence in your worldview - \"I use 80% of my time as effectively as possible\"&nbsp;</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">EA frames all value in terms of impact creation and this makes members sad</span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefipt32j7op0s\"><sup><a href=\"#fnipt32j7op0s\"><span class=\"by_cJnvyeYrotgZgfG8W\">[1]</span></a></sup></span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">How widespread is this?</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Many EAs don't feel this way</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Some people control orders of magnitude more resources than others. They could use their time and money to improve the lives of many other people. Whether they should is a different question, but it doesn't avoid the fact that this is true.</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">EA supports a culture of guilt [Kerry thread]</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">How does EA compare in terms of mental wellbeing to other communities centred around \"doing good\" eg \"Protestant Work Ethic\" and \"Catholic Guilt\"?</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">If you struggle with this, consider reading</span><a href=\"https://forum.effectivealtruism.org/s/a2LBRPLhvwB83DSGq\"><span class=\"by_cJnvyeYrotgZgfG8W\">&nbsp;</span><u><span class=\"by_cJnvyeYrotgZgfG8W\">Replacing Guilt</span></u></a><span class=\"by_cJnvyeYrotgZgfG8W\">, which is one of only 3 sequences with a permanent place sidebar of the EA Forum.</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">EA is spending too much money</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">EA is spending </span><i><span class=\"by_cJnvyeYrotgZgfG8W\">more</span></i><span class=\"by_cJnvyeYrotgZgfG8W\"> money but it's not immediately obvious it is spending too much. It might be spending too little. [Will MacAskill article]</span></li></ul></li></ul><h3 id=\"Criticisms_to_add\"><span class=\"by_cJnvyeYrotgZgfG8W\">Criticisms to add</span></h3><p><a href=\"https://stefanfschubert.com/blog/2020/12/30/five-common-ea-self-criticisms-i-disagree-with\"><u><span class=\"by_cJnvyeYrotgZgfG8W\">Stefan Shubert's criticisms and responses</span></u></a></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Kuhn, Ben (2013)&nbsp;</span><a href=\"https://www.benkuhn.net/ea-critique/\"><span class=\"by_cJnvyeYrotgZgfG8W\">A critique of effective altruism</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span><i><span class=\"by_cJnvyeYrotgZgfG8W\">Ben Kuhn’s Blog</span></i><span class=\"by_cJnvyeYrotgZgfG8W\">, December 2.</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">McMahan, Jeff (2016)&nbsp;</span><a href=\"https://doi.org/10.5840/tpm20167379\"><span class=\"by_cJnvyeYrotgZgfG8W\">Philosophical critiques of effective altruism</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span><i><span class=\"by_cJnvyeYrotgZgfG8W\">The Philosophers’ Magazine</span></i><span class=\"by_cJnvyeYrotgZgfG8W\">, vol. 73, pp. 92–99.</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Nielsen, Michael (2022)&nbsp;</span><a href=\"https://michaelnotebook.com/eanotes/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Notes on effective altruism</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span><i><span class=\"by_cJnvyeYrotgZgfG8W\">Michael’s Notebook</span></i><span class=\"by_cJnvyeYrotgZgfG8W\">, June 2.</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Rowe, Abraham (2022)&nbsp;</span><a href=\"https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read\"><span class=\"by_cJnvyeYrotgZgfG8W\">Critiques of EA that I want to read</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span><i><span class=\"by_cJnvyeYrotgZgfG8W\">Effective Altruism Forum</span></i><span class=\"by_cJnvyeYrotgZgfG8W\">, June 19.</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Wiblin, Robert &amp; Keiran Harris (2019)&nbsp;</span><a href=\"https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span><i><span class=\"by_cJnvyeYrotgZgfG8W\">80,000 Hours</span></i><span class=\"by_cJnvyeYrotgZgfG8W\">, September 3.</span></p><p><span class=\"by_cJnvyeYrotgZgfG8W\">Zhang, Linchuan (2021)&nbsp;</span><a href=\"https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism\"><span class=\"by_cJnvyeYrotgZgfG8W\">The motivated reasoning critique of effective altruism</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span><i><span class=\"by_cJnvyeYrotgZgfG8W\">Effective Altruism Forum</span></i><span class=\"by_cJnvyeYrotgZgfG8W\">, September 14.</span></p><h2 id=\"Related\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Related</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/altruism\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Altruism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/cause-prioritization\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Cause Prioritization</span></a></li><li><a href=\"https://lessestwrong.com/tag/utilitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utilitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_Xn6ACr6Cua8upALWQ\">S-risk</span></a></li><li><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_Xn6ACr6Cua8upALWQ\">X-risk</span></a></li></ul><h2 id=\"External_Resources\"><span class=\"by_Xn6ACr6Cua8upALWQ\">External Resources</span></h2><ul><li><a href=\"https://80000hours.org/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">80,000 Hours</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, who offer advice for how to have a maximally globally impactful career</span></li><li><a href=\"https://www.effectivealtruism.org/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Effective Altruism,</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> who offer support for local EA groups, </span><span class=\"by_cn4SiEmqWbu7K9em5\">as </span><span class=\"by_Xn6ACr6Cua8upALWQ\">well as articles</span><span class=\"by_cn4SiEmqWbu7K9em5\"> and </span><span class=\"by_Xn6ACr6Cua8upALWQ\">advice surrounding EA</span></span></li><li><a href=\"https://www.givewell.org/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">GiveWell,</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> a charity doing research into the effectiveness of other charities to provide information for donors</span></li><li><a href=\"https://www.thelifeyoucansave.org/the-book/?gclid=CjwKCAjwjqT5BRAPEiwAJlBuBXb3m1FKunezyfsYzYkjmgzSCHScRgZpzMH097cbAAGC5lmHUP-J3BoCcnAQAvD_BwE\"><span class=\"by_Xn6ACr6Cua8upALWQ\">The Life You Can Save</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, a free eBook outlining reasons for donating more and more effectively</span></li><li><a href=\"https://www.lesswrong.com/tag/center-on-long-term-risk-clr\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Center on Long-Term Risk</span></a></li></ul><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnipt32j7op0s\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefipt32j7op0s\"><span class=\"by_cJnvyeYrotgZgfG8W\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span class=\"by_cJnvyeYrotgZgfG8W\">https://twitter.com/KerryLVaughan/status/1545063368695898112?s=20&amp;t=xgaSuh22V6y44Wkcebo22Q</span></p></div></li></ol>",
      "sections": [
        {
          "title": "Concepts",
          "anchor": "Concepts",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "Total resources and how they are split",
          "anchor": "Total_resources_and_how_they_are_split",
          "level": 1
        },
        {
          "title": "Impact",
          "anchor": "Impact",
          "level": 1
        },
        {
          "title": "Global health and economic development",
          "anchor": "Global_health_and_economic_development",
          "level": 2
        },
        {
          "title": "Animal welfare",
          "anchor": "Animal_welfare",
          "level": 2
        },
        {
          "title": "Existential risk and the long-term future",
          "anchor": "Existential_risk_and_the_long_term_future",
          "level": 2
        },
        {
          "title": "Key ideas",
          "anchor": "Key_ideas",
          "level": 1
        },
        {
          "title": "Criticisms ",
          "anchor": "Criticisms_",
          "level": 1
        },
        {
          "title": "Criticisms to add",
          "anchor": "Criticisms_to_add",
          "level": 2
        },
        {
          "title": "Related",
          "anchor": "Related",
          "level": 1
        },
        {
          "title": "External Resources",
          "anchor": "External_Resources",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 13
    },
    "postCount": 170,
    "description": {
      "markdown": "**Effective Altruism** (EA) is a movement trying to invest time and money in causes that do the most possible good per unit investment. EA was at one point called **optimal philanthropy**.\n\nThe basic concept behind EA is that you would really struggle to donate 100 times more money or time to charity than you currently do--but (assuming you are approximately utilitarian), spending a little time researching who to donate to *could* have an impact on roughly this order of magnitude. The same argument works for doing good with your career or volunteer hours.\n\nConcepts\n--------\n\nDespite a broad diversity of ideas within the EA community on which areas are most pressing, there are a handful of criteria that are generally agreed make an area potentially impactful to work on (either directly or through donation). These are:\n\n*   The area is generally **neglected,** that is, it has capacity for more support either financially or in terms of skills\n*   The area has the potential for **large impact,** either in human lives saved, animal or human suffering alleviated, catastrophic crises averted, etc. Sometimes this is called \"scale\"\n*   The area is **tractable**\\-\\- it is a solvable problem, or is solvable with minimal resource investment (relative to other   \n    problem areas)\n\nA fourth semi-area is:\n\n*   Does the individual have good **personal fit**?  Do they have unique skills which will make them more effective in an area.\n\nFrom this, we can see a vast number of charities do not meet all or indeed any of these criteria. A major issue with EA is that some areas are much easier to track progress in than others (think tracking the cost per life saved of malaria nets vs existential [AI](https://www.lesswrong.com/tag/ai?showPostCount=false) risk, for instance). What is clear, however, is that some of the more effective charities (of those which *are* easy to track) have [far more benefit over the average charity than people think](https://80000hours.org/2017/05/most-people-report-believing-its-incredibly-cheap-to-save-lives-in-the-developing-world/)\\-\\- perhaps as much as 10,000% as effective.\n\nA large portion of the EA community are by and large, **longtermist**. This refers to the idea that, if there are many future generations (100s, 1000s or more), and their lives are as valuable as ours, then even very small impacts on all of their lives-- or things like moving good changes forwards in time or bad ones back-- far outweigh impacts on people who are currently alive. Because this concept is less broadly-accepted than charity for currently-alive people, longtermist solutions are also generally considered to be neglected. Longtermist interventions generally focus on [S-risks](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks) or [X-risks](https://www.lesswrong.com/tag/existential-risk).\n\nExamples of longtermist interventions include [AI](https://www.lesswrong.com/tag/ai) safety, pandemic preparedness, and [nanotechnology](https://www.lesswrong.com/tag/nanotechnology) security. Examples of other popular EA interventions include global poverty alleviation, malaria treatments, and vitamin supplementation in sub-saharan Africa.  \n \n\nThe **Effective Altruism movement** also has its own forum,   [**The EA Forum**](https://forum.effectivealtruism.org/). It runs on the same software as LessWrong.\n\nNotable Posts\n-------------\n\n*   [Efficient charity: do unto others...](https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/)\n*   [Optimal philanthropy for human beings](https://lessestwrong.com/lw/6py/optimal_philanthropy_for_human_beings/)\n*   [Why we can't take expected value estimates literally (even when they're unbiased)](https://lessestwrong.com/lw/745/why_we_cant_take_expected_value_estimates/)\n\nTotal resources and how they are split\n--------------------------------------\n\n\\[effectivealtruismdata.com\\]\n\n\\[How much money 80k has an article on this\\]\n\n\\[include graph\\]\n\nImpact\n------\n\n### Global health and economic development\n\n\\[estimated total amount of lives saved\\]\n\nThe [Against Malaria Foundation](https://www.againstmalaria.com/) has distributed more than 70 million bednets to protect people (mostly children) from a debilitating parasite. ([Source](https://www.againstmalaria.com/Distributions.aspx)) \\[number of lives saved\\]\n\n[GiveDirectly](https://givedirectly.org/) has facilitated more than $100 million in direct cash transfers to families living in extreme poverty, who determine for themselves how best to spend the money. ([Source](https://www.givedirectly.org/financials/)) \\[number of lives saved\\]\n\nThe [Schistosomiasis Control Initiative](https://www.imperial.ac.uk/schistosomiasis-control-initiative) and [Deworm the World Initiative](http://www.evidenceaction.org/dewormtheworld/) invests in people's health and future well-being by treating preventable diseases that often get little attention. They have given out hundreds of millions of deworming treatments to fight intestinal parasites, which may help people earn higher incomes later in life. (Sources for [SCI](https://schistosomiasiscontrolinitiative.org/reach) and [DWI](https://www.evidenceaction.org/dewormtheworld-2/))\n\n### Animal welfare\n\n\\[how much animal welfare in some reasonable metric\\]\n\n[The Humane League](https://thehumaneleague.org/) and [Mercy for Animals](https://mercyforanimals.org/), alongside many other organizations, have orchestrated corporate campaigns and legal reforms to fight the use of battery cages. Because of this work, more than 100 million hens that would have been caged instead live cage-free. (This includes all cage-free reform work, of which a sizable fraction was funded by EA-aligned donors.)\n\n[The Good Food Institute](https://gfi.org/) works with scientists, entrepreneurs, and investors to develop and promote meat alternatives that don't require the suffering of farmed animals.\n\n### Existential risk and the long-term future\n\n\\[how much lower higher? risk of existential catastrphe as a result\\]\n\nOrganizations like the [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) and the [Centre for the Study of Existential Risk](https://www.cser.ac.uk/) work on research and policy related to some of the biggest threats facing humanity, from pandemics and climate change to nuclear war and superintelligent AI systems.\n\nSome organizations in this space, like the [Center for Human-Compatible AI](https://humancompatible.ai/) and the [Machine Intelligence Research Institute](https://intelligence.org/), focus entirely on solving issues posed by advances in artificial intelligence. AI systems of the future could be very powerful and difficult to control --- a dangerous combination.\n\n[Sherlock Biosciences](https://sherlock.bio/) is developing a diagnostic platform that could reduce threats from viral pandemics. (They are a private company, but much of their capital comes from a [grant](https://www.openphilanthropy.org/focus/scientific-research/sherlock-biosciences-research-viral-diagnostics) made by Open Philanthropy, an EA-aligned grantmaker.)\n\nKey ideas\n---------\n\nCriticisms \n-----------\n\n*   EA is incoherent. Consequentialism applies to one's whole life, but many EAs don’t take it this seriously\n    *   This argument applies to virtue ethics too, but no one criticises it - “why aren’t you constantly seeking to always do the virtuous action”. People in practice seem to take statements from consequentialist philosophies more seriously than they do from others\n    *   It is more intellectually honest to surface incoherence in your worldview - \"I use 80% of my time as effectively as possible\" \n*   EA frames all value in terms of impact creation and this makes members sad^[\\[1\\]](#fnipt32j7op0s)^\n    *   How widespread is this?\n    *   Many EAs don't feel this way\n    *   Some people control orders of magnitude more resources than others. They could use their time and money to improve the lives of many other people. Whether they should is a different question, but it doesn't avoid the fact that this is true.\n*   EA supports a culture of guilt \\[Kerry thread\\]\n    *   How does EA compare in terms of mental wellbeing to other communities centred around \"doing good\" eg \"Protestant Work Ethic\" and \"Catholic Guilt\"?\n    *   If you struggle with this, consider reading[ Replacing Guilt](https://forum.effectivealtruism.org/s/a2LBRPLhvwB83DSGq), which is one of only 3 sequences with a permanent place sidebar of the EA Forum.\n*   EA is spending too much money\n    *   EA is spending *more* money but it's not immediately obvious it is spending too much. It might be spending too little. \\[Will MacAskill article\\]\n\n### Criticisms to add\n\n[Stefan Shubert's criticisms and responses](https://stefanfschubert.com/blog/2020/12/30/five-common-ea-self-criticisms-i-disagree-with)\n\nKuhn, Ben (2013) [A critique of effective altruism](https://www.benkuhn.net/ea-critique/), *Ben Kuhn’s Blog*, December 2.\n\nMcMahan, Jeff (2016) [Philosophical critiques of effective altruism](https://doi.org/10.5840/tpm20167379), *The Philosophers’ Magazine*, vol. 73, pp. 92–99.\n\nNielsen, Michael (2022) [Notes on effective altruism](https://michaelnotebook.com/eanotes/), *Michael’s Notebook*, June 2.\n\nRowe, Abraham (2022) [Critiques of EA that I want to read](https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read), *Effective Altruism Forum*, June 19.\n\nWiblin, Robert & Keiran Harris (2019) [Vitalik Buterin on effective altruism, better ways to fund public goods, the blockchain’s problems so far, and how it could yet change the world](https://80000hours.org/podcast/episodes/vitalik-buterin-new-ways-to-fund-public-goods/), *80,000 Hours*, September 3.\n\nZhang, Linchuan (2021) [The motivated reasoning critique of effective altruism](https://forum.effectivealtruism.org/posts/pxALB46SEkwNbfiNS/the-motivated-reasoning-critique-of-effective-altruism), *Effective Altruism Forum*, September 14.\n\nRelated\n-------\n\n*   [Altruism](https://www.lesswrong.com/tag/altruism)\n*   [Cause Prioritization](https://www.lesswrong.com/tag/cause-prioritization)\n*   [Utilitarianism](https://lessestwrong.com/tag/utilitarianism)\n*   [S-risk](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [X-risk](https://www.lesswrong.com/tag/existential-risk)\n\nExternal Resources\n------------------\n\n*   [80,000 Hours](https://80000hours.org/), who offer advice for how to have a maximally globally impactful career\n*   [Effective Altruism,](https://www.effectivealtruism.org/) who offer support for local EA groups, as well as articles and advice surrounding EA\n*   [GiveWell,](https://www.givewell.org/) a charity doing research into the effectiveness of other charities to provide information for donors\n*   [The Life You Can Save](https://www.thelifeyoucansave.org/the-book/?gclid=CjwKCAjwjqT5BRAPEiwAJlBuBXb3m1FKunezyfsYzYkjmgzSCHScRgZpzMH097cbAAGC5lmHUP-J3BoCcnAQAvD_BwE), a free eBook outlining reasons for donating more and more effectively\n*   [Center on Long-Term Risk](https://www.lesswrong.com/tag/center-on-long-term-risk-clr)\n\n1.  ^**[^](#fnrefipt32j7op0s)**^\n    \n    https://twitter.com/KerryLVaughan/status/1545063368695898112?s=20&t=xgaSuh22V6y44Wkcebo22Q"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "gHCNhqxuJq2bZ2akb",
    "name": "Social & Cultural Dynamics",
    "core": false,
    "slug": "social-and-cultural-dynamics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 255,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NrvXXL3iGjjxu5B7d",
    "name": "Machine Intelligence Research Institute (MIRI)",
    "core": false,
    "slug": "machine-intelligence-research-institute-miri",
    "tableOfContents": {
      "html": "<p><span class=\"by_XtphY3uYHwruKqDyG\">The </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">Machine Intelligence Research Institute</span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\">, formerly</span><span class=\"by_LoykQRMTxJFxwwdPy\"> known as the </span></span><a href=\"https://wiki.lesswrong.com/wiki/Singularity_Institute_for_Artificial_Intelligence\"><span class=\"by_LoykQRMTxJFxwwdPy\">Singularity Institute for Artificial Intelligence</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> (not</span><span class=\"by_LoykQRMTxJFxwwdPy\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">be confused with Singularity University)</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a non-profit research organization devoted to reducing </span></span><a href=\"https://lessestwrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">existential risk</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> from </span><a href=\"https://lessestwrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">unfriendly artificial intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and understanding problems related to </span><a href=\"https://lessestwrong.com/tag/friendly-artificial-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">friendly artificial intelligence</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">. </span><a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> was one of the early founders and continues to work there as a Research Fellow. The Machine Intelligence Research Institute created and currently owns the </span><a href=\"https://www.lesswrong.com/about\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> domain.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">External Links</span></p><ul><li><a href=\"http://intelligence.org/\"><span><span class=\"by_wDtHP5sk9gW3CzpiS\">Homepage</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of the Machine Intelligence Research Institute</span></span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/singularity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Technological singularity</span></a></li><li><a href=\"https://lessestwrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Existential risk</span></a></li><li><a href=\"https://lessestwrong.com/tag/intelligence-explosion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intelligence explosion</span></a></li><li><a href=\"https://lessestwrong.com/tag/friendly-artificial-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly artificial intelligence</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 134,
    "description": {
      "markdown": "The **Machine Intelligence Research Institute**, formerly known as the [Singularity Institute for Artificial Intelligence](https://wiki.lesswrong.com/wiki/Singularity_Institute_for_Artificial_Intelligence) (not to be confused with Singularity University) is a non-profit research organization devoted to reducing [existential risk](https://lessestwrong.com/tag/existential-risk) from [unfriendly artificial intelligence](https://lessestwrong.com/tag/unfriendly-artificial-intelligence) and understanding problems related to [friendly artificial intelligence](https://lessestwrong.com/tag/friendly-artificial-intelligence). [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) was one of the early founders and continues to work there as a Research Fellow. The Machine Intelligence Research Institute created and currently owns the [LessWrong](https://www.lesswrong.com/about) domain.\n\nExternal Links\n\n*   [Homepage of the Machine Intelligence Research Institute](http://intelligence.org/)\n\nSee Also\n--------\n\n*   [Technological singularity](https://www.lesswrong.com/tag/singularity)\n*   [Existential risk](https://lessestwrong.com/tag/existential-risk)\n*   [Intelligence explosion](https://lessestwrong.com/tag/intelligence-explosion)\n*   [Friendly artificial intelligence](https://lessestwrong.com/tag/friendly-artificial-intelligence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wfW6iL96u26mbatep",
    "name": "Bounded Rationality",
    "core": false,
    "slug": "bounded-rationality",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_XtphY3uYHwruKqDyG\">Bounded </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Rationality</span></span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\"> is rationality</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> for bounded agents. Not (primarily) about \"modelling irrationality\": may include models of irrational behavior, but the aspiration of bounded rationality is to explain why this is in some sense the best a bounded agent can do, or, a rational approach for a bounded agent to take given its limited resources and knowledge. In other words, bounded rationality is a type of rationality, not a type of irrationality.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 18,
    "description": {
      "markdown": "**Bounded Rationality** is rationality for bounded agents. Not (primarily) about \"modelling irrationality\": may include models of irrational behavior, but the aspiration of bounded rationality is to explain why this is in some sense the best a bounded agent can do, or, a rational approach for a bounded agent to take given its limited resources and knowledge. In other words, bounded rationality is a type of rationality, not a type of irrationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "X7v7Fyp9cgBYaMe2e",
    "name": "Center for Applied Rationality (CFAR)",
    "core": false,
    "slug": "center-for-applied-rationality-cfar",
    "tableOfContents": {
      "html": "<p><span class=\"by_XtphY3uYHwruKqDyG\">The </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">Center for Applied Rationality (CFAR) </span></strong><span class=\"by_XtphY3uYHwruKqDyG\">is a nonprofit research institute located in Berkeley, California, that was co-founded by </span><a href=\"https://www.lesswrong.com/users/eliezer_yudkowsky\"><span class=\"by_XtphY3uYHwruKqDyG\">Eliezer Yudkowsky</span></a><span class=\"by_XtphY3uYHwruKqDyG\">, with many top-contributors of LessWrong having participated in their workshops and many staff having written well-received posts on LessWrong.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 74,
    "description": {
      "markdown": "The **Center for Applied Rationality (CFAR)** is a nonprofit research institute located in Berkeley, California, that was co-founded by [Eliezer Yudkowsky](https://www.lesswrong.com/users/eliezer_yudkowsky), with many top-contributors of LessWrong having participated in their workshops and many staff having written well-received posts on LessWrong."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yTuPAtcPHSpc9r3kA",
    "name": "Impact Measures",
    "core": false,
    "slug": "impact-measures",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_pgi5MqvGrtvQozEH8\">Impact measures </span></strong><span class=\"by_pgi5MqvGrtvQozEH8\">penalize an AI for affecting us too much.</span><strong><span class=\"by_pgi5MqvGrtvQozEH8\"> </span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">To reduce the risk posed by a powerful AI, you might want to make it try accomplish its goals with as little impact on the world as possible. </span><span class=\"by_pgi5MqvGrtvQozEH8\">You reward the AI for crossing a room; to maximize time-discounted total reward, the optimal policy makes a huge mess as it sprints to the other side.</span></span><br><br><span><span class=\"by_pgi5MqvGrtvQozEH8\">How</span><span class=\"by_HoGziwmhpMGqGeWZy\"> do you rigorously define </span><span class=\"by_pgi5MqvGrtvQozEH8\">\"low impact\"</span><span class=\"by_HoGziwmhpMGqGeWZy\"> in a way that a computer can </span><span class=\"by_pgi5MqvGrtvQozEH8\">understand – how</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> do you measure </span><span class=\"by_pgi5MqvGrtvQozEH8\">impact? These questions are important for both prosaic and future AI systems: objective specification </span></span><a href=\"https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy\"><span class=\"by_pgi5MqvGrtvQozEH8\">is</span></a><span class=\"by_pgi5MqvGrtvQozEH8\"> </span><a href=\"https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity\"><span class=\"by_pgi5MqvGrtvQozEH8\">hard</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">; we don't want AI systems to rampantly disrupt their environment. In the limit of goal-directed intelligence, </span><a href=\"https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-provably-instrumentally-convergent-in\"><span class=\"by_pgi5MqvGrtvQozEH8\">theorems suggest that seeking power tends to be optimal</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">; we don't want highly capable AI systems to permanently wrench control of the future from us.&nbsp;</span></p><p><span><span class=\"by_pgi5MqvGrtvQozEH8\">Currently, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">impact</span><span class=\"by_pgi5MqvGrtvQozEH8\"> measurement research focuses on two approaches:</span></span></p><ul><li><a href=\"https://arxiv.org/pdf/1806.01186.pdf\"><span class=\"by_pgi5MqvGrtvQozEH8\">Relative reachability</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">: the AI preserves its ability to reach many kinds of world-states. The hope is that by staying able to reach many goal states, the AI stays able to reach the correct goal state.</span></li><li><a href=\"https://arxiv.org/abs/2006.06547\"><span class=\"by_pgi5MqvGrtvQozEH8\">Attainable</span></a><span class=\"by_pgi5MqvGrtvQozEH8\"> </span><a href=\"https://arxiv.org/abs/1902.09725\"><span class=\"by_pgi5MqvGrtvQozEH8\">utility</span></a><span class=\"by_pgi5MqvGrtvQozEH8\"> </span><a href=\"https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts\"><span class=\"by_pgi5MqvGrtvQozEH8\">preservation</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">: the AI preserves its ability to achieve one or more auxiliary goals. The hope is that by penalizing gaining or losing control over the future, the AI doesn't take control away from us.</span></li></ul><p><span class=\"by_pgi5MqvGrtvQozEH8\">For a review of earlier work, see </span><a href=\"https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/TPy4RJvzogqqupDKk\"><span class=\"by_pgi5MqvGrtvQozEH8\">A Survey of Early Impact Measures</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">.&nbsp;</span></p><p><span class=\"by_pgi5MqvGrtvQozEH8\">Sequences on impact measurement:</span></p><ul><li><a href=\"https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW\"><span class=\"by_pgi5MqvGrtvQozEH8\">Reframing Impact</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">: we're impacted when we become more or less able to achieve our goals. Seemingly, goal-directed AI systems are only incentivized to catastrophically impact us in order to gain power to achieve their own goals. To avoid catastrophic impact, what if we penalize the AI for gaining power?</span></li><li><a href=\"https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh\"><span class=\"by_pgi5MqvGrtvQozEH8\">Subagents and Impact Measures</span></a><span class=\"by_pgi5MqvGrtvQozEH8\"> explores how subagents can circumvent current impact measure formalizations.</span></li></ul><p><span class=\"by_pgi5MqvGrtvQozEH8\">Related tags: </span><a href=\"https://www.lesswrong.com/tag/instrumental-convergence\"><span class=\"by_pgi5MqvGrtvQozEH8\">Instrumental Convergence</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">, </span><a href=\"https://www.lesswrong.com/tag/corrigibility\"><span class=\"by_pgi5MqvGrtvQozEH8\">Corrigibility</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">, </span><a href=\"https://www.lesswrong.com/tag/mild-optimization\"><span class=\"by_pgi5MqvGrtvQozEH8\">Mild Optimization</span></a><span class=\"by_pgi5MqvGrtvQozEH8\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 49,
    "description": {
      "markdown": "**Impact measures** penalize an AI for affecting us too much.  To reduce the risk posed by a powerful AI, you might want to make it try accomplish its goals with as little impact on the world as possible. You reward the AI for crossing a room; to maximize time-discounted total reward, the optimal policy makes a huge mess as it sprints to the other side.  \n  \nHow do you rigorously define \"low impact\" in a way that a computer can understand – how do you measure impact? These questions are important for both prosaic and future AI systems: objective specification [is](https://www.lesswrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy) [hard](https://www.lesswrong.com/posts/7b2RJJQ76hjZwarnj/specification-gaming-the-flip-side-of-ai-ingenuity); we don't want AI systems to rampantly disrupt their environment. In the limit of goal-directed intelligence, [theorems suggest that seeking power tends to be optimal](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-provably-instrumentally-convergent-in); we don't want highly capable AI systems to permanently wrench control of the future from us. \n\nCurrently, impact measurement research focuses on two approaches:\n\n*   [Relative reachability](https://arxiv.org/pdf/1806.01186.pdf): the AI preserves its ability to reach many kinds of world-states. The hope is that by staying able to reach many goal states, the AI stays able to reach the correct goal state.\n*   [Attainable](https://arxiv.org/abs/2006.06547) [utility](https://arxiv.org/abs/1902.09725) [preservation](https://www.lesswrong.com/posts/75oMAADr4265AGK3L/attainable-utility-preservation-concepts): the AI preserves its ability to achieve one or more auxiliary goals. The hope is that by penalizing gaining or losing control over the future, the AI doesn't take control away from us.\n\nFor a review of earlier work, see [A Survey of Early Impact Measures](https://www.lesswrong.com/s/nMGrhBYXWjPhZoyNL/p/TPy4RJvzogqqupDKk). \n\nSequences on impact measurement:\n\n*   [Reframing Impact](https://www.lesswrong.com/s/7CdoznhJaLEKHwvJW): we're impacted when we become more or less able to achieve our goals. Seemingly, goal-directed AI systems are only incentivized to catastrophically impact us in order to gain power to achieve their own goals. To avoid catastrophic impact, what if we penalize the AI for gaining power?\n*   [Subagents and Impact Measures](https://www.lesswrong.com/s/iRwYCpcAXuFD24tHh) explores how subagents can circumvent current impact measure formalizations.\n\nRelated tags: [Instrumental Convergence](https://www.lesswrong.com/tag/instrumental-convergence), [Corrigibility](https://www.lesswrong.com/tag/corrigibility), [Mild Optimization](https://www.lesswrong.com/tag/mild-optimization)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ksdiAMKfgSyEeKMo6",
    "name": "Academic Papers",
    "core": false,
    "slug": "academic-papers",
    "tableOfContents": {
      "html": "<p><span class=\"by_qxJ28GN72aiJu96iF\">Posts either linking to, or summarizing, formal papers published elsewhere.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 102,
    "description": {
      "markdown": "Posts either linking to, or summarizing, formal papers published elsewhere."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PbShukhzpLsWpGXkM",
    "name": "Anthropics",
    "core": false,
    "slug": "anthropics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Anthropics</span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\"> is the study of how the fact that we </span><span class=\"by_qf77EiaoMw7tH3GSr\">succeed in making</span><span class=\"by_XtphY3uYHwruKqDyG\"> observations</span><span class=\"by_qf77EiaoMw7tH3GSr\"> of a given kind</span><span class=\"by_XtphY3uYHwruKqDyG\"> at all gives us evidence about the world we are living, independently of the </span><span class=\"by_qf77EiaoMw7tH3GSr\">content</span><span class=\"by_XtphY3uYHwruKqDyG\"> of the observations. </span><span class=\"by_qf77EiaoMw7tH3GSr\">As an</span><span class=\"by_XtphY3uYHwruKqDyG\"> example, for </span><span class=\"by_qf77EiaoMw7tH3GSr\">living beings, making any observations at all is only possible in</span><span class=\"by_XtphY3uYHwruKqDyG\"> a universe with physical laws that support life.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/sleeping-beauty-paradox\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Sleeping Beauty Paradox</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/filtered-evidence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Filtered Evidence</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/great-filter\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Great Filter</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 192,
    "description": {
      "markdown": "**Anthropics** is the study of how the fact that we succeed in making observations of a given kind at all gives us evidence about the world we are living, independently of the content of the observations. As an example, for living beings, making any observations at all is only possible in a universe with physical laws that support life.\n\n**Related Pages:** [Sleeping Beauty Paradox](https://www.lesswrong.com/tag/sleeping-beauty-paradox), [Filtered Evidence](https://www.lesswrong.com/tag/filtered-evidence), [Great Filter](https://www.lesswrong.com/tag/great-filter)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TiEFKWDvD3jsKumDx",
    "name": "AIXI",
    "core": false,
    "slug": "aixi",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_5yNJS8bxEYhgFD9XJ\">AIXI</span></strong><span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> is a mathematical formalism for a </span><span class=\"by_nmk3nLpQE89dMRzzN\">hypothetical </span></span><a href=\"https://www.lesswrong.com/tag/superintelligence\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">(super)</span><span class=\"by_W7ETRtvRMqYetyQE9\">intelligence</span></span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> developed by Marcus </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Hutter (2005, 2007). AIXI is not computable, and so does not serve as a design for a real-world AI, but is considered a valuable theoretical illustration with both positive and negative aspects (things AIXI would be able to do and things it arguably </span><span class=\"by_MzHvF5RbnRq6LepWb\">couldn'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">t do).</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also: </span></i><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI</span></a></p><p><span><span class=\"by_nmk3nLpQE89dMRzzN\">The AIXI formalism says roughly to consider</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> all possible </span><span class=\"by_nmk3nLpQE89dMRzzN\">computable models of</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the </span><span class=\"by_nmk3nLpQE89dMRzzN\">environment, Bayes-update them on past experiences, and use the resulting updated predictions</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> to </span><span class=\"by_nmk3nLpQE89dMRzzN\">model</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> the </span><span class=\"by_nmk3nLpQE89dMRzzN\">expected sensory reward of all possible strategies.</span><span class=\"by_qxJ28GN72aiJu96iF\"> This</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> is an </span><span class=\"by_qxJ28GN72aiJu96iF\">application</span><span class=\"by_5yNJS8bxEYhgFD9XJ\"> of </span></span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true\"><span><span class=\"by_5yNJS8bxEYhgFD9XJ\">Solomonoff </span><span class=\"by_qxJ28GN72aiJu96iF\">Induction</span></span></a><span class=\"by_5yNJS8bxEYhgFD9XJ\">.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI can be viewed as the border between AI problems that would be </span><span class=\"by_MzHvF5RbnRq6LepWb\">'simple'</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to solve using unlimited computing power and problems which are structurally </span><span class=\"by_MzHvF5RbnRq6LepWb\">'complicated'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></span></p><h2 id=\"How_AIXI_works\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">How AIXI works</span></strong></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Hutter (</span><a href=\"http://www.hutter1.net/ai/aixigentle.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2007</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">) describes AIXI as a combination of decision theory and algorithmic information theory: </span><span class=\"by_MzHvF5RbnRq6LepWb\">\"Decision</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. </span><span class=\"by_MzHvF5RbnRq6LepWb\">Solomonoff’</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence.</span><span class=\"by_MzHvF5RbnRq6LepWb\">\"</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI operates within the following agent model: There is an </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">agent</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, and an </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">environment</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, which is a computable function unknown to the agent. Thus the agent will need to have a probability distribution on the range of possible environments.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">On each clock tick, the agent receives an </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">observation</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> (a bitstring/number) from the environment, as well as a reward (another number).</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The agent then outputs an </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">action</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> (another number).</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">To do this, AIXI guesses at a probability distribution for its environment, using </span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, a formalization of </span><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><span><span class=\"by_MzHvF5RbnRq6LepWb\">Occam'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s razor</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">: Simpler computations are more likely </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">a priori</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> to describe the environment than more complex ones. This probability distribution is then Bayes-updated by how well each model fits the evidence (or more precisely, by throwing out all computations which have not exactly fit the environmental data so far, but for technical reasons this is roughly equivalent as a model). AIXI then calculates the expected reward of each action it might choose--weighting the likelihood of possible environments as mentioned. It chooses the best action by extrapolating its actions into its future time horizon recursively, using the assumption that at each step into the future it will again choose the best possible action using the same procedure.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Then, on each iteration, the environment provides an observation and reward as a function of the full history of the interaction; the agent likewise is choosing its action as a function of the full history.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><span class=\"by_MzHvF5RbnRq6LepWb\">agent'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s intelligence is defined by its expected reward across all environments, weighting their likelihood by their complexity.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI is not a feasible AI, because </span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is not computable, and because some environments may not interact over finite time horizons (AIXI only works over some finite time horizon, though any finite horizon can be chosen). A somewhat more computable variant is the time-space-bounded AIXItl. Real AI algorithms explicitly inspired by AIXItl, e.g. the Monte Carlo approximation by Veness et al. (2011) have shown interesting results in simple general-intelligence test problems.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">For a short (half-page) technical introduction to AIXI, see </span><a href=\"https://web.archive.org/web/20160425092747/http://www.jair.org/media/3125/live-3125-5397-jair.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Veness et al. 2011</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, page 1-2. For a full exposition of AIXI, see </span><a href=\"http://www.hutter1.net/ai/aixigentle.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hutter 2007</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h2 id=\"Relevance_to_Friendly_AI\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Relevance to Friendly AI</span></strong></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Because it abstracts optimization power away from human mental features, AIXI is valuable in considering the possibilities for future artificial general intelligence - a compact and non-anthropomorphic specification that is technically complete and closed; either some feature of AIXI follows from the equations or it does not. In particular, it acts as a constructive demonstration of an AGI which does not have human-like </span><a href=\"https://www.lesswrong.com/tag/terminal-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">terminal values</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and will act solely to maximize its reward function. (Yampolskiy &amp; Fox 2012).</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI has limitations as a model for future AGI, for example, the </span><a href=\"https://www.lesswrong.com/tag/anvil-problem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Anvil problem</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">: AIXI lacks a self-model. It extrapolates its own actions into the future indefinitely, on the assumption that it will keep working in the same way in the future. Though AIXI is an abstraction, any real AI would have a physical embodiment that could be damaged, and an implementation which could change its behavior due to bugs; and the AIXI formalism completely ignores these possibilities.</span></p><h2 id=\"References\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></strong></h2><ul><li><a href=\"https://intelligence.org/files/AGI-HMM.pdf\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">R.V. Yampolskiy, J. Fox (2012) Artificial General Intelligence and the Human Mental Model. In Amnon H. Eden, Johnny </span><span class=\"by_MzHvF5RbnRq6LepWb\">Sø</span><span class=\"by_qgdGA4ZEyW7zNdK84\">raker, James H. Moor, Eric Steinhart (Eds.), The Singularity Hypothesis.The Frontiers Collection. London: Springer.</span></span></a></li><li><a href=\"http://www.hutter1.net/ai/aixigentle.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">M. Hutter (2007) Universal Algorithmic Intelligence: A mathematical top-&gt;down approach</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. In Goertzel &amp; Pennachin (eds.), Artificial General Intelligence, 227-287. Berlin: Springer.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">M. Hutter, (2005) Universal Artificial Intelligence: Sequential decisions based on algorithmic probability. Berlin: Springer.</span></li><li><a href=\"http://www.jair.org/media/3125/live-3125-5397-jair.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">J. Veness, K.S. Ng, M. Hutter, W. Uther and D. Silver (2011) A Monte-Carlo AIXI Approximation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Journal of </span><span class=\"by_MzHvF5RbnRq6LepWb\">Artiﬁ</span><span class=\"by_qgdGA4ZEyW7zNdK84\">cial Intelligence Research</span></span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> 40, 95-142]</span></li></ul><h2 id=\"Blog_posts\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></strong></h2><ul><li><a href=\"https://www.lesswrong.com/lw/8qy/aixi_and_existential_despair/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI and Existential Despair</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://www.lesswrong.com/users/paulfchristiano\"><span class=\"by_qgdGA4ZEyW7zNdK84\">paulfchristiano</span></a></li><li><a href=\"https://www.lesswrong.com/r/discussion/lw/az7/video_paul_christianos_impromptu_tutorial_on_aixi/\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">[video] Paul </span><span class=\"by_MzHvF5RbnRq6LepWb\">Christiano'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s impromptu tutorial on AIXI and TDT</span></span></a></li></ul><h2 id=\"See_also\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></strong></h2><ul><li><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff induction</span></a></li><li><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory</span></a></li></ul>",
      "sections": [
        {
          "title": "How AIXI works",
          "anchor": "How_AIXI_works",
          "level": 1
        },
        {
          "title": "Relevance to Friendly AI",
          "anchor": "Relevance_to_Friendly_AI",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 30,
    "description": {
      "markdown": "**AIXI** is a mathematical formalism for a hypothetical [(super)intelligence](https://www.lesswrong.com/tag/superintelligence), developed by Marcus Hutter (2005, 2007). AIXI is not computable, and so does not serve as a design for a real-world AI, but is considered a valuable theoretical illustration with both positive and negative aspects (things AIXI would be able to do and things it arguably couldn't do).\n\n*See also:* [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), [Decision theory](https://www.lesswrong.com/tag/decision-theory), [AI](https://www.lesswrong.com/ai)\n\nThe AIXI formalism says roughly to consider all possible computable models of the environment, Bayes-update them on past experiences, and use the resulting updated predictions to model the expected sensory reward of all possible strategies. This is an application of [Solomonoff Induction](https://www.lesswrong.com/tag/solomonoff-induction?useTagName=true).\n\nAIXI can be viewed as the border between AI problems that would be 'simple' to solve using unlimited computing power and problems which are structurally 'complicated'.\n\n**How AIXI works**\n------------------\n\nHutter ([2007](http://www.hutter1.net/ai/aixigentle.htm)) describes AIXI as a combination of decision theory and algorithmic information theory: \"Decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff’s theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameterless theory of universal Artificial Intelligence.\"\n\nAIXI operates within the following agent model: There is an *agent*, and an *environment*, which is a computable function unknown to the agent. Thus the agent will need to have a probability distribution on the range of possible environments.\n\nOn each clock tick, the agent receives an *observation* (a bitstring/number) from the environment, as well as a reward (another number).\n\nThe agent then outputs an *action* (another number).\n\nTo do this, AIXI guesses at a probability distribution for its environment, using [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction), a formalization of [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor): Simpler computations are more likely *a priori* to describe the environment than more complex ones. This probability distribution is then Bayes-updated by how well each model fits the evidence (or more precisely, by throwing out all computations which have not exactly fit the environmental data so far, but for technical reasons this is roughly equivalent as a model). AIXI then calculates the expected reward of each action it might choose--weighting the likelihood of possible environments as mentioned. It chooses the best action by extrapolating its actions into its future time horizon recursively, using the assumption that at each step into the future it will again choose the best possible action using the same procedure.\n\nThen, on each iteration, the environment provides an observation and reward as a function of the full history of the interaction; the agent likewise is choosing its action as a function of the full history.\n\nThe agent's intelligence is defined by its expected reward across all environments, weighting their likelihood by their complexity.\n\nAIXI is not a feasible AI, because [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction) is not computable, and because some environments may not interact over finite time horizons (AIXI only works over some finite time horizon, though any finite horizon can be chosen). A somewhat more computable variant is the time-space-bounded AIXItl. Real AI algorithms explicitly inspired by AIXItl, e.g. the Monte Carlo approximation by Veness et al. (2011) have shown interesting results in simple general-intelligence test problems.\n\nFor a short (half-page) technical introduction to AIXI, see [Veness et al. 2011](https://web.archive.org/web/20160425092747/http://www.jair.org/media/3125/live-3125-5397-jair.pdf), page 1-2. For a full exposition of AIXI, see [Hutter 2007](http://www.hutter1.net/ai/aixigentle.htm).\n\n**Relevance to Friendly AI**\n----------------------------\n\nBecause it abstracts optimization power away from human mental features, AIXI is valuable in considering the possibilities for future artificial general intelligence - a compact and non-anthropomorphic specification that is technically complete and closed; either some feature of AIXI follows from the equations or it does not. In particular, it acts as a constructive demonstration of an AGI which does not have human-like [terminal values](https://www.lesswrong.com/tag/terminal-value) and will act solely to maximize its reward function. (Yampolskiy & Fox 2012).\n\nAIXI has limitations as a model for future AGI, for example, the [Anvil problem](https://www.lesswrong.com/tag/anvil-problem): AIXI lacks a self-model. It extrapolates its own actions into the future indefinitely, on the assumption that it will keep working in the same way in the future. Though AIXI is an abstraction, any real AI would have a physical embodiment that could be damaged, and an implementation which could change its behavior due to bugs; and the AIXI formalism completely ignores these possibilities.\n\n**References**\n--------------\n\n*   [R.V. Yampolskiy, J. Fox (2012) Artificial General Intelligence and the Human Mental Model. In Amnon H. Eden, Johnny Søraker, James H. Moor, Eric Steinhart (Eds.), The Singularity Hypothesis.The Frontiers Collection. London: Springer.](https://intelligence.org/files/AGI-HMM.pdf)\n*   [M. Hutter (2007) Universal Algorithmic Intelligence: A mathematical top->down approach](http://www.hutter1.net/ai/aixigentle.htm). In Goertzel & Pennachin (eds.), Artificial General Intelligence, 227-287. Berlin: Springer.\n*   M. Hutter, (2005) Universal Artificial Intelligence: Sequential decisions based on algorithmic probability. Berlin: Springer.\n*   [J. Veness, K.S. Ng, M. Hutter, W. Uther and D. Silver (2011) A Monte-Carlo AIXI Approximation](http://www.jair.org/media/3125/live-3125-5397-jair.pdf), *Journal of Artiﬁcial Intelligence Research* 40, 95-142\\]\n\n**Blog posts**\n--------------\n\n*   [AIXI and Existential Despair](https://www.lesswrong.com/lw/8qy/aixi_and_existential_despair/) by [paulfchristiano](https://www.lesswrong.com/users/paulfchristiano)\n*   [\\[video\\] Paul Christiano's impromptu tutorial on AIXI and TDT](https://www.lesswrong.com/r/discussion/lw/az7/video_paul_christianos_impromptu_tutorial_on_aixi/)\n\n**See also**\n------------\n\n*   [Solomonoff induction](https://www.lesswrong.com/tag/solomonoff-induction)\n*   [Decision theory](https://www.lesswrong.com/tag/decision-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Wi3EopKJ2aNdtxSWg",
    "name": "Neuroscience",
    "core": false,
    "slug": "neuroscience",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Neuroscience</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> is a field of study dealing with the structure or function of the brain. It is of particular interest to LessWrong both because it can shed light on </span><a href=\"https://www.lesswrong.com/tag/ai?showPostCount=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">AI</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, and because it is often helpful for </span><a href=\"https://www.lesswrong.com/tag/rationality?showPostCount=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">rationality</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">. For example, understanding how </span><a href=\"https://www.lesswrong.com/posts/rD57ysqawarsbry6v?lw_source=posts_sheet\"><span class=\"by_Xn6ACr6Cua8upALWQ\">attentional control</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> works can inform possible solutions.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 112,
    "description": {
      "markdown": "**Neuroscience** is a field of study dealing with the structure or function of the brain. It is of particular interest to LessWrong both because it can shed light on [AI](https://www.lesswrong.com/tag/ai?showPostCount=true), and because it is often helpful for [rationality](https://www.lesswrong.com/tag/rationality?showPostCount=true). For example, understanding how [attentional control](https://www.lesswrong.com/posts/rD57ysqawarsbry6v?lw_source=posts_sheet) works can inform possible solutions."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YAotJ9Le3S2rCJgf8",
    "name": "Predictive Processing",
    "core": false,
    "slug": "predictive-processing",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">(From </span><a href=\"https://en.wikipedia.org/wiki/Predictive_coding\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">) </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">predictive processing</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is a theory of brain function in which the brain is constantly generating and updating a mental model of the environment. The model is used to generate predictions of sensory input that are compared to actual sensory input. This comparison results in prediction errors that are then used to update and revise the mental model.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong><br><a href=\"https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Book Review: Surfing Uncertainty</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> - Introduction to predictive processing by Scott Alexander</span><br><a href=\"https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Predictive Processing And Perceptual Control</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/perceptual-control-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Perceptual Control Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/neuroscience\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Neuroscience</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 26,
    "description": {
      "markdown": "(From [Wikipedia](https://en.wikipedia.org/wiki/Predictive_coding)) **predictive processing** is a theory of brain function in which the brain is constantly generating and updating a mental model of the environment. The model is used to generate predictions of sensory input that are compared to actual sensory input. This comparison results in prediction errors that are then used to update and revise the mental model.\n\n**External Links:**  \n[Book Review: Surfing Uncertainty](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/) \\- Introduction to predictive processing by Scott Alexander  \n[Predictive Processing And Perceptual Control](https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/) by Scott Alexander\n\n**Related Pages:** [Perceptual Control Theory](https://www.lesswrong.com/tag/perceptual-control-theory), [Neuroscience](https://www.lesswrong.com/tag/neuroscience)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "69L5E2XPqdMF2B3gw",
    "name": "Internal Double Crux",
    "core": false,
    "slug": "internal-double-crux",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Internal Double Crux (IDC) </span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\">is a tool for resolving conflict in one's mind. It's a script for focusing on two conflicting inner voices and holding space for them to debate and compromise. A sort of internal couples therapy, if you will. It's built on the notion of double crux, which is a tool for resolving disagreements between two people, but in this situation applied to the inside of one's own mind as though it is made up of many smaller people. IDC was developed by staff at the Center for Applied Rationality.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">For those who find formal instructions helpful, these are the four instructions that CFAR staff give to attendees.</span></p><ol><li><span class=\"by_EQNTWXLKMeWMp2FQS\">Find an internal disagreement</span></li><li><span class=\"by_EQNTWXLKMeWMp2FQS\">Operationalize the disagreement</span></li><li><span class=\"by_EQNTWXLKMeWMp2FQS\">Seek double cruxes</span></li><li><span class=\"by_EQNTWXLKMeWMp2FQS\">Resonate</span></li></ol><p><span class=\"by_EQNTWXLKMeWMp2FQS\">The current best writeups are in </span><a href=\"https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all\"><span class=\"by_EQNTWXLKMeWMp2FQS\">the CFAR Handbook</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\"> and in Alkjash's </span><a href=\"https://www.lesswrong.com/posts/mQmx4kQQtHeBip9ZC/internal-double-crux\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Hammertime sequence</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">For a similar technique that IDC is arguably a special case of, see </span><a href=\"https://www.lesswrong.com/tag/internal-family-systems\"><span class=\"by_qxJ28GN72aiJu96iF\">Internal Family Systems</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "**Internal Double Crux (IDC)** is a tool for resolving conflict in one's mind. It's a script for focusing on two conflicting inner voices and holding space for them to debate and compromise. A sort of internal couples therapy, if you will. It's built on the notion of double crux, which is a tool for resolving disagreements between two people, but in this situation applied to the inside of one's own mind as though it is made up of many smaller people. IDC was developed by staff at the Center for Applied Rationality.\n\nFor those who find formal instructions helpful, these are the four instructions that CFAR staff give to attendees.\n\n1.  Find an internal disagreement\n2.  Operationalize the disagreement\n3.  Seek double cruxes\n4.  Resonate\n\nThe current best writeups are in [the CFAR Handbook](https://www.lesswrong.com/posts/Z9cbwuevS9cqaR96h/cfar-participant-handbook-now-available-to-all) and in Alkjash's [Hammertime sequence](https://www.lesswrong.com/posts/mQmx4kQQtHeBip9ZC/internal-double-crux).\n\nFor a similar technique that IDC is arguably a special case of, see [Internal Family Systems](https://www.lesswrong.com/tag/internal-family-systems)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5uHdFgR938LGGxMKQ",
    "name": "Subagents",
    "core": false,
    "slug": "subagents",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">Subagents</span></strong><span class=\"by_qxJ28GN72aiJu96iF\"> refers to the idea that rather than thinking of the mind as an entity with one set of goals and beliefs, it includes many independently acting components, each of which might have varying goals and beliefs. One intuitive way of expressing this is the expression \"one part of me wants X, but another part of me wants Y instead\".</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">While the name implies some degree of independent agency on part of the subagents, they may also be viewed as being more passive entities. For example, the \"parts\" in the above example may be considered different sets of beliefs, accessed one at a time by the same system.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">The </span><a href=\"https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip\"><span class=\"by_qxJ28GN72aiJu96iF\">Multiagent Models of Mind sequence</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> explores the notion of subagents in detail. </span><a href=\"https://www.lesswrong.com/tag/akrasia?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">Akrasia</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> (acting against one's better judgment, such as by procrastinating) may involve subagent disagreement. </span><a href=\"https://www.lesswrong.com/tag/internal-double-crux?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">Internal Double Crux</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> is one technique for getting subagents to agree with each other.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 82,
    "description": {
      "markdown": "**Subagents** refers to the idea that rather than thinking of the mind as an entity with one set of goals and beliefs, it includes many independently acting components, each of which might have varying goals and beliefs. One intuitive way of expressing this is the expression \"one part of me wants X, but another part of me wants Y instead\".\n\nWhile the name implies some degree of independent agency on part of the subagents, they may also be viewed as being more passive entities. For example, the \"parts\" in the above example may be considered different sets of beliefs, accessed one at a time by the same system.\n\nThe [Multiagent Models of Mind sequence](https://www.lesswrong.com/s/ZbmRyDN8TCpBTZSip) explores the notion of subagents in detail. [Akrasia](https://www.lesswrong.com/tag/akrasia?useTagName=true) (acting against one's better judgment, such as by procrastinating) may involve subagent disagreement. [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux?useTagName=true) is one technique for getting subagents to agree with each other."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "e9wHzopbGCAFwp9Rw",
    "name": "Human Genetics",
    "core": false,
    "slug": "human-genetics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 31,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AXhEhCkTrHZbjXXu3",
    "name": "Poetry",
    "core": false,
    "slug": "poetry",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 33,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cHoCqtfE9cF7aSs9d",
    "name": "Deception",
    "core": false,
    "slug": "deception",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Deception</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> is the act of sharing information in a way which intentionally misleads others.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/honesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Honesty</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/meta-honesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Meta-Honesty</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/self-deception\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Self-Deception</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/simulacrum-levels\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Simulacrum Levels</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 63,
    "description": {
      "markdown": "**Deception** is the act of sharing information in a way which intentionally misleads others.\n\n**Related Pages:** [Honesty](https://www.lesswrong.com/tag/honesty), [Meta-Honesty](https://www.lesswrong.com/tag/meta-honesty), [Self-Deception](https://www.lesswrong.com/tag/self-deception), [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nANxo5C4sPG9HQHzr",
    "name": "Honesty",
    "core": false,
    "slug": "honesty",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Honesty</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> means telling the truth and not being </span><a href=\"http://lesswrong.com/tag/deception\"><span class=\"by_sKAL2jzfkYkDbQmx9\">deceptive</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong><br><a href=\"https://slatestarcodex.com/2019/07/16/against-lie-inflation/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Against Lie Inflation</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Scott Alexander</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><a href=\"http://lesswrong.com/tag/meta-honesty\"><span class=\"by_HoGziwmhpMGqGeWZy\">Meta-Honesty</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">,</span><a href=\"http://lesswrong.com/tag/deception\"><span class=\"by_HoGziwmhpMGqGeWZy\"> Deception</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 45,
    "description": {
      "markdown": "**Honesty** means telling the truth and not being [deceptive](http://lesswrong.com/tag/deception).\n\n**External Links:**  \n[Against Lie Inflation](https://slatestarcodex.com/2019/07/16/against-lie-inflation/) by Scott Alexander\n\n**Related Pages:** [Meta-Honesty](http://lesswrong.com/tag/meta-honesty), [Deception](http://lesswrong.com/tag/deception)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "W9aNkPwtPhMrcfgj7",
    "name": "Sex & Gender",
    "core": false,
    "slug": "sex-and-gender",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 55,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7Gh5dSvySMNbA6niL",
    "name": "Prioritization",
    "core": false,
    "slug": "prioritization",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xeqCTxje765k79Q78",
    "name": "Phenomenology",
    "core": false,
    "slug": "phenomenology",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">Phenomenology is the study of experience. It is also an approach to philosophy and a bundle of philosophical techniques and methods.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "Phenomenology is the study of experience. It is also an approach to philosophy and a bundle of philosophical techniques and methods."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xgpBASEThXPuKRhbS",
    "name": "Epistemology",
    "core": false,
    "slug": "epistemology",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemology</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the study of how we know the world. It's both a topic in philosophy and a practical concern for how we come to believe things are true.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Highly Advanced Epistemology 101 for Beginners</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Concepts in formal epistemology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/s/GTEay24Lxm3xoE4hy\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Novum Organum</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 197,
    "description": {
      "markdown": "**Epistemology** is the study of how we know the world. It's both a topic in philosophy and a practical concern for how we come to believe things are true.\n\n**Related Sequences:** [Highly Advanced Epistemology 101 for Beginners](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs), [Concepts in formal epistemology](https://www.lesswrong.com/s/FYMiCeXEgMzsB5stm), [Novum Organum](https://www.lesswrong.com/s/GTEay24Lxm3xoE4hy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3ee9k6NJfcGzL6kMS",
    "name": "Emotions",
    "core": false,
    "slug": "emotions",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Contrary to the </span><span class=\"by_LoykQRMTxJFxwwdPy\">stereotype,</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_qf77EiaoMw7tH3GSr\">rationality</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> doesn't mean denying </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">emotion</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">.</span><span class=\"by_qf77EiaoMw7tH3GSr\"> When emotion is appropriate to the reality of the situation, it should be embraced; only when emotion isn't appropriate should it be suppressed.</span></span></p><h2 id=\"External_links\"><span class=\"by_LoykQRMTxJFxwwdPy\">External links</span></h2><ul><li><a href=\"http://www.youtube.com/watch?v=tLgNZ9aTEwc\"><span class=\"by_LoykQRMTxJFxwwdPy\">The Straw Vulcan</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, a talk introducing rationality, by </span><a href=\"http://lesswrong.com/user/Julia_Galef/\"><span class=\"by_LoykQRMTxJFxwwdPy\">Julia Galef</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> (</span><a href=\"https://www.lesswrong.com/lw/90n/summary_of_the_straw_vulcan/\"><span class=\"by_LoykQRMTxJFxwwdPy\">summary</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">)</span></li><li><a href=\"http://www.overcomingbias.com/2006/12/vulcan_logic.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">Vulcan Logic</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by </span><a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\"><span class=\"by_HoGziwmhpMGqGeWZy\">Hal Finney</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/alief\"><span class=\"by_LoykQRMTxJFxwwdPy\">Alief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\"><span class=\"by_9c2mQkLQq6gQSksMs\">Truth</span></a><span class=\"by_9c2mQkLQq6gQSksMs\">, </span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_9c2mQkLQq6gQSksMs\">Rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/litany-of-tarski\"><span class=\"by_9c2mQkLQq6gQSksMs\">Litany of Tarski</span></a></li><li><a href=\"https://www.lesswrong.com/tag/hollywood-rationality\"><span class=\"by_9c2mQkLQq6gQSksMs\">Hollywood rationality</span></a></li></ul>",
      "sections": [
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 129,
    "description": {
      "markdown": "Contrary to the stereotype, [rationality](https://www.lesswrong.com/tag/rationality) doesn't mean denying **emotion**. When emotion is appropriate to the reality of the situation, it should be embraced; only when emotion isn't appropriate should it be suppressed.\n\nExternal links\n--------------\n\n*   [The Straw Vulcan](http://www.youtube.com/watch?v=tLgNZ9aTEwc), a talk introducing rationality, by [Julia Galef](http://lesswrong.com/user/Julia_Galef/) ([summary](https://www.lesswrong.com/lw/90n/summary_of_the_straw_vulcan/))\n*   [Vulcan Logic](http://www.overcomingbias.com/2006/12/vulcan_logic.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n\nSee also\n--------\n\n*   [Alief](https://www.lesswrong.com/tag/alief)\n*   [Truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning), [Rationality](https://www.lesswrong.com/tag/rationality)\n*   [Litany of Tarski](https://www.lesswrong.com/tag/litany-of-tarski)\n*   [Hollywood rationality](https://www.lesswrong.com/tag/hollywood-rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "csMv9MvvjYJyeHqoo",
    "name": "Physics",
    "core": false,
    "slug": "physics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 138,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q6hq54EXkrw8LQQE7",
    "name": "Gears-Level",
    "core": false,
    "slug": "gears-level",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">gears-level </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">model </span><span class=\"by_Xn6ACr6Cua8upALWQ\">is 'well-constrained' in</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">sense that there is a strong connection between each of the things you observe-</span><span class=\"by_qgdGA4ZEyW7zNdK84\">- </span><span class=\"by_Xn6ACr6Cua8upALWQ\">it would be hard for you to imagine that one of the variables could be different while all of the others remained the same. </span></span></p><p><em><span class=\"by_Xn6ACr6Cua8upALWQ\">Related Tags: </span><a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Anticipated Experiences</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Double Crux</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Empiricism</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Falsifiability</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Map and Territory</span></a></em></p><br><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The term </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">gears-level</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> was first described on LW in the post </span><a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\"><span class=\"by_Xn6ACr6Cua8upALWQ\">\"Gears in Understanding\"</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">:</span></p><blockquote><span class=\"by_Xn6ACr6Cua8upALWQ\">This property is </span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">how deterministically interconnected the variables of the model are</span></em><span class=\"by_Xn6ACr6Cua8upALWQ\">. There are a few </span><a href=\"https://en.wikipedia.org/wiki/Goodhart%27s_law\"><span class=\"by_Xn6ACr6Cua8upALWQ\">tests</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> I know of to see to what extent a model has this property, though I don't know if this list is exhaustive and would be a little surprised if it were:</span></blockquote><blockquote><span class=\"by_Xn6ACr6Cua8upALWQ\"> 1. Does the model p</span><a href=\"https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">ay rent?</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> If it does, and if it were falsified, how much (and how precisely) could you infer other things from the falsification?</span></blockquote><blockquote><span class=\"by_Xn6ACr6Cua8upALWQ\"> 2. How incoherent is it to imagine that the model is accurate but that a given variable </span><a href=\"https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">could be different</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">?</span></blockquote><blockquote><span class=\"by_Xn6ACr6Cua8upALWQ\"> 3. If you knew the model were accurate but you were to forget the value of one variable, </span><a href=\"https://www.lesswrong.com/lw/la/truly_part_of_you/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">could you rederive it</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">?</span></blockquote><p><span class=\"by_Xn6ACr6Cua8upALWQ\">An example from </span><a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Gears in Understanding</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> of a gears-level model is (surprise) a box of gears. If you can see a series of  interlocked gears, alternately turning clockwise, then counterclockwise, and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">so </span><span class=\"by_Xn6ACr6Cua8upALWQ\">on, then you're able </span><span class=\"by_qgdGA4ZEyW7zNdK84\">to </span><span class=\"by_Xn6ACr6Cua8upALWQ\">anticipate the direction of any given, even if you cannot see it.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> It </span><span class=\"by_Xn6ACr6Cua8upALWQ\">would be very difficult to imagine all of the gears turning as they are but only one of them changing direction whilst remaining interlocked. And finally, you would be able to rederive the direction of any given gear if you forgot it. </span></span></p><br><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Note that the author of </span><a href=\"https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Gears in Understanding</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/users/valentine\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Valentine</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, was careful to point out that these tests do not fully </span><em><span class=\"by_Xn6ACr6Cua8upALWQ\">define</span></em><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> the property 'gears-level', and that  \"Gears-ness is not the same as goodness\"-- there are other things that are valuable in</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> a </span><span class=\"by_Xn6ACr6Cua8upALWQ\">model,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><span class=\"by_Xn6ACr6Cua8upALWQ\">many things cannot practically be modelled in this fashion. If you intend to use the term</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> it </span><span class=\"by_Xn6ACr6Cua8upALWQ\">is highly recommended you read the post beforehand,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> as </span><span class=\"by_Xn6ACr6Cua8upALWQ\">the concept is not easily defined.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 49,
    "description": {
      "markdown": "A **gears-level** model is 'well-constrained' in the sense that there is a strong connection between each of the things you observe-- it would be hard for you to imagine that one of the variables could be different while all of the others remained the same.\n\n_Related Tags: [Anticipated Experiences](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&useTagName=true), [Double Crux](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&useTagName=true), [Empiricism](https://www.lesswrong.com/tag/empiricism?showPostCount=true&useTagName=true), [Falsifiability](https://www.lesswrong.com/tag/falsifiability?showPostCount=true&useTagName=true), [Map and Territory](https://www.lesswrong.com/tag/map-and-territory?showPostCount=true&useTagName=true)_\n\n  \n\nThe term **gears-level** was first described on LW in the post [\"Gears in Understanding\"](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding):\n\n> This property is _how deterministically interconnected the variables of the model are_. There are a few [tests](https://en.wikipedia.org/wiki/Goodhart%27s_law) I know of to see to what extent a model has this property, though I don't know if this list is exhaustive and would be a little surprised if it were:\n\n> 1\\. Does the model p[ay rent?](https://www.lesswrong.com/lw/i3/making_beliefs_pay_rent_in_anticipated_experiences/) If it does, and if it were falsified, how much (and how precisely) could you infer other things from the falsification?\n\n> 2\\. How incoherent is it to imagine that the model is accurate but that a given variable [could be different](https://www.lesswrong.com/lw/if/your_strength_as_a_rationalist/)?\n\n> 3\\. If you knew the model were accurate but you were to forget the value of one variable, [could you rederive it](https://www.lesswrong.com/lw/la/truly_part_of_you/)?\n\nAn example from [Gears in Understanding](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding) of a gears-level model is (surprise) a box of gears. If you can see a series of interlocked gears, alternately turning clockwise, then counterclockwise, and so on, then you're able to anticipate the direction of any given, even if you cannot see it. It would be very difficult to imagine all of the gears turning as they are but only one of them changing direction whilst remaining interlocked. And finally, you would be able to rederive the direction of any given gear if you forgot it.\n\n  \n\nNote that the author of [Gears in Understanding](https://www.lesswrong.com/posts/B7P97C27rvHPz3s9B/gears-in-understanding), [Valentine](https://www.lesswrong.com/users/valentine), was careful to point out that these tests do not fully _define_ the property 'gears-level', and that \"Gears-ness is not the same as goodness\"-- there are other things that are valuable in a model, and many things cannot practically be modelled in this fashion. If you intend to use the term it is highly recommended you read the post beforehand, as the concept is not easily defined."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oFpCNzqBd6tzCuxLa",
    "name": "World Modeling Techniques",
    "core": false,
    "slug": "world-modeling-techniques",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">world-modeling technique</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a general-purpose method for generating world models [LINK], or a pattern which recurs in world models, across many domains.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Examples:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Causal Models [LINK Yudkowsky's intro]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Bayesian Model Comparison [LINK]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Markovity [external LINK?]</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Causality [LINK tag]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">World Modeling</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Efficient Market Hypothesis</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": {
      "markdown": "A **world-modeling technique** is a general-purpose method for generating world models \\[LINK\\], or a pattern which recurs in world models, across many domains.\n\nExamples:\n\n*   Causal Models \\[LINK Yudkowsky's intro\\]\n*   Bayesian Model Comparison \\[LINK\\]\n*   Markovity \\[external LINK?\\]\n\nSee also:\n\n*   Causality \\[LINK tag\\]\n*   World Modeling\n*   Efficient Market Hypothesis\n*   Game Theory"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "w2CXH4hsQtihvwT4v",
    "name": "Crowdfunding",
    "core": false,
    "slug": "crowdfunding",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HqaByfeGvDLKSaK2W",
    "name": "Debate (AI safety technique)",
    "core": false,
    "slug": "debate-ai-safety-technique-1",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">Debate</span></strong><span><span class=\"by_gXeEWGjTWyqgrQTzR\"> is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the </span><span class=\"by_Sp5wM4aRAhNERd4oY\">answers.</span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnref7clr966emb9\"><sup><a href=\"#fn7clr966emb9\"><span><span class=\"by_gXeEWGjTWyqgrQTzR\">[</span><span class=\"by_Sp5wM4aRAhNERd4oY\">1]</span></span></a></sup></span><span><span class=\"by_Sp5wM4aRAhNERd4oY\">&nbsp;</span><span class=\"by_gXeEWGjTWyqgrQTzR\">The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue).&nbsp;</span></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefvrcbanw2zz\"><sup><a href=\"#fnvrcbanw2zz\"><span class=\"by_Sp5wM4aRAhNERd4oY\">[2]</span></a></sup></span><span class=\"footnote-reference\" role=\"doc-noteref\" id=\"fnrefnwfhnzy6a3e\"><sup><a href=\"#fnnwfhnzy6a3e\"><span class=\"by_Sp5wM4aRAhNERd4oY\">[3]</span></a></sup></span></p><ol class=\"footnotes\" role=\"doc-endnotes\"><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fn7clr966emb9\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnref7clr966emb9\"><span class=\"by_Sp5wM4aRAhNERd4oY\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span class=\"by_Sp5wM4aRAhNERd4oY\">https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1</span></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnvrcbanw2zz\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefvrcbanw2zz\"><span class=\"by_Sp5wM4aRAhNERd4oY\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span class=\"by_Sp5wM4aRAhNERd4oY\">https://ought.org/mission</span></p></div></li><li class=\"footnote-item\" role=\"doc-endnote\" id=\"fnnwfhnzy6a3e\"><span class=\"footnote-back-link\"><sup><strong><a href=\"#fnrefnwfhnzy6a3e\"><span class=\"by_Sp5wM4aRAhNERd4oY\">^</span></a></strong></sup></span><div class=\"footnote-content\"><p><span class=\"by_Sp5wM4aRAhNERd4oY\">https://openai.com/blog/debate/</span></p></div></li></ol>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 42,
    "description": {
      "markdown": "**Debate** is a proposed technique for allowing human evaluators to get correct and helpful answers from experts, even if the evaluator is not themselves an expert or able to fully verify the answers.^[\\[1\\]](#fn7clr966emb9)^ The technique was suggested as part of an approach to build advanced AI systems that are aligned with human values, and to safely apply machine learning techniques to problems that have high stakes, but are not well-defined (such as advancing science or increase a company's revenue). ^[\\[2\\]](#fnvrcbanw2zz)^^[\\[3\\]](#fnnwfhnzy6a3e)^\n\n1.  ^**[^](#fnref7clr966emb9)**^\n    \n    https://www.lesswrong.com/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1\n    \n2.  ^**[^](#fnrefvrcbanw2zz)**^\n    \n    https://ought.org/mission\n    \n3.  ^**[^](#fnrefnwfhnzy6a3e)**^\n    \n    https://openai.com/blog/debate/"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Rz5jb3cYHTSRmqNnN",
    "name": "Existential Risk",
    "core": false,
    "slug": "existential-risk",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">An </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">existential risk</span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">(or</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span></span><strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\">x-</span><span class=\"by_qgdGA4ZEyW7zNdK84\">risk</span></span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">) is a risk </span><span class=\"by_2aoRX3ookcCozcb3m\">that poses astronomically</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> large negative</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> consequences </span><span class=\"by_2aoRX3ookcCozcb3m\">for humanity,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> such as</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><span class=\"by_2aoRX3ookcCozcb3m\">human extinction or permanent global totalitarianism.</span></span></p><p><a href=\"https://lessestwrong.com/tag/nick-bostrom\"><span class=\"by_2aoRX3ookcCozcb3m\">Nick Bostrom</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> introduced </span><span class=\"by_EQNTWXLKMeWMp2FQS\">the </span><span class=\"by_2aoRX3ookcCozcb3m\">term \"existential risk\" in</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> his </span><span class=\"by_2aoRX3ookcCozcb3m\">2002</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> paper </span><span class=\"by_2aoRX3ookcCozcb3m\">\"</span></span><a href=\"https://www.nickbostrom.com/existential/risks.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\"</span><a href=\"https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn1\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></sup></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> In the paper,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Bostrom defined an existential risk as:</span></span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.</span></p></blockquote><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><span class=\"by_2aoRX3ookcCozcb3m\">Oxford </span></span><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute-fhi\"><span><span class=\"by_2aoRX3ookcCozcb3m\">Future</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of </span><span class=\"by_2aoRX3ookcCozcb3m\">Humanity Institute</span></span></a><span class=\"by_2aoRX3ookcCozcb3m\"> (FHI) was founded by Bostrom in 2005 in part to study existential risks. Other institutions with a generalist focus on existential risk include the </span><a href=\"https://www.cser.ac.uk/\"><span class=\"by_2aoRX3ookcCozcb3m\">Centre for the Study of Existential Risk</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">FHI's </span><a href=\"https://www.existential-risk.org/faq.html\"><span class=\"by_2aoRX3ookcCozcb3m\">existential-risk.org FAQ</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> notes regarding the definition of \"existential risk\":</span></p><blockquote><p><span><span class=\"by_2aoRX3ookcCozcb3m\">An</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> existential risk is one </span><span class=\"by_2aoRX3ookcCozcb3m\">that threatens the entire future </span><span class=\"by_qgdGA4ZEyW7zNdK84\">of </span><span class=\"by_2aoRX3ookcCozcb3m\">humanity. [...]</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">“Humanity”, in this context, does not mean “the biological species </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Homo sapiens</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\">”. If we humans were to evolve into another species, or merge or replace ourselves with intelligent machines, this would not necessarily mean that an existential catastrophe had occurred — although it might if </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the </span><span class=\"by_2aoRX3ookcCozcb3m\">quality of</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> life </span><span class=\"by_2aoRX3ookcCozcb3m\">enjoyed by those new</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> life </span><span class=\"by_2aoRX3ookcCozcb3m\">forms turns out to be far inferior to that enjoyed by humans.</span></span></p></blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Classification_of_Existential_Risks\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Classification of Existential Risks</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom</span><a href=\"https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn2\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> proposes a series of classifications for existential risks:</span></p><ul><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Bangs</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Earthly intelligent life is extinguished relatively suddenly by any cause; the prototypical end of humanity. Examples of bangs include deliberate or accidental misuse of nanotechnology, nuclear holocaust, </span><a href=\"https://lessestwrong.com/tag/simulation-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">the end of our simulation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, or an </span><a href=\"https://wiki.lesswrong.com/wiki/unfriendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">unfriendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Crunches</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> - The potential humanity had to enhance itself indefinitely is forever eliminated, although humanity continues. Possible crunches include an exhaustion of resources, social or governmental pressure ending technological development, and even future technological development proving an unsurpassable challenge before the creation of a </span><a href=\"https://lessestwrong.com/tag/superintelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">superintelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Shrieks</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Humanity enhances itself, but explores only a narrow portion of its desirable possibilities. As the </span><a href=\"https://lessestwrong.com/tag/complexity-of-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">criteria for desirability haven't been defined yet</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, this category is mainly undefined. However, a flawed </span><a href=\"https://wiki.lesswrong.com/wiki/friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> incorrectly interpreting our values, a superhuman </span><a href=\"https://wiki.lesswrong.com/wiki/WBE\"><span class=\"by_qgdGA4ZEyW7zNdK84\">upload</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> deciding its own values and imposing them on the rest of humanity, and an intolerant government outlawing social progress would certainly qualify.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Whimpers</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Though humanity is enduring, only a fraction of our potential is ever achieved. Spread across the galaxy and expanding at near light-speed, we might find ourselves doomed by ours or another being's catastrophic physics experimentation, destroying reality at light-speed. A prolonged galactic war leading to our extinction or severe limitation would also be a whimper. More darkly, humanity might develop until its </span><a href=\"https://lessestwrong.com/tag/complexity-of-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">values</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> were disjoint with ours today, making their civilization worthless by present values.</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The total negative results of an existential risk could amount to the total of potential future lives not being realized. A rough and conservative calculation</span><a href=\"https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn3\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> gives us a total of 10^54 potential future humans lives – smarter, happier and kinder then we are. Hence, almost no other task would amount to so much positive impact than existential risk reduction.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Existential risks also present an unique challenge because of their irreversible nature. We will never, by definition, experience and survive an extinction risk</span><a href=\"https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn4\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">4</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and so cannot learn from our mistakes. They are subject to strong </span><a href=\"https://lessestwrong.com/tag/observation-selection-effect\"><span class=\"by_qgdGA4ZEyW7zNdK84\">observational selection effects</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn5\"><sup><span class=\"by_qgdGA4ZEyW7zNdK84\">5</span></sup></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. One cannot estimate their future probability based on the past, because </span><a href=\"https://lessestwrong.com/tag/bayesian-probability\"><span class=\"by_qgdGA4ZEyW7zNdK84\">bayesianly</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> speaking, the conditional probability of a past existential catastrophe given our present existence is always 0, no matter how high the probability of an existential risk really is. Instead, indirect estimates have to be used, such as possible existential catastrophes happening elsewhere. A high extinction risk probability could be functioning as a </span><a href=\"https://lessestwrong.com/tag/great-filter\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Great Filter</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and explain why there is no evidence of spacial colonization.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Another related idea is that of a </span><a href=\"https://lessestwrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_qgdGA4ZEyW7zNdK84\">suffering risk</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> (or s-risk)</span><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"History\"><span class=\"by_EQNTWXLKMeWMp2FQS\">History</span></h2><p><span><span class=\"by_EQNTWXLKMeWMp2FQS\">The focus on </span><span class=\"by_mPipmBTniuABY5PQy\">existential</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_EQNTWXLKMeWMp2FQS\">risks on LessWrong dates back</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> to </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">Bostrom'</span><span class=\"by_2YpRin5m5vBJu8Tg9\">s</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> 2002</span><span class=\"by_woC2b5rav5sGrAo3E\"> paper </span></span><a href=\"https://www.nickbostrom.com/astronomical/waste.html\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Astronomical Waste: The Opportunity Cost of Delayed Technological Development</span></a><span><span class=\"by_EQNTWXLKMeWMp2FQS\">. It argues that </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"the</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> chief goal for utilitarians should be to reduce existential </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">risk\"</span><span class=\"by_EQNTWXLKMeWMp2FQS\">. Bostrom writes:</span></span></p><blockquote><p><span><span class=\"by_EQNTWXLKMeWMp2FQS\">If what we are concerned with is (something like) maximizing</span><span class=\"by_woC2b5rav5sGrAo3E\"> the </span><span class=\"by_EQNTWXLKMeWMp2FQS\">expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. We might fall victim to</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> an </span></span><i><span class=\"by_2YpRin5m5vBJu8Tg9\">existential risk</span></i><span><span class=\"by_EQNTWXLKMeWMp2FQS\">, one</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> where an adverse outcome would either annihilate Earth-originating intelligent life</span><span class=\"by_mPipmBTniuABY5PQy\"> or </span><span class=\"by_2YpRin5m5vBJu8Tg9\">permanently and drastically curtail its potential.</span><span class=\"by_EQNTWXLKMeWMp2FQS\">[8] Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.</span></span><br><span class=\"by_EQNTWXLKMeWMp2FQS\">Therefore, if our actions have even the slightest effect on </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">the probability</span></i><span class=\"by_EQNTWXLKMeWMp2FQS\"> of eventual colonization, this will outweigh their effect on </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">when</span></i><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> colonization takes place. For standard utilitarians, priority number one, two, three and four should consequently be to reduce existential risk. The utilitarian imperative </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">“Maximize</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> expected aggregate utility!</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">”</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> can be simplified to the maxim </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">“Minimize</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> existential risk!</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">”</span><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></span></p></blockquote><p><span><span class=\"by_woC2b5rav5sGrAo3E\">The </span><span class=\"by_EQNTWXLKMeWMp2FQS\">concept</span><span class=\"by_woC2b5rav5sGrAo3E\"> is </span><span class=\"by_EQNTWXLKMeWMp2FQS\">expanded upon in his 2012 paper </span></span><a href=\"https://www.existential-risk.org/concept.html\"><span><span class=\"by_SdZmP36R37riQrHAw\">Existential Risk</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> Prevention as </span><span class=\"by_DH3Hiv6kJp93dDF4J\">Global </span><span class=\"by_EQNTWXLKMeWMp2FQS\">Priority</span></span></a></p><h2 id=\"Organizations\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Organizations</span></h2><ul><li><a href=\"http://intelligence.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Machine Intelligence Research Institute</span></u></a></li><li><a href=\"http://www.fhi.ox.ac.uk/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Future of Humanity Institute</span></u></a></li><li><a href=\"http://www.futuretech.ox.ac.uk/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Oxford Martin Programme on the Impacts of Future Technology</span></u></a></li><li><a href=\"http://www.gcrinstitute.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Global Catastrophic Risk Institute</span></u></a></li><li><a href=\"http://shfhs.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Saving Humanity from Homo Sapiens</span></u></a></li><li><a href=\"http://www.skollglobalthreats.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Skoll Global Threats Fund (To Safeguard Humanity from Global Threats)</span></u></a></li><li><a href=\"http://www.foresight.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Foresight Institute</span></u></a></li><li><a href=\"http://nuclearrisk.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Defusing the Nuclear Threat</span></u></a></li><li><a href=\"http://www.leverageresearch.org/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Leverage Research</span></u></a></li><li><a href=\"http://lifeboat.com/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Lifeboat Foundation</span></u></a><br><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span></li></ul><h2 id=\"References\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">References</span></h2><ol><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">BOSTROM, Nick. (2002) \"</span><a href=\"http://www.nickbostrom.com/existential/risks.pdf\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards</span></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">\". Journal of Evolution and Technology, Vol. 9, March 2002.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">BOSTROM, Nick. (2012) \"</span><a href=\"http://www.existential-risk.org/concept.pdf\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Existential Risk Reduction as the Most Important Task for Humanity</span></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">\". Global Policy, forthcoming, 2012.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">BOSTROM, Nick &amp; SANDBERG, Anders &amp; CIRKOVIC, Milan. (2010) \"Anthropic Shadow: Observation Selection Effects and Human Extinction Risks\" Risk Analysis, Vol. 30, No. 10 (2010): 1495-1506.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Nick Bostrom, Milan M. Ćirković, ed (2008). </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Global Catastrophic Risks</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Oxford University Press.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Milan M. Ćirković (2008). </span><a href=\"http://books.google.com/books?id=-Jxc88RuJhgC&amp;lpg=PP1&amp;pg=PA120#v=onepage&amp;q=&amp;f=false\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"Observation Selection Effects and global catastrophic risks\"</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Global Catastrophic Risks</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Oxford University Press.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Eliezer S. Yudkowsky (2008). </span><a href=\"http://yudkowsky.net/rational/cognitive-biases\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"Cognitive Biases Potentially Affecting Judgment of Global Risks\"</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Global Catastrophic Risks</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Oxford University Press. (</span><a href=\"http://intelligence.org/files/CognitiveBiases.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">PDF</span></u></a><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">)</span></u></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Richard A. Posner (2004). </span><a href=\"http://books.google.ca/books?id=SDe59lXSrY8C\"><i><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Catastrophe Risk and Response</span></u></i></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Oxford University Press. (</span><a href=\"http://www.avturchin.narod.ru/posner.doc\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">DOC</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">)</span></li></ol>",
      "sections": [
        {
          "title": "Classification of Existential Risks",
          "anchor": "Classification_of_Existential_Risks",
          "level": 1
        },
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "Organizations",
          "anchor": "Organizations",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 164,
    "description": {
      "markdown": "An **existential risk** (or **x-risk**) is a risk that poses astronomically large negative consequences for humanity, such as human extinction or permanent global totalitarianism.\n\n[Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) introduced the term \"existential risk\" in his 2002 paper \"[Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards](https://www.nickbostrom.com/existential/risks.pdf).\"[^1^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn1) In the paper, Bostrom defined an existential risk as:\n\n> One where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\n\nThe Oxford [Future of Humanity Institute](https://www.lesswrong.com/tag/future-of-humanity-institute-fhi) (FHI) was founded by Bostrom in 2005 in part to study existential risks. Other institutions with a generalist focus on existential risk include the [Centre for the Study of Existential Risk](https://www.cser.ac.uk/).\n\nFHI's [existential-risk.org FAQ](https://www.existential-risk.org/faq.html) notes regarding the definition of \"existential risk\":\n\n> An existential risk is one that threatens the entire future of humanity. \\[...\\]\n> \n> “Humanity”, in this context, does not mean “the biological species *Homo sapiens*”. If we humans were to evolve into another species, or merge or replace ourselves with intelligent machines, this would not necessarily mean that an existential catastrophe had occurred — although it might if the quality of life enjoyed by those new life forms turns out to be far inferior to that enjoyed by humans.\n\nClassification of Existential Risks\n-----------------------------------\n\nBostrom[^2^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn2) proposes a series of classifications for existential risks:\n\n*   **Bangs** \\- Earthly intelligent life is extinguished relatively suddenly by any cause; the prototypical end of humanity. Examples of bangs include deliberate or accidental misuse of nanotechnology, nuclear holocaust, [the end of our simulation](https://lessestwrong.com/tag/simulation-argument), or an [unfriendly AI](https://wiki.lesswrong.com/wiki/unfriendly_AI).\n*   **Crunches** \\- The potential humanity had to enhance itself indefinitely is forever eliminated, although humanity continues. Possible crunches include an exhaustion of resources, social or governmental pressure ending technological development, and even future technological development proving an unsurpassable challenge before the creation of a [superintelligence](https://lessestwrong.com/tag/superintelligence).\n*   **Shrieks** \\- Humanity enhances itself, but explores only a narrow portion of its desirable possibilities. As the [criteria for desirability haven't been defined yet](https://lessestwrong.com/tag/complexity-of-value), this category is mainly undefined. However, a flawed [friendly AI](https://wiki.lesswrong.com/wiki/friendly_AI) incorrectly interpreting our values, a superhuman [upload](https://wiki.lesswrong.com/wiki/WBE) deciding its own values and imposing them on the rest of humanity, and an intolerant government outlawing social progress would certainly qualify.\n*   **Whimpers** \\- Though humanity is enduring, only a fraction of our potential is ever achieved. Spread across the galaxy and expanding at near light-speed, we might find ourselves doomed by ours or another being's catastrophic physics experimentation, destroying reality at light-speed. A prolonged galactic war leading to our extinction or severe limitation would also be a whimper. More darkly, humanity might develop until its [values](https://lessestwrong.com/tag/complexity-of-value) were disjoint with ours today, making their civilization worthless by present values.\n\nThe total negative results of an existential risk could amount to the total of potential future lives not being realized. A rough and conservative calculation[^3^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn3) gives us a total of 10^54 potential future humans lives – smarter, happier and kinder then we are. Hence, almost no other task would amount to so much positive impact than existential risk reduction.\n\nExistential risks also present an unique challenge because of their irreversible nature. We will never, by definition, experience and survive an extinction risk[^4^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn4) and so cannot learn from our mistakes. They are subject to strong [observational selection effects](https://lessestwrong.com/tag/observation-selection-effect) [^5^](https://lessestwrong.com/tag/existential-risk?revision=0.0.39#fn5). One cannot estimate their future probability based on the past, because [bayesianly](https://lessestwrong.com/tag/bayesian-probability) speaking, the conditional probability of a past existential catastrophe given our present existence is always 0, no matter how high the probability of an existential risk really is. Instead, indirect estimates have to be used, such as possible existential catastrophes happening elsewhere. A high extinction risk probability could be functioning as a [Great Filter](https://lessestwrong.com/tag/great-filter) and explain why there is no evidence of spacial colonization.\n\nAnother related idea is that of a [suffering risk](https://lessestwrong.com/tag/risks-of-astronomical-suffering-s-risks) (or s-risk).\n\nHistory\n-------\n\nThe focus on existential risks on LessWrong dates back to Bostrom's 2002 paper [Astronomical Waste: The Opportunity Cost of Delayed Technological Development](https://www.nickbostrom.com/astronomical/waste.html). It argues that \"the chief goal for utilitarians should be to reduce existential risk\". Bostrom writes:\n\n> If what we are concerned with is (something like) maximizing the expected number of worthwhile lives that we will create, then in addition to the opportunity cost of delayed colonization, we have to take into account the risk of failure to colonize at all. We might fall victim to an *existential risk*, one where an adverse outcome would either annihilate Earth-originating intelligent life or permanently and drastically curtail its potential.\\[8\\] Because the lifespan of galaxies is measured in billions of years, whereas the time-scale of any delays that we could realistically affect would rather be measured in years or decades, the consideration of risk trumps the consideration of opportunity cost. For example, a single percentage point of reduction of existential risks would be worth (from a utilitarian expected utility point-of-view) a delay of over 10 million years.  \n> Therefore, if our actions have even the slightest effect on *the probability* of eventual colonization, this will outweigh their effect on *when* colonization takes place. For standard utilitarians, priority number one, two, three and four should consequently be to reduce existential risk. The utilitarian imperative “Maximize expected aggregate utility!” can be simplified to the maxim “Minimize existential risk!”.\n\nThe concept is expanded upon in his 2012 paper [Existential Risk Prevention as Global Priority](https://www.existential-risk.org/concept.html)\n\nOrganizations\n-------------\n\n*   [Machine Intelligence Research Institute](http://intelligence.org/)\n*   [The Future of Humanity Institute](http://www.fhi.ox.ac.uk/)\n*   [The Oxford Martin Programme on the Impacts of Future Technology](http://www.futuretech.ox.ac.uk/)\n*   [Global Catastrophic Risk Institute](http://www.gcrinstitute.org/)\n*   [Saving Humanity from Homo Sapiens](http://shfhs.org/)\n*   [Skoll Global Threats Fund (To Safeguard Humanity from Global Threats)](http://www.skollglobalthreats.org/)\n*   [Foresight Institute](http://www.foresight.org/)\n*   [Defusing the Nuclear Threat](http://nuclearrisk.org/)\n*   [Leverage Research](http://www.leverageresearch.org/)\n*   [The Lifeboat Foundation](http://lifeboat.com/)  \n     \n\nReferences\n----------\n\n1.  BOSTROM, Nick. (2002) \"[Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards](http://www.nickbostrom.com/existential/risks.pdf)\". Journal of Evolution and Technology, Vol. 9, March 2002.\n2.  BOSTROM, Nick. (2012) \"[Existential Risk Reduction as the Most Important Task for Humanity](http://www.existential-risk.org/concept.pdf)\". Global Policy, forthcoming, 2012.\n3.  BOSTROM, Nick & SANDBERG, Anders & CIRKOVIC, Milan. (2010) \"Anthropic Shadow: Observation Selection Effects and Human Extinction Risks\" Risk Analysis, Vol. 30, No. 10 (2010): 1495-1506.\n4.  Nick Bostrom, Milan M. Ćirković, ed (2008). *Global Catastrophic Risks*. Oxford University Press.\n5.  Milan M. Ćirković (2008). [\"Observation Selection Effects and global catastrophic risks\"](http://books.google.com/books?id=-Jxc88RuJhgC&lpg=PP1&pg=PA120#v=onepage&q=&f=false). *Global Catastrophic Risks*. Oxford University Press.\n6.  Eliezer S. Yudkowsky (2008). [\"Cognitive Biases Potentially Affecting Judgment of Global Risks\"](http://yudkowsky.net/rational/cognitive-biases). *Global Catastrophic Risks*. Oxford University Press. ([PDF](http://intelligence.org/files/CognitiveBiases.pdf))\n7.  Richard A. Posner (2004). [*Catastrophe Risk and Response*](http://books.google.ca/books?id=SDe59lXSrY8C). Oxford University Press. ([DOC](http://www.avturchin.narod.ru/posner.doc))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8byoqYZfdwHffYLZ6",
    "name": "Newsletters",
    "core": false,
    "slug": "newsletters",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Newsletters</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are collected summaries of recent events, posts, and academic papers.</span></p><p><span><span class=\"by_du6SPHKnnPrPmxWNT\">The</span><span class=\"by_HoGziwmhpMGqGeWZy\"> most prolific newsletter</span><span class=\"by_du6SPHKnnPrPmxWNT\"> on Less Wrong</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is Rohin Shah's weekly Alignment Newsletter.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 179,
    "description": {
      "markdown": "**Newsletters** are collected summaries of recent events, posts, and academic papers.\n\nThe most prolific newsletter on Less Wrong is Rohin Shah's weekly Alignment Newsletter."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "oiRp4T6u5poc8r9Tj",
    "name": "AI Takeoff",
    "core": false,
    "slug": "ai-takeoff",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">AI</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_HoGziwmhpMGqGeWZy\">Takeoff</span></span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> refers to </span><span class=\"by_HoGziwmhpMGqGeWZy\">the process of an </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Artificial General Intelligence</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\"> going from </span><span class=\"by_NRg5Bw8H2DCYTpmHE\">a </span><span class=\"by_HoGziwmhpMGqGeWZy\">certain threshold of capability (often discussed as \"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">human-</span><span class=\"by_HoGziwmhpMGqGeWZy\">level\") to being super-intelligent and capable enough to control</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> the </span><span class=\"by_HoGziwmhpMGqGeWZy\">fate of civilization. There has been much debate about whether AI takeoff is more</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> likely</span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> to </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">be </span><span class=\"by_qgdGA4ZEyW7zNdK84\">slow vs fast, i.e., </span><span class=\"by_HoGziwmhpMGqGeWZy\">\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">soft</span><span class=\"by_HoGziwmhpMGqGeWZy\">\" </span><span class=\"by_qgdGA4ZEyW7zNdK84\">vs</span><span class=\"by_HoGziwmhpMGqGeWZy\"> \"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">hard</span><span class=\"by_HoGziwmhpMGqGeWZy\">\".</span></span></p><p><em><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/ai-timelines\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Timelines</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/seed-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Seed AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/singularity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Singularity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intelligence explosion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Recursive self-improvement</span></a></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">AI</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> takeoff is </span><span class=\"by_HoGziwmhpMGqGeWZy\">sometimes casually referred</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to as </span></span><strong><span><span class=\"by_RNTWCoLXwK5DKhFDA\">AI </span><span class=\"by_HoGziwmhpMGqGeWZy\">FOOM.</span></span></strong></p><h1 id=\"Soft_takeoff\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Soft takeoff</span></h1><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">soft takeoff</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Whole brain emulation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Biological Cognitive Enhancement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, and software-based strong AGI [</span><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fn1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]. By maintaining control of the AGI's ascent it should be easier for a </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> to emerge.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Vernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.</span></p><h1 id=\"Hard_takeoff\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hard takeoff</span></h1><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">hard takeoff</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> (or an AI going \"</span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">FOOM</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">\" [</span><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fn2\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. </span><a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Unfriendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">). It is one of the main ideas supporting the </span><a href=\"https://www.lesswrong.com/tag/intelligence-explosion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intelligence explosion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> hypothesis.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The feasibility of hard takeoff has been addressed by Hugo de Garis, </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/ben-goertzel\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ben Goertzel</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Nick Bostrom</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large </span><a href=\"https://www.lesswrong.com/tag/computing-overhang\"><span class=\"by_qgdGA4ZEyW7zNdK84\">resources overhangs</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> or the fact that small improvements seem to have a large impact in a mind's general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) [</span><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fn3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">].</span></p><h1 id=\"Notable_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable posts</span></h1><ul><li><a href=\"https://www.lesswrong.com/lw/wf/hard_takeoff/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hard Takeoff</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Eliezer Yudkowsky</span></li></ul><h1 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h1><ul><li><a href=\"http://www.kurzweilai.net/the-age-of-virtuous-machines\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Age of Virtuous Machines</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by J. Storrs Hall President of The Foresight Institute</span></li><li><a href=\"http://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hard take off Hypothesis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Ben Goertzel.</span></li><li><a href=\"http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Extensive archive of Hard takeoff Essays</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> from Accelerating Future</span></li><li><a href=\"http://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Can we avoid a hard take off?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Vernor Vinge</span></li><li><a href=\"http://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robot: Mere Machine to Transcendent Mind</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Hans Moravec</span></li><li><a href=\"http://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1339495098&amp;sr=1-1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Singularity is Near</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Ray Kurzweil</span></li></ul><p><strong id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></strong></p><ol><li><a href=\"http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html↩\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html</span></a><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fnref1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><a href=\"http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/↩\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/</span></a><a href=\"https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&amp;lw_source=import_sheet#fnref2\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li><li><a href=\"https://www.lesswrong.com/lw/wf/hard_takeoff/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">http://lesswrong.com/lw/wf/hard_takeoff/</span></a><a href=\"http://lesswrong.com/lw/wf/hard_takeoff/↩\"><span class=\"by_qgdGA4ZEyW7zNdK84\">↩</span></a></li></ol>",
      "sections": [
        {
          "title": "Soft takeoff",
          "anchor": "Soft_takeoff",
          "level": 1
        },
        {
          "title": "Hard takeoff",
          "anchor": "Hard_takeoff",
          "level": 1
        },
        {
          "title": "Notable posts",
          "anchor": "Notable_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 128,
    "description": {
      "markdown": "**AI Takeoff** refers to the process of an [Artificial General Intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence) going from a certain threshold of capability (often discussed as \"human-level\") to being super-intelligent and capable enough to control the fate of civilization. There has been much debate about whether AI takeoff is more likely to be slow vs fast, i.e., \"soft\" vs \"hard\".\n\n_See also_: [AI Timelines](https://www.lesswrong.com/tag/ai-timelines), [Seed AI](https://www.lesswrong.com/tag/seed-ai), [Singularity](https://www.lesswrong.com/tag/singularity), [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion), [Recursive self-improvement](https://www.lesswrong.com/tag/recursive-self-improvement)\n\nAI takeoff is sometimes casually referred to as **AI FOOM.**\n\nSoft takeoff\n============\n\nA **soft takeoff** refers to an AGI that would self-improve over a period of years or decades. This could be due to either the learning algorithm being too demanding for the hardware or because the AI relies on experiencing feedback from the real-world that would have to be played out in real-time. Possible methods that could deliver a soft takeoff, by slowly building on human-level intelligence, are [Whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), [Biological Cognitive Enhancement](https://www.lesswrong.com/tag/nootropics-and-other-cognitive-enhancement), and software-based strong AGI \\[[1](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn1)\\]. By maintaining control of the AGI's ascent it should be easier for a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) to emerge.\n\nVernor Vinge, Hans Moravec and have all expressed the view that soft takeoff is preferable to a hard takeoff as it would be both safer and easier to engineer.\n\nHard takeoff\n============\n\nA **hard takeoff** (or an AI going \"**FOOM**\" \\[[2](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn2)\\]) refers to AGI expansion in a matter of minutes, days, or months. It is a fast, abruptly, local increase in capability. This scenario is widely considered much more precarious, as this involves an AGI rapidly ascending in power without human control. This may result in unexpected or undesired behavior (i.e. [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)). It is one of the main ideas supporting the [Intelligence explosion](https://www.lesswrong.com/tag/intelligence-explosion) hypothesis.\n\nThe feasibility of hard takeoff has been addressed by Hugo de Garis, [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky), [Ben Goertzel](https://www.lesswrong.com/tag/ben-goertzel), [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom), and Michael Anissimov. It is widely agreed that a hard takeoff is something to be avoided due to the risks. Yudkowsky points out several possibilities that would make a hard takeoff more likely than a soft takeoff such as the existence of large [resources overhangs](https://www.lesswrong.com/tag/computing-overhang) or the fact that small improvements seem to have a large impact in a mind's general intelligence (i.e.: the small genetic difference between humans and chimps lead to huge increases in capability) \\[[3](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fn3)\\].\n\nNotable posts\n=============\n\n*   [Hard Takeoff](https://www.lesswrong.com/lw/wf/hard_takeoff/) by Eliezer Yudkowsky\n\nExternal links\n==============\n\n*   [The Age of Virtuous Machines](http://www.kurzweilai.net/the-age-of-virtuous-machines) by J. Storrs Hall President of The Foresight Institute\n*   [Hard take off Hypothesis](http://multiverseaccordingtoben.blogspot.co.uk/2011/01/hard-takeoff-hypothesis.html) by Ben Goertzel.\n*   [Extensive archive of Hard takeoff Essays](http://www.acceleratingfuture.com/michael/blog/2011/05/hard-takeoff-sources/) from Accelerating Future\n*   [Can we avoid a hard take off?](http://www-rohan.sdsu.edu/faculty/vinge/misc/ac2005/) by Vernor Vinge\n*   [Robot: Mere Machine to Transcendent Mind](http://www.amazon.co.uk/Robot-Mere-Machine-Transcendent-Mind/dp/0195136306) by Hans Moravec\n*   [The Singularity is Near](http://www.amazon.co.uk/The-Singularity-Near-Raymond-Kurzweil/dp/0715635611/ref=sr_1_1?s=books&ie=UTF8&qid=1339495098&sr=1-1) by Ray Kurzweil\n\n**References**\n\n1.  [http://www.aleph.se/andart/archives/2010/10/why\\_early\\_singularities\\_are\\_softer.html](http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html↩)[↩](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref1)\n2.  [http://lesswrong.com/lw/63t/requirements\\_for\\_ai\\_to\\_go_foom/](http://lesswrong.com/lw/63t/requirements_for_ai_to_go_foom/↩)[↩](https://www.lesswrong.com/tag/ai-takeoff?revision=0.0.24&lw_source=import_sheet#fnref2)\n3.  [http://lesswrong.com/lw/wf/hard_takeoff/](https://www.lesswrong.com/lw/wf/hard_takeoff/)[↩](http://lesswrong.com/lw/wf/hard_takeoff/↩)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zwv9eHi7KGg5KA9oM",
    "name": "Introspection",
    "core": false,
    "slug": "introspection",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 52,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ipJwbLxhR83ZksN6Z",
    "name": "Mechanism Design",
    "core": false,
    "slug": "mechanism-design",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_gXeEWGjTWyqgrQTzR\">Mechanism</span><span class=\"by_r38pkCm7wF4M44MDQ\"> Design</span></span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is the theory of how to design </span><a href=\"https://www.lesswrong.com/tag/incentives\"><span class=\"by_sKAL2jzfkYkDbQmx9\">incentives</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> for strategic agents, such that the agents acting according to their selfish interests will result in a desired outcome. It can be applied to things like institution design, </span><a href=\"https://www.lesswrong.com/tag/voting-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">voting systems</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, school admissions, regulation of monopolists, market design, and auction design. Think of it as the engineering side of game theory, thinking backward from a desired goal, and designing structures that lead strategic agents to behave in a way that achieves that goal.</span></p><h3 id=\"Important_Concepts\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Important Concepts</span></h3><ul><li><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Incentive Compatibility:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://en.wikipedia.org/wiki/Incentive_compatibility\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf/p/N4gDA5HPpGC4mbTEZ\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LessWrong</span></a></li><li><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Revelation Principle:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://en.wikipedia.org/wiki/Revelation_principle\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf/p/N4gDA5HPpGC4mbTEZ\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LessWrong</span></a></li></ul><p><strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Related </span><span class=\"by_qf77EiaoMw7tH3GSr\">Pages:</span></span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Game Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/incentives\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Incentives</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/principal-agent-problems\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Principal-Agent Problems</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/cryptocurrency-and-blockchain\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Cryptocurrencies and blockchain</span></a><span class=\"by_gXeEWGjTWyqgrQTzR\">, </span><a href=\"https://www.lesswrong.com/tag/public-discourse\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Public discourse</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences: </span></strong><a href=\"https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Mechanism Design</span></a></p>",
      "sections": [
        {
          "title": "Important Concepts",
          "anchor": "Important_Concepts",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 99,
    "description": {
      "markdown": "**Mechanism Design** is the theory of how to design [incentives](https://www.lesswrong.com/tag/incentives) for strategic agents, such that the agents acting according to their selfish interests will result in a desired outcome. It can be applied to things like institution design, [voting systems](https://www.lesswrong.com/tag/voting-theory), school admissions, regulation of monopolists, market design, and auction design. Think of it as the engineering side of game theory, thinking backward from a desired goal, and designing structures that lead strategic agents to behave in a way that achieves that goal.\n\n### Important Concepts\n\n*   **Incentive Compatibility:** [Wikipedia](https://en.wikipedia.org/wiki/Incentive_compatibility), [LessWrong](https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf/p/N4gDA5HPpGC4mbTEZ)\n*   **Revelation Principle:** [Wikipedia](https://en.wikipedia.org/wiki/Revelation_principle), [LessWrong](https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf/p/N4gDA5HPpGC4mbTEZ)\n\n**Related Pages:** [Game Theory](https://www.lesswrong.com/tag/game-theory), [Incentives](https://www.lesswrong.com/tag/incentives), [Principal-Agent Problems](https://www.lesswrong.com/tag/principal-agent-problems), [Cryptocurrencies and blockchain](https://www.lesswrong.com/tag/cryptocurrency-and-blockchain), [Public discourse](https://www.lesswrong.com/tag/public-discourse)\n\n**Related Sequences:** [Mechanism Design](https://www.lesswrong.com/s/Yh4YsGDD9WYiZqRnf)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZsWDPoXcchbGneaMX",
    "name": "UI Design",
    "core": false,
    "slug": "ui-design",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "feSdiScf9o6zgrwgG",
    "name": "Memory and Mnemonics",
    "core": false,
    "slug": "memory-and-mnemonics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 11,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wzgcQCrwKfETcBpR9",
    "name": "Disagreement",
    "core": false,
    "slug": "disagreement",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Disagreement</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is when</span><span class=\"by_qf77EiaoMw7tH3GSr\"> two people </span><span class=\"by_HoGziwmhpMGqGeWZy\">have different beliefs.</span></span></p><h2 id=\"Aumann_s_Agreement_Theorem\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">Aumann'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s Agreement Theorem</span></span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Considered of particular relevance to disagreement between people trying to be rational, </span><a href=\"https://lessestwrong.com/tag/aumann-s-agreement-theorem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aumann's agreement theorem</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> can be </span><a href=\"https://lessestwrong.com/tag/aumann-agreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">informally interpreted</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> as suggesting that if two people are honest seekers of truth, and both </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">believe</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> each other to be honest, then they should update on each other's opinions and quickly reach agreement. The very fact that a person believes something is </span><a href=\"https://lessestwrong.com/tag/rational-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rational evidence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> that that something is true, and so this fact </span><a href=\"http://www.overcomingbias.com/2007/01/extraordinary_c.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">should be taken into account</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> when forming your belief.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Outside of well-functioning </span><a href=\"https://lessestwrong.com/tag/prediction-markets\"><span class=\"by_qgdGA4ZEyW7zNdK84\">prediction markets</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Aumann agreement can probably only be approximated by careful deliberative discourse. Interest in Aumann agreement has waned in recent years within the Rationalist community, perhaps out of a sense Aumann agreement cannot be practically achieved by humans – there is too much background information to be exchanged. Instead, people now focus on things more like Double-Crux</span></p><h2 id=\"External_Posts\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">External</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Posts</span></span></h2><ul><li><a href=\"http://www.overcomingbias.com/2006/12/reasonable_disa.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Reasonable Disagreement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Nicholas Shackel</span></li><li><a href=\"http://www.overcomingbias.com/2006/12/agreeing_to_agr.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Agreeing to Agree</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hal Finney</span></a></li><li><a href=\"http://www.overcomingbias.com/2006/12/you_are_never_e.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">You Are Never Entitled to Your Opinion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"http://www.overcomingbias.com/2006/12/normative_bayes.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Normative Bayesianism and Disagreement</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Nicholas Shackel</span></li><li><a href=\"http://www.overcomingbias.com/2009/01/disagreement-is-nearfar-bias.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreement is Near/Far Bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"http://www.spencergreenberg.com/2011/12/the-seven-causes-of-disagreement/\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">The Seven Causes of</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Disagreement</span></span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><span class=\"by_HoGziwmhpMGqGeWZy\">Spencer Greenberg</span></span></li><li><a href=\"http://www.paulgraham.com/disagree.html\"><span class=\"by_HoGziwmhpMGqGeWZy\">How to Disagree</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by Paul Graham</span></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/aumann-s-agreement-theorem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aumann's agreement theorem</span></a></li><li><a href=\"https://lessestwrong.com/tag/modesty-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Modesty argument</span></a></li><li><a href=\"https://lessestwrong.com/tag/disagreements-on-less-wrong\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreements on Less Wrong</span></a></li><li><a href=\"https://lessestwrong.com/tag/arguments-as-soldiers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Arguments as soldiers</span></a></li><li><a href=\"https://www.lesswrong.com/tag/double-crux\"><span class=\"by_HoGziwmhpMGqGeWZy\">Double-Crux</span></a></li><li><a href=\"https://www.lesswrong.com/tag/conversation-topic\"><span class=\"by_HoGziwmhpMGqGeWZy\">Conversation</span></a></li></ul><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span><a href=\"http://hanson.gmu.edu/deceive.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">PDF</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Talk video</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></p><ul><li><a href=\"http://cowles.econ.yale.edu/P/cp/p05b/p0552.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">We Can't Disagree Forever</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by John Geanakoplos and Heraklis Polemarchakis</span></li><li><a href=\"http://www.kellogg.northwestern.edu/research/math/papers/377.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information, Trade, and Common Knowledge</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Paul Milgrom and Nancy Stokey</span></li></ul>",
      "sections": [
        {
          "title": "Aumann's Agreement Theorem",
          "anchor": "Aumann_s_Agreement_Theorem",
          "level": 1
        },
        {
          "title": "External Posts",
          "anchor": "External_Posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 93,
    "description": {
      "markdown": "**Disagreement** is when two people have different beliefs.\n\nAumann's Agreement Theorem\n--------------------------\n\nConsidered of particular relevance to disagreement between people trying to be rational, [Aumann's agreement theorem](https://lessestwrong.com/tag/aumann-s-agreement-theorem) can be [informally interpreted](https://lessestwrong.com/tag/aumann-agreement) as suggesting that if two people are honest seekers of truth, and both *believe* each other to be honest, then they should update on each other's opinions and quickly reach agreement. The very fact that a person believes something is [Rational evidence](https://lessestwrong.com/tag/rational-evidence) that that something is true, and so this fact [should be taken into account](http://www.overcomingbias.com/2007/01/extraordinary_c.html) when forming your belief.\n\nOutside of well-functioning [prediction markets](https://lessestwrong.com/tag/prediction-markets), Aumann agreement can probably only be approximated by careful deliberative discourse. Interest in Aumann agreement has waned in recent years within the Rationalist community, perhaps out of a sense Aumann agreement cannot be practically achieved by humans – there is too much background information to be exchanged. Instead, people now focus on things more like Double-Crux\n\nExternal Posts\n--------------\n\n*   [Reasonable Disagreement](http://www.overcomingbias.com/2006/12/reasonable_disa.html) by Nicholas Shackel\n*   [Agreeing to Agree](http://www.overcomingbias.com/2006/12/agreeing_to_agr.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cypherpunk))\n*   [You Are Never Entitled to Your Opinion](http://www.overcomingbias.com/2006/12/you_are_never_e.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Normative Bayesianism and Disagreement](http://www.overcomingbias.com/2006/12/normative_bayes.html) by Nicholas Shackel\n*   [Disagreement is Near/Far Bias](http://www.overcomingbias.com/2009/01/disagreement-is-nearfar-bias.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [The Seven Causes of Disagreement](http://www.spencergreenberg.com/2011/12/the-seven-causes-of-disagreement/) by Spencer Greenberg\n*   [How to Disagree](http://www.paulgraham.com/disagree.html) by Paul Graham\n\nSee also\n--------\n\n*   [Aumann's agreement theorem](https://lessestwrong.com/tag/aumann-s-agreement-theorem)\n*   [Modesty argument](https://lessestwrong.com/tag/modesty-argument)\n*   [Disagreements on Less Wrong](https://lessestwrong.com/tag/disagreements-on-less-wrong)\n*   [Arguments as soldiers](https://lessestwrong.com/tag/arguments-as-soldiers)\n*   [Double-Crux](https://www.lesswrong.com/tag/double-crux)\n*   [Conversation](https://www.lesswrong.com/tag/conversation-topic)\n\nReferences\n----------\n\n([PDF](http://hanson.gmu.edu/deceive.pdf), [Talk video](http://www.newmedia.ufm.edu/gsm/index.php?title=Are_Disagreements_Honest%3F))\n\n*   [We Can't Disagree Forever](http://cowles.econ.yale.edu/P/cp/p05b/p0552.pdf) by John Geanakoplos and Heraklis Polemarchakis\n*   [Information, Trade, and Common Knowledge](http://www.kellogg.northwestern.edu/research/math/papers/377.pdf) by Paul Milgrom and Nancy Stokey"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "3uE2pXvbcnS9nnZRE",
    "name": "World Modeling",
    "core": true,
    "slug": "world-modeling",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">World Modeling</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is </span><span class=\"by_nLbwLhBaQeG6tCNDN\">getting curious about</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> how the world </span><span class=\"by_EQNTWXLKMeWMp2FQS\">works. </span><span class=\"by_nLbwLhBaQeG6tCNDN\">It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, </span></span><i><span class=\"by_nLbwLhBaQeG6tCNDN\">finding out how the damn thing works,</span></i><span><span class=\"by_nLbwLhBaQeG6tCNDN\"> and writing it down for the rest</span><span class=\"by_r38pkCm7wF4M44MDQ\"> of </span><span class=\"by_nLbwLhBaQeG6tCNDN\">us.</span></span></p><blockquote><p><i><span class=\"by_r38pkCm7wF4M44MDQ\">The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.</span></i></p><p><span class=\"by_nLbwLhBaQeG6tCNDN\">-- </span><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality\"><u><span class=\"by_nLbwLhBaQeG6tCNDN\">Twelve Virtues of Rationality</span></u></a></p></blockquote><hr><h1 id=\"________________________________World_Modeling_Sub_Topics\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Modeling Sub-Topics</span></strong></h1><figure style=\"width:100%;\"><table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%);\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%;\"><p><strong id=\"Mathematical_Sciences\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mathematical Sciences</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Abstraction</span></a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Anthropics</span></a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Category Theory</span></a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Causality</span></a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information Theory</span></a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Logic &amp; Mathematics</span></a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Probability &amp; Statistics</span></a></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Specifics</span></i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner's Dilemma</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33.33%;\"><p><strong id=\"General_Science___Eng\"><span class=\"by_qgdGA4ZEyW7zNdK84\">General Science &amp; Eng</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Machine Learning</span></a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Nanotechnology</span></a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Physics</span></a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Programming</span></a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Space Exploration &amp; Colonization</span></a></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Specifics</span></i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Great Filter</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%;\"><p><strong id=\"Meta___Misc\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meta / Misc</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Academic Papers</span></a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Book Reviews</span></a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Distillation &amp; Pedagogy</span></a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fact Posts</span></a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Research Agendas</span></a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship &amp; Learning</span></a></p></td></tr><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;\"><p><strong id=\"Social___Economic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Social &amp; Economic</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Economics</span></a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Financial Investing</span></a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">History</span></a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Politics</span></a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Progress Studies</span></a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Social and Cultural Dynamics</span></a></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Specifics</span></i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Conflict vs Mistake Theory</span></a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cost Disease</span></a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Efficient Market Hypothesis</span></a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Industrial Revolution</span></a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Moral Mazes</span></a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Signaling</span></a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Social Reality</span></a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Social Status</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:25px;padding:0px;vertical-align:top;\"><p><strong id=\"Biological___Psychological\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Biological &amp; Psychological</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aging</span></a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Biology</span></a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consciousness</span></a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolution</span></a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolutionary Psychology</span></a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Medicine</span></a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Neuroscience</span></a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Qualia</span></a></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Specifics</span></i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Coronavirus</span></a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">General Intelligence</span></a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">IQ / g-factor</span></u></a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Neocortex</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;\"><p><strong id=\"The_Practice_of_Modeling\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Practice of Modeling</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic Review</span></a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Expertise</span></a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Gears-Level Models</span></a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Falsifiability</span></a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasting &amp; Prediction</span></a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasts (Lists of)</span></a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inside/Outside View</span></a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Jargon (meta)</span></a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Practice and Philosophy of Science</span></a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prediction Markets</span></a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Reductionism</span></a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Replicability</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td></tr></tbody></table></figure><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><h2 id=\"A_definition_by_elimination\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A definition by elimination</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Properly considered, the overwhelming majority of content LessWrong is about </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">modeling how the world is</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of </span><a href=\"https://www.lesswrong.com/tag/rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, and </span><a href=\"https://www.lesswrong.com/tag/ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><a href=\"https://www.lesswrong.com/tag/rationality\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag is for content that is about </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how the world is </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">in relation to how minds works and what one ought to do in order to reach true beliefs. The question for that category is </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">does this relate to how I ought to think?</span></i></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag is for content about </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how the world is </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">which is relevant to choosing actions in a relatively immediate way. By this definition, it encompasses most posts discussing altruistic methods and targets, as well practical personal advice. The question for that category is </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">is this content motivated by the desire to optimize the world?</span></i></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><a href=\"https://www.lesswrong.com/tag/ai\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">AI</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag is for content about </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how the world is </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">which is relevant to questions of how advanced artificial intelligence will affect the world and how to ensure outcomes are good. The question is </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">does this help me make predictions about AI or ensure AI will have good outcomes?</span></i></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">If content warrants a no to all of the above questions, then it is likely to be both relatively pure world modeling (not about optimizing in any direct way) and not already covered by an existing major category. It is then a good fit for the World Modeling category. Stuff like math, science, history</span></p><h2 id=\"Some_more_examples\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Some more examples</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A study of how people historically exercised is World Modeling. Advice on the optimal way to exercise in the present day is </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. A study of the Fall of Rome would be World Modeling. A review of current policies being discussed by people who want to cause changes in a present government should be classified as </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. It would be World Modeling too only if it is expected to be of interest to people with no immediate plans to try to alter government, for example a review on the effects of marijuana on productivity, driving, IQ, etc.</span></p>",
      "sections": [
        {
          "title": "                                World Modeling Sub-Topics",
          "anchor": "________________________________World_Modeling_Sub_Topics",
          "level": 1
        },
        {
          "title": "Mathematical Sciences",
          "anchor": "Mathematical_Sciences",
          "level": 3
        },
        {
          "title": "General Science & Eng",
          "anchor": "General_Science___Eng",
          "level": 3
        },
        {
          "title": "Meta / Misc",
          "anchor": "Meta___Misc",
          "level": 3
        },
        {
          "title": "Social & Economic",
          "anchor": "Social___Economic",
          "level": 3
        },
        {
          "title": "Biological & Psychological",
          "anchor": "Biological___Psychological",
          "level": 3
        },
        {
          "title": "The Practice of Modeling",
          "anchor": "The_Practice_of_Modeling",
          "level": 3
        },
        {
          "title": "A definition by elimination",
          "anchor": "A_definition_by_elimination",
          "level": 2
        },
        {
          "title": "Some more examples",
          "anchor": "Some_more_examples",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 10
    },
    "postCount": 2286,
    "description": {
      "markdown": "**World Modeling** is getting curious about how the world works. It’s diving into wikipedia, it’s running a survey to get data from your friends, it’s dropping balls from different heights and measuring how long they take to fall. Empiricism, scholarship, googling, introspection, data-gathering, science. Applying your epistemology and curiosity, *finding out how the damn thing works,* and writing it down for the rest of us.\n\n> *The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains.*\n> \n> \\-\\- [Twelve Virtues of Rationality](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/the-twelve-virtues-of-rationality)\n\n* * *\n\n**                                World Modeling Sub-Topics**\n=============================================================\n\n<table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Mathematical Sciences</strong></p><p><a href=\"http://www.lesswrong.com/tag/abstraction?showPostCount=true&amp;useTagName=true\">Abstraction</a><br><a href=\"https://www.lesswrong.com/tag/anthropics?showPostCount=true&amp;useTagName=true\">Anthropics</a><br><a href=\"http://www.lesswrong.com/tag/category-theory?showPostCount=true&amp;useTagName=true\">Category Theory</a><br><a href=\"https://www.lesswrong.com/tag/causality?showPostCount=true&amp;useTagName=true\">Causality</a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a><br><a href=\"http://www.lesswrong.com/tag/information-theory?showPostCount=true&amp;useTagName=true\">Information Theory</a><br><a href=\"https://www.lesswrong.com/tag/logic-and-mathematics?showPostCount=true&amp;useTagName=true\">Logic &amp; Mathematics</a><br><a href=\"https://www.lesswrong.com/tag/probability-and-statistics?showPostCount=true&amp;useTagName=false\">Probability &amp; Statistics</a></p><p><i>Specifics</i><br><a href=\"http://www.lesswrong.com/tag/prisoner-s-dilemma?showPostCount=true&amp;useTagName=true\">Prisoner's Dilemma</a><br>&nbsp;</p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33.33%\"><p><strong>General Science &amp; Eng</strong></p><p><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning</a><br><a href=\"https://www.lesswrong.com/tag/nanotechnology?showPostCount=true&amp;useTagName=true\">Nanotechnology</a><br><a href=\"https://www.lesswrong.com/tag/physics?showPostCount=true&amp;useTagName=true\">Physics</a><br><a href=\"https://www.lesswrong.com/tag/programming?showPostCount=true&amp;useTagName=true\">Programming</a><br><a href=\"http://www.lesswrong.com/tag/space-exploration-and-colonization?showPostCount=true&amp;useTagName=true\">Space Exploration &amp; Colonization</a></p><p><i>Specifics</i><br><a href=\"https://www.lesswrong.com/tag/great-filter?showPostCount=true&amp;useTagName=true\">The Great Filter</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Meta / Misc</strong></p><p><a href=\"https://www.lesswrong.com/tag/academic-papers?showPostCount=true&amp;useTagName=true\">Academic Papers</a><br><a href=\"https://www.lesswrong.com/tag/book-reviews?showPostCount=true&amp;useTagName=true\">Book Reviews</a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy</a><br><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\">Fact Posts</a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas</a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning</a></p></td></tr><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Social &amp; Economic</strong></p><p><a href=\"https://www.lesswrong.com/tag/economics?showPostCount=true&amp;useTagName=true\">Economics</a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing</a><br><a href=\"https://www.lesswrong.com/tag/history?showPostCount=true&amp;useTagName=true\">History</a><br><a href=\"https://www.lesswrong.com/tag/politics?showPostCount=true&amp;useTagName=true\">Politics</a><br><a href=\"https://www.lesswrong.com/tag/progress-studies?showPostCount=true&amp;useTagName=true\">Progress Studies</a><br><a href=\"https://www.lesswrong.com/tag/social-and-cultural-dynamics?showPostCount=true&amp;useTagName=true\">Social and Cultural Dynamics</a></p><p><i>Specifics</i><br><a href=\"https://www.lesswrong.com/tag/conflict-vs-mistake?showPostCount=true&amp;useTagName=true\">Conflict vs Mistake Theory</a><br><a href=\"https://www.lesswrong.com/tag/cost-disease?showPostCount=true&amp;useTagName=true\">Cost Disease</a><br><a href=\"https://www.lesswrong.com/tag/efficient-market-hypothesis?showPostCount=true&amp;useTagName=true\">Efficient Market Hypothesis</a><br><a href=\"https://www.lesswrong.com/tag/industrial-revolution?showPostCount=true&amp;useTagName=true\">Industrial Revolution</a><br><a href=\"https://www.lesswrong.com/tag/moral-mazes?showPostCount=true&amp;useTagName=true\">Moral Mazes</a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling</a><br><a href=\"https://www.lesswrong.com/tag/social-reality?showPostCount=true&amp;useTagName=true\">Social Reality</a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:25px;padding:0px;vertical-align:top\"><p><strong>Biological &amp; Psychological</strong></p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging</a><br><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\">Biology</a><br><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness</a><br><a href=\"https://www.lesswrong.com/tag/evolution?showPostCount=true&amp;useTagName=true\">Evolution</a><br><a href=\"http://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\">Evolutionary Psychology</a><br><a href=\"https://www.lesswrong.com/tag/medicine?showPostCount=true&amp;useTagName=true\">Medicine</a><br><a href=\"https://www.lesswrong.com/tag/neuroscience?showPostCount=true&amp;useTagName=true\">Neuroscience</a><br><a href=\"https://www.lesswrong.com/tag/qualia?showPostCount=true&amp;useTagName=true\">Qualia</a></p><p><i>Specifics</i><br><a href=\"https://www.lesswrong.com/tag/coronavirus?showPostCount=true&amp;useTagName=true\">Coronavirus</a><br><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\">General Intelligence</a><br><a href=\"http://www.lesswrong.com/tag/iq-g-factor?showPostCount=true&amp;useTagName=true\"><u>IQ / g-factor</u></a><br><a href=\"http://www.lesswrong.com/tag/neocortex?showPostCount=true&amp;useTagName=true\">Neocortex</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>The Practice of Modeling</strong></p><p><a href=\"https://www.lesswrong.com/tag/epistemic-review?showPostCount=true&amp;useTagName=true\">Epistemic Review</a><br><a href=\"https://www.lesswrong.com/tag/expertise?showPostCount=true&amp;useTagName=true\">Expertise</a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\">Gears-Level Models</a><br><a href=\"http://www.lesswrong.com/tag/falsifiability?showPostCount=true&amp;useTagName=true\">Falsifiability</a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction</a><br><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of?showPostCount=true&amp;useTagName=true\">Forecasts (Lists of)</a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View</a><br><a href=\"http://www.lesswrong.com/tag/jargon-meta?showPostCount=true&amp;useTagName=true\">Jargon (meta)</a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\">Practice and Philosophy of Science</a><br><a href=\"https://www.lesswrong.com/tag/prediction-markets?showPostCount=true&amp;useTagName=true\">Prediction Markets</a><br><a href=\"http://www.lesswrong.com/tag/reductionism?showPostCount=true&amp;useTagName=true\">Reductionism</a><br><a href=\"https://www.lesswrong.com/tag/replicability?showPostCount=true&amp;useTagName=true\">Replicability</a><br>&nbsp;</p></td></tr></tbody></table>\n\nA definition by elimination\n---------------------------\n\nProperly considered, the overwhelming majority of content LessWrong is about *modeling how the world is*, including almost all posts on Rationality and all practical advice. The intended usage of World Modeling is to capture all content describing how the world is that is not captured by the more specific major tags of [Rationality](https://www.lesswrong.com/tag/rationality), [World Optimization](https://www.lesswrong.com/tag/world-optimization), and [AI](https://www.lesswrong.com/tag/ai).\n\n*   The [Rationality](https://www.lesswrong.com/tag/rationality) tag is for content that is about *how the world is* in relation to how minds works and what one ought to do in order to reach true beliefs. The question for that category is *does this relate to how I ought to think?*\n*   The [World Optimization](https://www.lesswrong.com/tag/world-optimization) tag is for content about *how the world is* which is relevant to choosing actions in a relatively immediate way. By this definition, it encompasses most posts discussing altruistic methods and targets, as well practical personal advice. The question for that category is *is this content motivated by the desire to optimize the world?*\n*   The [AI](https://www.lesswrong.com/tag/ai) tag is for content about *how the world is* which is relevant to questions of how advanced artificial intelligence will affect the world and how to ensure outcomes are good. The question is *does this help me make predictions about AI or ensure AI will have good outcomes?*\n\nIf content warrants a no to all of the above questions, then it is likely to be both relatively pure world modeling (not about optimizing in any direct way) and not already covered by an existing major category. It is then a good fit for the World Modeling category. Stuff like math, science, history\n\nSome more examples\n------------------\n\nA study of how people historically exercised is World Modeling. Advice on the optimal way to exercise in the present day is [World Optimization](https://www.lesswrong.com/tag/world-optimization). A study of the Fall of Rome would be World Modeling. A review of current policies being discussed by people who want to cause changes in a present government should be classified as [Optimization](https://www.lesswrong.com/tag/world-optimization). It would be World Modeling too only if it is expected to be of interest to people with no immediate plans to try to alter government, for example a review on the effects of marijuana on productivity, driving, IQ, etc."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PDJ6KqJBRzvKPfuS3",
    "name": "Economics",
    "core": null,
    "slug": "economics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_TWDkQT6f9mgyNXQ4p\">Economics</span></strong><span class=\"by_TWDkQT6f9mgyNXQ4p\"> is the social science that studies how humans and other agents interact in a universe with scarce resources. It deals with topics such as trade, specialization of labor, accumulation of capital, technology, and resource consumption. Agents in economics are generally assumed to have utility functions, which they try to maximize under various constraints.</span></p><p><span class=\"by_TWDkQT6f9mgyNXQ4p\">Economics is usually separated into microeconomics and macroeconomics. Microeconomics concerns the behavior of agents as they interact in a market. More narrowly, it studies the price mechanism, a decentralized system of allocating goods and services based on an evolving system of prices and trade, which all actors in a market economy contribute towards. The price mechanism is closely related to the concept of the </span><a href=\"https://en.wikipedia.org/wiki/Invisible_hand\"><span class=\"by_TWDkQT6f9mgyNXQ4p\">invisible hand</span></a><span class=\"by_TWDkQT6f9mgyNXQ4p\">, first introduced by </span><a href=\"https://en.wikipedia.org/wiki/Adam_Smith\"><span class=\"by_TWDkQT6f9mgyNXQ4p\">Adam Smith</span></a><span class=\"by_TWDkQT6f9mgyNXQ4p\">. </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_TWDkQT6f9mgyNXQ4p\">Game theory</span></a><span class=\"by_TWDkQT6f9mgyNXQ4p\"> is the mathematical study of rational agency, which formalizes many standard results in microeconomics.</span></p><p><span class=\"by_TWDkQT6f9mgyNXQ4p\">Macroeconomics concerns the aggregate behavior of entire economies. For example, it studies economic growth, inflation, international trade and unemployment. An ongoing debate concerns to what extent the </span><a href=\"https://www.lesswrong.com/tag/economic-consequences-of-agi\"><span class=\"by_TWDkQT6f9mgyNXQ4p\">impacts of artificial intelligence</span></a><span class=\"by_TWDkQT6f9mgyNXQ4p\"> should be viewed through the lens of economics.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 241,
    "description": {
      "markdown": "**Economics** is the social science that studies how humans and other agents interact in a universe with scarce resources. It deals with topics such as trade, specialization of labor, accumulation of capital, technology, and resource consumption. Agents in economics are generally assumed to have utility functions, which they try to maximize under various constraints.\n\nEconomics is usually separated into microeconomics and macroeconomics. Microeconomics concerns the behavior of agents as they interact in a market. More narrowly, it studies the price mechanism, a decentralized system of allocating goods and services based on an evolving system of prices and trade, which all actors in a market economy contribute towards. The price mechanism is closely related to the concept of the [invisible hand](https://en.wikipedia.org/wiki/Invisible_hand), first introduced by [Adam Smith](https://en.wikipedia.org/wiki/Adam_Smith). [Game theory](https://www.lesswrong.com/tag/game-theory) is the mathematical study of rational agency, which formalizes many standard results in microeconomics.\n\nMacroeconomics concerns the aggregate behavior of entire economies. For example, it studies economic growth, inflation, international trade and unemployment. An ongoing debate concerns to what extent the [impacts of artificial intelligence](https://www.lesswrong.com/tag/economic-consequences-of-agi) should be viewed through the lens of economics."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tNsqhzTibgGJKPEWB",
    "name": "Covid-19",
    "core": false,
    "slug": "covid-19",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_r38pkCm7wF4M44MDQ\">The</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span></span><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">2019 Novel Coronavirus</span></strong><span><span class=\"by_nLbwLhBaQeG6tCNDN\"> (aka COVID-19, SARS-CoV-2)</span><span class=\"by_r38pkCm7wF4M44MDQ\"> is a pandemic sweeping the world.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 853,
    "description": {
      "markdown": "The **2019 Novel Coronavirus** (aka COVID-19, SARS-CoV-2) is a pandemic sweeping the world."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sYm3HiWcfZvrGu3ui",
    "name": "AI",
    "core": true,
    "slug": "ai",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Artificial Intelligence</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the study of </span><span class=\"by_EQNTWXLKMeWMp2FQS\">creating </span><span class=\"by_qgdGA4ZEyW7zNdK84\">intelligence in </span><span class=\"by_nLbwLhBaQeG6tCNDN\">algorithms.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> On LessWrong, </span><span class=\"by_EQNTWXLKMeWMp2FQS\">the primary focus</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of AI </span><span class=\"by_EQNTWXLKMeWMp2FQS\">discussion </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is to </span><span class=\"by_nLbwLhBaQeG6tCNDN\">ensure that as humanity builds increasingly powerful AI systems, the outcome will be </span><span class=\"by_qgdGA4ZEyW7zNdK84\">good.</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> The</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> central concern is that </span><span class=\"by_nLbwLhBaQeG6tCNDN\">a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> an existential </span><span class=\"by_nLbwLhBaQeG6tCNDN\">threat</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to </span><span class=\"by_nLbwLhBaQeG6tCNDN\">the future of humanity.</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> This is known as the </span></span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">AI alignment</span></i><span class=\"by_EQNTWXLKMeWMp2FQS\"> problem.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Common terms in this space are </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI. </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.</span></p><p><strong id=\"AI_Alignment\"><span class=\"by_EQNTWXLKMeWMp2FQS\">AI Alignment</span></strong></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">There are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">But both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.</span></p><p><span class=\"by_qxJ28GN72aiJu96iF\">See also </span><a href=\"https://www.lesswrong.com/tag/general-intelligence\"><span class=\"by_qxJ28GN72aiJu96iF\">General Intelligence</span></a><span class=\"by_qxJ28GN72aiJu96iF\">.</span></p><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border:10px solid #f8f8f8\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong id=\"Basic_Alignment_Theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Basic Alignment Theory</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI</span></a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Coherent Extrapolated Volition</span></a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Complexity of Value</span></a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Corrigibility</span></a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Embedded Agency</span></a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fixed Point Theorems</span></a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart's Law</span></a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Goal-Directedness</span></a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Infra-Bayesianism</span></a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inner Alignment</span></a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental Convergence</span></a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Intelligence Explosion</span></a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Logical Induction</span></a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Logical Uncertainty</span></a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mesa-Optimization</span></a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Myopia</span></a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's Problem</span></a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimization</span></a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Orthogonality Thesis</span></a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Outer Alignment</span></a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Paperclip Maximizer</span></a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Recursive Self-Improvement</span></a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff Induction</span></a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Treacherous Turn</span></a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility Functions</span></a></p></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong id=\"Engineering_Alignment\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Engineering Alignment</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Boxing (Containment)</span></a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Conservatism (AI)</span></a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Debate</span><span class=\"by_Sp5wM4aRAhNERd4oY\"> (AI safety technique)</span></span></a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Factored Cognition</span></a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Humans Consulting HCH</span></a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Impact Measures</span></a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Inverse Reinforcement Learning</span></a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Iterated Amplification</span></a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Mild Optimization</span></a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Oracle AI</span></a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Reward Functions</span></a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Tool AI</span></a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Transparency / Interpretability</span></a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Tripwire</span></a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Value Learning</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><p><strong id=\"Strategy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Strategy</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/ai-governance?showPostCount=true&amp;useTagName=true\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">AI </span><span class=\"by_Sp5wM4aRAhNERd4oY\">Governance</span></span></a><br><a href=\"https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Risk</span></a><br><a href=\"http://www.lesswrong.com/tag/ai-services-cais?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Services (CAIS)</span></u></a><br><a href=\"https://www.lesswrong.com/tag/ai-takeoff?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Takeoff</span></a><br><a href=\"https://www.lesswrong.com/tag/ai-timelines?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Timelines</span></a><br><a href=\"https://www.lesswrong.com/tag/computing-overhang?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Computing Overhang</span></a><br><a href=\"https://www.lesswrong.com/tag/regulation-and-ai-risk?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Regulation and AI Risk</span></a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Transformative AI</span></a></p></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\"><p><strong id=\"Organizations\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Organizations</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">AI Safety Camp</span></a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Centre for Human-Compatible AI</span></a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">DeepMind</span></a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Future of Humanity Institute</span></a><br><a href=\"https://www.lesswrong.com/tag/future-of-life-institute-fli?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Future of Life Institute</span></a><br><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Machine Intelligence Research Institute</span></a><br><a href=\"https://www.lesswrong.com/tag/openai?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">OpenAI</span></a><br><a href=\"https://www.lesswrong.com/tag/ought?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ought</span></a></p><p><span class=\"by_Sp5wM4aRAhNERd4oY\">&nbsp;</span></p><p><strong id=\"Other\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Other</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-capabilities?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">AI Capabilities</span></a><br><a href=\"https://www.lesswrong.com/tag/gpt?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">GPT</span></a><br><a href=\"https://www.lesswrong.com/tag/language-models?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Language Models</span></a><br><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Machine Learning</span></a><br><a href=\"https://www.lesswrong.com/tag/narrow-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Narrow AI</span></a><br><a href=\"https://www.lesswrong.com/tag/neuromorphic-ai?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Neuromorphic AI</span></a><br><a href=\"https://www.lesswrong.com/tag/reinforcement-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Reinforcement Learning</span></a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Research Agendas</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Superintelligence</span></a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Whole Brain Emulation</span></a></p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top\"><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td></tr></tbody></table></figure>",
      "sections": [
        {
          "title": "AI Alignment",
          "anchor": "AI_Alignment",
          "level": 1
        },
        {
          "title": "Basic Alignment Theory",
          "anchor": "Basic_Alignment_Theory",
          "level": 1
        },
        {
          "title": "Engineering Alignment",
          "anchor": "Engineering_Alignment",
          "level": 1
        },
        {
          "title": "Strategy",
          "anchor": "Strategy",
          "level": 1
        },
        {
          "title": "Organizations",
          "anchor": "Organizations",
          "level": 1
        },
        {
          "title": "Other",
          "anchor": "Other",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 3179,
    "description": {
      "markdown": "**Artificial Intelligence** is the study of creating intelligence in algorithms. On LessWrong, the primary focus of AI discussion is to ensure that as humanity builds increasingly powerful AI systems, the outcome will be good. The central concern is that a powerful enough AI, if not designed and implemented with sufficient understanding, would optimize something unintended by its creators and pose an existential threat to the future of humanity. This is known as the *AI alignment* problem.\n\nCommon terms in this space are *superintelligence, AI Alignment, AI Safety, Friendly AI, Transformative AI, human-level-intelligence, AI Governance, and Beneficial AI.* This entry and the associated tag roughly encompass all of these topics: anything part of the broad cluster of understanding AI and its future impacts on our civilization deserves this tag.\n\n**AI Alignment**\n\nThere are narrow conceptions of alignment, where you’re trying to get it to do something like cure Alzheimer’s disease without destroying the rest of the world. And there’s much more ambitious notions of alignment, where you’re trying to get it to do the right thing and achieve a happy intergalactic civilization.\n\nBut both the narrow and the ambitious alignment have in common that you’re trying to have the AI do that thing rather than making a lot of paperclips.\n\nSee also [General Intelligence](https://www.lesswrong.com/tag/general-intelligence).\n\n<table style=\"background-color:rgb(255, 255, 255);border:10px solid #f8f8f8\"><tbody><tr><td style=\"border:1px solid hsl(0, 0%, 100%);padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Basic Alignment Theory</strong></p><p><a href=\"https://www.lesswrong.com/tag/aixi?showPostCount=true&amp;useTagName=true\">AIXI</a><br><a href=\"http://www.lesswrong.com/tag/coherent-extrapolated-volition?showPostCount=true&amp;useTagName=true\">Coherent Extrapolated Volition</a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a><br><a href=\"https://www.lesswrong.com/tag/corrigibility?showPostCount=true&amp;useTagName=true\">Corrigibility</a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a><br><a href=\"https://www.lesswrong.com/tag/embedded-agency?showPostCount=true&amp;useTagName=true\">Embedded Agency</a><br><a href=\"https://www.lesswrong.com/tag/fixed-point-theorems?showPostCount=true&amp;useTagName=true\">Fixed Point Theorems</a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart's Law</a><br><a href=\"https://www.lesswrong.com/tag/goal-directedness?showPostCount=true&amp;useTagName=true\">Goal-Directedness</a><br><a href=\"http://www.lesswrong.com/tag/infra-bayesianism?showPostCount=true&amp;useTagName=true\">Infra-Bayesianism</a><br><a href=\"https://www.lesswrong.com/tag/inner-alignment?showPostCount=true&amp;useTagName=true\">Inner Alignment</a><br><a href=\"https://www.lesswrong.com/tag/instrumental-convergence?showPostCount=true&amp;useTagName=true\">Instrumental Convergence</a><br><a href=\"https://www.lesswrong.com/tag/intelligence-explosion?showPostCount=true&amp;useTagName=true\">Intelligence Explosion</a><br><a href=\"https://www.lesswrong.com/tag/logical-induction?showPostCount=true&amp;useTagName=true\">Logical Induction</a><br><a href=\"http://www.lesswrong.com/tag/logical-uncertainty?showPostCount=true&amp;useTagName=true\">Logical Uncertainty</a><br><a href=\"https://www.lesswrong.com/tag/mesa-optimization?showPostCount=true&amp;useTagName=true\">Mesa-Optimization</a><br><a href=\"https://www.lesswrong.com/tag/myopia?showPostCount=true&amp;useTagName=true\">Myopia</a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem</a><br><a href=\"https://www.lesswrong.com/tag/optimization?showPostCount=true&amp;useTagName=true\">Optimization</a><br><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis?showPostCount=true&amp;useTagName=true\">Orthogonality Thesis</a><br><a href=\"https://www.lesswrong.com/tag/outer-alignment?showPostCount=true&amp;useTagName=true\">Outer Alignment</a><br><a href=\"http://www.lesswrong.com/tag/paperclip-maximizer?showPostCount=true&amp;useTagName=true\">Paperclip Maximizer</a><br><a href=\"https://www.lesswrong.com/tag/recursive-self-improvement?showPostCount=true&amp;useTagName=true\">Recursive Self-Improvement</a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction</a><br><a href=\"https://www.lesswrong.com/tag/treacherous-turn?showPostCount=true&amp;useTagName=true\">Treacherous Turn</a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a></p></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\" rowspan=\"2\"><p><strong>Engineering Alignment</strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-boxing-containment?showPostCount=true&amp;useTagName=true\">AI Boxing (Containment)</a><br><a href=\"https://www.lesswrong.com/tag/conservatism-ai?showPostCount=true&amp;useTagName=true\">Conservatism (AI)</a><br><a href=\"https://www.lesswrong.com/tag/ai-safety-via-debate?showPostCount=true&amp;useTagName=true\">Debate (AI safety technique)</a><br><a href=\"https://www.lesswrong.com/tag/factored-cognition?showPostCount=true&amp;useTagName=true\">Factored Cognition</a><br><a href=\"https://www.lesswrong.com/tag/hch?showPostCount=true&amp;useTagName=true\">Humans Consulting HCH</a><br><a href=\"https://www.lesswrong.com/tag/impact-measures?showPostCount=true&amp;useTagName=true\">Impact Measures</a><br><a href=\"https://www.lesswrong.com/tag/inverse-reinforcement-learning?showPostCount=true&amp;useTagName=true\">Inverse Reinforcement Learning</a><br><a href=\"https://www.lesswrong.com/tag/iterated-amplification?showPostCount=true&amp;useTagName=true\">Iterated Amplification</a><br><a href=\"http://www.lesswrong.com/tag/mild-optimization?showPostCount=true&amp;useTagName=true\">Mild Optimization</a><br><a href=\"https://www.lesswrong.com/tag/oracle-ai?showPostCount=true&amp;useTagName=true\">Oracle AI</a><br><a href=\"https://www.lesswrong.com/tag/reward-functions?showPostCount=true&amp;useTagName=true\">Reward Functions</a><br><a href=\"http://www.lesswrong.com/tag/tool-ai?showPostCount=true&amp;useTagName=true\">Tool AI</a><br><a href=\"https://www.lesswrong.com/tag/transparency-interpretability-ml-and-ai?showPostCount=true\">Transparency / Interpretability</a><br><a href=\"https://www.lesswrong.com/tag/tripwire?showPostCount=true&amp;useTagName=true\">Tripwire</a><br><a href=\"https://www.lesswrong.com/tag/value-learning?showPostCount=true&amp;useTagName=true\">Value Learning</a></p><p>&nbsp;</p><p><strong>Strategy</strong></p><p><a href=\"http://www.lesswrong.com/tag/ai-governance?showPostCount=true&amp;useTagName=true\">AI Governance</a><br><a href=\"https://www.lesswrong.com/tag/ai-risk?showPostCount=true&amp;useTagName=true\">AI Risk</a><br><a href=\"http://www.lesswrong.com/tag/ai-services-cais?showPostCount=true&amp;useTagName=true\"><u>AI Services (CAIS)</u></a><br><a href=\"https://www.lesswrong.com/tag/ai-takeoff?showPostCount=true&amp;useTagName=true\">AI Takeoff</a><br><a href=\"https://www.lesswrong.com/tag/ai-timelines?showPostCount=true&amp;useTagName=true\">AI Timelines</a><br><a href=\"https://www.lesswrong.com/tag/computing-overhang?showPostCount=true&amp;useTagName=true\">Computing Overhang</a><br><a href=\"https://www.lesswrong.com/tag/regulation-and-ai-risk?showPostCount=true&amp;useTagName=true\">Regulation and AI Risk</a><br><a href=\"https://www.lesswrong.com/tag/transformative-ai?showPostCount=true&amp;useTagName=true\">Transformative AI</a></p></td><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:10px;vertical-align:top;width:33.33%\"><p><strong>Organizations</strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-safety-camp?showPostCount=true&amp;useTagName=true\">AI Safety Camp</a><br><a href=\"https://www.lesswrong.com/tag/centre-for-human-compatible-ai?showPostCount=true&amp;useTagName=true\">Centre for Human-Compatible AI</a><br><a href=\"https://www.lesswrong.com/tag/alpha-algorithm-family?showPostCount=true&amp;useTagName=true\">DeepMind</a><br><a href=\"https://www.lesswrong.com/tag/future-of-humanity-institute?showPostCount=true&amp;useTagName=true\">Future of Humanity Institute</a><br><a href=\"https://www.lesswrong.com/tag/future-of-life-institute-fli?showPostCount=true&amp;useTagName=true\">Future of Life Institute</a><br><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri?showPostCount=true\">Machine Intelligence Research Institute</a><br><a href=\"https://www.lesswrong.com/tag/openai?showPostCount=true&amp;useTagName=true\">OpenAI</a><br><a href=\"https://www.lesswrong.com/tag/ought?showPostCount=true&amp;useTagName=true\">Ought</a></p><p>&nbsp;</p><p><strong>Other</strong></p><p><a href=\"https://www.lesswrong.com/tag/ai-capabilities?showPostCount=true&amp;useTagName=true\">AI Capabilities</a><br><a href=\"https://www.lesswrong.com/tag/gpt?showPostCount=true&amp;useTagName=true\">GPT</a><br><a href=\"https://www.lesswrong.com/tag/language-models?showPostCount=true&amp;useTagName=true\">Language Models</a><br><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=true&amp;useTagName=true\">Machine Learning</a><br><a href=\"https://www.lesswrong.com/tag/narrow-ai?showPostCount=true&amp;useTagName=true\">Narrow AI</a><br><a href=\"https://www.lesswrong.com/tag/neuromorphic-ai?showPostCount=true&amp;useTagName=true\">Neuromorphic AI</a><br><a href=\"https://www.lesswrong.com/tag/reinforcement-learning?showPostCount=true&amp;useTagName=true\">Reinforcement Learning</a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas</a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/superintelligence?showPostCount=true&amp;useTagName=true\">Superintelligence</a><br><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation?showPostCount=true&amp;useTagName=true\">Whole Brain Emulation</a></p></td></tr><tr><td style=\"border-color:hsl(0, 0%, 100%);border-style:solid;padding:0px;vertical-align:top\">&nbsp;</td></tr></tbody></table>"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Ng8Gice9KNkncxqcj",
    "name": "Rationality",
    "core": true,
    "slug": "rationality",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_7r4pRYHgRRMuxw9fL\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the </span><span class=\"by_nLbwLhBaQeG6tCNDN\">art</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of </span><span class=\"by_nLbwLhBaQeG6tCNDN\">thinking</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> in ways that </span><span class=\"by_EQNTWXLKMeWMp2FQS\">result in </span></span><a href=\"https://www.lesswrong.com/tag/world-modeling\"><span><span class=\"by_nLbwLhBaQeG6tCNDN\">accurate</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> beliefs</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span><span class=\"by_nLbwLhBaQeG6tCNDN\">good</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_pbREHuM5F5t5nyWqh\">decisions</span></span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> It </span><span class=\"by_nLbwLhBaQeG6tCNDN\">is the primary topic of LessWrong.</span></span><br><br><span class=\"by_nLbwLhBaQeG6tCNDN\">Rationality is not only about avoiding the vices of </span><a href=\"https://www.lesswrong.com/tag/self-deception\"><span class=\"by_nLbwLhBaQeG6tCNDN\">self-deception</span></a><span><span class=\"by_nLbwLhBaQeG6tCNDN\"> and </span><span class=\"by_pbREHuM5F5t5nyWqh\">obfuscation (the failure to </span></span><a href=\"https://www.lesswrong.com/tag/conversation-topic\"><span class=\"by_pbREHuM5F5t5nyWqh\">communicate clearly</span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">),</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> but also about the virtue of </span></span><a href=\"https://www.lesswrong.com/tag/curiosity\"><span class=\"by_pbREHuM5F5t5nyWqh\">curiosity</span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">,</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> seeing the world more clearly than before, and </span></span><a href=\"https://www.lesswrong.com/tag/ambition\"><span class=\"by_nLbwLhBaQeG6tCNDN\">achieving things</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><a href=\"https://www.lesswrong.com/tag/skill-building\"><span class=\"by_nLbwLhBaQeG6tCNDN\">previously unreachable</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><a href=\"https://www.lesswrong.com/tag/coordination-cooperation\"><span><span class=\"by_nLbwLhBaQeG6tCNDN\">to </span><span class=\"by_pbREHuM5F5t5nyWqh\">you</span></span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">.</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> The study of rationality on LessWrong includes</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> a theoretical understanding of ideal </span><span class=\"by_EQNTWXLKMeWMp2FQS\">cognitive algorithms,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> as well </span><span class=\"by_EQNTWXLKMeWMp2FQS\">as </span><span class=\"by_nLbwLhBaQeG6tCNDN\">building a practice that uses</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> these </span><span class=\"by_qgdGA4ZEyW7zNdK84\">idealized</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> algorithms </span><span class=\"by_nLbwLhBaQeG6tCNDN\">to inform</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span></span><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\"><span class=\"by_pbREHuM5F5t5nyWqh\">heuristics</span></a><span class=\"by_pbREHuM5F5t5nyWqh\">, </span><a href=\"https://www.lesswrong.com/tag/habits\"><span class=\"by_pbREHuM5F5t5nyWqh\">habits</span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span></span><a href=\"https://www.lesswrong.com/tag/techniques\"><span class=\"by_pbREHuM5F5t5nyWqh\">techniques</span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to </span><span class=\"by_nLbwLhBaQeG6tCNDN\">successfully reason</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> and </span><span class=\"by_nLbwLhBaQeG6tCNDN\">make decisions</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the real </span><span class=\"by_EQNTWXLKMeWMp2FQS\">world.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Topics covered </span><span class=\"by_EQNTWXLKMeWMp2FQS\">in</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> rationality include </span><span class=\"by_EQNTWXLKMeWMp2FQS\">(but</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> are not limited </span><span class=\"by_EQNTWXLKMeWMp2FQS\">to): </span><span class=\"by_qgdGA4ZEyW7zNdK84\">normative and theoretical explorations of </span></span><a href=\"https://www.lesswrong.com/tag/solomonoff-induction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">ideal</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/probability-and-statistics\"><span class=\"by_pbREHuM5F5t5nyWqh\">reasoning</span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">; </span><span class=\"by_EQNTWXLKMeWMp2FQS\">the </span></span><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology\"><span class=\"by_EQNTWXLKMeWMp2FQS\">capabilities and limitations</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><a href=\"https://www.lesswrong.com/tag/neuroscience\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">of </span><span class=\"by_EQNTWXLKMeWMp2FQS\">our </span><span class=\"by_pbREHuM5F5t5nyWqh\">brain</span></span></a><span class=\"by_pbREHuM5F5t5nyWqh\">, </span><a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">mind and </span><span class=\"by_pbREHuM5F5t5nyWqh\">psychology</span></span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">;</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">applied advice such as </span></span><a href=\"https://www.lesswrong.com/tag/introspection\"><span class=\"by_qgdGA4ZEyW7zNdK84\">introspection</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> techniques and </span><a href=\"https://www.lesswrong.com/tag/group-rationality\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">how to achieve truth </span><span class=\"by_pbREHuM5F5t5nyWqh\">collaboratively</span></span></a><span><span class=\"by_pbREHuM5F5t5nyWqh\">;</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> practical </span><span class=\"by_qgdGA4ZEyW7zNdK84\">techniques and methodologies for figuring out what’s true ranging from </span><span class=\"by_EQNTWXLKMeWMp2FQS\">rough</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> quantitative </span><span class=\"by_EQNTWXLKMeWMp2FQS\">modeling</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to full research guides.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Note </span><span class=\"by_EQNTWXLKMeWMp2FQS\">that</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> content about </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how the world is </span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">can be found under</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span></span><a href=\"https://www.lesswrong.com/tag/world-modeling\"><span class=\"by_EQNTWXLKMeWMp2FQS\">World Modeling</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, and practical advice about </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">how to change the world</span></i><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">categorized under </span></span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><a href=\"/tag/practical\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Practical</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p><hr><figure class=\"table\" style=\"width:100%\"><table style=\"background-color:rgb(255, 255, 255);border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong id=\"Theory___Concepts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Theory / Concepts</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Anticipated Experiences</span></u></a><br><a href=\"http://www.lesswrong.com/tag/aumann-s-agreement-theorem?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aumann's Agreement Theorem</span></a><br><a href=\"http://www.lesswrong.com/tag/bayes-theorem?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Bayes Theorem</span></u></a><br><a href=\"https://www.lesswrong.com/tag/bounded-rationality?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bounded Rationality</span></a><br><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Conservation of Expected</span></a><br><a href=\"http://www.lesswrong.com/tag/contrarianism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Contrarianism</span></a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></a><br><a href=\"http://www.lesswrong.com/tag/epistemology?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemology</span></u></a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Gears-Level</span></u></a><br><a href=\"http://www.lesswrong.com/tag/hansonian-pre-rationality?useTagName=true&amp;showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hansonian Pre-Rationality</span></a><br><a href=\"https://www.lesswrong.com/tag/law-thinking?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Law-Thinking</span></a><br><a href=\"http://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Map and Territory</span></a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's Problem</span></a><br><a href=\"http://www.lesswrong.com/tag/occam-s-razor?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Occam's razor</span></a><br><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robust Agents</span></a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solomonoff Induction</span></a><br><a href=\"http://www.lesswrong.com/tag/truth-semantics-and-meaning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Truth, Semantics, &amp; Meaning</span></a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility Functions</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong id=\"Applied_Topics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Applied Topics</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/alief?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Alief</span></u></a><br><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Betting</span></a><br><a href=\"http://www.lesswrong.com/tag/cached-thoughts?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cached Thoughts</span></a><br><a href=\"http://www.lesswrong.com/tag/calibration?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Calibration</span></a><br><a href=\"https://www.lesswrong.com/tag/dark-arts?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Dark Arts</span></a><br><a href=\"http://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Empiricism</span></a><br><a href=\"http://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic Modesty</span></a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasting &amp; Prediction</span></a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Group Rationality</span></a><br><a href=\"https://www.lesswrong.com/tag/identity?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Identity</span></a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inside/Outside View</span></a><br><a href=\"http://www.lesswrong.com/tag/introspection?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Introspection</span></u></a><br><a href=\"http://www.lesswrong.com/tag/intuition?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Intuition</span></a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Practice &amp; Philosophy of Science</span></u></a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship &amp; Learning</span></a><br><a href=\"http://www.lesswrong.com/tag/taking-ideas-seriously?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Taking Ideas Seriously</span></a><br><a href=\"https://www.lesswrong.com/tag/value-of-information?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Value of Information</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong id=\"Failure_Modes\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Failure Modes</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Affect Heuristic</span></a><br><a href=\"https://www.lesswrong.com/tag/bucket-errors?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bucket Errors</span></a><br><a href=\"https://www.lesswrong.com/tag/compartmentalization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Compartmentalization</span></a><br><a href=\"https://www.lesswrong.com/tag/confirmation-bias?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Confirmation Bias</span></u></a><br><a href=\"https://www.lesswrong.com/tag/logical-fallacies?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fallacies</span></a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart’s Law</span></a><br><a href=\"http://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Groupthink</span></u></a><br><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Heuristics and Biases</span></a><br><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind Projection Fallacy</span></a><br><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivated Reasoning</span></u></a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pica</span></a><br><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pitfalls of Rationality</span></a><br><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><br><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-Deception</span></a><br><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sunk-Cost Fallacy</span></a></p></td></tr><tr><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong id=\"Communication\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Communication</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Common Knowledge</span></u></a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Conversation</span></u></a><br><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Decoupling vs Contextualizing</span></u></a><br><a href=\"https://www.lesswrong.com/tag/disagreement?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreement</span></u></a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Distillation &amp; Pedagogy</span></a><br><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Double-Crux</span></u></a><br><a href=\"http://www.lesswrong.com/tag/good-explanations-advice?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Good Explanations (Advice)</span></a><br><a href=\"http://www.lesswrong.com/tag/ideological-turing-tests?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ideological Turing Tests</span></a><br><a href=\"https://www.lesswrong.com/tag/inferential-distance?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inferential Distance</span></a><br><a href=\"https://www.lesswrong.com/tag/information-cascades?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information Cascades</span></a><br><a href=\"https://www.lesswrong.com/tag/memetic-immune-system?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Memetic Immune System</span></a><br><a href=\"https://www.lesswrong.com/tag/philosophy-of-language?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Philosophy of Language</span></u></a><br><a href=\"https://www.lesswrong.com/tag/steelmanning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Steelmanning</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong id=\"Techniques\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Techniques</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Double-Crux</span></u></a><br><a href=\"https://www.lesswrong.com/tag/focusing?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Focusing</span></a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goal Factoring</span></a><br><a href=\"https://www.lesswrong.com/tag/internal-double-crux?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Internal Double Crux</span></a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Hamming Questions</span></u></a><br><a href=\"https://www.lesswrong.com/tag/noticing?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Noticing</span></a><br><a href=\"https://www.lesswrong.com/tag/techniques?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Techniques</span></a><br><a href=\"https://www.lesswrong.com/tag/trigger-action-planning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Trigger Action Planning/Patterns</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong id=\"Models_of_the_Mind\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Models of the Mind</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consciousness</span></a><br><a href=\"https://www.lessestwrong.com/tag/dual-process-theory-system-1-and-system-2?showPostCount=true&amp;useTagName=false\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Dual Process Theory (System 1 &amp; 2)</span></a><br><a href=\"http://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">General Intelligence</span></u></a><br><a href=\"https://www.lesswrong.com/tag/subagents?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Subagents</span></u></a><br><a href=\"https://www.lesswrong.com/tag/predictive-processing?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Predictive Processing</span></u></a><br><a href=\"https://www.lesswrong.com/tag/perceptual-control-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Perceptual Control Theory</span></a><br><a href=\"http://www.lesswrong.com/tag/zombies?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Zombies</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td></tr><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px\"><p><strong id=\"Other\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Other</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar?showPostCount=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Center for Applied Rationality</span></u></a><br><a href=\"https://www.lesswrong.com/tag/curiosity?useTagName=true&amp;showPostCount=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Curiosity</span></u></a><br><a href=\"https://www.lesswrong.com/tag/rationality-quotes?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality Quotes</span></a><br><a href=\"http://www.lesswrong.com/tag/updated-beliefs-examples-of?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Updated Beliefs (examples of)</span></a></p></td></tr></tbody></table></figure><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">This list is not comprehensive! The tagging system is new. Many needed tags have not been created and/or added to the above list.</span></i></p><hr><h2 id=\"What_we_re_calling__rationality_\"><span class=\"by_qgdGA4ZEyW7zNdK84\">What we're calling \"rationality\"</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A good heuristic is that rationality is about cognitive algorithms. Rather than being a synonym for </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">true</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">optimal</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, the term </span><i><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">rational</span></strong></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> should be reserved for describing whether or not a cognitive algorithm results in true beliefs and optimal actions.</span></p><p><span><span class=\"by_nLbwLhBaQeG6tCNDN\">This</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is </span><span class=\"by_nLbwLhBaQeG6tCNDN\">distinct from </span></span><a href=\"https://www.lesswrong.com/tag/practical-advice\"><span class=\"by_qgdGA4ZEyW7zNdK84\">practical advice</span></a><span><span class=\"by_nLbwLhBaQeG6tCNDN\">,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> such as how to improve relationships or implement productivity </span><span class=\"by_nLbwLhBaQeG6tCNDN\">systems, which</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> should not be considered \"rationality\" per se. Some have pushed against labeling self-help as \"rational dating\", etc. for reasons along </span><span class=\"by_nLbwLhBaQeG6tCNDN\">these lines</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> [</span></span><a href=\"https://www.lesswrong.com/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/posts/DFHhuAMexXAi8T6AY/the-rational-rationalist-s-guide-to-rationally-using\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">], and they are probably correct.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">In accordance with this, </span><span class=\"by_MxvxpRHcxbKfj46vG\">LessWrong </span><span class=\"by_qgdGA4ZEyW7zNdK84\">classifies most self-help type advice under the </span></span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag and not the Rationality tag.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Similarly, most object-level material about </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how the world is</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, e.g. math, biology, history, etc. is tagged under </span><a href=\"https://www.lesswrong.com/tag/world-modeling\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Modeling</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag, with exceptions for neuroscience and probability theory, etc., which have concrete consequences for how one ought to think.</span></p><h2 id=\"Heuristics_and_Biases\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Heuristics and Biases</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Early material on LessWrong frequently describes rationality with reference to heuristics and biases [</span><a href=\"https://www.lesswrong.com/posts/xLm9mgJRPvmPGpo7Q/the-cognitive-science-of-rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1,</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]. Indeed, LessWrong grew out of the blog </span><a href=\"https://www.overcomingbias.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Overcoming Bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and even </span><a href=\"https://www.lesswrong.com/rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality: A-Z</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> opens with a discussion of biases [</span><a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM/p/ptxnyfLWqRZ98wnYi\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">] with the opening chapter titled </span><a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Predictably Wrong</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. The idea is </span><span class=\"by_MxvxpRHcxbKfj46vG\">that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">human mind has been shown to systematically make certain errors of reasoning, like confirmation bias. Rationality then consists of overcoming these biases.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Apart from the issue of the replication crises which discredited many examples of bias that were commonly referenced on LessWrong, e.g. priming, the \"overcoming biases\" frame of rationality </span><span class=\"by_MxvxpRHcxbKfj46vG\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">too limited. Rationality requires the development of many </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">positive </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">skills, not just removing </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">negative </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">biases to reveal underlying perfect reasoning. These are skills such as how to update the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more.</span></p><h2 id=\"Instrumental_vs_Epistemic_Rationality\"><span><span class=\"by_iBcH2a3HdWGS2JEZA\">Instrumental</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> vs </span><span class=\"by_iBcH2a3HdWGS2JEZA\">Epistemic</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Rationality</span></span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Classically, on LessWrong, a distinction has been made between </span><i><span><span class=\"by_iBcH2a3HdWGS2JEZA\">instrumental</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">rationality</span><i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">and </span><i><span><span class=\"by_iBcH2a3HdWGS2JEZA\">epistemic</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">rationality, however, these terms may be misleading – it's not as though epistemic rationality can be traded off for gains in instrumental rationality. Only apparently, and to think one should do this is a trap.</span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental rationality</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is defined as being concerned with achieving goals. More specifically, instrumental rationality is the art of choosing and </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">implementing</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> actions that steer the future toward outcomes ranked higher in one's preferences. Said preferences are not limited to 'selfish' preferences or unshared values; they include anything one cares about.</span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic rationality</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is defined as the part of rationality which involves achieving accurate beliefs about the world. It involves </span><a href=\"https://wiki.lesswrong.com/wiki/updating\"><span class=\"by_qgdGA4ZEyW7zNdK84\">updating</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> on receiving new </span><a href=\"https://lessestwrong.com/tag/evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">evidence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, mitigating cognitive biases, and examining why you believe what you believe. It can be seen as a form of instrumental rationality in which knowledge and truth are goals in themselves, whereas in other forms of instrumental rationality, knowledge and truth are only potential aids to achieving goals. Someone practicing instrumental rationality might even find falsehood useful.</span></p><h2 id=\"The_Art_and_Science_of_Rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Art and Science of Rationality</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In a field like biology, we can draw a distinction between the </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">science of biology</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, which involves various theories and empirical data about biological life, and the </span><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">art </span><span class=\"by_MxvxpRHcxbKfj46vG\">of being </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a biologis</span></span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">t, which is the specific way that a biologist thinks</span><span class=\"by_MxvxpRHcxbKfj46vG\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">plays with ideas and interacts</span><span class=\"by_MxvxpRHcxbKfj46vG\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the world around them. Similarly, rationality is both a science and an art. There’s study of the iron-clad laws of reasoning and mechanics of the human mind, but there’s also the general training to be the kind of person who reasons well.</span></span></p><h2 id=\"Rationalist\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalist</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The term </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">rationalist</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> as a description of people is used in a couple of ways. It can refer to someone who endeavors to think better and implement as much rationality as they can. Many prefer </span><span class=\"by_7r4pRYHgRRMuxw9fL\">the </span><span class=\"by_MxvxpRHcxbKfj46vG\">term </span></span><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">aspiring</span></u><span class=\"by_qgdGA4ZEyW7zNdK84\"> rationalist</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to convey </span><span class=\"by_MxvxpRHcxbKfj46vG\">that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the identifier is a claim to the goal of being more rational rather than a claim of having attained </span><span class=\"by_MxvxpRHcxbKfj46vG\">it </span><span class=\"by_qgdGA4ZEyW7zNdK84\">already.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Perhaps more commonly, </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">rationalist</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_MxvxpRHcxbKfj46vG\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">used to refer culturally to someone associated with various rationalist communities separate from their efforts to improve their rationality.</span></span></p>",
      "sections": [
        {
          "title": "Theory / Concepts",
          "anchor": "Theory___Concepts",
          "level": 2
        },
        {
          "title": "Applied Topics",
          "anchor": "Applied_Topics",
          "level": 2
        },
        {
          "title": "Failure Modes",
          "anchor": "Failure_Modes",
          "level": 2
        },
        {
          "title": "Communication",
          "anchor": "Communication",
          "level": 2
        },
        {
          "title": "Techniques",
          "anchor": "Techniques",
          "level": 2
        },
        {
          "title": "Models of the Mind",
          "anchor": "Models_of_the_Mind",
          "level": 2
        },
        {
          "title": "Other",
          "anchor": "Other",
          "level": 2
        },
        {
          "title": "What we're calling \"rationality\"",
          "anchor": "What_we_re_calling__rationality_",
          "level": 1
        },
        {
          "title": "Heuristics and Biases",
          "anchor": "Heuristics_and_Biases",
          "level": 1
        },
        {
          "title": "Instrumental vs Epistemic Rationality",
          "anchor": "Instrumental_vs_Epistemic_Rationality",
          "level": 1
        },
        {
          "title": "The Art and Science of Rationality",
          "anchor": "The_Art_and_Science_of_Rationality",
          "level": 1
        },
        {
          "title": "Rationalist",
          "anchor": "Rationalist",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 13
    },
    "postCount": 2237,
    "description": {
      "markdown": "**Rationality** is the art of thinking in ways that result in [accurate beliefs](https://www.lesswrong.com/tag/world-modeling) and [good decisions](https://www.lesswrong.com/tag/decision-theory). It is the primary topic of LessWrong.  \n  \nRationality is not only about avoiding the vices of [self-deception](https://www.lesswrong.com/tag/self-deception) and obfuscation (the failure to [communicate clearly](https://www.lesswrong.com/tag/conversation-topic)), but also about the virtue of [curiosity](https://www.lesswrong.com/tag/curiosity), seeing the world more clearly than before, and [achieving things](https://www.lesswrong.com/tag/ambition) [previously unreachable](https://www.lesswrong.com/tag/skill-building) [to you](https://www.lesswrong.com/tag/coordination-cooperation). The study of rationality on LessWrong includes a theoretical understanding of ideal cognitive algorithms, as well as building a practice that uses these idealized algorithms to inform [heuristics](https://www.lesswrong.com/tag/heuristics-and-biases), [habits](https://www.lesswrong.com/tag/habits), and [techniques](https://www.lesswrong.com/tag/techniques), to successfully reason and make decisions in the real world.\n\nTopics covered in rationality include (but are not limited to): normative and theoretical explorations of [ideal](https://www.lesswrong.com/tag/solomonoff-induction) [reasoning](https://www.lesswrong.com/tag/probability-and-statistics); the [capabilities and limitations](https://www.lesswrong.com/tag/evolutionary-psychology) [of our brain](https://www.lesswrong.com/tag/neuroscience), [mind and psychology](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2); applied advice such as [introspection](https://www.lesswrong.com/tag/introspection) techniques and [how to achieve truth collaboratively](https://www.lesswrong.com/tag/group-rationality); practical techniques and methodologies for figuring out what’s true ranging from rough quantitative modeling to full research guides.\n\nNote that content about *how the world is* can be found under [World Modeling](https://www.lesswrong.com/tag/world-modeling), and practical advice about *how to change the world* is categorized under [World Optimization](https://www.lesswrong.com/tag/world-optimization) or [Practical](/tag/practical).\n\n* * *\n\n<table style=\"background-color:rgb(255, 255, 255);border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Theory / Concepts</strong></p><p><a href=\"http://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><u>Anticipated Experiences</u></a><br><a href=\"http://www.lesswrong.com/tag/aumann-s-agreement-theorem?showPostCount=true&amp;useTagName=true\">Aumann's Agreement Theorem</a><br><a href=\"http://www.lesswrong.com/tag/bayes-theorem?showPostCount=true&amp;useTagName=true\"><u>Bayes Theorem</u></a><br><a href=\"https://www.lesswrong.com/tag/bounded-rationality?showPostCount=true&amp;useTagName=true\">Bounded Rationality</a><br><a href=\"https://www.lesswrong.com/tag/conservation-of-expected-evidence?showPostCount=true&amp;useTagName=true\">Conservation of Expected</a><br><a href=\"http://www.lesswrong.com/tag/contrarianism?showPostCount=true&amp;useTagName=true\">Contrarianism</a><br><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\">Decision Theory</a><br><a href=\"http://www.lesswrong.com/tag/epistemology?showPostCount=true&amp;useTagName=true\"><u>Epistemology</u></a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a><br><a href=\"https://www.lesswrong.com/tag/gears-level?showPostCount=true&amp;useTagName=true\"><u>Gears-Level</u></a><br><a href=\"http://www.lesswrong.com/tag/hansonian-pre-rationality?useTagName=true&amp;showPostCount=true\">Hansonian Pre-Rationality</a><br><a href=\"https://www.lesswrong.com/tag/law-thinking?showPostCount=true&amp;useTagName=true\">Law-Thinking</a><br><a href=\"http://www.lesswrong.com/tag/map-and-territory?showPostCount=true&amp;useTagName=true\">Map and Territory</a><br><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem?showPostCount=true&amp;useTagName=true\">Newcomb's Problem</a><br><a href=\"http://www.lesswrong.com/tag/occam-s-razor?showPostCount=true&amp;useTagName=true\">Occam's razor</a><br><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\">Robust Agents</a><br><a href=\"https://www.lesswrong.com/tag/solomonoff-induction?showPostCount=true&amp;useTagName=true\">Solomonoff Induction</a><br><a href=\"http://www.lesswrong.com/tag/truth-semantics-and-meaning?showPostCount=true&amp;useTagName=true\">Truth, Semantics, &amp; Meaning</a><br><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\">Utility Functions</a><br>&nbsp;</p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Applied Topics</strong></p><p><a href=\"http://www.lesswrong.com/tag/alief?showPostCount=true&amp;useTagName=true\"><u>Alief</u></a><br><a href=\"https://www.lesswrong.com/tag/betting?showPostCount=true&amp;useTagName=true\">Betting</a><br><a href=\"http://www.lesswrong.com/tag/cached-thoughts?showPostCount=true&amp;useTagName=true\">Cached Thoughts</a><br><a href=\"http://www.lesswrong.com/tag/calibration?showPostCount=true&amp;useTagName=true\">Calibration</a><br><a href=\"https://www.lesswrong.com/tag/dark-arts?showPostCount=true&amp;useTagName=true\">Dark Arts</a><br><a href=\"http://www.lesswrong.com/tag/empiricism?showPostCount=true&amp;useTagName=true\">Empiricism</a><br><a href=\"http://www.lesswrong.com/tag/epistemic-modesty?showPostCount=true&amp;useTagName=true\">Epistemic Modesty</a><br><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction?showPostCount=true&amp;useTagName=true\">Forecasting &amp; Prediction</a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality</a><br><a href=\"https://www.lesswrong.com/tag/identity?showPostCount=true&amp;useTagName=true\">Identity</a><br><a href=\"http://www.lesswrong.com/tag/inside-outside-view?showPostCount=true&amp;useTagName=true\">Inside/Outside View</a><br><a href=\"http://www.lesswrong.com/tag/introspection?showPostCount=true&amp;useTagName=true\"><u>Introspection</u></a><br><a href=\"http://www.lesswrong.com/tag/intuition?showPostCount=true&amp;useTagName=true\">Intuition</a><br><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science?showPostCount=true&amp;useTagName=true\"><u>Practice &amp; Philosophy of Science</u></a><br><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning?showPostCount=true&amp;useTagName=true\">Scholarship &amp; Learning</a><br><a href=\"http://www.lesswrong.com/tag/taking-ideas-seriously?showPostCount=true&amp;useTagName=true\">Taking Ideas Seriously</a><br><a href=\"https://www.lesswrong.com/tag/value-of-information?showPostCount=true&amp;useTagName=true\">Value of Information</a><br>&nbsp;</p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33.33%\"><p><strong>Failure Modes</strong></p><p><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\">Affect Heuristic</a><br><a href=\"https://www.lesswrong.com/tag/bucket-errors?showPostCount=true&amp;useTagName=true\">Bucket Errors</a><br><a href=\"https://www.lesswrong.com/tag/compartmentalization?showPostCount=true&amp;useTagName=true\">Compartmentalization</a><br><a href=\"https://www.lesswrong.com/tag/confirmation-bias?showPostCount=true&amp;useTagName=true\"><u>Confirmation Bias</u></a><br><a href=\"https://www.lesswrong.com/tag/logical-fallacies?showPostCount=true&amp;useTagName=true\">Fallacies</a><br><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\">Goodhart’s Law</a><br><a href=\"http://www.lesswrong.com/tag/groupthink?showPostCount=true&amp;useTagName=true\"><u>Groupthink</u></a><br><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases?showPostCount=true&amp;useTagName=true\">Heuristics and Biases</a><br><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy?showPostCount=true&amp;useTagName=true\">Mind Projection Fallacy</a><br><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><u>Motivated Reasoning</u></a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true&amp;useTagName=true\">Pica</a><br><a href=\"https://www.lesswrong.com/tag/pitfalls-of-rationality?showPostCount=true&amp;useTagName=true\">Pitfalls of Rationality</a><br><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\">Rationalization</a>&nbsp;<br><a href=\"https://www.lesswrong.com/tag/self-deception?showPostCount=true&amp;useTagName=true\">Self-Deception</a><br><a href=\"https://www.lesswrong.com/tag/sunk-cost-fallacy?showPostCount=true&amp;useTagName=true\">Sunk-Cost Fallacy</a></p></td></tr><tr><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Communication</strong></p><p><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge</u></a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true\"><u>Conversation</u></a><br><a href=\"https://www.lesswrong.com/tag/decoupling-vs-contextualizing?showPostCount=true&amp;useTagName=true\"><u>Decoupling vs Contextualizing</u></a><br><a href=\"https://www.lesswrong.com/tag/disagreement?showPostCount=true&amp;useTagName=true\"><u>Disagreement</u></a><br><a href=\"http://www.lesswrong.com/tag/distillation-and-pedagogy?showPostCount=true&amp;useTagName=true\">Distillation &amp; Pedagogy</a><br><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux</u></a><br><a href=\"http://www.lesswrong.com/tag/good-explanations-advice?showPostCount=true&amp;useTagName=true\">Good Explanations (Advice)</a><br><a href=\"http://www.lesswrong.com/tag/ideological-turing-tests?showPostCount=true&amp;useTagName=true\">Ideological Turing Tests</a><br><a href=\"https://www.lesswrong.com/tag/inferential-distance?showPostCount=true&amp;useTagName=true\">Inferential Distance</a><br><a href=\"https://www.lesswrong.com/tag/information-cascades?showPostCount=true&amp;useTagName=true\">Information Cascades</a><br><a href=\"https://www.lesswrong.com/tag/memetic-immune-system?showPostCount=true&amp;useTagName=true\">Memetic Immune System</a><br><a href=\"https://www.lesswrong.com/tag/philosophy-of-language?showPostCount=true&amp;useTagName=true\"><u>Philosophy of Language</u></a><br><a href=\"https://www.lesswrong.com/tag/steelmanning?showPostCount=true&amp;useTagName=true\">Steelmanning</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\" rowspan=\"2\"><p><strong>Techniques</strong></p><p><a href=\"http://www.lesswrong.com/tag/double-crux?showPostCount=true&amp;useTagName=true\"><u>Double-Crux</u></a><br><a href=\"https://www.lesswrong.com/tag/focusing?showPostCount=true&amp;useTagName=true\">Focusing</a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring</a><br><a href=\"https://www.lesswrong.com/tag/internal-double-crux?showPostCount=true&amp;useTagName=true\">Internal Double Crux</a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\"><u>Hamming Questions</u></a><br><a href=\"https://www.lesswrong.com/tag/noticing?showPostCount=true&amp;useTagName=true\">Noticing</a><br><a href=\"https://www.lesswrong.com/tag/techniques?showPostCount=true&amp;useTagName=true\">Techniques</a><br><a href=\"https://www.lesswrong.com/tag/trigger-action-planning?showPostCount=true&amp;useTagName=true\">Trigger Action Planning/Patterns</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><p><strong>Models of the Mind</strong></p><p><a href=\"https://www.lesswrong.com/tag/consciousness?showPostCount=true&amp;useTagName=true\">Consciousness</a><br><a href=\"https://www.lessestwrong.com/tag/dual-process-theory-system-1-and-system-2?showPostCount=true&amp;useTagName=false\">Dual Process Theory (System 1 &amp; 2)</a><br><a href=\"http://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><u>General Intelligence</u></a><br><a href=\"https://www.lesswrong.com/tag/subagents?showPostCount=true&amp;useTagName=true\"><u>Subagents</u></a><br><a href=\"https://www.lesswrong.com/tag/predictive-processing?showPostCount=true&amp;useTagName=true\"><u>Predictive Processing</u></a><br><a href=\"https://www.lesswrong.com/tag/perceptual-control-theory?showPostCount=true&amp;useTagName=true\">Perceptual Control Theory</a><br><a href=\"http://www.lesswrong.com/tag/zombies?showPostCount=true&amp;useTagName=true\">Zombies</a><br>&nbsp;</p></td></tr><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px\"><p><strong>Other</strong></p><p><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar?showPostCount=true\"><u>Center for Applied Rationality</u></a><br><a href=\"https://www.lesswrong.com/tag/curiosity?useTagName=true&amp;showPostCount=true\"><u>Curiosity</u></a><br><a href=\"https://www.lesswrong.com/tag/rationality-quotes?showPostCount=true&amp;useTagName=true\">Rationality Quotes</a><br><a href=\"http://www.lesswrong.com/tag/updated-beliefs-examples-of?showPostCount=true&amp;useTagName=true\">Updated Beliefs (examples of)</a></p></td></tr></tbody></table>\n\n*This list is not comprehensive! The tagging system is new. Many needed tags have not been created and/or added to the above list.*\n\n* * *\n\nWhat we're calling \"rationality\"\n--------------------------------\n\nA good heuristic is that rationality is about cognitive algorithms. Rather than being a synonym for *true* or *optimal*, the term ***rational*** should be reserved for describing whether or not a cognitive algorithm results in true beliefs and optimal actions.\n\nThis is distinct from [practical advice](https://www.lesswrong.com/tag/practical-advice), such as how to improve relationships or implement productivity systems, which should not be considered \"rationality\" per se. Some have pushed against labeling self-help as \"rational dating\", etc. for reasons along these lines \\[[1](https://www.lesswrong.com/posts/HcCpvYLoSFP4iAqSz/rationality-appreciating-cognitive-algorithms), [2](https://www.lesswrong.com/posts/DFHhuAMexXAi8T6AY/the-rational-rationalist-s-guide-to-rationally-using)\\], and they are probably correct.\n\nIn accordance with this, LessWrong classifies most self-help type advice under the [World Optimization](https://www.lesswrong.com/tag/world-optimization) tag and not the Rationality tag.\n\nSimilarly, most object-level material about *how the world is*, e.g. math, biology, history, etc. is tagged under [World Modeling](https://www.lesswrong.com/tag/world-modeling) tag, with exceptions for neuroscience and probability theory, etc., which have concrete consequences for how one ought to think.\n\nHeuristics and Biases\n---------------------\n\nEarly material on LessWrong frequently describes rationality with reference to heuristics and biases \\[[1,](https://www.lesswrong.com/posts/xLm9mgJRPvmPGpo7Q/the-cognitive-science-of-rationality) [2](https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic)\\]. Indeed, LessWrong grew out of the blog [Overcoming Bias](https://www.overcomingbias.com/) and even [Rationality: A-Z](https://www.lesswrong.com/rationality) opens with a discussion of biases \\[[1](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM/p/ptxnyfLWqRZ98wnYi)\\] with the opening chapter titled [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM). The idea is that human mind has been shown to systematically make certain errors of reasoning, like confirmation bias. Rationality then consists of overcoming these biases.\n\nApart from the issue of the replication crises which discredited many examples of bias that were commonly referenced on LessWrong, e.g. priming, the \"overcoming biases\" frame of rationality is too limited. Rationality requires the development of many *positive* skills, not just removing *negative* biases to reveal underlying perfect reasoning. These are skills such as how to update the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more.\n\nInstrumental vs Epistemic Rationality\n-------------------------------------\n\nClassically, on LessWrong, a distinction has been made between *instrumental* rationality  and *epistemic* rationality, however, these terms may be misleading – it's not as though epistemic rationality can be traded off for gains in instrumental rationality. Only apparently, and to think one should do this is a trap.\n\n**Instrumental rationality** is defined as being concerned with achieving goals. More specifically, instrumental rationality is the art of choosing and *implementing* actions that steer the future toward outcomes ranked higher in one's preferences. Said preferences are not limited to 'selfish' preferences or unshared values; they include anything one cares about.\n\n**Epistemic rationality** is defined as the part of rationality which involves achieving accurate beliefs about the world. It involves [updating](https://wiki.lesswrong.com/wiki/updating) on receiving new [evidence](https://lessestwrong.com/tag/evidence), mitigating cognitive biases, and examining why you believe what you believe. It can be seen as a form of instrumental rationality in which knowledge and truth are goals in themselves, whereas in other forms of instrumental rationality, knowledge and truth are only potential aids to achieving goals. Someone practicing instrumental rationality might even find falsehood useful.\n\nThe Art and Science of Rationality\n----------------------------------\n\nIn a field like biology, we can draw a distinction between the *science of biology*, which involves various theories and empirical data about biological life, and the *art of being a biologis*t, which is the specific way that a biologist thinks and plays with ideas and interacts to the world around them. Similarly, rationality is both a science and an art. There’s study of the iron-clad laws of reasoning and mechanics of the human mind, but there’s also the general training to be the kind of person who reasons well.\n\nRationalist\n-----------\n\nThe term *rationalist* as a description of people is used in a couple of ways. It can refer to someone who endeavors to think better and implement as much rationality as they can. Many prefer the term *aspiring rationalist* to convey that the identifier is a claim to the goal of being more rational rather than a claim of having attained it already.\n\nPerhaps more commonly, *rationalist* is used to refer culturally to someone associated with various rationalist communities separate from their efforts to improve their rationality."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pnSDArjzAjkvAF5Jo",
    "name": "Efficient Market Hypothesis",
    "core": false,
    "slug": "efficient-market-hypothesis",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Efficient Market Hypothesis</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> states that existing market prices already account for all available information, and that it is therefore impossible to exploit the market unless you have information other traders don't.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 38,
    "description": {
      "markdown": "The **Efficient Market Hypothesis** states that existing market prices already account for all available information, and that it is therefore impossible to exploit the market unless you have information other traders don't."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2EFq8dJbxKNzforjM",
    "name": "Social Status",
    "core": false,
    "slug": "social-status",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Social Status </span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\">is an abstraction to model how people relate to each other, how</span><span class=\"by_LoykQRMTxJFxwwdPy\"> social </span><span class=\"by_XtphY3uYHwruKqDyG\">hierarchies are formed,</span><span class=\"by_LoykQRMTxJFxwwdPy\"> and how </span><span class=\"by_XtphY3uYHwruKqDyG\">people facilitate trade in the absence of financial accounting (as well as a variety of other stuff). I mean, everyone knows what status is, but here</span><span class=\"by_LoykQRMTxJFxwwdPy\"> is </span><span class=\"by_XtphY3uYHwruKqDyG\">where we break that down into </span><span class=\"by_BFEqRp75ExagMkz34\">its</span><span class=\"by_XtphY3uYHwruKqDyG\"> components and really try to understand what's happening on a mechanistic level.</span></span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/signaling\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Signaling</span></a></li></ul><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/13s/the_nature_of_offense/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Nature of Offense</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"http://weidai.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wei Dai</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - People are </span><a href=\"https://lessestwrong.com/tag/offense\"><span class=\"by_qgdGA4ZEyW7zNdK84\">offended</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by grabs for status.</span></li><li><a href=\"https://lessestwrong.com/lw/154/why_real_men_wear_pink/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why Real Men Wear Pink</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Yvain</span></a></li><li><a href=\"http://www.overcomingbias.com/2009/08/actors-see-status.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Actors See Status</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, quoting </span><a href=\"https://en.wikipedia.org/wiki/Keith_Johnstone\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Keith Johnstone</span></a></li><li><a href=\"https://lessestwrong.com/lw/1kr/that_other_kind_of_status/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">That Other Kind of Status</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Yvain</span></li></ul><h2 id=\"External\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Melting Asphault (by Kevin Simler) has many great posts on status</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Elephant in the Brain by Simler and Hanson</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Impro (book on improv covering status relations)</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Writings by Venkatesh Rao such as Gervais Principle and something, something Psychopath</span></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "External",
          "anchor": "External",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 88,
    "description": {
      "markdown": "**Social Status** is an abstraction to model how people relate to each other, how social hierarchies are formed, and how people facilitate trade in the absence of financial accounting (as well as a variety of other stuff). I mean, everyone knows what status is, but here is where we break that down into its components and really try to understand what's happening on a mechanistic level.\n\nSee Also\n--------\n\n*   [Signaling](https://lessestwrong.com/tag/signaling)\n\nNotable Posts\n-------------\n\n*   [The Nature of Offense](https://lessestwrong.com/lw/13s/the_nature_of_offense/) by [Wei Dai](http://weidai.com/) \\- People are [offended](https://lessestwrong.com/tag/offense) by grabs for status.\n*   [Why Real Men Wear Pink](https://lessestwrong.com/lw/154/why_real_men_wear_pink/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Actors See Status](http://www.overcomingbias.com/2009/08/actors-see-status.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson), quoting [Keith Johnstone](https://en.wikipedia.org/wiki/Keith_Johnstone)\n*   [That Other Kind of Status](https://lessestwrong.com/lw/1kr/that_other_kind_of_status/) by Yvain\n\nExternal\n--------\n\n*   Melting Asphault (by Kevin Simler) has many great posts on status\n*   Elephant in the Brain by Simler and Hanson\n*   Impro (book on improv covering status relations)\n*   Writings by Venkatesh Rao such as Gervais Principle and something, something Psychopath"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fkABsGCJZ6y9qConW",
    "name": "Practical",
    "core": true,
    "slug": "practical",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Practical</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_r38pkCm7wF4M44MDQ\">posts give direct,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> actionable advice on how to achieve goals and generally succeed. </span><span class=\"by_nLbwLhBaQeG6tCNDN\">The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many </span><span class=\"by_EQNTWXLKMeWMp2FQS\">places</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> on the internet will give you advice; here, we </span><span class=\"by_EQNTWXLKMeWMp2FQS\">value</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.</span></span></p><p><span><span class=\"by_nLbwLhBaQeG6tCNDN\">Material</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> that is directly about </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how to think better</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> can be found at </span><a href=\"https://www.lessestwrong.com/tag/rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><h1 id=\"___________________________________________Practical_Sub_Topics\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Practical Sub-Topics</span></strong></h1><figure style=\"width:100%;\"><table style=\"background-color:rgb(255, 255, 255);border-bottom:2px solid hsl(0, 0%, 90%);border-left:2px solid hsl(0, 0%, 90%);border-right:2px solid hsl(0, 0%, 90%);border-top:2px solid hsl(0, 0%, 90%);\"><tbody><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%;\" rowspan=\"2\"><p><strong id=\"Domains_of_Well_being\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Domains of Well-being</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Careers</span></a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Emotions</span></a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Exercise (Physical)</span></a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Financial Investing</span></a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Gratitude</span></a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Happiness</span></a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Human Bodies</span></a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Nutrition</span></a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Parenting</span></a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack</span></a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sleep</span></a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Well-being</span></a></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%;\" rowspan=\"2\"><p><strong id=\"Skills__Tools__Techniques\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Skills, Tools, Techniques</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cryonics</span></a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Emotions</span></a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goal Factoring</span></a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Habits</span></a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hamming Questions</span></a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Life Improvements</span></a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meditation</span></a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">More Dakka</span></a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Pica</span></u></a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Planning &amp; Decision-Making</span></a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Self Experimentation</span></a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Skill Building</span></a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Software Tools</span></a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Spaced Repetition</span></a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Virtues (Instrumental)</span></a></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33%;\"><p><strong id=\"Productivity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Productivity</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Akrasia</span></a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivations</span></a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prioritization</span></a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Procrastination</span></a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Productivity</span></a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Willpower</span></a></p></td></tr><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Interpersonal</span></strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Circling</span></u></a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Conversation (topic)</span></a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Communication Cultures</span></a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Relationship</span></u></a></td></tr></tbody></table></figure>",
      "sections": [
        {
          "title": "                                           Practical Sub-Topics",
          "anchor": "___________________________________________Practical_Sub_Topics",
          "level": 1
        },
        {
          "title": "Domains of Well-being",
          "anchor": "Domains_of_Well_being",
          "level": 2
        },
        {
          "title": "Skills, Tools, Techniques",
          "anchor": "Skills__Tools__Techniques",
          "level": 2
        },
        {
          "title": "Productivity",
          "anchor": "Productivity",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 1615,
    "description": {
      "markdown": "**Practical** posts give direct, actionable advice on how to achieve goals and generally succeed. The art of rationality would be useless if it did not connect to the real world; we must take our ideas and abstractions and collide them with reality. Many places on the internet will give you advice; here, we value survey data, literature reviews, self-blinded trials, quantitative estimates, and theoretical models that aim to explain the phenomena.\n\nMaterial that is directly about *how to think better* can be found at [Rationality](https://www.lessestwrong.com/tag/rationality).\n\n**                                           Practical Sub-Topics**\n===================================================================\n\n<table style=\"background-color:rgb(255, 255, 255);border-bottom:2px solid hsl(0, 0%, 90%);border-left:2px solid hsl(0, 0%, 90%);border-right:2px solid hsl(0, 0%, 90%);border-top:2px solid hsl(0, 0%, 90%)\"><tbody><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Domains of Well-being</strong></p><p><a href=\"http://www.lesswrong.com/tag/careers?showPostCount=true&amp;useTagName=true\">Careers</a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions</a><br><a href=\"http://www.lesswrong.com/tag/exercise-physical?showPostCount=true&amp;useTagName=true\">Exercise (Physical)</a><br><a href=\"https://www.lesswrong.com/tag/financial-investing?showPostCount=true&amp;useTagName=true\">Financial Investing</a><br><a href=\"http://www.lesswrong.com/tag/gratitude?showPostCount=true&amp;useTagName=true\">Gratitude</a><br><a href=\"http://www.lesswrong.com/tag/happiness-1?showPostCount=true&amp;useTagName=true\">Happiness</a><br><a href=\"http://www.lesswrong.com/tag/human-bodies?showPostCount=true&amp;useTagName=true\">Human Bodies</a><br><a href=\"http://www.lesswrong.com/tag/nutrition?showPostCount=true&amp;useTagName=true\">Nutrition</a><br><a href=\"https://www.lesswrong.com/tag/parenting?showPostCount=true&amp;useTagName=true\">Parenting</a><br><a href=\"https://www.lesswrong.com/tag/slack?showPostCount=true&amp;useTagName=true\">Slack</a><br><a href=\"https://www.lesswrong.com/tag/sleep?showPostCount=true&amp;useTagName=true\">Sleep</a><br><a href=\"https://www.lesswrong.com/tag/well-being?showPostCount=true&amp;useTagName=true\">Well-being</a></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\" rowspan=\"2\"><p><strong>Skills, Tools, Techniques</strong></p><p><a href=\"https://www.lesswrong.com/tag/cryonics?showPostCount=true&amp;useTagName=true\">Cryonics</a><br><a href=\"https://www.lesswrong.com/tag/emotions?showPostCount=true&amp;useTagName=true\">Emotions</a><br><a href=\"https://www.lesswrong.com/tag/goal-factoring?showPostCount=true&amp;useTagName=true\">Goal Factoring</a><br><a href=\"http://www.lesswrong.com/tag/habits?showPostCount=true&amp;useTagName=true\">Habits</a><br><a href=\"https://www.lesswrong.com/tag/hamming-questions?showPostCount=true&amp;useTagName=true\">Hamming Questions</a><br><a href=\"http://www.lesswrong.com/tag/life-improvements?showPostCount=true&amp;useTagName=true\">Life Improvements</a><br><a href=\"https://www.lesswrong.com/tag/meditation?showPostCount=true&amp;useTagName=true\">Meditation</a><br><a href=\"http://www.lesswrong.com/tag/more-dakka?showPostCount=true&amp;useTagName=true\">More Dakka</a><br><a href=\"https://www.lesswrong.com/tag/pica?showPostCount=true\"><u>Pica</u></a><br><a href=\"https://www.lesswrong.com/tag/planning-and-decision-making?showPostCount=true&amp;useTagName=true\">Planning &amp; Decision-Making</a><br><a href=\"https://www.lesswrong.com/tag/self-experimentation?showPostCount=true&amp;useTagName=true\">Self Experimentation</a><br><a href=\"http://www.lesswrong.com/tag/skill-building?showPostCount=true&amp;useTagName=true\">Skill Building</a><br><a href=\"https://www.lesswrong.com/tag/software-tools?showPostCount=true&amp;useTagName=true\">Software Tools</a><br><a href=\"https://www.lesswrong.com/tag/spaced-repetition?showPostCount=true&amp;useTagName=true\">Spaced Repetition</a><br><a href=\"https://www.lesswrong.com/tag/virtues-instrumental?showPostCount=true&amp;useTagName=false\">Virtues (Instrumental)</a></p></td><td style=\"border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Productivity</strong></p><p><a href=\"https://www.lesswrong.com/tag/akrasia?showPostCount=true&amp;useTagName=true\">Akrasia</a><br><a href=\"https://www.lesswrong.com/tag/motivations?showPostCount=true&amp;useTagName=true\">Motivations</a><br><a href=\"https://www.lesswrong.com/tag/prioritization?showPostCount=true&amp;useTagName=true\">Prioritization</a><br><a href=\"https://www.lesswrong.com/tag/procrastination?showPostCount=true&amp;useTagName=true\">Procrastination</a><br><a href=\"https://www.lesswrong.com/tag/productivity?showPostCount=true&amp;useTagName=true\">Productivity</a><br><a href=\"https://www.lesswrong.com/tag/willpower?showPostCount=true&amp;useTagName=true\">Willpower</a></p></td></tr><tr><td style=\"border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top\"><strong>Interpersonal</strong><br><a href=\"http://www.lesswrong.com/tag/circling?showPostCount=true&amp;useTagName=true\"><u>Circling</u></a><br><a href=\"https://www.lesswrong.com/tag/conversation-topic?showPostCount=true&amp;useTagName=true\">Conversation (topic)</a><br><a href=\"https://www.lesswrong.com/tag/communication-cultures?showPostCount=true&amp;useTagName=true\">Communication Cultures</a><br><a href=\"http://www.lesswrong.com/tag/relationships-interpersonal?showPostCount=true&amp;useTagName=false\"><u>Relationship</u></a></td></tr></tbody></table>"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "b8FHrKqyXuYGWc6vn",
    "name": "Game Theory",
    "core": null,
    "slug": "game-theory",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Game</span><span class=\"by_cn4SiEmqWbu7K9em5\"> theory</span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_woC2b5rav5sGrAo3E\">is the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">formal study of how rational actors interact</span><span class=\"by_woC2b5rav5sGrAo3E\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">pursue incentives. It investigates situations</span><span class=\"by_woC2b5rav5sGrAo3E\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">conflict</span><span class=\"by_woC2b5rav5sGrAo3E\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">cooperation.</span></span></p><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">See also: </span></i><a href=\"https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Coalition/coordination</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Coalitional Instincts</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Decision theory</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Moloch</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Utility functions</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://lessestwrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/prisoner-s-dilemma\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner's Dilemma</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Game theory is an extremely powerful and robust tool in analyzing much more complex situations, such as: mergers and acquisitions, political economy, voting systems, war bargaining and biological evolution. Eight game-theorists have won the Nobel Prize in Economic Sciences.</span></p><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ul><li><a href=\"http://levine.sscnet.ucla.edu/general/whatis.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Naïve introduction to Game Theory</span></a></li><li><a href=\"http://plato.stanford.edu/entries/game-theory/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Stanford Encyclopedia entry on Game Theory</span></a></li></ul>",
      "sections": [
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 177,
    "description": {
      "markdown": "**Game theory** is the formal study of how rational actors interact to pursue incentives. It investigates situations of conflict and cooperation.\n\n*See also:* [Coalition/coordination](https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&useTagName=true), [Coalitional Instincts](https://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&useTagName=true), [Decision theory](https://www.lesswrong.com/tag/decision-theory), [Moloch](https://www.lesswrong.com/tag/moloch?showPostCount=true&useTagName=true), [Utility functions](https://www.lesswrong.com/tag/utility-functions), [Decision Theory](https://lessestwrong.com/tag/decision-theory), [Prisoner's Dilemma](https://lessestwrong.com/tag/prisoner-s-dilemma)\n\nGame theory is an extremely powerful and robust tool in analyzing much more complex situations, such as: mergers and acquisitions, political economy, voting systems, war bargaining and biological evolution. Eight game-theorists have won the Nobel Prize in Economic Sciences.\n\nReferences\n----------\n\n*   [Naïve introduction to Game Theory](http://levine.sscnet.ucla.edu/general/whatis.htm)\n*   [Stanford Encyclopedia entry on Game Theory](http://plato.stanford.edu/entries/game-theory/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "izp6eeJJEg9v5zcur",
    "name": "Community",
    "core": true,
    "slug": "community",
    "tableOfContents": {
      "html": "<p><span class=\"by_nLbwLhBaQeG6tCNDN\">The </span><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">LessWrong</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Community </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is </span><span class=\"by_nLbwLhBaQeG6tCNDN\">the people who write on </span><span class=\"by_EQNTWXLKMeWMp2FQS\">LessWrong </span><span class=\"by_nLbwLhBaQeG6tCNDN\">and</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> who</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> contribute to </span><span class=\"by_EQNTWXLKMeWMp2FQS\">its</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> mission of refining the art of human rationality. This</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag </span><span class=\"by_nLbwLhBaQeG6tCNDN\">includes community events, analysis of the </span><span class=\"by_EQNTWXLKMeWMp2FQS\">health, norms and</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> direction of the community, and space to understand communities in </span><span class=\"by_EQNTWXLKMeWMp2FQS\">general.</span></span><br><br><span class=\"by_nLbwLhBaQeG6tCNDN\">LessWrong also has many brothers and sisters like the Berkeley Rationality Community, </span><a href=\"https://www.reddit.com/r/slatestarcodex/\"><span class=\"by_nLbwLhBaQeG6tCNDN\">SlateStarCodex</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, </span><a href=\"https://www.reddit.com/r/rational/\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Rational Fiction</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\">, </span><a href=\"https://forum.effectivealtruism.org/\"><span class=\"by_nLbwLhBaQeG6tCNDN\">Effective Altruism</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\">, </span><a href=\"https://www.alignmentforum.org/\"><span class=\"by_nLbwLhBaQeG6tCNDN\">AI Alignment</span></a><span><span class=\"by_nLbwLhBaQeG6tCNDN\">, and more,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> who participate </span><span class=\"by_nLbwLhBaQeG6tCNDN\">here. To see upcoming</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> LessWrong </span><span class=\"by_nLbwLhBaQeG6tCNDN\">events, go to the </span></span><a href=\"https://www.lesswrong.com/community\"><span class=\"by_nLbwLhBaQeG6tCNDN\">community section</span></a><span class=\"by_nLbwLhBaQeG6tCNDN\">.</span></p><hr><h2 id=\"___________________________________Community_Sub_Topics\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;Community Sub-Topics</span></strong></h2><figure style=\"width:100%;\"><table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%);\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%;\"><p><strong id=\"All\"><span class=\"by_qgdGA4ZEyW7zNdK84\">All</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/bounties-active?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bounties (active)</span></a><br><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities?showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Grants &amp; Fundraising</span></a><br><a href=\"http://www.lesswrong.com/tag/growth-stories?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Growth Stories</span></a><br><a href=\"https://www.lesswrong.com/tag/online-socialization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Online Socialization</span></a><br><a href=\"https://www.lesswrong.com/tag/petrov-day?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Petrov Day</span></a><br><a href=\"https://www.lesswrong.com/tag/public-discourse?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Public Discourse</span></a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Research Agendas</span></a><br><a href=\"https://www.lesswrong.com/tag/ritual?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ritual</span></a><br><a href=\"https://www.lesswrong.com/tag/solstice-celebration?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Solstice Celebration</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%;\"><p><strong id=\"LessWrong\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/events-community?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Events (Community)</span></a><br><a href=\"https://www.lesswrong.com/tag/site-meta?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Site Meta</span></a><br><a href=\"https://www.lesswrong.com/tag/greaterwrong-meta?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">GreaterWrong Meta</span></a><br><a href=\"https://www.lesswrong.com/tag/lesswrong-events?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong Events</span></a><br><a href=\"http://www.lesswrong.com/tag/lw-moderation?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LW Moderation</span></a><br><a href=\"http://www.lesswrong.com/tag/meetups-topic?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meetups (topic)</span></a><br><a href=\"http://www.lesswrong.com/tag/moderation-topic?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Moderation (topic)</span></a><br><a href=\"http://www.lesswrong.com/tag/the-sf-bay-area?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The SF Bay Area</span></a><br><a href=\"http://www.lesswrong.com/tag/tagging?showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Tagging</span></a></p></td></tr></tbody></table></figure><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Not all Community posts are tagged with subtopics.</span></i></p><hr><p><span class=\"by_qgdGA4ZEyW7zNdK84\">This tag applies to any post about:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Specific projects, orgs, and prizes [e.g. </span><a href=\"http://www.lesswrong.com/posts/xFGQdgJndLcthgWoE\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">4</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">5</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Requests and offers for help [</span><a href=\"http://www.lesswrong.com/posts/bSWavBThj6ebB62gD\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/LuL7LLqcdmM7TTYvW\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/x72ta8C3dKu2QRfPv\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Announcements, retrospectives, funding requests, and AMAs from orgs [</span><a href=\"http://www.lesswrong.com/posts/XJiNtvxoiLCpBn6FH\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/posts/96N8BT9tJvybLbn5z/we-run-the-center-for-applied-rationality-ama\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">4</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/posts/tCHsm5ZyAca8HfJSG\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">5</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Discussions of the orgs in the LessWrong, Rationalist cluster [</span><a href=\"http://www.lesswrong.com/posts/KpnyCT7CZy4Qe6kx6\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Discussions about the LessWrong, Rationalist, and related communities [</span><a href=\"http://www.lesswrong.com/posts/2Ee5DPBxowTTXZ6zf\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://www.lesswrong.com/posts/yGycR8tFA3JJbvApp\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/posts/zAqoj79A7QuhJKKvi\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">While the </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization</span></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">core tag is for posts discussing how to do good in general, the Community tag is for the specific, concrete efforts of our community to execute plans.</span></p>",
      "sections": [
        {
          "title": "                                   Community Sub-Topics",
          "anchor": "___________________________________Community_Sub_Topics",
          "level": 1
        },
        {
          "title": "All",
          "anchor": "All",
          "level": 2
        },
        {
          "title": "LessWrong",
          "anchor": "LessWrong",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 1082,
    "description": {
      "markdown": "The **LessWrong** **Community** is the people who write on LessWrong and who contribute to its mission of refining the art of human rationality. This tag includes community events, analysis of the health, norms and direction of the community, and space to understand communities in general.  \n  \nLessWrong also has many brothers and sisters like the Berkeley Rationality Community, [SlateStarCodex](https://www.reddit.com/r/slatestarcodex/), [Rational Fiction](https://www.reddit.com/r/rational/), [Effective Altruism](https://forum.effectivealtruism.org/), [AI Alignment](https://www.alignmentforum.org/), and more, who participate here. To see upcoming LessWrong events, go to the [community section](https://www.lesswrong.com/community).\n\n* * *\n\n**                                   Community Sub-Topics**\n-----------------------------------------------------------\n\n<table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%\"><p><strong>All</strong></p><p><a href=\"http://www.lesswrong.com/tag/bounties-active?showPostCount=true&amp;useTagName=true\">Bounties (active)</a><br><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities?showPostCount=true\">Grants &amp; Fundraising</a><br><a href=\"http://www.lesswrong.com/tag/growth-stories?showPostCount=true&amp;useTagName=true\">Growth Stories</a><br><a href=\"https://www.lesswrong.com/tag/online-socialization?showPostCount=true&amp;useTagName=true\">Online Socialization</a><br><a href=\"https://www.lesswrong.com/tag/petrov-day?showPostCount=true&amp;useTagName=true\">Petrov Day</a><br><a href=\"https://www.lesswrong.com/tag/public-discourse?showPostCount=true&amp;useTagName=true\">Public Discourse</a><br><a href=\"https://www.lesswrong.com/tag/research-agendas?showPostCount=true&amp;useTagName=true\">Research Agendas</a><br><a href=\"https://www.lesswrong.com/tag/ritual?showPostCount=true&amp;useTagName=true\">Ritual</a><br><a href=\"https://www.lesswrong.com/tag/solstice-celebration?showPostCount=true&amp;useTagName=true\">Solstice Celebration</a><br>&nbsp;</p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:50%\"><p><strong>LessWrong</strong></p><p><a href=\"http://www.lesswrong.com/tag/events-community?showPostCount=true&amp;useTagName=true\">Events (Community)</a><br><a href=\"https://www.lesswrong.com/tag/site-meta?showPostCount=true&amp;useTagName=true\">Site Meta</a><br><a href=\"https://www.lesswrong.com/tag/greaterwrong-meta?showPostCount=true&amp;useTagName=true\">GreaterWrong Meta</a><br><a href=\"https://www.lesswrong.com/tag/lesswrong-events?showPostCount=true&amp;useTagName=true\">LessWrong Events</a><br><a href=\"http://www.lesswrong.com/tag/lw-moderation?showPostCount=true&amp;useTagName=true\">LW Moderation</a><br><a href=\"http://www.lesswrong.com/tag/meetups-topic?showPostCount=true&amp;useTagName=true\">Meetups (topic)</a><br><a href=\"http://www.lesswrong.com/tag/moderation-topic?showPostCount=true&amp;useTagName=true\">Moderation (topic)</a><br><a href=\"http://www.lesswrong.com/tag/the-sf-bay-area?showPostCount=true&amp;useTagName=true\">The SF Bay Area</a><br><a href=\"http://www.lesswrong.com/tag/tagging?showPostCount=true\">Tagging</a></p></td></tr></tbody></table>\n\n*Not all Community posts are tagged with subtopics.*\n\n* * *\n\nThis tag applies to any post about:\n\n*   Specific projects, orgs, and prizes \\[e.g. [1](http://www.lesswrong.com/posts/xFGQdgJndLcthgWoE), [2](http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH), [3](http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN), [4](http://www.lesswrong.com/posts/cSzaxcmeYW6z7cgtc), [5](https://www.lesswrong.com/posts/nDHbgjdddG5EN6ocg)\\]\n*   Requests and offers for help \\[[1](http://www.lesswrong.com/posts/bSWavBThj6ebB62gD), [2](http://www.lesswrong.com/posts/LuL7LLqcdmM7TTYvW), [3](http://www.lesswrong.com/posts/x72ta8C3dKu2QRfPv)\\]\n*   Announcements, retrospectives, funding requests, and AMAs from orgs \\[[1](http://www.lesswrong.com/posts/XJiNtvxoiLCpBn6FH) [2](https://www.lesswrong.com/posts/96N8BT9tJvybLbn5z/we-run-the-center-for-applied-rationality-ama) [3](http://www.lesswrong.com/posts/KgFrtaajjfSnBSZoH), [4](http://www.lesswrong.com/posts/auL2gAGTb3MsYhCeN), [5](https://www.lesswrong.com/posts/tCHsm5ZyAca8HfJSG)\\]\n*   Discussions of the orgs in the LessWrong, Rationalist cluster \\[[1](http://www.lesswrong.com/posts/KpnyCT7CZy4Qe6kx6), [2](https://www.lesswrong.com/posts/6SGqkCgHuNr7d4yJm/thoughts-on-the-singularity-institute-si)\\]\n*   Discussions about the LessWrong, Rationalist, and related communities \\[[1](http://www.lesswrong.com/posts/2Ee5DPBxowTTXZ6zf), [2](http://www.lesswrong.com/posts/yGycR8tFA3JJbvApp), [3](https://www.lesswrong.com/posts/zAqoj79A7QuhJKKvi)\\]\n\nWhile the [World Optimization](https://www.lesswrong.com/tag/world-optimization)  core tag is for posts discussing how to do good in general, the Community tag is for the specific, concrete efforts of our community to execute plans."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xexCWMyds6QLWognu",
    "name": "World Optimization",
    "core": true,
    "slug": "world-optimization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">World Optimization </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is the </span><span class=\"by_EQNTWXLKMeWMp2FQS\">full use of our agency.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_nLbwLhBaQeG6tCNDN\">It is extending the reach of human civilization. It is building cities and democracies and economic systems and computers and flight and science and space rockets and the internet. World optimization is about adding to that list.&nbsp;</span></span><br><br><span class=\"by_nLbwLhBaQeG6tCNDN\">But it’s not just about growth, it’s also about preservation. We are still in the dawn of civilization, with most of civilization in the billions of years ahead. We mustn’t let this light go out.</span></p><hr><h1 id=\"______________________________World_Optimization_Sub_Topics\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; World Optimization Sub-Topics</span></strong></h1><figure style=\"width:100%;\"><table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%);\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33%;\"><p><strong id=\"Moral_Theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Moral Theory</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/altruism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Altruism</span></a><br><a href=\"https://www.lesswrong.com/tag/consequentialism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consequentialism</span></a><br><a href=\"https://www.lesswrong.com/tag/deontology?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Deontology</span></a><br><a href=\"http://www.lesswrong.com/tag/ethics-and-morality?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Ethics &amp; Morality</span></u></a><br><a href=\"https://www.lesswrong.com/tag/metaethics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics</span></a><br><a href=\"http://www.lesswrong.com/tag/moral-uncertainty?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Moral Uncertainty</span></u></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%;\"><p><strong id=\"Causes___Interventions\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Causes / Interventions</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aging</span></a><br><a href=\"https://www.lesswrong.com/tag/animal-welfare?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Animal Welfare</span></a><br><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Existential Risk</span></a><br><a href=\"http://www.lesswrong.com/tag/futurism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Futurism</span></a><br><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind Uploading</span></a><br><a href=\"https://www.lesswrong.com/tag/life-extension?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Life Extension</span></a><br><a href=\"http://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&amp;useTagName=false\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">S-risks</span></u></a><br><a href=\"https://www.lesswrong.com/tag/transhumanism?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Transhumanism</span></u></a><br><a href=\"https://www.lesswrong.com/tag/voting-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Voting Theory</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%;\"><p><strong id=\"Working_with_Humans\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Working with Humans</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Coalitional Instincts</span></u></a><br><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Common Knowledge</span></u></a><br><a href=\"http://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Coordination / Cooperation</span></a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Group Rationality</span></a><br><a href=\"https://www.lesswrong.com/tag/institution-design?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Institution Design</span></a><br><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch</span></a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Signaling</span></a><br><a href=\"https://www.lesswrong.com/tag/simulacrum-levels?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Simulacrum Levels</span></a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Social Status</span></a></p></td></tr><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top;\"><p><strong id=\"Applied_Topics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Applied Topics</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/blackmail?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blackmail</span></a><br><a href=\"http://www.lesswrong.com/tag/censorship?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Censorship</span></a><br><a href=\"http://www.lesswrong.com/tag/chesterton-s-fence?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Chesterton's Fence</span></a><br><a href=\"http://www.lesswrong.com/tag/death?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Death</span></a><br><a href=\"https://www.lesswrong.com/tag/deception?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Deception</span></a><br><a href=\"https://www.lesswrong.com/tag/honesty?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Honesty</span></a><br><a href=\"https://www.lesswrong.com/tag/hypocrisy?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hypocrisy</span></a><br><a href=\"https://www.lesswrong.com/tag/information-hazards?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information Hazards</span></a><br><a href=\"https://www.lesswrong.com/tag/meta-honesty?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meta-Honesty</span></a><br><a href=\"http://www.lesswrong.com/tag/pascal-s-mugging?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal's Mugging</span></a><br><a href=\"http://www.lesswrong.com/tag/war?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">War</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:25px;padding:0px;vertical-align:top;\"><p><strong id=\"Value___Virtue\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Value &amp; Virtue</span></strong></p><p><a href=\"http://www.lesswrong.com/tag/ambition?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ambition</span></a><br><a href=\"https://www.lesswrong.com/tag/art?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Art</span></a><br><a href=\"https://www.lesswrong.com/tag/aesthetics?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aesthetics</span></a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of Value</span></a><br><a href=\"http://www.lesswrong.com/tag/courage?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Courage</span></a><br><a href=\"http://www.lesswrong.com/tag/fun-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fun Theory</span></a><br><a href=\"http://www.lesswrong.com/tag/principles?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Principles</span></a><br><a href=\"http://www.lesswrong.com/tag/suffering?showPostCount=true&amp;useTagName=true\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Suffering</span></u></a><br><a href=\"https://www.lesswrong.com/tag/superstimuli?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Superstimuli</span></a><br><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wireheading</span></a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top;\"><p><strong id=\"Meta\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meta</span></strong></p><p><a href=\"https://www.lesswrong.com/tag/cause-prioritization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cause Prioritization</span></a><br><a href=\"http://www.lesswrong.com/tag/center-on-long-term-risk-clr?useTagName=true&amp;showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Center for Long-term Risk</span></a><br><a href=\"https://www.lesswrong.com/tag/effective-altruism?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Effective Altruism</span></a><br><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Heroic Responsibility</span></a><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p></td></tr></tbody></table></figure><hr><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Content which describes </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how the world is </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">that directly bears upon choices one makes to optimize the world fall under this tag. Examples include discussion of the moral patienthood of different animals, the potential of human civilization, and the most effective interventions against a global health threat.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Some material has both immediate relevance to world optimization decisions but also can inform broader world models. This material might be included under both </span><a href=\"https://www.lesswrong.com/tag/world-modeling\"><span class=\"by_qgdGA4ZEyW7zNdK84\">World Modeling</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag and this tag.</span></p>",
      "sections": [
        {
          "title": "                              World Optimization Sub-Topics",
          "anchor": "______________________________World_Optimization_Sub_Topics",
          "level": 1
        },
        {
          "title": "Moral Theory",
          "anchor": "Moral_Theory",
          "level": 2
        },
        {
          "title": "Causes / Interventions",
          "anchor": "Causes___Interventions",
          "level": 2
        },
        {
          "title": "Working with Humans",
          "anchor": "Working_with_Humans",
          "level": 2
        },
        {
          "title": "Applied Topics",
          "anchor": "Applied_Topics",
          "level": 2
        },
        {
          "title": "Value & Virtue",
          "anchor": "Value___Virtue",
          "level": 2
        },
        {
          "title": "Meta",
          "anchor": "Meta",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 8
    },
    "postCount": 1478,
    "description": {
      "markdown": "**World Optimization** is the full use of our agency. It is extending the reach of human civilization. It is building cities and democracies and economic systems and computers and flight and science and space rockets and the internet. World optimization is about adding to that list.   \n  \nBut it’s not just about growth, it’s also about preservation. We are still in the dawn of civilization, with most of civilization in the billions of years ahead. We mustn’t let this light go out.\n\n* * *\n\n**                              World Optimization Sub-Topics**\n===============================================================\n\n<table style=\"border-bottom:20px solid hsl(0, 0%, 100%);border-left:20px solid hsl(0, 0%, 100%);border-right:20px solid hsl(0, 0%, 100%);border-top:20px solid hsl(0, 0%, 100%)\"><tbody><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:50%;padding:0px;vertical-align:top;width:33%\"><p><strong>Moral Theory</strong></p><p><a href=\"https://www.lesswrong.com/tag/altruism?showPostCount=true&amp;useTagName=true\">Altruism</a><br><a href=\"https://www.lesswrong.com/tag/consequentialism?showPostCount=true&amp;useTagName=true\">Consequentialism</a><br><a href=\"https://www.lesswrong.com/tag/deontology?showPostCount=true&amp;useTagName=true\">Deontology</a><br><a href=\"http://www.lesswrong.com/tag/ethics-and-morality?showPostCount=true&amp;useTagName=true\"><u>Ethics &amp; Morality</u></a><br><a href=\"https://www.lesswrong.com/tag/metaethics?showPostCount=true&amp;useTagName=true\">Metaethics</a><br><a href=\"http://www.lesswrong.com/tag/moral-uncertainty?showPostCount=true&amp;useTagName=true\"><u>Moral Uncertainty</u></a></p><p>&nbsp;</p><p>&nbsp;</p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\"><p><strong>Causes / Interventions</strong></p><p><a href=\"https://www.lesswrong.com/tag/aging?showPostCount=true&amp;useTagName=true\">Aging</a><br><a href=\"https://www.lesswrong.com/tag/animal-welfare?showPostCount=true&amp;useTagName=true\">Animal Welfare</a><br><a href=\"https://www.lesswrong.com/tag/existential-risk?showPostCount=true&amp;useTagName=true\">Existential Risk</a><br><a href=\"http://www.lesswrong.com/tag/futurism?showPostCount=true&amp;useTagName=true\">Futurism</a><br><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=true&amp;useTagName=true\">Mind Uploading</a><br><a href=\"https://www.lesswrong.com/tag/life-extension?showPostCount=true&amp;useTagName=true\">Life Extension</a><br><a href=\"http://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks?showPostCount=true&amp;useTagName=false\"><u>S-risks</u></a><br><a href=\"https://www.lesswrong.com/tag/transhumanism?showPostCount=true&amp;useTagName=true\"><u>Transhumanism</u></a><br><a href=\"https://www.lesswrong.com/tag/voting-theory?showPostCount=true&amp;useTagName=true\">Voting Theory</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);padding:0px;vertical-align:top;width:33%\"><p><strong>Working with Humans</strong></p><p><a href=\"http://www.lesswrong.com/tag/coalitional-instincts?showPostCount=true&amp;useTagName=true\"><u>Coalitional Instincts</u></a><br><a href=\"https://www.lesswrong.com/tag/common-knowledge?showPostCount=true&amp;useTagName=true\"><u>Common Knowledge</u></a><br><a href=\"http://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\">Coordination / Cooperation</a><br><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\">Game Theory</a><br><a href=\"http://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\">Group Rationality</a><br><a href=\"https://www.lesswrong.com/tag/institution-design?showPostCount=true&amp;useTagName=true\">Institution Design</a><br><a href=\"https://www.lesswrong.com/tag/moloch?showPostCount=true&amp;useTagName=true\">Moloch</a><br><a href=\"https://www.lesswrong.com/tag/signaling?showPostCount=true&amp;useTagName=true\">Signaling</a><br><a href=\"https://www.lesswrong.com/tag/simulacrum-levels?showPostCount=true&amp;useTagName=true\">Simulacrum Levels</a><br><a href=\"https://www.lesswrong.com/tag/social-status?showPostCount=true&amp;useTagName=true\">Social Status</a></p></td></tr><tr><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Applied Topics</strong></p><p><a href=\"https://www.lesswrong.com/tag/blackmail?showPostCount=true&amp;useTagName=true\">Blackmail</a><br><a href=\"http://www.lesswrong.com/tag/censorship?showPostCount=true&amp;useTagName=true\">Censorship</a><br><a href=\"http://www.lesswrong.com/tag/chesterton-s-fence?showPostCount=true&amp;useTagName=true\">Chesterton's Fence</a><br><a href=\"http://www.lesswrong.com/tag/death?showPostCount=true&amp;useTagName=true\">Death</a><br><a href=\"https://www.lesswrong.com/tag/deception?showPostCount=true&amp;useTagName=true\">Deception</a><br><a href=\"https://www.lesswrong.com/tag/honesty?showPostCount=true&amp;useTagName=true\">Honesty</a><br><a href=\"https://www.lesswrong.com/tag/hypocrisy?showPostCount=true&amp;useTagName=true\">Hypocrisy</a><br><a href=\"https://www.lesswrong.com/tag/information-hazards?showPostCount=true&amp;useTagName=true\">Information Hazards</a><br><a href=\"https://www.lesswrong.com/tag/meta-honesty?showPostCount=true&amp;useTagName=true\">Meta-Honesty</a><br><a href=\"http://www.lesswrong.com/tag/pascal-s-mugging?showPostCount=true&amp;useTagName=true\">Pascal's Mugging</a><br><a href=\"http://www.lesswrong.com/tag/war?showPostCount=true&amp;useTagName=true\">War</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:solid hsl(0, 0%, 100%);border-left:solid hsl(0, 0%, 100%);border-right:solid hsl(0, 0%, 100%);border-top:solid hsl(0, 0%, 100%);height:25px;padding:0px;vertical-align:top\"><p><strong>Value &amp; Virtue</strong></p><p><a href=\"http://www.lesswrong.com/tag/ambition?showPostCount=true&amp;useTagName=true\">Ambition</a><br><a href=\"https://www.lesswrong.com/tag/art?showPostCount=true&amp;useTagName=true\">Art</a><br><a href=\"https://www.lesswrong.com/tag/aesthetics?showPostCount=true&amp;useTagName=true\">Aesthetics</a><br><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\">Complexity of Value</a><br><a href=\"http://www.lesswrong.com/tag/courage?showPostCount=true&amp;useTagName=true\">Courage</a><br><a href=\"http://www.lesswrong.com/tag/fun-theory?showPostCount=true&amp;useTagName=true\">Fun Theory</a><br><a href=\"http://www.lesswrong.com/tag/principles?showPostCount=true&amp;useTagName=true\">Principles</a><br><a href=\"http://www.lesswrong.com/tag/suffering?showPostCount=true&amp;useTagName=true\"><u>Suffering</u></a><br><a href=\"https://www.lesswrong.com/tag/superstimuli?showPostCount=true&amp;useTagName=true\">Superstimuli</a><br><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\">Wireheading</a></p></td><td style=\"background-color:hsl(0,0%,100%);border-bottom:1px solid hsl(0, 0%, 100%);border-left:1px solid hsl(0, 0%, 100%);border-right:1px solid hsl(0, 0%, 100%);border-top:1px solid hsl(0, 0%, 100%);padding:0em;vertical-align:top\"><p><strong>Meta</strong></p><p><a href=\"https://www.lesswrong.com/tag/cause-prioritization?showPostCount=true&amp;useTagName=true\">Cause Prioritization</a><br><a href=\"http://www.lesswrong.com/tag/center-on-long-term-risk-clr?useTagName=true&amp;showPostCount=true\">Center for Long-term Risk</a><br><a href=\"https://www.lesswrong.com/tag/effective-altruism?showPostCount=true&amp;useTagName=true\">Effective Altruism</a><br><a href=\"https://www.lesswrong.com/tag/heroic-responsibility?showPostCount=true&amp;useTagName=true\">Heroic Responsibility</a><br>&nbsp;</p></td></tr></tbody></table>\n\n* * *\n\nContent which describes *how the world is* that directly bears upon choices one makes to optimize the world fall under this tag. Examples include discussion of the moral patienthood of different animals, the potential of human civilization, and the most effective interventions against a global health threat.\n\nSome material has both immediate relevance to world optimization decisions but also can inform broader world models. This material might be included under both [World Modeling](https://www.lesswrong.com/tag/world-modeling) tag and this tag."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z8wZZLeLMJ3NSK7kR",
    "name": "Metaethics",
    "core": null,
    "slug": "metaethics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_2YpRin5m5vBJu8Tg9\">Metaethics</span></strong><span><span class=\"by_2YpRin5m5vBJu8Tg9\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">one of the three branches of ethics usually recognized by philosophers, the others being </span></span><a href=\"http://en.wikipedia.org/wiki/Normative_ethics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">normative ethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"http://en.wikipedia.org/wiki/Applied_ethics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">applied ethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. It’s a field of study that tries to understand the metaphysical, epistemological and semantic characteristics as well as the foundations and scope of moral values. It worries about questions and problems such as \"Are moral judgments objective or subjective, relative or absolute?\", \"Do moral facts exist?\" or “How do we learn moral values?”. (As distinct from object-level moral questions like, \"Ought I to steal from banks in order to give the money to the deserving poor?\")</span></p><h2 id=\"Metaethics_on_LessWrong\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics on LessWrong</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky wrote a Sequence about metaethics, the </span><a href=\"https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics sequence</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">, which Yudkowsky worried failed to convey his central </span><span class=\"by_sKAL2jzfkYkDbQmx9\">point (</span></span><a href=\"https://www.lesswrong.com/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">this post by Luke</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> tried to clarify);</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> he approached the same problem again from a different angle in </span></span><a href=\"https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Highly Advanced Epistemology 101 for Beginners</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. From a standard philosophical standpoint, Yudkowsky's philosophy is closest to Frank Jackson's moral functionalism / analytic descriptivism; Yudkowsky could be loosely characterized as moral cognitivist - someone who believes moral sentences are either true or false - but not a moral realist - thus </span><span class=\"by_sKAL2jzfkYkDbQmx9\">denying</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> that moral sentences refer to facts</span><span class=\"by_woC2b5rav5sGrAo3E\"> about the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">world. Yudkowsky believes that</span><span class=\"by_nmk3nLpQE89dMRzzN\"> moral </span><span class=\"by_qgdGA4ZEyW7zNdK84\">cognition in any single human</span><span class=\"by_nmk3nLpQE89dMRzzN\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">at least potentially about a subject matter that is 'logical'</span><span class=\"by_nmk3nLpQE89dMRzzN\"> in</span><span class=\"by_LedhurJxi3baDAKDZ\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">sense that its semantics can be pinned down by axioms, and hence that moral cognition can bear truth-values; also that human beings both using similar words like \"morality\" can be talking about highly overlapping subject matter; but not that all possible minds would find the truths about this subject matter to be psychologically compelling.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Luke Muehlhauser has written a sequence, </span><a href=\"https://www.lessestwrong.com/s/bQgRsy23biR52poMf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">No-Nonsense Metaethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, where he claims that many of the questions of metaethics can be answered today using modern neuroscience and rationality. He explains how conventional metaethics or \"Austere Metaethics\" is capable of, after assuming a definition of 'right', choosing the right action given a situation - but useless without assuming some criteria for 'right'. He proposes instead \"Empathic Metaethics\" which utilizes your underlying cognitive algorithms to understand what you think 'right' means, helps clarify any emotional and cognitive contradictions in it, and then tells you what the right thing to do is, </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">according to your definition of right</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. This approach is highly relevant for the </span><a href=\"https://www.lesswrong.com/tag/friendly-artificial-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> problem as a way of defining human-like goals and motivations when designing AIs.</span></p><h2 id=\"Further_Reading___References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Further Reading &amp; References</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Garner, Richard T.; Bernard Rosen (1967). Moral Philosophy: A Systematic Introduction to Normative Ethics and Meta-ethics. New York: Macmillan. pp. 215</span></li><li><a href=\"http://plato.stanford.edu/entries/metaethics/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> in the Stanford Encyclopedia of Philosophy</span></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/complexity-of-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of value</span></a></li><li><a href=\"https://lessestwrong.com/tag/utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility</span></a></li></ul>",
      "sections": [
        {
          "title": "Metaethics on LessWrong",
          "anchor": "Metaethics_on_LessWrong",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 68,
    "description": {
      "markdown": "**Metaethics** is one of the three branches of ethics usually recognized by philosophers, the others being [normative ethics](http://en.wikipedia.org/wiki/Normative_ethics) and [applied ethics](http://en.wikipedia.org/wiki/Applied_ethics). It’s a field of study that tries to understand the metaphysical, epistemological and semantic characteristics as well as the foundations and scope of moral values. It worries about questions and problems such as \"Are moral judgments objective or subjective, relative or absolute?\", \"Do moral facts exist?\" or “How do we learn moral values?”. (As distinct from object-level moral questions like, \"Ought I to steal from banks in order to give the money to the deserving poor?\")\n\nMetaethics on LessWrong\n-----------------------\n\nEliezer Yudkowsky wrote a Sequence about metaethics, the [Metaethics sequence](https://www.lesswrong.com/s/W2fkmatEzyrmbbrDt), which Yudkowsky worried failed to convey his central point ([this post by Luke](https://www.lesswrong.com/posts/3R2vH2Ar5AbC9m8Qj/what-is-eliezer-yudkowsky-s-meta-ethical-theory) tried to clarify); he approached the same problem again from a different angle in [Highly Advanced Epistemology 101 for Beginners](https://www.lesswrong.com/s/SqFbMbtxGybdS2gRs). From a standard philosophical standpoint, Yudkowsky's philosophy is closest to Frank Jackson's moral functionalism / analytic descriptivism; Yudkowsky could be loosely characterized as moral cognitivist - someone who believes moral sentences are either true or false - but not a moral realist - thus denying that moral sentences refer to facts about the world. Yudkowsky believes that moral cognition in any single human is at least potentially about a subject matter that is 'logical' in the sense that its semantics can be pinned down by axioms, and hence that moral cognition can bear truth-values; also that human beings both using similar words like \"morality\" can be talking about highly overlapping subject matter; but not that all possible minds would find the truths about this subject matter to be psychologically compelling.\n\nLuke Muehlhauser has written a sequence, [No-Nonsense Metaethics](https://www.lessestwrong.com/s/bQgRsy23biR52poMf), where he claims that many of the questions of metaethics can be answered today using modern neuroscience and rationality. He explains how conventional metaethics or \"Austere Metaethics\" is capable of, after assuming a definition of 'right', choosing the right action given a situation - but useless without assuming some criteria for 'right'. He proposes instead \"Empathic Metaethics\" which utilizes your underlying cognitive algorithms to understand what you think 'right' means, helps clarify any emotional and cognitive contradictions in it, and then tells you what the right thing to do is, *according to your definition of right*. This approach is highly relevant for the [Friendly AI](https://www.lesswrong.com/tag/friendly-artificial-intelligence) problem as a way of defining human-like goals and motivations when designing AIs.\n\nFurther Reading & References\n----------------------------\n\n*   Garner, Richard T.; Bernard Rosen (1967). Moral Philosophy: A Systematic Introduction to Normative Ethics and Meta-ethics. New York: Macmillan. pp. 215\n*   [Metaethics](http://plato.stanford.edu/entries/metaethics/) in the Stanford Encyclopedia of Philosophy\n\nSee Also\n--------\n\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Utility](https://lessestwrong.com/tag/utility)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "etDohXtBrXd8WqCtR",
    "name": "Fiction",
    "core": null,
    "slug": "fiction",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Fiction</span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn't </span><span class=\"by_EQNTWXLKMeWMp2FQS\">literal truth, but when done well it captures truths and intuitions that are difficult to explain directly. (It’s also damn fun to read.)</span></span></p><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">“Nonfiction conveys </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">knowledge,</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> fiction conveys </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">experience.</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">” - Eliezer Yudkowsky&nbsp;</span></p></blockquote><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Eliezer Yudkowsky helped kickstart the genre of </span><a href=\"https://www.lesswrong.com/posts/q79vYjHAE9KHcAjSs/rationalist-fiction\"><i><span class=\"by_EQNTWXLKMeWMp2FQS\">rationalist fiction</span></i></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, which is about characters who solve the problems in their world by thinking, in a way where the reader </span><i><span class=\"by_EQNTWXLKMeWMp2FQS\">could figure it out too</span></i><span><span class=\"by_EQNTWXLKMeWMp2FQS\">. Not where the genius character explains it afterward like Sherlock Holmes</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><span class=\"by_EQNTWXLKMeWMp2FQS\">Artemis Fowl,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> but </span><span class=\"by_EQNTWXLKMeWMp2FQS\">where the problem is fair and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you </span><span class=\"by_EQNTWXLKMeWMp2FQS\">could’ve figured it out first. Eliezer has written about this in his short online book </span></span><a href=\"https://yudkowsky.tumblr.com/writing\"><u><span><span class=\"by_EQNTWXLKMeWMp2FQS\">The Abridged Guide</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to </span><span class=\"by_EQNTWXLKMeWMp2FQS\">Intelligent Characters</span></span></u></a><span class=\"by_XtphY3uYHwruKqDyG\">.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Other fiction on the site is in the spirit of hard science fiction, and often involves taking the laws of a universe or the rules of a system to their extreme conclusions, and munchkining your way to become god (or something similar). They also share much of the parts of sci-fi that engage with difficult moral quandaries.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Fiction on this site also tends to have puns. I’m so sorry.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Much more fiction can be found at </span><a href=\"https://www.reddit.com/r/rational\"><span><span class=\"by_EQNTWXLKMeWMp2FQS\">r/</span><span class=\"by_sKAL2jzfkYkDbQmx9\">Rational</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> which is a subreddit devoted to rationalist fiction.</span></span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">This is a tag for works of fiction, not for analysis or discussion of literature. For that see </span><a href=\"https://www.lesswrong.com/tag/fiction-topic\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Fiction (topic)</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">.</span></p><h2 id=\"Fiction_Sequences\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Fiction </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Sequences</span></span></h2><ul><li><a href=\"https://www.lesswrong.com/hpmor\"><span class=\"by_sKAL2jzfkYkDbQmx9\">HPMOR</span></a></li><li><a href=\"https://www.lesswrong.com/s/qWoFR4ytMpQ5vw3FT\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Three Worlds Collide</span></a></li><li><a href=\"https://www.lesswrong.com/s/LAop879LCQWrM5YnE\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Bayesian Conspiracy</span></a></li><li><a href=\"https://www.lesswrong.com/s/4C33PKt2cQdA7oyfJ\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Murphy's Quest</span></a></li><li><a href=\"https://www.lesswrong.com/s/TF77XsD5PbucbJsG3\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Luna Lovegood and the Chamber of Secrets</span></a></li><li><a href=\"https://www.lesswrong.com/s/TjdhvTSptCYakw3Lc\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Bayeswatch</span></a></li><li><a href=\"https://www.lesswrong.com/s/qMtriMPLdriNkAfSJ\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Short stories</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by lsusr</span></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"https://www.reddit.com/r/rational/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">/r/rational/</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> on </span><a href=\"https://lessestwrong.com/tag/reddit\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Reddit</span></a></li><li><a href=\"http://tvtropes.org/pmwiki/pmwiki.php/Main/RationalFic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">RationalFic</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> on TV Tropes</span></li><li><a href=\"http://yudkowsky.tumblr.com/writing\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky's guide to writing intelligent characters</span></a></li><li><a href=\"http://rationalreads.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">rationalreads.com</span></a></li></ul>",
      "sections": [
        {
          "title": "Fiction Sequences",
          "anchor": "Fiction_Sequences",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 410,
    "description": {
      "markdown": "**Fiction** isn't literal truth, but when done well it captures truths and intuitions that are difficult to explain directly. (It’s also damn fun to read.)\n\n> “Nonfiction conveys *knowledge,* fiction conveys *experience.*” \\- Eliezer Yudkowsky \n\nEliezer Yudkowsky helped kickstart the genre of [*rationalist fiction*](https://www.lesswrong.com/posts/q79vYjHAE9KHcAjSs/rationalist-fiction), which is about characters who solve the problems in their world by thinking, in a way where the reader *could figure it out too*. Not where the genius character explains it afterward like Sherlock Holmes or Artemis Fowl, but where the problem is fair and you could’ve figured it out first. Eliezer has written about this in his short online book [The Abridged Guide to Intelligent Characters](https://yudkowsky.tumblr.com/writing).\n\nOther fiction on the site is in the spirit of hard science fiction, and often involves taking the laws of a universe or the rules of a system to their extreme conclusions, and munchkining your way to become god (or something similar). They also share much of the parts of sci-fi that engage with difficult moral quandaries.\n\nFiction on this site also tends to have puns. I’m so sorry.\n\nMuch more fiction can be found at [r/Rational](https://www.reddit.com/r/rational), which is a subreddit devoted to rationalist fiction.\n\nThis is a tag for works of fiction, not for analysis or discussion of literature. For that see [Fiction (topic)](https://www.lesswrong.com/tag/fiction-topic).\n\nFiction Sequences\n-----------------\n\n*   [HPMOR](https://www.lesswrong.com/hpmor)\n*   [Three Worlds Collide](https://www.lesswrong.com/s/qWoFR4ytMpQ5vw3FT)\n*   [The Bayesian Conspiracy](https://www.lesswrong.com/s/LAop879LCQWrM5YnE)\n*   [Murphy's Quest](https://www.lesswrong.com/s/4C33PKt2cQdA7oyfJ)\n*   [Luna Lovegood and the Chamber of Secrets](https://www.lesswrong.com/s/TF77XsD5PbucbJsG3)\n*   [Bayeswatch](https://www.lesswrong.com/s/TjdhvTSptCYakw3Lc)\n*   [Short stories](https://www.lesswrong.com/s/qMtriMPLdriNkAfSJ) by lsusr\n\nExternal links\n--------------\n\n*   [/r/rational/](https://www.reddit.com/r/rational/) on [Reddit](https://lessestwrong.com/tag/reddit)\n*   [RationalFic](http://tvtropes.org/pmwiki/pmwiki.php/Main/RationalFic) on TV Tropes\n*   [Eliezer Yudkowsky's guide to writing intelligent characters](http://yudkowsky.tumblr.com/writing)\n*   [rationalreads.com](http://rationalreads.com/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "udPbn9RthmgTtHMiG",
    "name": "Productivity",
    "core": false,
    "slug": "productivity",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 155,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dqx5k65wjFfaiJ9sQ",
    "name": "Procrastination",
    "core": false,
    "slug": "procrastination",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Procrastination</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is [TODO: finish tag description]</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 32,
    "description": {
      "markdown": "**Procrastination** is \\[TODO: finish tag description\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kJrjorSx3hXa7q7CJ",
    "name": "Surveys",
    "core": false,
    "slug": "surveys",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Surveys</span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> and polls</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> of users of LessWrong and related communities, results, and analysis of the resulting data.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 66,
    "description": {
      "markdown": "**Surveys** and polls of users of LessWrong and related communities, results, and analysis of the resulting data."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9YFoDPFwMoWthzgkY",
    "name": "Pitfalls of Rationality",
    "core": false,
    "slug": "pitfalls-of-rationality",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Pitfalls of Rationality</span></strong><span class=\"by_XtphY3uYHwruKqDyG\"> are frequent </span><a href=\"https://www.lesswrong.com/tag/failure-mode\"><span><span class=\"by_XtphY3uYHwruKqDyG\">error </span><span class=\"by_sKAL2jzfkYkDbQmx9\">modes</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_XtphY3uYHwruKqDyG\"> obstacles or problems that arise when people try to practice rationality, or engage with rationality-related materials. Related concepts include the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"valley</span><span class=\"by_XtphY3uYHwruKqDyG\"> of bad rationality\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></span><br><br><span class=\"by_qgdGA4ZEyW7zNdK84\">There are two threads touched in posts under this tag:</span></p><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Things that go wrong when people try to be more rational and they unintentionally end up making things worse.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Arguably, why haven't rationalists visible succeeded at their bold and ambitious goals yet?</span></li></ol><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Regarding the first point, from </span><a href=\"https://www.lesswrong.com/posts/oZNXmHcdhb4m7vwsv/incremental-progress-and-the-valley\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Incremental Progress and the Valley</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span></p><blockquote><p><span class=\"by_XtphY3uYHwruKqDyG\">Ah.&nbsp; Well, here's the the thing:&nbsp; An </span><i><span class=\"by_XtphY3uYHwruKqDyG\">incremental</span></i><span class=\"by_XtphY3uYHwruKqDyG\"> step in the direction of rationality, if the result is still irrational in other ways, does not have to yield </span><i><span class=\"by_XtphY3uYHwruKqDyG\">incrementally </span></i><span class=\"by_XtphY3uYHwruKqDyG\">more winning.</span></p><p><span class=\"by_XtphY3uYHwruKqDyG\">The optimality theorems that we have for probability theory and decision theory, are for </span><i><span class=\"by_XtphY3uYHwruKqDyG\">perfect</span></i><span class=\"by_XtphY3uYHwruKqDyG\"> probability theory and decision theory.&nbsp; There is no companion theorem which says that, starting from some flawed initial form, every </span><i><span class=\"by_XtphY3uYHwruKqDyG\">incremental</span></i><span class=\"by_XtphY3uYHwruKqDyG\"> modification of the algorithm that takes the structure closer to the ideal, must yield an </span><i><span class=\"by_XtphY3uYHwruKqDyG\">incremental</span></i><span class=\"by_XtphY3uYHwruKqDyG\"> improvement in performance.&nbsp; This has not yet been proven, because it is not, in fact, true.</span></p></blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement\"><span class=\"by_HoGziwmhpMGqGeWZy\">Criticisms of the Rationalist Movement</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/value-of-rationality\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Value of</span><span class=\"by_HoGziwmhpMGqGeWZy\"> Rationality</span></span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 68,
    "description": {
      "markdown": "**Pitfalls of Rationality** are frequent [error modes](https://www.lesswrong.com/tag/failure-mode), obstacles or problems that arise when people try to practice rationality, or engage with rationality-related materials. Related concepts include the \"valley of bad rationality\".  \n  \nThere are two threads touched in posts under this tag:\n\n1.  Things that go wrong when people try to be more rational and they unintentionally end up making things worse.\n2.  Arguably, why haven't rationalists visible succeeded at their bold and ambitious goals yet?\n\nRegarding the first point, from [Incremental Progress and the Valley](https://www.lesswrong.com/posts/oZNXmHcdhb4m7vwsv/incremental-progress-and-the-valley):\n\n> Ah.  Well, here's the the thing:  An *incremental* step in the direction of rationality, if the result is still irrational in other ways, does not have to yield *incrementally* more winning.\n> \n> The optimality theorems that we have for probability theory and decision theory, are for *perfect* probability theory and decision theory.  There is no companion theorem which says that, starting from some flawed initial form, every *incremental* modification of the algorithm that takes the structure closer to the ideal, must yield an *incremental* improvement in performance.  This has not yet been proven, because it is not, in fact, true.\n\nSee also: [Criticisms of the Rationalist Movement](https://www.lesswrong.com/tag/criticisms-of-the-rationalist-movement), [Value of Rationality](https://www.lesswrong.com/tag/value-of-rationality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7w6XkYe5YPx9YL59j",
    "name": "Information Hazards",
    "core": null,
    "slug": "information-hazards",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">An </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Information Hazard</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">some true information that could harm </span><span class=\"by_4utSx6E5fffNYHj47\">people, or other sentient beings,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> if known. It is tricky to determine policies on information hazards. Some information might genuinely be dangerous, but excessive controls on information has its own perils.&nbsp;</span></span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">This tag is for discussing the phenomenon of Information Hazards and what to do with them. Not for actual Information Hazards themselves.</span></strong><br><br><span><span class=\"by_qgdGA4ZEyW7zNdK84\">An example might be </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">formula for easily creating cold fusion in your </span><span class=\"by_63CvXxSWvMAxrKQYz\">garage,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> which would be very dangerous. </span><span class=\"by_63CvXxSWvMAxrKQYz\">Alternatively,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> it might be an idea </span><span class=\"by_63CvXxSWvMAxrKQYz\">that</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> causes great mental harm to people.</span></span></p><h2 id=\"Bostrom_s_Typology_of_Information_Hazards\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom's Typology of Information Hazards</span></h2><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Nick Bostrom </span><span class=\"by_qgdGA4ZEyW7zNdK84\">coined the term </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">information hazard </span></i><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">in a 2011 paper</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> [1]</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> for Review of Contemporary Philosophy. He defines it as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">follows:</span></span></p><blockquote><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Information hazard: A risk that arises from the dissemination or the potential dissemination of (true) information that may cause harm or enable some agent to cause harm.</span></span></p></blockquote><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">Bostrom points out that this is in contrast to the generally accepted principle of information freedom and that, while rare, the possibility of information hazards needs to be considered when making information policies. He proceeds to categorize and define a large number of sub-types of information hazards. For example, he defines artificial intelligence hazard </span><span class=\"by_qgdGA4ZEyW7zNdK84\">as:</span></span></p><blockquote><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Artificial intelligence hazard: There could be computer-related risks in which the threat would derive primarily from the cognitive sophistication of the program rather than the specific properties of any actuators to which the system initially has access.</span></span></p></blockquote><p><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">The </span><span class=\"by_qgdGA4ZEyW7zNdK84\">following </span><span class=\"by_5wu9jG4pm9q6xjZ9R\">table is reproduced from </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom 2011 [1]</span><span class=\"by_5wu9jG4pm9q6xjZ9R\">.</span></span></p><figure class=\"table\"><table style=\"background-color:white\"><tbody><tr><td style=\"text-align:center\" colspan=\"3\"><strong><span class=\"by_5wu9jG4pm9q6xjZ9R\">TYPOLOGY OF INFORMATION HAZARDS</span></strong></td></tr><tr><td colspan=\"3\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">I. By information transfer mode</span></td></tr><tr><td rowspan=\"6\"><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Data hazard</span></td><td rowspan=\"6\"><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Idea hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Attention hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Template hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Signaling hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Evocation hazard</span></td></tr><tr><td colspan=\"3\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">II. By effect</span></td></tr><tr><td><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">TYPE</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">SUBTYPE</span></td></tr><tr><td rowspan=\"4\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">ADVERSARIAL RISKS</span></td><td rowspan=\"4\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Competiveness hazard</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Enemy Hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Intellectual property hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Commitment hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Knowing-too-much hazard</span></td></tr><tr><td rowspan=\"3\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">RISKS TO SOCIAL ORGANIZATION AND MARKETS</span></td><td rowspan=\"3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Norm hazard</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Information asymmetry Hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Unveiling hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Recognition hazard</span></td></tr><tr><td rowspan=\"7\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">RISKS OF IRRATIONALITY AND ERROR</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Ideological hazard</span></td><td rowspan=\"7\"><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Distraction and temptation hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Role model hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Biasing hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">De-biasing hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Neuropsychological hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Information-burying hazard</span></td></tr><tr><td rowspan=\"5\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">RISKS TO VALUABLE STATES AND ACTIVITIES</span></td><td rowspan=\"3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Psychological reaction hazard</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Disappointment hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Spoiler hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Mindset hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Belief-constituted value hazard</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">(mixed)</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Embarrassment hazard</span></td></tr><tr><td rowspan=\"3\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">RISKS FROM INFORMATION TECHNOLOGY SYSTEMS</span></td><td rowspan=\"3\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information system hazard</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Information infrastructure failure hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Information infrastructure misuse hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">Artificial intelligence hazard</span></td></tr><tr><td><span class=\"by_5wu9jG4pm9q6xjZ9R\">RISKS FROM DEVELOPMENT</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">Development hazard</span></td><td><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></td></tr></tbody></table></figure><h2 id=\"See_Also\"><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/dangerous-knowledge\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Dangerous Knowledge</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Computation_Hazards\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">Computation Hazards</span></a></li></ul><h2 id=\"References\"><span class=\"by_5wu9jG4pm9q6xjZ9R\">References</span></h2><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom, N. (2011). \"</span><a href=\"http://www.nickbostrom.com/information-hazards.pdf\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Information Hazards: A Typology of Potential Harms from Knowledge</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">\". </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Review of Contemporary Philosophy</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">10</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: 44-79.</span></li></ol>",
      "sections": [
        {
          "title": "Bostrom's Typology of Information Hazards",
          "anchor": "Bostrom_s_Typology_of_Information_Hazards",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 37,
    "description": {
      "markdown": "An **Information Hazard** is some true information that could harm people, or other sentient beings, if known. It is tricky to determine policies on information hazards. Some information might genuinely be dangerous, but excessive controls on information has its own perils. \n\n**This tag is for discussing the phenomenon of Information Hazards and what to do with them. Not for actual Information Hazards themselves.**  \n  \nAn example might be a formula for easily creating cold fusion in your garage, which would be very dangerous. Alternatively, it might be an idea that causes great mental harm to people.\n\nBostrom's Typology of Information Hazards\n-----------------------------------------\n\nNick Bostrom coined the term *information hazard* in a 2011 paper \\[1\\] for Review of Contemporary Philosophy. He defines it as follows:\n\n> Information hazard: A risk that arises from the dissemination or the potential dissemination of (true) information that may cause harm or enable some agent to cause harm.\n\nBostrom points out that this is in contrast to the generally accepted principle of information freedom and that, while rare, the possibility of information hazards needs to be considered when making information policies. He proceeds to categorize and define a large number of sub-types of information hazards. For example, he defines artificial intelligence hazard as:\n\n> Artificial intelligence hazard: There could be computer-related risks in which the threat would derive primarily from the cognitive sophistication of the program rather than the specific properties of any actuators to which the system initially has access.\n\nThe following table is reproduced from Bostrom 2011 \\[1\\].\n\n<table style=\"background-color:white\"><tbody><tr><td style=\"text-align:center\" colspan=\"3\"><strong>TYPOLOGY OF INFORMATION HAZARDS</strong></td></tr><tr><td colspan=\"3\">I. By information transfer mode</td></tr><tr><td rowspan=\"6\">&nbsp;</td><td>Data hazard</td><td rowspan=\"6\">&nbsp;</td></tr><tr><td>Idea hazard</td></tr><tr><td>Attention hazard</td></tr><tr><td>Template hazard</td></tr><tr><td>Signaling hazard</td></tr><tr><td>Evocation hazard</td></tr><tr><td colspan=\"3\">II. By effect</td></tr><tr><td>&nbsp;</td><td>TYPE</td><td>SUBTYPE</td></tr><tr><td rowspan=\"4\">ADVERSARIAL RISKS</td><td rowspan=\"4\">Competiveness hazard</td><td>Enemy Hazard</td></tr><tr><td>Intellectual property hazard</td></tr><tr><td>Commitment hazard</td></tr><tr><td>Knowing-too-much hazard</td></tr><tr><td rowspan=\"3\">RISKS TO SOCIAL ORGANIZATION AND MARKETS</td><td rowspan=\"3\">Norm hazard</td><td>Information asymmetry Hazard</td></tr><tr><td>Unveiling hazard</td></tr><tr><td>Recognition hazard</td></tr><tr><td rowspan=\"7\">RISKS OF IRRATIONALITY AND ERROR</td><td>Ideological hazard</td><td rowspan=\"7\">&nbsp;</td></tr><tr><td>Distraction and temptation hazard</td></tr><tr><td>Role model hazard</td></tr><tr><td>Biasing hazard</td></tr><tr><td>De-biasing hazard</td></tr><tr><td>Neuropsychological hazard</td></tr><tr><td>Information-burying hazard</td></tr><tr><td rowspan=\"5\">RISKS TO VALUABLE STATES AND ACTIVITIES</td><td rowspan=\"3\">Psychological reaction hazard</td><td>Disappointment hazard</td></tr><tr><td>Spoiler hazard</td></tr><tr><td>Mindset hazard</td></tr><tr><td>Belief-constituted value hazard</td><td>&nbsp;</td></tr><tr><td>(mixed)</td><td>Embarrassment hazard</td></tr><tr><td rowspan=\"3\">RISKS FROM INFORMATION TECHNOLOGY SYSTEMS</td><td rowspan=\"3\">Information system hazard</td><td>Information infrastructure failure hazard</td></tr><tr><td>Information infrastructure misuse hazard</td></tr><tr><td>Artificial intelligence hazard</td></tr><tr><td>RISKS FROM DEVELOPMENT</td><td>Development hazard</td><td>&nbsp;</td></tr></tbody></table>\n\nSee Also\n--------\n\n*   [Dangerous Knowledge](https://lessestwrong.com/tag/dangerous-knowledge)\n*   [Computation Hazards](https://wiki.lesswrong.com/wiki/Computation_Hazards)\n\nReferences\n----------\n\n1.  Bostrom, N. (2011). \"[Information Hazards: A Typology of Potential Harms from Knowledge](http://www.nickbostrom.com/information-hazards.pdf)\". *Review of Contemporary Philosophy* **10**: 44-79."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "MfpEPj6kJneT9gWT6",
    "name": "Site Meta",
    "core": true,
    "slug": "site-meta",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Site Meta </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is the category for </span><span class=\"by_r38pkCm7wF4M44MDQ\">discussion about</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><span class=\"by_LGNsmxuTq4yMww6u5\">the AI Alignment Forum</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span><span class=\"by_r38pkCm7wF4M44MDQ\">website. It includes technical updates. It</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> applies </span><span class=\"by_r38pkCm7wF4M44MDQ\">to</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> team announcements such as updates, features, events, moderation activity</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">policy, downtime, requests for feedback, as well as site documentation, &nbsp;and the team’s writings about site philosophy/strategic thinking.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The tag also applies to any discussion of the site norms/moderation, feature requests, questions, and ideas about what the site should do – regardless of author.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 541,
    "description": {
      "markdown": "**Site Meta** is the category for discussion about the AI Alignment Forum website. It includes technical updates. It applies to team announcements such as updates, features, events, moderation activity and policy, downtime, requests for feedback, as well as site documentation,  and the team’s writings about site philosophy/strategic thinking.\n\nThe tag also applies to any discussion of the site norms/moderation, feature requests, questions, and ideas about what the site should do – regardless of author."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jgcAJnksReZRuvgzp",
    "name": "Financial Investing",
    "core": false,
    "slug": "financial-investing",
    "tableOfContents": {
      "html": "<p><span class=\"by_XtphY3uYHwruKqDyG\">The </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">Financial Investing</span></strong><span class=\"by_XtphY3uYHwruKqDyG\"> tag covers concrete personal investment advice, specific investment opportunities (like Bitcoin), and analysis of existing financial investing practices, as well as broad analyses of things like the efficient market hypothesis.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 119,
    "description": {
      "markdown": "The **Financial Investing** tag covers concrete personal investment advice, specific investment opportunities (like Bitcoin), and analysis of existing financial investing practices, as well as broad analyses of things like the efficient market hypothesis."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fF9GEdWXKJ3z73TmB",
    "name": "Scholarship & Learning",
    "core": null,
    "slug": "scholarship-and-learning",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship &amp; Learning. </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Here be posts on how to study, research, and learn.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Topics include, but are not limited to: how to research, how to understand material deeply, note-taking, and useful scholarship resources.</span></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. – </span></i><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Twelve Virtues of Rationality</span></i></a></p></blockquote><h2 id=\"See_Also\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">See </span><span class=\"by_QBvPFLFyZyuHcBwFm\">Also</span></span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/spaced-repetition\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Spaced Repetition</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a technique for long-term retention of learned material.</span></li><li><a href=\"https://www.lesswrong.com/tag/fact-posts?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Fact Posts</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> are pieces of writing that attempt to build an understanding of the world, starting bottom up with empirical facts rather than </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"opinions\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span></span></li><li><span class=\"by_QBvPFLFyZyuHcBwFm\">The other </span><a href=\"https://www.lesswrong.com/tag/virtues?showPostCount=true&amp;useTagName=true\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Virtues</span></a><span class=\"by_QBvPFLFyZyuHcBwFm\"> of Rationality.</span></li></ul><h2 id=\"Top_Resources\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Top Resources</span></h2><ul><li><a href=\"https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship: How to Do It Efficiently</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a guide to quickly researching topics</span><span class=\"by_LoykQRMTxJFxwwdPy\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">understanding what is known within a field.</span></span></li><li><a href=\"https://www.lesswrong.com/posts/RKz7pc6snBttndxXz/literature-review-for-academic-outsiders-what-how-and-why-1\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Literature Review For Academic Outsiders: What, How, and Why</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> similar to the first resource, contains many links to further resources.</span></li><li><a href=\"https://www.lesswrong.com/posts/gxbGKa2AnQsrn3Gni/how-do-you-assess-the-quality-reliability-of-a-scientific\"><span class=\"by_qgdGA4ZEyW7zNdK84\">[Question] How do you assess the quality / reliability of a scientific study?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> A question post with many highly excellent lengthy responses, several which received bounty payouts.</span></li><li><a href=\"https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things\"><span class=\"by_qgdGA4ZEyW7zNdK84\">On learning difficult things</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> covers techniques and methods</span><span class=\"by_LoykQRMTxJFxwwdPy\"> for </span><span class=\"by_qgdGA4ZEyW7zNdK84\">studying difficult topics.</span></span></li><li><a href=\"https://www.lesswrong.com/posts/TPjbTXntR54XSZ3F2/paper-reading-for-gears\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Paper-Reading for Gears</span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a guide studying to actually build up a mechanistic, gears-level understanding of a topic.</span></span></li><li><a href=\"https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">The </span><span class=\"by_qgdGA4ZEyW7zNdK84\">3 Books Technique for Learning a New Skilll</span></span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a short post suggests finding a What, How, and Why book for any skill or topic you wish to learn.</span></span></li><li><a href=\"https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject\"><span class=\"by_LoykQRMTxJFxwwdPy\">The Best Textbooks on Every Subject</span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">crowd-sourced list where every recommendation requires that the recommender have read three books on the topic and can explain why one textbook is better than others.</span></span></li><li><a href=\"https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forum participation as a research strategy</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> argues that participation on discussion forums on a research topic is actually a great way for researchers to make progress.</span></li><li><a href=\"https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fact Posts: How and Why</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is guide on exploring empirical question by starting with raw facts rather than expert opinion and prior analysis. Compared more typical research, the Fact Post method helps you ground your understanding in facts and see the topic freshly.</span></li><li><a href=\"https://www.lesswrong.com/posts/tRQek3Xb9cKZ2o6iA/how-to-not-do-a-literature-review\"><span class=\"by_qgdGA4ZEyW7zNdK84\">How to (not) do a literature review</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> which contains a very concrete list</span><span class=\"by_LoykQRMTxJFxwwdPy\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">steps for literature reviews, including mistakes to avoid.</span></span></li></ul><p><strong id=\"External_Resources\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Resources</span></strong></p><ul><li><a href=\"https://www.gwern.net/Search\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Internet Search Tips</span></a><span><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Gwern Branwen is a long, extremely detailed practical guide on how to conduct an online search for references, papers,</span><span class=\"by_LoykQRMTxJFxwwdPy\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">books that are difficult</span><span class=\"by_LoykQRMTxJFxwwdPy\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">find, including 13 case studies.</span></span></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "Top Resources",
          "anchor": "Top_Resources",
          "level": 1
        },
        {
          "title": "External Resources",
          "anchor": "External_Resources",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 241,
    "description": {
      "markdown": "**Scholarship & Learning.** Here be posts on how to study, research, and learn.\n\nTopics include, but are not limited to: how to research, how to understand material deeply, note-taking, and useful scholarship resources.\n\n> *The eleventh virtue is scholarship. Study many sciences and absorb their power as your own. Each field that you consume makes you larger. If you swallow enough sciences the gaps between them will diminish and your knowledge will become a unified whole. If you are gluttonous you will become vaster than mountains. –* [*Twelve Virtues of Rationality*](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality)\n\nSee Also\n--------\n\n*   [Spaced Repetition](https://www.lesswrong.com/tag/spaced-repetition) is a technique for long-term retention of learned material.\n*   [Fact Posts](https://www.lesswrong.com/tag/fact-posts?showPostCount=true&useTagName=true) are pieces of writing that attempt to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".\n*   The other [Virtues](https://www.lesswrong.com/tag/virtues?showPostCount=true&useTagName=true) of Rationality.\n\nTop Resources\n-------------\n\n*   [Scholarship: How to Do It Efficiently](https://www.lesswrong.com/posts/37sHjeisS9uJufi4u/scholarship-how-to-do-it-efficiently) is a guide to quickly researching topics and understanding what is known within a field.\n*   [Literature Review For Academic Outsiders: What, How, and Why](https://www.lesswrong.com/posts/RKz7pc6snBttndxXz/literature-review-for-academic-outsiders-what-how-and-why-1) similar to the first resource, contains many links to further resources.\n*   [\\[Question\\] How do you assess the quality / reliability of a scientific study?](https://www.lesswrong.com/posts/gxbGKa2AnQsrn3Gni/how-do-you-assess-the-quality-reliability-of-a-scientific) A question post with many highly excellent lengthy responses, several which received bounty payouts.\n*   [On learning difficult things](https://www.lesswrong.com/posts/w5F4w8tNZc6LcBKRP/on-learning-difficult-things) covers techniques and methods for studying difficult topics.\n*   [Paper-Reading for Gears](https://www.lesswrong.com/posts/TPjbTXntR54XSZ3F2/paper-reading-for-gears) is a guide studying to actually build up a mechanistic, gears-level understanding of a topic.\n*   [The 3 Books Technique for Learning a New Skilll](https://www.lesswrong.com/posts/oPEWyxJjRo4oKHzMu/the-3-books-technique-for-learning-a-new-skilll) is a short post suggests finding a What, How, and Why book for any skill or topic you wish to learn.\n*   [The Best Textbooks on Every Subject](https://www.lesswrong.com/posts/xg3hXCYQPJkwHyik2/the-best-textbooks-on-every-subject) crowd-sourced list where every recommendation requires that the recommender have read three books on the topic and can explain why one textbook is better than others.\n*   [Forum participation as a research strategy](https://www.lesswrong.com/posts/rBkZvbGDQZhEymReM/forum-participation-as-a-research-strategy) argues that participation on discussion forums on a research topic is actually a great way for researchers to make progress.\n*   [Fact Posts: How and Why](https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why) is guide on exploring empirical question by starting with raw facts rather than expert opinion and prior analysis. Compared more typical research, the Fact Post method helps you ground your understanding in facts and see the topic freshly.\n*   [How to (not) do a literature review](https://www.lesswrong.com/posts/tRQek3Xb9cKZ2o6iA/how-to-not-do-a-literature-review) which contains a very concrete list of steps for literature reviews, including mistakes to avoid.\n\n**External Resources**\n\n*   [Internet Search Tips](https://www.gwern.net/Search) by Gwern Branwen is a long, extremely detailed practical guide on how to conduct an online search for references, papers, and books that are difficult to find, including 13 case studies."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XSryTypw5Hszpa4TS",
    "name": "Consciousness",
    "core": null,
    "slug": "consciousness",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_2aoRX3ookcCozcb3m\">The word </span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span></span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">consciousness</span></strong><span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> is used in a variety of different ways, and there are large disagreements about the reality and nature (and even coherence) of some of the things people profess to mean by \"consciousness.\"</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Colloquially, the word \"conscious\" is used to pick out a few different things:</span></p><ul><li><strong><span class=\"by_2aoRX3ookcCozcb3m\">Wakefulness </span></strong><span class=\"by_2aoRX3ookcCozcb3m\">- The property that distinguishes, e.g., a person who is awake from a person who is asleep.</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">We call people \"unconscious\" in this sense based on observed features like \"sharply reduced mobility,\" though we wouldn't normally call someone unconscious if we think they're merely </span><i><span class=\"by_2aoRX3ookcCozcb3m\">paralyzed</span></i><span class=\"by_2aoRX3ookcCozcb3m\">. Instead, calling someone \"unconscious\" tends to imply reduced ability to perceive and/or reason about events in one's environment.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">An unconscious person (in this sense) might or might not be dreaming; and if dreaming, they might or might not be lucid.</span></li></ul></li><li><strong><span class=\"by_2aoRX3ookcCozcb3m\">Having experiences </span></strong><span class=\"by_2aoRX3ookcCozcb3m\">- The property that distinguishes, e.g., a comatose person who is having experiences from a comatose person who is not having experiences.</span></li><li><strong><span class=\"by_2aoRX3ookcCozcb3m\">Knowledge, perception, </span></strong><span class=\"by_2aoRX3ookcCozcb3m\">and/or </span><strong><span class=\"by_2aoRX3ookcCozcb3m\">attention</span></strong><span class=\"by_2aoRX3ookcCozcb3m\"> - E.g., we might say that someone becomes \"conscious of\" a fact when they first learn that fact. Or we might say that they become \"conscious of\" something whenever they're currently perceiving it, or whenever they're </span><i><span class=\"by_2aoRX3ookcCozcb3m\">paying attention</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> to it.</span></li><li><strong><span class=\"by_2aoRX3ookcCozcb3m\">Meta-cognition </span></strong><span class=\"by_2aoRX3ookcCozcb3m\">or </span><strong><span class=\"by_2aoRX3ookcCozcb3m\">reflective awareness</span></strong><span class=\"by_2aoRX3ookcCozcb3m\"> - Knowing, perceiving, and/or attending to your own mental states; or knowing, perceiving, and/or attending to </span><i><span class=\"by_2aoRX3ookcCozcb3m\">the fact that</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> you have certain mental states.</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">E.g., we might say that someone is less \"conscious\" when they're fully immersed in a novel than when they're thinking about their own experiences, directing attention to the fact that they're reading a book, etc.</span></li></ul></li><li><strong><span class=\"by_2aoRX3ookcCozcb3m\">Self-awareness</span></strong><span class=\"by_2aoRX3ookcCozcb3m\"> - Knowing, perceiving, and/or attending to your own existence or your own central properties.</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">Depending on what exactly is meant by \"self-awareness,\" the \"immersed in a novel\" example might also involve less self-awareness. In some weaker senses of \"self-aware,\" one might instead claim that humans who are experiencing anything are always \"self-aware.\"</span></li></ul></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">Reasonably mainstream academic overviews of \"consciousness\" can be found in the </span><a href=\"https://plato.stanford.edu/entries/consciousness/\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Stanford Encyclopedia of Philosophy</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\"> and the </span><a href=\"http://www.mkdavies.net/Martin_Davies/Mind_files/ConsciousnessMITECS.pdf\"><i><span class=\"by_2aoRX3ookcCozcb3m\">MIT Encyclopedia of the Cognitive Sciences</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">This tag is </span><i><span class=\"by_2aoRX3ookcCozcb3m\">tentatively and provisionally</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> about the \"</span><strong><span class=\"by_2aoRX3ookcCozcb3m\">having experiences</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">\" meaning(s) of \"consciousness.\" For wakefulness and dreaming, see </span><a href=\"https://www.lesswrong.com/tag/sleep\"><span class=\"by_2aoRX3ookcCozcb3m\">sleep</span></a><span class=\"by_2aoRX3ookcCozcb3m\">. For knowledge, perception, and attention, see </span><a href=\"https://www.lesswrong.com/tag/attention\"><span class=\"by_2aoRX3ookcCozcb3m\">attention</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> and </span><a href=\"https://www.lesswrong.com/tag/cognitive-science\"><span class=\"by_2aoRX3ookcCozcb3m\">cognitive science</span></a><span class=\"by_2aoRX3ookcCozcb3m\">. And for reflective awareness and self-awareness, see </span><a href=\"https://www.lesswrong.com/tag/identity\"><span class=\"by_2aoRX3ookcCozcb3m\">identity</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/personal-identity\"><span class=\"by_2aoRX3ookcCozcb3m\">personal identity</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, and </span><a href=\"https://www.lesswrong.com/tag/reflective-reasoning\"><span class=\"by_2aoRX3ookcCozcb3m\">reflective reasoning</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">This tag's focus is tentative and provisional because it is not altogether clear that \"consciousness in the sense of having experiences\" is a coherent idea, or one that's distinct from the other categories above. This tag is a practical tool for organizing discussion on a family of related topics, and isn't intended as a strong statement \"this is the right way of </span><a href=\"https://www.lesswrong.com/posts/d5NyJ2Lf6N22AD9PB/where-to-draw-the-boundary\"><span class=\"by_2aoRX3ookcCozcb3m\">carving nature at its joints</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\"</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Suffice to say that (as of December 8, 2020) </span><i><span class=\"by_2aoRX3ookcCozcb3m\">enough LessWrongers find consciousness confusing enough</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, and disagree enough about what's going on here, for it to make sense to use this page to organize discussion of those disagreements, rather than \"picking a winner\" immediately and running with it.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"_Having_experiences___Practical_implications\"><span class=\"by_2aoRX3ookcCozcb3m\">\"Having experiences\": Practical implications</span></h2><p><span class=\"by_2aoRX3ookcCozcb3m\">Beyond sheer curiosity about how the mind works, there are several sub-questions that have caused thinkers to take a special interest in the question \"what is 'having an experience'?\":</span></p><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">1. When should I care about something else's welfare?</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">1.1. </span><a href=\"https://www.lesswrong.com/tag/animal-welfare\"><span class=\"by_2aoRX3ookcCozcb3m\">Animal welfare</span></a><span class=\"by_2aoRX3ookcCozcb3m\">: Pain, pleasure, desire, etc. are commonly taken to be </span><i><span class=\"by_2aoRX3ookcCozcb3m\">experiences</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, and experiences of great moral importance. Knowing which species are capable of \"having experiences,\" then, could matter decisively in assessing the morality of factory farming and the morality of policies affecting wild animals.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1.2. Machine welfare and </span><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_2aoRX3ookcCozcb3m\">s-risks</span></a><span class=\"by_2aoRX3ookcCozcb3m\">: Similarly, knowing which kinds of (actual or potential) software have \"</span><a href=\"https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates\"><span class=\"by_2aoRX3ookcCozcb3m\">experiences</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" could tell us a great deal about which programs are morally important.</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2. When should I think of something as \"me\" (or \"relevantly me-like\")?</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">2.1. </span><a href=\"https://www.lesswrong.com/tag/personal-identity\"><span class=\"by_2aoRX3ookcCozcb3m\">Personal identity</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/whole-brain-emulation\"><span class=\"by_2aoRX3ookcCozcb3m\">whole brain emulation</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, and </span><a href=\"https://www.lesswrong.com/tag/simulation\"><span class=\"by_2aoRX3ookcCozcb3m\">simulations</span></a><span class=\"by_2aoRX3ookcCozcb3m\">: Normally, people care about their future selves (at least in part) because they anticipate </span><i><span class=\"by_2aoRX3ookcCozcb3m\">having those selves' experiences</span></i><span class=\"by_2aoRX3ookcCozcb3m\">. Thus, one might say: \"It doesn't make sense for me to sign up for </span><a href=\"https://www.lesswrong.com/tag/cryonics\"><span class=\"by_2aoRX3ookcCozcb3m\">cryonics</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, because a cryo-revived copy of me wouldn't be </span><i><span class=\"by_2aoRX3ookcCozcb3m\">me</span></i><span class=\"by_2aoRX3ookcCozcb3m\">.\" (Or, replying to 1.2 above, one might say \"It doesn't make sense for me to sign up for cryonics, because a cryo-revived emulation of me would be a mere automaton with no experiences.\")</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2.2. </span><a href=\"https://www.lesswrong.com/tag/anthropics\"><span class=\"by_2aoRX3ookcCozcb3m\">Anthropics</span></a><span class=\"by_2aoRX3ookcCozcb3m\">: Anthropic questions turn on how many copies of \"you\" exist, or how many copies of \"observers similar to you\" exist. One could speculate that this is related to the question of what makes a copy of you conscious, and what \"consciousness\" is in the first place.</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">3. Does the existence or nature of subjective experience imply any major updates about the world as a whole, about scientific methodology, etc.?</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">3.1. </span><a href=\"https://www.lesswrong.com/tag/reductionism\"><span class=\"by_2aoRX3ookcCozcb3m\">Reductionism</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, physicalism, and naturalism: Can experience be a mere matter of, uh, matter? If experience turned out to be irreducibly unphysical (and real), this would falsify some of the most well-established generalizations in science.</span></li></ul></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">LessWrong writers have typically been strongly on board with physicalism (3.1), and on board with the idea that an emulation of me is \"me\" (and conscious) in every sense that matters (2.1). Beyond that, however, views vary. (By comparison, ~74% of Anglophone philosophers of mind endorsed \"physicalism\" as opposed to \"non-physicalism\" </span><a href=\"https://philpapers.org/surveys/results.pl?affil=Target+faculty&amp;areas0=16&amp;areas_max=1&amp;grain=fine\"><span class=\"by_2aoRX3ookcCozcb3m\">in 2009</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.)</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"_Having_experiences___Pre_LessWrong_discussion\"><span class=\"by_2aoRX3ookcCozcb3m\">\"Having experiences\": Pre-LessWrong discussion</span></h2><p><span class=\"by_2aoRX3ookcCozcb3m\">How does this \"having experiences\" thing work, then? Well, this wiki page's editors haven't agreed on an answer yet. As a cop-out, we instead provide a list of highlights from the history of other people thinking about this.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">For concreteness, we'll list particular years, authors, and texts, even though this makes some choices of what to highlight more arbitrary. Philosophy also shows up much more than psychology or neuroscience proper, not because philosophy is necessarily the right way to make progress here, but because the philosophy highlights are more \"meta\" and therefore choosing what to include relies less on a LessWrong consensus about consciousness itself.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Highlights:</span></p><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">A long time ago BC: Someone comes up with the idea that \"minds\" are a pretty basic and fundamental feature of the world. Maybe gods have minds; maybe trees; maybe rivers; and so on. See also </span><a href=\"https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo/p/f4RJtHBPvDRJcCTva\"><span class=\"by_2aoRX3ookcCozcb3m\">When Anthropomorphism Became Stupid</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> and </span><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\"><span class=\"by_2aoRX3ookcCozcb3m\">Mind Projection Fallacy</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">~400 BC: Democritus proposes that all human-scale phenomena, including psychological phenomena, are the result of small physical parts bouncing off each other. From </span><a href=\"https://www.britannica.com/topic/materialism-philosophy/History-of-materialism\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Encyclopedia Britannica</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">: \"Democritus thought that the soul consists of smooth, round atoms and that perceptions consist of motions caused in the soul atoms by the atoms in the perceived thing.\"</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1641: René Descartes, </span><a href=\"https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes%27%20Meditations.pdf\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Meditations on First Philosophy</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">. Descartes argues that mind and matter must be irreducibly distinct (</span><strong><span class=\"by_2aoRX3ookcCozcb3m\">mind-body dualism</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">), because (e.g.) material things are spatially extended, while thoughts are not. Descartes speculates that minds interact with the physical world via a specific part of the brain, the pineal gland.</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">Descartes also popularizes the idea that everyone knows their own conscious experiences with certainty: at any given moment, we are infallible about </span><i><span class=\"by_2aoRX3ookcCozcb3m\">the fact </span></i><span class=\"by_2aoRX3ookcCozcb3m\">that we are having an experience (the \"cogito\"), and we are also infallible about the </span><i><span class=\"by_2aoRX3ookcCozcb3m\">contents</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> of that experience.</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1651: Thomas Hobbes, </span><a href=\"https://www.csus.edu/indiv/s/simpsonl/hist162/hobbes.pdf\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Leviathan</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">. Hobbes </span><a href=\"https://plato.stanford.edu/entries/hobbes/#3\"><span class=\"by_2aoRX3ookcCozcb3m\">insistently</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> asserts that everything (including the mind) is material, and can be thought of as a mechanism or machine.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1714: Gottfried Leibniz. </span><a href=\"https://plato.stanford.edu/entries/leibniz-mind/\"><i><span class=\"by_2aoRX3ookcCozcb3m\">The Monadology</span></i></a><i><span class=\"by_2aoRX3ookcCozcb3m\">.</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> Leibniz argues that mind can't be reduced to matter:</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">\"One is obliged to admit that </span><i><span class=\"by_2aoRX3ookcCozcb3m\">perception</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> and what depends upon it is inexplicable on mechanical principles, that is, by figures and motions. In imagining that there is a machine whose construction would enable it to think, to sense, and to have perception, one could conceive it enlarged while retaining the same proportions, so that one could enter into it, just like into a windmill. Supposing this, one should, when visiting within it, find only parts pushing one another, and never anything by which to explain a perception. Thus it is in the simple substance, and not in the composite or in the machine, that one must look for perception.\"</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1866: Charles Sanders Peirce, Lowell Lectures. Peirce </span><a href=\"https://colorysemiotica.files.wordpress.com/2014/08/peirce-collectedpapers.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">introduces</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> the term \"</span><a href=\"https://plato.stanford.edu/entries/qualia/\"><i><strong><span class=\"by_2aoRX3ookcCozcb3m\">qualia</span></strong></i></a><span class=\"by_2aoRX3ookcCozcb3m\">\" to refer to what it's like to have a specific experience — e.g., the particular experience of redness. </span><i><span class=\"by_2aoRX3ookcCozcb3m\">Qualia</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> is the plural of </span><i><span class=\"by_2aoRX3ookcCozcb3m\">quale</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, Latin for \"what kind of thing?\" and source of the English word </span><i><span class=\"by_2aoRX3ookcCozcb3m\">quality</span></i><span class=\"by_2aoRX3ookcCozcb3m\">.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1874: Thomas Huxley, \"</span><a href=\"http://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Huxley-English.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">On the Hypothesis that Animals Are Automata, and Its History</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" Huxley argues for </span><strong><span class=\"by_2aoRX3ookcCozcb3m\">epiphenomenalism</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">, the view that consciousness is </span><i><span class=\"by_2aoRX3ookcCozcb3m\">caused</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> by physical processes, but has no effects of its own.</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">\"The consciousness of brutes would appear to be related to the mechanism of their body simply as a collateral product of its working, and to be as completely without any power of modifying that working as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery. Their volition, if they have any, is an emotion indicative of physical changes, not a cause of such changes.\" And: \"to the best of my judgment, the argumentation which applies to brutes holds equally good of men\".</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1888: Santiago Ramón y Cajal, \"Estructura de los centros nerviosos de las aves.\" Using Camillo Golgi's staining method, Ramón y Cajal discovers that brains are made of neurons.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1903: G.E. Moore, \"</span><a href=\"https://fewd.univie.ac.at/fileadmin/user_upload/inst_ethik_wiss_dialog/Moore__G._1903._The_refutation_of_Idealism._in_MInd.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">The Refutation of Idealism</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" The early 20th century saw sharp moves away from spiritualism and supernaturalism in intellectual circles, beginning with the \"</span><a href=\"https://en.wikipedia.org/wiki/Bertrand_Russell%27s_philosophical_views#Analytic_philosophy\"><span class=\"by_2aoRX3ookcCozcb3m\">Cambridge revolt against idealism</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" Mysticism and metaphysical proclamations about the mind became increasingly unfashionable, as intellectuals grew more skeptical and more inclined to demand testable operationalizations of claims. Extreme manifestations of this attitude included logical positivism in the 1930s-1950s and behaviorism in the 1920s-1960s.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1943: McCulloch and Pitts, \"</span><a href=\"https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">A Logical Calculus of the Ideas Immanent in Nervous Activity</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" </span><a href=\"https://plato.stanford.edu/entries/computational-mind/\"><i><span class=\"by_2aoRX3ookcCozcb3m\">SEP</span></i></a><i><span class=\"by_2aoRX3ookcCozcb3m\"> </span></i><span class=\"by_2aoRX3ookcCozcb3m\">writes that this paper \"first suggested that something resembling the Turing machine might provide a good model for the mind.\" Subsequent developments in this direction include the cognitive revolution and the rise of </span><a href=\"https://plato.stanford.edu/entries/functionalism/\"><strong><span class=\"by_2aoRX3ookcCozcb3m\">functionalist</span></strong></a><span class=\"by_2aoRX3ookcCozcb3m\"> and </span><a href=\"https://plato.stanford.edu/entries/computational-mind/\"><strong><span class=\"by_2aoRX3ookcCozcb3m\">computational</span></strong></a><span class=\"by_2aoRX3ookcCozcb3m\"> accounts of the mind, which supplanted behaviorism.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1968: David Armstrong, </span><i><span class=\"by_2aoRX3ookcCozcb3m\">A Materialist Theory of the Mind</span></i><span class=\"by_2aoRX3ookcCozcb3m\">. An early attempt to sketch a theory of consciousness (specifically, a </span><a href=\"https://plato.stanford.edu/entries/consciousness-higher/\"><span class=\"by_2aoRX3ookcCozcb3m\">higher-order</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> theory). For an overview of popular theories or sketches-of-theories in the following decades, see </span><i><span class=\"by_2aoRX3ookcCozcb3m\">SEP</span></i><span class=\"by_2aoRX3ookcCozcb3m\">'s review article \"</span><a href=\"https://plato.stanford.edu/entries/consciousness-neuroscience/\"><span class=\"by_2aoRX3ookcCozcb3m\">The Neuroscience of Consciousness</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\"</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1974: Thomas Nagel, \"</span><a href=\"http://www.esalq.usp.br/lepse/imgs/conteudo_thumb/What-Is-It-Like-to-Be-a-Bat-1.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">What Is It Like To Be A Bat?</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" Nagel writes that \"fundamentally an organism has conscious mental states if and only if there is something that it is like to </span><i><span class=\"by_2aoRX3ookcCozcb3m\">be</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> that organism—something it is like for the organism.\" And:</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">\"If physicalism is to be defended, the phenomenological features [i.e., what it's like to have certain experiences] must themselves be given a physical account. But when we examine their subjective character it seems that such a result is impossible. The reason is that every subjective phenomenon is essentially connected with a single point of view, and it seems inevitable that an objective, physical theory will abandon that point of view.\"</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">Subsequent authors have tended to use terms like \"</span><strong><span class=\"by_2aoRX3ookcCozcb3m\">what it's like</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">,\" \"</span><strong><span class=\"by_2aoRX3ookcCozcb3m\">phenomenal consciousness</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">\" (derived from </span><i><span class=\"by_2aoRX3ookcCozcb3m\">phenomena</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> in the sense of \"appearances\"), and </span><i><span class=\"by_2aoRX3ookcCozcb3m\">qualia </span></i><span class=\"by_2aoRX3ookcCozcb3m\">to gesture at this apparent puzzle. These are closely related terms, used in slightly different ways by different authors.</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1974: Robert Kirk, \"</span><a href=\"https://academic.oup.com/aristoteliansupp/article-abstract/48/1/135/1779753?redirectedFrom=fulltext\"><span class=\"by_2aoRX3ookcCozcb3m\">Zombies v. Materialists</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" This paper introduces the </span><strong><span class=\"by_2aoRX3ookcCozcb3m\">philosophical zombie</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">, or </span><strong><span class=\"by_2aoRX3ookcCozcb3m\">p-zombie</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">: a hypothetical being that is physically identical to a conscious person, but lacks consciousness. If the idea of p-zombies has no hidden logical inconsistencies, it is argued, then consciousness is not logically entailed by organisms' physical properties, which would make physicalism false.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1982: Frank Jackson, \"</span><a href=\"https://www.sfu.ca/~jillmc/JacksonfromJStore.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">Epiphenomenal Qualia</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">.\" Jackson argues that we can imagine a scientist, Mary, who knows all the physical facts about color but has never seen the color red for herself. If she then sees red, it seems as though she learns a </span><span class=\"by_Lo9GspCs3L2YQvqrW\">new</span><span class=\"by_2aoRX3ookcCozcb3m\"> fact—she learns what it's like to experience redness. Jackson takes this to mean that there are further facts beyond the physical facts, and that physicalism is therefore false. (For subsequent discussion, see </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">SEP</span></i><span class=\"by_2aoRX3ookcCozcb3m\">'s \"</span><a href=\"https://plato.stanford.edu/entries/qualia-knowledge/\"><span class=\"by_2aoRX3ookcCozcb3m\">Qualia: The Knowledge Argument</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\")</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">1996: David Chalmers, </span><a href=\"http://consc.net/books/tcm/intro.html\"><i><span class=\"by_2aoRX3ookcCozcb3m\">The Conscious Mind: In Search of a Fundamental Theory</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\">. Chalmers argues against physicalism, leaning heavily on the zombie argument and the Mary argument.</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">Chalmers speaks of the \"</span><strong><span class=\"by_2aoRX3ookcCozcb3m\">hard problem of consciousness</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">,\" the problem of explaining why we are phenomenally conscious (i.e., why we aren't p-zombies). \"Many books and articles on consciousness have appeared in the last few years, and one might think that we are making progress. But on a closer look, most of this work leaves the hardest problems about consciousness untouched. Often, this work addresses what might be called the 'easy' problems of consciousness: How does the brain process environmental stimulation? How does it integrate information? How do we produce reports on internal states? These are important questions, but to answer them is not to solve the hard problem: why is all this processing accompanied by an experienced inner life?\"</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">While Chalmers discussed consciousness earlier (e.g., in </span><a href=\"http://consc.net/papers/qualia.html\"><span class=\"by_2aoRX3ookcCozcb3m\">1993</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"http://consc.net/papers/facing.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">1994</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, and </span><a href=\"http://consc.net/papers/moving.html\"><span class=\"by_2aoRX3ookcCozcb3m\">1996</span></a><span class=\"by_2aoRX3ookcCozcb3m\">), </span><i><span class=\"by_2aoRX3ookcCozcb3m\">The Conscious Mind</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> is the work that brought dualistic and quasi-dualistic views back into the intellectual almost-mainstream for the first time in a century. In spite of its crazy-sounding conclusions, the book is unusually clear, rigorous, and thorough, anticipating almost all of the obvious objections; and Chalmers attempts to make the irreducibility of consciousness more palatable to scientists by endorsing what he calls \"naturalistic dualism\": the view that consciousness is lawful, predictable, and not specific to humans. Chalmers argues that our consciousness depends on stable (but contingent) \"psychophysical laws\" that would also (for example) make a whole-brain emulation conscious.</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2003. Max Tegmark, \"</span><a href=\"https://space.mit.edu/home/tegmark/multiverse.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">Parallel Universes</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" Although not explicitly concerned with consciousness, Tegmark's picture raises problems for </span><a href=\"https://www.lesswrong.com/tag/anthropics\"><span class=\"by_2aoRX3ookcCozcb3m\">anthropics</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> and our understanding of what makes an observer \"real.\"</span></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"_Having_experiences___Recent_discussion\"><span class=\"by_2aoRX3ookcCozcb3m\">\"Having experiences\": Recent discussion</span></h2><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">2008. Eliezer Yudkowsky, \"</span><a href=\"https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies\"><span class=\"by_2aoRX3ookcCozcb3m\">Zombies! Zombies?</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" This and other posts from </span><a href=\"https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo\"><span class=\"by_2aoRX3ookcCozcb3m\">Physicalism 201</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> argue that we can be confident physicalism is true, even without knowing how to solve (or </span><a href=\"https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question\"><span class=\"by_2aoRX3ookcCozcb3m\">dissolve</span></a><span class=\"by_2aoRX3ookcCozcb3m\">) the \"hard problem of consciousness\".</span><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">In particular, Yudkowsky argues that accepting the possibility of p-zombies is tantamount to accepting epiphenomenalism, and that epiphenomenalism is crazy. If our claims about consciousness are </span><i><span class=\"by_2aoRX3ookcCozcb3m\">true</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> even though consciousness has no causal effect on what we claim (because a p-zombie would move its lips and pen exactly as we do), then our claims would have to be true </span><i><span class=\"by_2aoRX3ookcCozcb3m\">by coincidence</span></i><span class=\"by_2aoRX3ookcCozcb3m\">, which is absurd given the Bayesian understanding of evidence and knowledge.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">More generally, LessWrong writers' views on consciousness have been heavily influenced by the intuition pumps and reasoning rules Yudkowsky writes about in the Sequences (2006–2009), such as: </span><a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><span class=\"by_2aoRX3ookcCozcb3m\">Making Beliefs Pay Rent</span></a><span class=\"by_2aoRX3ookcCozcb3m\">; </span><a href=\"https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available\"><span class=\"by_2aoRX3ookcCozcb3m\">Making History Available</span></a><span class=\"by_2aoRX3ookcCozcb3m\">; </span><a href=\"https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take\"><span class=\"by_2aoRX3ookcCozcb3m\">How Much Evidence Does It Take?</span></a><span class=\"by_2aoRX3ookcCozcb3m\">; </span><a href=\"https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition\"><span class=\"by_2aoRX3ookcCozcb3m\">The Second Law of Thermodynamics and Engines of Cognition</span></a><span class=\"by_2aoRX3ookcCozcb3m\">; and </span><a href=\"https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/TynBiYt6zg42StRbb\"><span class=\"by_2aoRX3ookcCozcb3m\">My Kind of Reflection</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></li></ul></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2008. Eliezer Yudkowsky, \"</span><a href=\"https://www.lesswrong.com/posts/xsZnufn3cQw7tJeQ3/collapse-postulates\"><span class=\"by_2aoRX3ookcCozcb3m\">Collapse Postulates</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" This and other posts from the </span><a href=\"https://www.lesswrong.com/posts/hc9Eg6erp6hk9bWhn/the-quantum-physics-sequence\"><span class=\"by_2aoRX3ookcCozcb3m\">Quantum Physics Sequence</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> argue that physicists' belief that observers or consciousness play a privileged role in quantum phenomena is based on a series of confusions and misunderstandings.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2013. David Chalmers, \"</span><a href=\"http://consc.net/papers/panpsychism.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">Panpsychism and Panprotopsychism</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">.\" Chalmers argues that everything in the universe (down </span><span class=\"by_Lo9GspCs3L2YQvqrW\">to</span><span class=\"by_2aoRX3ookcCozcb3m\"> the subatomic level) is \"conscious\" or \"proto-conscious.\"</span></span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2014. Benya Fallenstein. \"</span><a href=\"https://www.lesswrong.com/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies\"><span class=\"by_2aoRX3ookcCozcb3m\">L-zombies! L-zombies?</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" Fallenstein asks how we can distinguish between instantiated observers and uninstantiated (\"merely logical\") observers.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2016: Keith Frankish. \"</span><a href=\"https://nbviewer.jupyter.org/github/k0711/kf_articles/blob/master/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">Illusionism as a Theory of Consciousness</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" Frankish argues that \"experiences do not really have qualitative, 'what-it’s-like' properties.\" Instead, subjective experience seems \"unphysical\" or \"irreducible\" because of a sort of introspective illusion.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2017: Luke Muehlhauser, \"</span><a href=\"https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood\"><span class=\"by_2aoRX3ookcCozcb3m\">2017 Report on Consciousness and Moral Patienthood</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" The single largest work of scholarship on consciousness by the rationality community.</span></li><li><span class=\"by_2aoRX3ookcCozcb3m\">2018: David Chalmers, \"</span><a href=\"https://philpapers.org/archive/chatmo-32.pdf\"><span class=\"by_2aoRX3ookcCozcb3m\">The Meta-Problem of Consciousness</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.\" Chalmers discusses \"the problem of explaining why we think consciousness poses a hard problem\".</span></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Related_pages\"><span class=\"by_2aoRX3ookcCozcb3m\">Related pages</span></h2><ul><li><span class=\"by_2aoRX3ookcCozcb3m\">Non-tags: </span><a href=\"https://www.lesswrong.com/tag/anthropomorphism\"><span class=\"by_2aoRX3ookcCozcb3m\">Anthropomorphism</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/how-an-algorithm-feels\"><span class=\"by_2aoRX3ookcCozcb3m\">How an algorithm feels</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/zombies\"><span class=\"by_2aoRX3ookcCozcb3m\">Zombies</span></a></li><li><a href=\"https://www.lesswrong.com/tag/identity\"><span class=\"by_2aoRX3ookcCozcb3m\">Identity</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/personal-identity\"><span class=\"by_2aoRX3ookcCozcb3m\">Personal identity</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/reflective-reasoning\"><span class=\"by_2aoRX3ookcCozcb3m\">Reflective reasoning</span></a></li><li><a href=\"https://www.lesswrong.com/tag/animal-welfare\"><span class=\"by_2aoRX3ookcCozcb3m\">Animal welfare</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/suffering\"><span class=\"by_2aoRX3ookcCozcb3m\">Suffering</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks\"><span class=\"by_2aoRX3ookcCozcb3m\">Risks of astronomical suffering (s-risks)</span></a></li><li><a href=\"https://www.lesswrong.com/tag/reductionism\"><span class=\"by_2aoRX3ookcCozcb3m\">Reductionism</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, </span><a href=\"https://www.lesswrong.com/tag/mind-projection-fallacy\"><span class=\"by_2aoRX3ookcCozcb3m\">Mind Projection Fallacy</span></a></li><li><a href=\"https://www.lesswrong.com/tag/quantum-mechanics\"><span class=\"by_2aoRX3ookcCozcb3m\">Quantum mechanics</span></a></li></ul>",
      "sections": [
        {
          "title": "\"Having experiences\": Practical implications",
          "anchor": "_Having_experiences___Practical_implications",
          "level": 1
        },
        {
          "title": "\"Having experiences\": Pre-LessWrong discussion",
          "anchor": "_Having_experiences___Pre_LessWrong_discussion",
          "level": 1
        },
        {
          "title": "\"Having experiences\": Recent discussion",
          "anchor": "_Having_experiences___Recent_discussion",
          "level": 1
        },
        {
          "title": "Related pages",
          "anchor": "Related_pages",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 129,
    "description": {
      "markdown": "The word \"**consciousness**\" is used in a variety of different ways, and there are large disagreements about the reality and nature (and even coherence) of some of the things people profess to mean by \"consciousness.\"\n\nColloquially, the word \"conscious\" is used to pick out a few different things:\n\n*   **Wakefulness** \\- The property that distinguishes, e.g., a person who is awake from a person who is asleep.\n    *   We call people \"unconscious\" in this sense based on observed features like \"sharply reduced mobility,\" though we wouldn't normally call someone unconscious if we think they're merely *paralyzed*. Instead, calling someone \"unconscious\" tends to imply reduced ability to perceive and/or reason about events in one's environment.\n    *   An unconscious person (in this sense) might or might not be dreaming; and if dreaming, they might or might not be lucid.\n*   **Having experiences** \\- The property that distinguishes, e.g., a comatose person who is having experiences from a comatose person who is not having experiences.\n*   **Knowledge, perception,** and/or **attention** \\- E.g., we might say that someone becomes \"conscious of\" a fact when they first learn that fact. Or we might say that they become \"conscious of\" something whenever they're currently perceiving it, or whenever they're *paying attention* to it.\n*   **Meta-cognition** or **reflective awareness** \\- Knowing, perceiving, and/or attending to your own mental states; or knowing, perceiving, and/or attending to *the fact that* you have certain mental states.\n    *   E.g., we might say that someone is less \"conscious\" when they're fully immersed in a novel than when they're thinking about their own experiences, directing attention to the fact that they're reading a book, etc.\n*   **Self-awareness** \\- Knowing, perceiving, and/or attending to your own existence or your own central properties.\n    *   Depending on what exactly is meant by \"self-awareness,\" the \"immersed in a novel\" example might also involve less self-awareness. In some weaker senses of \"self-aware,\" one might instead claim that humans who are experiencing anything are always \"self-aware.\"\n\nReasonably mainstream academic overviews of \"consciousness\" can be found in the [*Stanford Encyclopedia of Philosophy*](https://plato.stanford.edu/entries/consciousness/) and the [*MIT Encyclopedia of the Cognitive Sciences*](http://www.mkdavies.net/Martin_Davies/Mind_files/ConsciousnessMITECS.pdf).\n\nThis tag is *tentatively and provisionally* about the \"**having experiences**\" meaning(s) of \"consciousness.\" For wakefulness and dreaming, see [sleep](https://www.lesswrong.com/tag/sleep). For knowledge, perception, and attention, see [attention](https://www.lesswrong.com/tag/attention) and [cognitive science](https://www.lesswrong.com/tag/cognitive-science). And for reflective awareness and self-awareness, see [identity](https://www.lesswrong.com/tag/identity), [personal identity](https://www.lesswrong.com/tag/personal-identity), and [reflective reasoning](https://www.lesswrong.com/tag/reflective-reasoning).\n\nThis tag's focus is tentative and provisional because it is not altogether clear that \"consciousness in the sense of having experiences\" is a coherent idea, or one that's distinct from the other categories above. This tag is a practical tool for organizing discussion on a family of related topics, and isn't intended as a strong statement \"this is the right way of [carving nature at its joints](https://www.lesswrong.com/posts/d5NyJ2Lf6N22AD9PB/where-to-draw-the-boundary).\"\n\nSuffice to say that (as of December 8, 2020) *enough LessWrongers find consciousness confusing enough*, and disagree enough about what's going on here, for it to make sense to use this page to organize discussion of those disagreements, rather than \"picking a winner\" immediately and running with it.\n\n\"Having experiences\": Practical implications\n--------------------------------------------\n\nBeyond sheer curiosity about how the mind works, there are several sub-questions that have caused thinkers to take a special interest in the question \"what is 'having an experience'?\":\n\n*   1\\. When should I care about something else's welfare?\n    *   1.1. [Animal welfare](https://www.lesswrong.com/tag/animal-welfare): Pain, pleasure, desire, etc. are commonly taken to be *experiences*, and experiences of great moral importance. Knowing which species are capable of \"having experiences,\" then, could matter decisively in assessing the morality of factory farming and the morality of policies affecting wild animals.\n    *   1.2. Machine welfare and [s-risks](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks): Similarly, knowing which kinds of (actual or potential) software have \"[experiences](https://www.lesswrong.com/posts/wqDRRx9RqwKLzWt7R/nonperson-predicates)\" could tell us a great deal about which programs are morally important.\n*   2\\. When should I think of something as \"me\" (or \"relevantly me-like\")?\n    *   2.1. [Personal identity](https://www.lesswrong.com/tag/personal-identity), [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation), and [simulations](https://www.lesswrong.com/tag/simulation): Normally, people care about their future selves (at least in part) because they anticipate *having those selves' experiences*. Thus, one might say: \"It doesn't make sense for me to sign up for [cryonics](https://www.lesswrong.com/tag/cryonics), because a cryo-revived copy of me wouldn't be *me*.\" (Or, replying to 1.2 above, one might say \"It doesn't make sense for me to sign up for cryonics, because a cryo-revived emulation of me would be a mere automaton with no experiences.\")\n    *   2.2. [Anthropics](https://www.lesswrong.com/tag/anthropics): Anthropic questions turn on how many copies of \"you\" exist, or how many copies of \"observers similar to you\" exist. One could speculate that this is related to the question of what makes a copy of you conscious, and what \"consciousness\" is in the first place.\n*   3\\. Does the existence or nature of subjective experience imply any major updates about the world as a whole, about scientific methodology, etc.?\n    *   3.1. [Reductionism](https://www.lesswrong.com/tag/reductionism), physicalism, and naturalism: Can experience be a mere matter of, uh, matter? If experience turned out to be irreducibly unphysical (and real), this would falsify some of the most well-established generalizations in science.\n\nLessWrong writers have typically been strongly on board with physicalism (3.1), and on board with the idea that an emulation of me is \"me\" (and conscious) in every sense that matters (2.1). Beyond that, however, views vary. (By comparison, ~74% of Anglophone philosophers of mind endorsed \"physicalism\" as opposed to \"non-physicalism\" [in 2009](https://philpapers.org/surveys/results.pl?affil=Target+faculty&areas0=16&areas_max=1&grain=fine).)\n\n\"Having experiences\": Pre-LessWrong discussion\n----------------------------------------------\n\nHow does this \"having experiences\" thing work, then? Well, this wiki page's editors haven't agreed on an answer yet. As a cop-out, we instead provide a list of highlights from the history of other people thinking about this.\n\nFor concreteness, we'll list particular years, authors, and texts, even though this makes some choices of what to highlight more arbitrary. Philosophy also shows up much more than psychology or neuroscience proper, not because philosophy is necessarily the right way to make progress here, but because the philosophy highlights are more \"meta\" and therefore choosing what to include relies less on a LessWrong consensus about consciousness itself.\n\nHighlights:\n\n*   A long time ago BC: Someone comes up with the idea that \"minds\" are a pretty basic and fundamental feature of the world. Maybe gods have minds; maybe trees; maybe rivers; and so on. See also [When Anthropomorphism Became Stupid](https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo/p/f4RJtHBPvDRJcCTva) and [Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy).\n*   ~400 BC: Democritus proposes that all human-scale phenomena, including psychological phenomena, are the result of small physical parts bouncing off each other. From [*Encyclopedia Britannica*](https://www.britannica.com/topic/materialism-philosophy/History-of-materialism): \"Democritus thought that the soul consists of smooth, round atoms and that perceptions consist of motions caused in the soul atoms by the atoms in the perceived thing.\"\n*   1641: René Descartes, [*Meditations on First Philosophy*](https://yale.learningu.org/download/041e9642-df02-4eed-a895-70e472df2ca4/H2665_Descartes%27%20Meditations.pdf). Descartes argues that mind and matter must be irreducibly distinct (**mind-body dualism**), because (e.g.) material things are spatially extended, while thoughts are not. Descartes speculates that minds interact with the physical world via a specific part of the brain, the pineal gland.\n    *   Descartes also popularizes the idea that everyone knows their own conscious experiences with certainty: at any given moment, we are infallible about *the fact* that we are having an experience (the \"cogito\"), and we are also infallible about the *contents* of that experience.\n*   1651: Thomas Hobbes, [*Leviathan*](https://www.csus.edu/indiv/s/simpsonl/hist162/hobbes.pdf). Hobbes [insistently](https://plato.stanford.edu/entries/hobbes/#3) asserts that everything (including the mind) is material, and can be thought of as a mechanism or machine.\n*   1714: Gottfried Leibniz. [*The Monadology*](https://plato.stanford.edu/entries/leibniz-mind/)*.* Leibniz argues that mind can't be reduced to matter:\n    *   \"One is obliged to admit that *perception* and what depends upon it is inexplicable on mechanical principles, that is, by figures and motions. In imagining that there is a machine whose construction would enable it to think, to sense, and to have perception, one could conceive it enlarged while retaining the same proportions, so that one could enter into it, just like into a windmill. Supposing this, one should, when visiting within it, find only parts pushing one another, and never anything by which to explain a perception. Thus it is in the simple substance, and not in the composite or in the machine, that one must look for perception.\"\n*   1866: Charles Sanders Peirce, Lowell Lectures. Peirce [introduces](https://colorysemiotica.files.wordpress.com/2014/08/peirce-collectedpapers.pdf) the term \"[***qualia***](https://plato.stanford.edu/entries/qualia/)\" to refer to what it's like to have a specific experience — e.g., the particular experience of redness. *Qualia* is the plural of *quale*, Latin for \"what kind of thing?\" and source of the English word *quality*.\n*   1874: Thomas Huxley, \"[On the Hypothesis that Animals Are Automata, and Its History](http://opessoa.fflch.usp.br/sites/opessoa.fflch.usp.br/files/Huxley-English.pdf).\" Huxley argues for **epiphenomenalism**, the view that consciousness is *caused* by physical processes, but has no effects of its own.\n    *   \"The consciousness of brutes would appear to be related to the mechanism of their body simply as a collateral product of its working, and to be as completely without any power of modifying that working as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery. Their volition, if they have any, is an emotion indicative of physical changes, not a cause of such changes.\" And: \"to the best of my judgment, the argumentation which applies to brutes holds equally good of men\".\n*   1888: Santiago Ramón y Cajal, \"Estructura de los centros nerviosos de las aves.\" Using Camillo Golgi's staining method, Ramón y Cajal discovers that brains are made of neurons.\n*   1903: G.E. Moore, \"[The Refutation of Idealism](https://fewd.univie.ac.at/fileadmin/user_upload/inst_ethik_wiss_dialog/Moore__G._1903._The_refutation_of_Idealism._in_MInd.pdf).\" The early 20th century saw sharp moves away from spiritualism and supernaturalism in intellectual circles, beginning with the \"[Cambridge revolt against idealism](https://en.wikipedia.org/wiki/Bertrand_Russell%27s_philosophical_views#Analytic_philosophy).\" Mysticism and metaphysical proclamations about the mind became increasingly unfashionable, as intellectuals grew more skeptical and more inclined to demand testable operationalizations of claims. Extreme manifestations of this attitude included logical positivism in the 1930s-1950s and behaviorism in the 1920s-1960s.\n*   1943: McCulloch and Pitts, \"[A Logical Calculus of the Ideas Immanent in Nervous Activity](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf).\" [*SEP*](https://plato.stanford.edu/entries/computational-mind/)  writes that this paper \"first suggested that something resembling the Turing machine might provide a good model for the mind.\" Subsequent developments in this direction include the cognitive revolution and the rise of [**functionalist**](https://plato.stanford.edu/entries/functionalism/) and [**computational**](https://plato.stanford.edu/entries/computational-mind/) accounts of the mind, which supplanted behaviorism.\n*   1968: David Armstrong, *A Materialist Theory of the Mind*. An early attempt to sketch a theory of consciousness (specifically, a [higher-order](https://plato.stanford.edu/entries/consciousness-higher/) theory). For an overview of popular theories or sketches-of-theories in the following decades, see *SEP*'s review article \"[The Neuroscience of Consciousness](https://plato.stanford.edu/entries/consciousness-neuroscience/).\"\n*   1974: Thomas Nagel, \"[What Is It Like To Be A Bat?](http://www.esalq.usp.br/lepse/imgs/conteudo_thumb/What-Is-It-Like-to-Be-a-Bat-1.pdf)\" Nagel writes that \"fundamentally an organism has conscious mental states if and only if there is something that it is like to *be* that organism—something it is like for the organism.\" And:\n    *   \"If physicalism is to be defended, the phenomenological features \\[i.e., what it's like to have certain experiences\\] must themselves be given a physical account. But when we examine their subjective character it seems that such a result is impossible. The reason is that every subjective phenomenon is essentially connected with a single point of view, and it seems inevitable that an objective, physical theory will abandon that point of view.\"\n    *   Subsequent authors have tended to use terms like \"**what it's like**,\" \"**phenomenal consciousness**\" (derived from *phenomena* in the sense of \"appearances\"), and *qualia* to gesture at this apparent puzzle. These are closely related terms, used in slightly different ways by different authors.\n*   1974: Robert Kirk, \"[Zombies v. Materialists](https://academic.oup.com/aristoteliansupp/article-abstract/48/1/135/1779753?redirectedFrom=fulltext).\" This paper introduces the **philosophical zombie**, or **p-zombie**: a hypothetical being that is physically identical to a conscious person, but lacks consciousness. If the idea of p-zombies has no hidden logical inconsistencies, it is argued, then consciousness is not logically entailed by organisms' physical properties, which would make physicalism false.\n*   1982: Frank Jackson, \"[Epiphenomenal Qualia](https://www.sfu.ca/~jillmc/JacksonfromJStore.pdf).\" Jackson argues that we can imagine a scientist, Mary, who knows all the physical facts about color but has never seen the color red for herself. If she then sees red, it seems as though she learns a new fact—she learns what it's like to experience redness. Jackson takes this to mean that there are further facts beyond the physical facts, and that physicalism is therefore false. (For subsequent discussion, see *SEP*'s \"[Qualia: The Knowledge Argument](https://plato.stanford.edu/entries/qualia-knowledge/).\")\n*   1996: David Chalmers, [*The Conscious Mind: In Search of a Fundamental Theory*](http://consc.net/books/tcm/intro.html). Chalmers argues against physicalism, leaning heavily on the zombie argument and the Mary argument.\n    *   Chalmers speaks of the \"**hard problem of consciousness**,\" the problem of explaining why we are phenomenally conscious (i.e., why we aren't p-zombies). \"Many books and articles on consciousness have appeared in the last few years, and one might think that we are making progress. But on a closer look, most of this work leaves the hardest problems about consciousness untouched. Often, this work addresses what might be called the 'easy' problems of consciousness: How does the brain process environmental stimulation? How does it integrate information? How do we produce reports on internal states? These are important questions, but to answer them is not to solve the hard problem: why is all this processing accompanied by an experienced inner life?\"\n    *   While Chalmers discussed consciousness earlier (e.g., in [1993](http://consc.net/papers/qualia.html), [1994](http://consc.net/papers/facing.pdf), and [1996](http://consc.net/papers/moving.html)), *The Conscious Mind* is the work that brought dualistic and quasi-dualistic views back into the intellectual almost-mainstream for the first time in a century. In spite of its crazy-sounding conclusions, the book is unusually clear, rigorous, and thorough, anticipating almost all of the obvious objections; and Chalmers attempts to make the irreducibility of consciousness more palatable to scientists by endorsing what he calls \"naturalistic dualism\": the view that consciousness is lawful, predictable, and not specific to humans. Chalmers argues that our consciousness depends on stable (but contingent) \"psychophysical laws\" that would also (for example) make a whole-brain emulation conscious.\n*   2003\\. Max Tegmark, \"[Parallel Universes](https://space.mit.edu/home/tegmark/multiverse.pdf).\" Although not explicitly concerned with consciousness, Tegmark's picture raises problems for [anthropics](https://www.lesswrong.com/tag/anthropics) and our understanding of what makes an observer \"real.\"\n\n\"Having experiences\": Recent discussion\n---------------------------------------\n\n*   2008\\. Eliezer Yudkowsky, \"[Zombies! Zombies?](https://www.lesswrong.com/posts/fdEWWr8St59bXLbQr/zombies-zombies)\" This and other posts from [Physicalism 201](https://www.lesswrong.com/s/FqgKAHZAiZn9JAjDo) argue that we can be confident physicalism is true, even without knowing how to solve (or [dissolve](https://www.lesswrong.com/posts/Mc6QcrsbH5NRXbCRX/dissolving-the-question)) the \"hard problem of consciousness\".\n    *   In particular, Yudkowsky argues that accepting the possibility of p-zombies is tantamount to accepting epiphenomenalism, and that epiphenomenalism is crazy. If our claims about consciousness are *true* even though consciousness has no causal effect on what we claim (because a p-zombie would move its lips and pen exactly as we do), then our claims would have to be true *by coincidence*, which is absurd given the Bayesian understanding of evidence and knowledge.\n    *   More generally, LessWrong writers' views on consciousness have been heavily influenced by the intuition pumps and reasoning rules Yudkowsky writes about in the Sequences (2006–2009), such as: [Making Beliefs Pay Rent](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences); [Making History Available](https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available); [How Much Evidence Does It Take?](https://www.lesswrong.com/posts/nj8JKFoLSMEmD3RGp/how-much-evidence-does-it-take); [The Second Law of Thermodynamics and Engines of Cognition](https://www.lesswrong.com/posts/QkX2bAkwG2EpGvNug/the-second-law-of-thermodynamics-and-engines-of-cognition); and [My Kind of Reflection](https://www.lesswrong.com/s/9bvAELWc8y2gYjRav/p/TynBiYt6zg42StRbb).\n*   2008\\. Eliezer Yudkowsky, \"[Collapse Postulates](https://www.lesswrong.com/posts/xsZnufn3cQw7tJeQ3/collapse-postulates).\" This and other posts from the [Quantum Physics Sequence](https://www.lesswrong.com/posts/hc9Eg6erp6hk9bWhn/the-quantum-physics-sequence) argue that physicists' belief that observers or consciousness play a privileged role in quantum phenomena is based on a series of confusions and misunderstandings.\n*   2013\\. David Chalmers, \"[Panpsychism and Panprotopsychism](http://consc.net/papers/panpsychism.pdf).\" Chalmers argues that everything in the universe (down to the subatomic level) is \"conscious\" or \"proto-conscious.\"\n*   2014\\. Benya Fallenstein. \"[L-zombies! L-zombies?](https://www.lesswrong.com/posts/7nAxgQYGYrEY5ZCAD/l-zombies-l-zombies)\" Fallenstein asks how we can distinguish between instantiated observers and uninstantiated (\"merely logical\") observers.\n*   2016: Keith Frankish. \"[Illusionism as a Theory of Consciousness](https://nbviewer.jupyter.org/github/k0711/kf_articles/blob/master/Frankish_Illusionism%20as%20a%20theory%20of%20consciousness_eprint.pdf).\" Frankish argues that \"experiences do not really have qualitative, 'what-it’s-like' properties.\" Instead, subjective experience seems \"unphysical\" or \"irreducible\" because of a sort of introspective illusion.\n*   2017: Luke Muehlhauser, \"[2017 Report on Consciousness and Moral Patienthood](https://www.openphilanthropy.org/2017-report-consciousness-and-moral-patienthood).\" The single largest work of scholarship on consciousness by the rationality community.\n*   2018: David Chalmers, \"[The Meta-Problem of Consciousness](https://philpapers.org/archive/chatmo-32.pdf).\" Chalmers discusses \"the problem of explaining why we think consciousness poses a hard problem\".\n\nRelated pages\n-------------\n\n*   Non-tags: [Anthropomorphism](https://www.lesswrong.com/tag/anthropomorphism), [How an algorithm feels](https://www.lesswrong.com/tag/how-an-algorithm-feels), [Zombies](https://www.lesswrong.com/tag/zombies)\n*   [Identity](https://www.lesswrong.com/tag/identity), [Personal identity](https://www.lesswrong.com/tag/personal-identity), [Reflective reasoning](https://www.lesswrong.com/tag/reflective-reasoning)\n*   [Animal welfare](https://www.lesswrong.com/tag/animal-welfare), [Suffering](https://www.lesswrong.com/tag/suffering), [Risks of astronomical suffering (s-risks)](https://www.lesswrong.com/tag/risks-of-astronomical-suffering-s-risks)\n*   [Reductionism](https://www.lesswrong.com/tag/reductionism), [Mind Projection Fallacy](https://www.lesswrong.com/tag/mind-projection-fallacy)\n*   [Quantum mechanics](https://www.lesswrong.com/tag/quantum-mechanics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fpEBgFE7fgpxTm9BF",
    "name": "Machine Learning  (ML)",
    "core": null,
    "slug": "machine-learning-ml",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_LedhurJxi3baDAKDZ\">Machine Learning</span></strong><span><span class=\"by_LedhurJxi3baDAKDZ\"> refers to the general field of study that deals with automated statistical learning and pattern detection by non-biological systems. It can be seen as a sub-domain of </span><span class=\"by_mcKSiwq2TBrTMZS6X\">artificial</span><span class=\"by_LedhurJxi3baDAKDZ\"> intelligence that specifically deals with modeling and prediction through the knowledge extracted from </span><span class=\"by_mcKSiwq2TBrTMZS6X\">training data.</span><span class=\"by_LedhurJxi3baDAKDZ\"> As a multi-disciplinary area, it has borrowed concepts and ideas from</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> other areas like</span><span class=\"by_LedhurJxi3baDAKDZ\"> pure mathematics </span><span class=\"by_mcKSiwq2TBrTMZS6X\">and</span><span class=\"by_LedhurJxi3baDAKDZ\"> cognitive </span><span class=\"by_mcKSiwq2TBrTMZS6X\">science.</span></span></p><h2 id=\"Understanding_different_machine_learning_algorithms\"><span><span class=\"by_mcKSiwq2TBrTMZS6X\">Understanding different machine</span><span class=\"by_LedhurJxi3baDAKDZ\"> learning algorithms</span></span></h2><p><span><span class=\"by_mcKSiwq2TBrTMZS6X\">The</span><span class=\"by_LedhurJxi3baDAKDZ\"> most widely </span><span class=\"by_mcKSiwq2TBrTMZS6X\">used </span><span class=\"by_LedhurJxi3baDAKDZ\">distinction is between unsupervised </span><span class=\"by_mcKSiwq2TBrTMZS6X\">(e.g.</span><span class=\"by_LedhurJxi3baDAKDZ\"> k-means clustering, </span><span class=\"by_mcKSiwq2TBrTMZS6X\">principal component analysis)</span><span class=\"by_LedhurJxi3baDAKDZ\"> vs supervised </span><span class=\"by_mcKSiwq2TBrTMZS6X\">(e.g. Support</span><span class=\"by_LedhurJxi3baDAKDZ\"> Vector Machines, </span><span class=\"by_mcKSiwq2TBrTMZS6X\">logistic regression) methods. The</span><span class=\"by_LedhurJxi3baDAKDZ\"> first </span><span class=\"by_mcKSiwq2TBrTMZS6X\">approach identifies interesting patterns (e.g. clusters and latent dimensions) in unlabeled training data, whereas</span><span class=\"by_LedhurJxi3baDAKDZ\"> the second </span><span class=\"by_mcKSiwq2TBrTMZS6X\">takes labeled training data and tries to predict</span><span class=\"by_LedhurJxi3baDAKDZ\"> the </span><span class=\"by_mcKSiwq2TBrTMZS6X\">label for unlabeled data points from</span><span class=\"by_LedhurJxi3baDAKDZ\"> the </span><span class=\"by_mcKSiwq2TBrTMZS6X\">same distribution.</span></span></p><p><span><span class=\"by_mcKSiwq2TBrTMZS6X\">Another important</span><span class=\"by_LedhurJxi3baDAKDZ\"> distinction </span><span class=\"by_mcKSiwq2TBrTMZS6X\">relates</span><span class=\"by_LedhurJxi3baDAKDZ\"> to the </span><span class=\"by_mcKSiwq2TBrTMZS6X\">bias/</span><span class=\"by_LedhurJxi3baDAKDZ\">variance </span><span class=\"by_mcKSiwq2TBrTMZS6X\">tradeoff -- some machine learning methods are are capable of recognizing more complex patterns, but</span><span class=\"by_LedhurJxi3baDAKDZ\"> the </span><span class=\"by_mcKSiwq2TBrTMZS6X\">tradeoff is</span><span class=\"by_LedhurJxi3baDAKDZ\"> that </span><span class=\"by_mcKSiwq2TBrTMZS6X\">these methods can overfit and generalize poorly if there'</span><span class=\"by_LedhurJxi3baDAKDZ\">s </span><span class=\"by_mcKSiwq2TBrTMZS6X\">noise in</span><span class=\"by_LedhurJxi3baDAKDZ\"> the training data </span><span class=\"by_mcKSiwq2TBrTMZS6X\">-- especially if there's not much training data available.</span></span></p><p><span><span class=\"by_mcKSiwq2TBrTMZS6X\">There are also subfields of machine learning devoted</span><span class=\"by_LedhurJxi3baDAKDZ\"> to </span><span class=\"by_mcKSiwq2TBrTMZS6X\">operating on specific kinds of data. For example, Hidden Markov Models and recurrent neural networks operate on time series data. Convolutional neural networks are commonly applied to image data.</span></span></p><h2 id=\"Applications\"><span class=\"by_LedhurJxi3baDAKDZ\">Applications</span></h2><p><span><span class=\"by_LedhurJxi3baDAKDZ\">The use of </span><span class=\"by_mcKSiwq2TBrTMZS6X\">machine learning</span><span class=\"by_LedhurJxi3baDAKDZ\"> has been widespread since </span><span class=\"by_qxJ28GN72aiJu96iF\">its</span><span class=\"by_LedhurJxi3baDAKDZ\"> formal definition in the 50’s. The ability to make predictions based on </span><span class=\"by_mcKSiwq2TBrTMZS6X\">data</span><span class=\"by_LedhurJxi3baDAKDZ\"> has been extensively used in areas such as </span><span class=\"by_mcKSiwq2TBrTMZS6X\">analysis of financial markets,</span><span class=\"by_LedhurJxi3baDAKDZ\"> natural language processing </span><span class=\"by_mcKSiwq2TBrTMZS6X\">and</span><span class=\"by_LedhurJxi3baDAKDZ\"> even brain-computer interfaces. Amazon’s </span><span class=\"by_mcKSiwq2TBrTMZS6X\">product suggestion system makes use</span><span class=\"by_LedhurJxi3baDAKDZ\"> of </span><span class=\"by_mcKSiwq2TBrTMZS6X\">training data in the form of past customer purchases in order to predict what customers might want to buy in the future.</span></span></p><p><span><span class=\"by_mcKSiwq2TBrTMZS6X\">In addition</span><span class=\"by_LedhurJxi3baDAKDZ\"> to </span><span class=\"by_mcKSiwq2TBrTMZS6X\">its practical usefulness,</span><span class=\"by_LedhurJxi3baDAKDZ\"> machine learning has also </span><span class=\"by_mcKSiwq2TBrTMZS6X\">offered insight</span><span class=\"by_LedhurJxi3baDAKDZ\"> into human cognitive organization. </span><span class=\"by_mcKSiwq2TBrTMZS6X\">It</span><span class=\"by_LedhurJxi3baDAKDZ\"> seems</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> likely</span><span class=\"by_LedhurJxi3baDAKDZ\"> machine learning will </span><span class=\"by_mcKSiwq2TBrTMZS6X\">play an</span><span class=\"by_LedhurJxi3baDAKDZ\"> important </span><span class=\"by_mcKSiwq2TBrTMZS6X\">role in</span><span class=\"by_LedhurJxi3baDAKDZ\"> the development of </span></span><a href=\"https://www.lesswrong.com/tag/artificial-general-intelligence\"><span class=\"by_LedhurJxi3baDAKDZ\">artificial general intelligence</span></a><span class=\"by_LedhurJxi3baDAKDZ\">.</span></p><h2 id=\"Further_Reading___References\"><span class=\"by_LedhurJxi3baDAKDZ\">Further Reading &amp; References</span></h2><ul><li><a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\"><span><span class=\"by_mcKSiwq2TBrTMZS6X\">The Elements of Statistical</span><span class=\"by_LedhurJxi3baDAKDZ\"> Learning</span></span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_LedhurJxi3baDAKDZ\">See Also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_LedhurJxi3baDAKDZ\">Prediction</span></a></li></ul>",
      "sections": [
        {
          "title": "Understanding different machine learning algorithms",
          "anchor": "Understanding_different_machine_learning_algorithms",
          "level": 1
        },
        {
          "title": "Applications",
          "anchor": "Applications",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 221,
    "description": {
      "markdown": "**Machine Learning** refers to the general field of study that deals with automated statistical learning and pattern detection by non-biological systems. It can be seen as a sub-domain of artificial intelligence that specifically deals with modeling and prediction through the knowledge extracted from training data. As a multi-disciplinary area, it has borrowed concepts and ideas from other areas like pure mathematics and cognitive science.\n\nUnderstanding different machine learning algorithms\n---------------------------------------------------\n\nThe most widely used distinction is between unsupervised (e.g. k-means clustering, principal component analysis) vs supervised (e.g. Support Vector Machines, logistic regression) methods. The first approach identifies interesting patterns (e.g. clusters and latent dimensions) in unlabeled training data, whereas the second takes labeled training data and tries to predict the label for unlabeled data points from the same distribution.\n\nAnother important distinction relates to the bias/variance tradeoff -- some machine learning methods are are capable of recognizing more complex patterns, but the tradeoff is that these methods can overfit and generalize poorly if there's noise in the training data -- especially if there's not much training data available.\n\nThere are also subfields of machine learning devoted to operating on specific kinds of data. For example, Hidden Markov Models and recurrent neural networks operate on time series data. Convolutional neural networks are commonly applied to image data.\n\nApplications\n------------\n\nThe use of machine learning has been widespread since its formal definition in the 50’s. The ability to make predictions based on data has been extensively used in areas such as analysis of financial markets, natural language processing and even brain-computer interfaces. Amazon’s product suggestion system makes use of training data in the form of past customer purchases in order to predict what customers might want to buy in the future.\n\nIn addition to its practical usefulness, machine learning has also offered insight into human cognitive organization. It seems likely machine learning will play an important role in the development of [artificial general intelligence](https://www.lesswrong.com/tag/artificial-general-intelligence).\n\nFurther Reading & References\n----------------------------\n\n*   [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)\n\nSee Also\n--------\n\n*   [Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bh7uxTTqmsQ8jZJdB",
    "name": "Probability & Statistics",
    "core": false,
    "slug": "probability-and-statistics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 222,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2wjPMY34by2gXEXA2",
    "name": "Techniques",
    "core": null,
    "slug": "techniques",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">technique </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">rationality technique </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is a set of actions (including \"mental actions\") for improving one's thinking so as to form accurate beliefs and/or make better decisions. Ideally, techniques are refined to the point that they can be taught and trained.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 93,
    "description": {
      "markdown": "A **technique** or **rationality technique** is a set of actions (including \"mental actions\") for improving one's thinking so as to form accurate beliefs and/or make better decisions. Ideally, techniques are refined to the point that they can be taught and trained."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "iP2X4jQNHMWHRNPne",
    "name": "Motivations",
    "core": false,
    "slug": "motivations",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Motivations</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> are the reasons why we think and do the things that we do. Related: </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Desire, Values</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">. Many questions can asked about motivation such as: i) what does/could/should motivate people? ii) which stated motivations are true motivations for belief and behavior? iii) which motivations are </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">valid</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> vs </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">invalid</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">? iv) How does motivation even work?</span><span class=\"by_r38pkCm7wF4M44MDQ\">&nbsp;</span></span></p><p><i><span class=\"by_r38pkCm7wF4M44MDQ\">Note: This tag is a work in progress</span></i></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></p><ul><li><a href=\"www.lesswrong.com/stub\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inspirational</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">CEV</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility Functions</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Elephant in the Brain by Simler and Hanson</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Signaling</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Multi-Agent Theories of Mind</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is the act of finding reasons to support a desired conclusion rather than reasoning in ways which reach the true conclusion.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivated Cognition </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is when one's thinking does not purely follow processes for generating truth, and are instead influenced by desires/motivation to reach certain conclusion.</span></li></ul><h2 id=\"Motivation_and_Belief\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivation and Belief</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In the context of belief, a valid motivation for believing something might be having encountered Bayesian evidence for it; in contrast, simply wishing something were true is a poor motivation for believing and often results </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">motivated reasoning [link need].</span></i></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The Litanies of Gendlin and Tarsky [links] are often invoked to elicit feels which motivate truth-seeking behaviors.</span></p><h2 id=\"Motivated_Cognition__Confirmation_Bias__Rationalization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivated Cognition, Confirmation Bias, Rationalization</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">...</span></p><h2 id=\"Stated_vs_Actual_Motivation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Stated vs Actual Motivation</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">It is no secret that often the reasons people give for their actions and beliefs are probably not the real ones driving their behavior. Is that your real objection? The work of Hanson....Signaling...</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Act of Charity</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Player vs Character</span></li></ul><h2 id=\"The_Cognitive_Science_of_Motivation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Cognitive Science of Motivation</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">While most people can recognize the feeling of motivation, it is a much more complication question on how agents, particularly humans, implement </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">motivation.</span></i></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In 20xx, Lukeprog wrote &lt;Neuroscience Review&gt;. Lengthy and thorough. Unknown uptodateness.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Related to the question of Motivation is subagents. Is one's overall self actually made up of subagents each with their own desires. Kaj Sotala explores this in his Multiagent Theories of Mind Sequences. CFAR techniques: Internal Double Crux are aimed harmonizing between the desires/motivations of different \"parts\" of oneself.</span></p><h2 id=\"Aligning_Motivations\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aligning Motivations</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Overcoming akrasia...</span></li></ul><h2 id=\"Practical_Techniques_for_Motivation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Practical Techniques for Motivation</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Propagating Urges</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Mental Contrasting (external)</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Propagating Urges</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Internal Double Crux</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Habitual Productivity and Nate's Writing</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Something to Protect</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See also Motivated Reasoning</span></p>",
      "sections": [
        {
          "title": "Motivation and Belief",
          "anchor": "Motivation_and_Belief",
          "level": 1
        },
        {
          "title": "Motivated Cognition, Confirmation Bias, Rationalization",
          "anchor": "Motivated_Cognition__Confirmation_Bias__Rationalization",
          "level": 1
        },
        {
          "title": "Stated vs Actual Motivation",
          "anchor": "Stated_vs_Actual_Motivation",
          "level": 1
        },
        {
          "title": "The Cognitive Science of Motivation",
          "anchor": "The_Cognitive_Science_of_Motivation",
          "level": 1
        },
        {
          "title": "Aligning Motivations",
          "anchor": "Aligning_Motivations",
          "level": 1
        },
        {
          "title": "Practical Techniques for Motivation",
          "anchor": "Practical_Techniques_for_Motivation",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 153,
    "description": {
      "markdown": "**Motivations** are the reasons why we think and do the things that we do. Related: **Desire, Values**. Many questions can asked about motivation such as: i) what does/could/should motivate people? ii) which stated motivations are true motivations for belief and behavior? iii) which motivations are *valid* vs *invalid*? iv) How does motivation even work? \n\n*Note: This tag is a work in progress*\n\nSee also:\n\n*   [Inspirational](www.lesswrong.com/stub)\n*   CEV\n*   Utility Functions\n*   Elephant in the Brain by Simler and Hanson\n*   Signaling\n*   Multi-Agent Theories of Mind\n*   **Rationalization** is the act of finding reasons to support a desired conclusion rather than reasoning in ways which reach the true conclusion.\n*   **Motivated Cognition** is when one's thinking does not purely follow processes for generating truth, and are instead influenced by desires/motivation to reach certain conclusion.\n\nMotivation and Belief\n---------------------\n\nIn the context of belief, a valid motivation for believing something might be having encountered Bayesian evidence for it; in contrast, simply wishing something were true is a poor motivation for believing and often results *motivated reasoning \\[link need\\].*\n\nThe Litanies of Gendlin and Tarsky \\[links\\] are often invoked to elicit feels which motivate truth-seeking behaviors.\n\nMotivated Cognition, Confirmation Bias, Rationalization\n-------------------------------------------------------\n\n...\n\nStated vs Actual Motivation\n---------------------------\n\nIt is no secret that often the reasons people give for their actions and beliefs are probably not the real ones driving their behavior. Is that your real objection? The work of Hanson....Signaling...\n\n*   Act of Charity\n*   Player vs Character\n\nThe Cognitive Science of Motivation\n-----------------------------------\n\nWhile most people can recognize the feeling of motivation, it is a much more complication question on how agents, particularly humans, implement *motivation.*\n\nIn 20xx, Lukeprog wrote <Neuroscience Review>. Lengthy and thorough. Unknown uptodateness.\n\nRelated to the question of Motivation is subagents. Is one's overall self actually made up of subagents each with their own desires. Kaj Sotala explores this in his Multiagent Theories of Mind Sequences. CFAR techniques: Internal Double Crux are aimed harmonizing between the desires/motivations of different \"parts\" of oneself.\n\nAligning Motivations\n--------------------\n\n*   Overcoming akrasia...\n\nPractical Techniques for Motivation\n-----------------------------------\n\n*   Propagating Urges\n*   Mental Contrasting (external)\n*   Propagating Urges\n*   Internal Double Crux\n\nHabitual Productivity and Nate's Writing\n\nSomething to Protect\n\nSee also Motivated Reasoning"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YSyvvi4uXvxAARX2D",
    "name": "Slack",
    "core": null,
    "slug": "slack",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is absence of binding constraints on behavior. The term is usually capitalized to distinguish it from the ordinary English meaning. Not to be confused with the communication app by the same name.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">From the post which introduced this usage, </span><a href=\"https://www.lessestwrong.com/posts/yLLkWMDbC9ZNKbjDG/slack\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack</span></a><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span></strong></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Poor is the person without Slack. Lack of Slack compounds and traps.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack means margin for error. You can relax.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack allows pursuing opportunities. You can explore. You can trade.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack prevents desperation. You can avoid bad trades and wait for better spots. You can be efficient.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack permits planning for the long term. You can invest.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack enables doing things for your own amusement. You can play games. You can have fun.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can have a code.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack presents things as they are without concern for how things look or what others think. You can be honest.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">You can do some of these things, and choose not to do others. Because you don’t have to.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Only with slack can one be a righteous dude.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Slack is life.</span></i></p></blockquote><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequence:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/HXkpm9b8o964jbQ89\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Slack and the Sabbath</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 34,
    "description": {
      "markdown": "**Slack** is absence of binding constraints on behavior. The term is usually capitalized to distinguish it from the ordinary English meaning. Not to be confused with the communication app by the same name.\n\nFrom the post which introduced this usage, [Slack](https://www.lessestwrong.com/posts/yLLkWMDbC9ZNKbjDG/slack)**:**\n\n> *Poor is the person without Slack. Lack of Slack compounds and traps.*\n> \n> *Slack means margin for error. You can relax.*\n> \n> *Slack allows pursuing opportunities. You can explore. You can trade.*\n> \n> *Slack prevents desperation. You can avoid bad trades and wait for better spots. You can be efficient.*\n> \n> *Slack permits planning for the long term. You can invest.*\n> \n> *Slack enables doing things for your own amusement. You can play games. You can have fun.*\n> \n> *Slack enables doing the right thing. Stand by your friends. Reward the worthy. Punish the wicked. You can have a code.*\n> \n> *Slack presents things as they are without concern for how things look or what others think. You can be honest.*\n> \n> *You can do some of these things, and choose not to do others. Because you don’t have to.*\n> \n> *Only with slack can one be a righteous dude.*\n> \n> *Slack is life.*\n\n**Related Sequence:** [Slack and the Sabbath](https://www.lesswrong.com/s/HXkpm9b8o964jbQ89)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AiNyf5iwbpc7mehiX",
    "name": "Meditation",
    "core": null,
    "slug": "meditation",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_nmk3nLpQE89dMRzzN\">This page is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">for the </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">meditation practice</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_nmk3nLpQE89dMRzzN\"> as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">in, Vipassana meditation. For meditation in</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">sense</span><span class=\"by_nmk3nLpQE89dMRzzN\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Koan, see </span></span><a href=\"/tag/meditation-koan\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Meditation / Koan</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 79,
    "description": {
      "markdown": "This page is for the **meditation practice**, as in, Vipassana meditation. For meditation in the sense of Koan, see [*Meditation / Koan*](/tag/meditation-koan)*.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q55STnFh6gbSezRuR",
    "name": "Parenting",
    "core": false,
    "slug": "parenting",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Parenting</span></strong><span class=\"by_XtphY3uYHwruKqDyG\">, i.e. how to raise children well.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages: </span></strong><a href=\"https://www.lesswrong.com/tag/education\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Education</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/developmental-psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Developmental Psychology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/santa-claus\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Santa Claus</span></a><span class=\"by_j8TwwtYJusmkqvGfh\">, </span><a href=\"https://www.lesswrong.com/tag/family-planning\"><span class=\"by_j8TwwtYJusmkqvGfh\">family planning</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External links: </span></strong><a href=\"https://en.wikipedia.org/wiki/Baby_sign_language\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Baby sign language</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 108,
    "description": {
      "markdown": "**Parenting**, i.e. how to raise children well.\n\n**Related pages:** [Education](https://www.lesswrong.com/tag/education), [Developmental Psychology](https://www.lesswrong.com/tag/developmental-psychology), [Santa Claus](https://www.lesswrong.com/tag/santa-claus), [family planning](https://www.lesswrong.com/tag/family-planning)\n\n**External links:** [Baby sign language](https://en.wikipedia.org/wiki/Baby_sign_language)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AADZcNS24mmSfPp2w",
    "name": "Communication Cultures",
    "core": null,
    "slug": "communication-cultures",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span><span class=\"by_r38pkCm7wF4M44MDQ\">Communication </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Culture </span></span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a set of norms, expectations, </span><span class=\"by_NmLgHDBdX3etsSSJp\">and</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> assumptions that a group of people adopts around communication. It is </span><span class=\"by_XtphY3uYHwruKqDyG\">probable</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> that some Communication </span><span class=\"by_r38pkCm7wF4M44MDQ\">Cultures are</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> objectively better than others, but is definite that difficult clashes occur when people operating under different cultures interact.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Awareness of Communication Cultures is therefore key to getting along with others not perfectly sharing our background</span><span class=\"by_r38pkCm7wF4M44MDQ\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">preferences.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Communication Cultures (these are usually contrasted along some dimension) are: </span><a href=\"https://www.lessestwrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ask vs Guess</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (and </span><a href=\"https://www.lessestwrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Tell</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">/</span><a href=\"https://malcolmocean.com/2015/06/reveal-culture/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Reveal</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">); </span><a href=\"https://www.lessestwrong.com/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wait vs Interrupt</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">; and </span><a href=\"https://www.lessestwrong.com/posts/ExssKjAaXEEYcnzPd/conversational-cultures-combat-vs-nurture-v2\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Combat vs Nurture</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/simulacrum-levels\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Simulacrum Levels</span></a></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Tag Status: C-Class</span></i></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 75,
    "description": {
      "markdown": "A **Communication Culture** is a set of norms, expectations, and assumptions that a group of people adopts around communication. It is probable that some Communication Cultures are objectively better than others, but is definite that difficult clashes occur when people operating under different cultures interact.\n\nAwareness of Communication Cultures is therefore key to getting along with others not perfectly sharing our background and preferences.\n\nNotable Communication Cultures (these are usually contrasted along some dimension) are: [Ask vs Guess](https://www.lessestwrong.com/posts/vs3kzjLhbdKsndnBy/ask-and-guess) (and [Tell](https://www.lessestwrong.com/posts/rEBXN3x6kXgD4pLxs/tell-culture)/[Reveal](https://malcolmocean.com/2015/06/reveal-culture/)); [Wait vs Interrupt](https://www.lessestwrong.com/posts/LuXb6CZG4x7pDRBP8/wait-vs-interrupt-culture); and [Combat vs Nurture](https://www.lessestwrong.com/posts/ExssKjAaXEEYcnzPd/conversational-cultures-combat-vs-nurture-v2).\n\nSee also: [Simulacrum Levels](https://www.lesswrong.com/tag/simulacrum-levels)\n\n*Tag Status: C-Class*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NzSTgAtKwgivkfeYm",
    "name": "Heroic Responsibility",
    "core": false,
    "slug": "heroic-responsibility",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Heroic responsibility</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the responsibility to get the job done no matter what, including not shifting any responsibility for its completion on to others.</span></p><p><i><span><span class=\"by_XtphY3uYHwruKqDyG\">\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">You</span><span class=\"by_PXCeXYzvwEeqqitqH\"> could call it </span></span><strong><span><span class=\"by_PXCeXYzvwEeqqitqH\">heroic </span><span class=\"by_XtphY3uYHwruKqDyG\">responsibility</span></span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\">,</span><span class=\"by_PXCeXYzvwEeqqitqH\"> maybe,</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span><span class=\"by_PXCeXYzvwEeqqitqH\"> Harry Potter said. </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“Not</span><span class=\"by_PXCeXYzvwEeqqitqH\"> like the usual sort. It means that whatever happens, no matter what, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">it’</span><span class=\"by_PXCeXYzvwEeqqitqH\">s always your fault. Even if you tell Professor McGonagall, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">she’</span><span class=\"by_PXCeXYzvwEeqqitqH\">s not responsible for what happens, you are. Following the school rules </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t an excuse, someone else being in charge </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t an excuse, even trying your best </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t an excuse. There just </span><span class=\"by_qgdGA4ZEyW7zNdK84\">aren’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t any excuses, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you’</span><span class=\"by_PXCeXYzvwEeqqitqH\">ve got to get the job done no matter what.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">” Harry’</span><span class=\"by_PXCeXYzvwEeqqitqH\">s face tightened. </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“That’</span><span class=\"by_PXCeXYzvwEeqqitqH\">s why I say </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you’</span><span class=\"by_PXCeXYzvwEeqqitqH\">re not thinking responsibly, Hermione. Thinking that your job is done when you tell Professor </span><span class=\"by_qgdGA4ZEyW7zNdK84\">McGonagall—</span><span class=\"by_PXCeXYzvwEeqqitqH\">that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t heroine thinking. Like Hannah being beat up is okay then, because it </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t your fault anymore. Being a heroine means your job </span><span class=\"by_qgdGA4ZEyW7zNdK84\">isn’</span><span class=\"by_PXCeXYzvwEeqqitqH\">t finished until </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you’</span><span class=\"by_PXCeXYzvwEeqqitqH\">ve done whatever it takes to protect the other girls, permanently.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">” In Harry’s voice was a touch of the steel he had acquired since the day Fawkes had been on his shoulder. “You can’t think as if just following the rules means you’ve done your duty. –</span></span></i><a href=\"http://hpmor.com/chapter/75\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">HPMOR</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\">, chapter 75.</span></i><br><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><h2 id=\"External_Links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Links</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The discussion at this </span><a href=\"http://www.reddit.com/r/HPMOR/comments/yj2kb/ethical_solipsism_chapter_75/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Reddit post</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is excellent. </span><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">This </span><span class=\"by_bdsoCfqiFJEwTE7Wp\">wiki</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> requires work.</span></span></i></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 27,
    "description": {
      "markdown": "**Heroic responsibility** is the responsibility to get the job done no matter what, including not shifting any responsibility for its completion on to others.\n\n*\"You could call it **heroic responsibility**, maybe,” Harry Potter said. “Not like the usual sort. It means that whatever happens, no matter what, it’s always your fault. Even if you tell Professor McGonagall, she’s not responsible for what happens, you are. Following the school rules isn’t an excuse, someone else being in charge isn’t an excuse, even trying your best isn’t an excuse. There just aren’t any excuses, you’ve got to get the job done no matter what.” Harry’s face tightened. “That’s why I say you’re not thinking responsibly, Hermione. Thinking that your job is done when you tell Professor McGonagall—that isn’t heroine thinking. Like Hannah being beat up is okay then, because it isn’t your fault anymore. Being a heroine means your job isn’t finished until you’ve done whatever it takes to protect the other girls, permanently.” In Harry’s voice was a touch of the steel he had acquired since the day Fawkes had been on his shoulder. “You can’t think as if just following the rules means you’ve done your duty. –*[*HPMOR*](http://hpmor.com/chapter/75)*, chapter 75.*  \n \n\nExternal Links\n--------------\n\n*   The discussion at this [Reddit post](http://www.reddit.com/r/HPMOR/comments/yj2kb/ethical_solipsism_chapter_75/) is excellent. *This wiki requires work.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vg4LDxjdwHLotCm8w",
    "name": "Replication Crisis",
    "core": null,
    "slug": "replication-crisis",
    "tableOfContents": {
      "html": "<p><span class=\"by_nLbwLhBaQeG6tCNDN\">The </span><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Replication Crisis</span></strong><span><span class=\"by_nLbwLhBaQeG6tCNDN\"> was</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> the </span><span class=\"by_nLbwLhBaQeG6tCNDN\">discovery</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> that many </span><span class=\"by_nLbwLhBaQeG6tCNDN\">fields of so-called science were producing experimental results that could not</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> be </span><span class=\"by_nLbwLhBaQeG6tCNDN\">replicated, because they were illusions resulting from bad statistical and experimental practices.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">The replication crisis </span><span class=\"by_nLbwLhBaQeG6tCNDN\">began</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> in the </span><span class=\"by_nLbwLhBaQeG6tCNDN\">early 2010s when several high-profile irreproducible</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> results</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> inspired mass replication attempts, revealing that the majority of papers checked</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> in psychology </span><span class=\"by_nLbwLhBaQeG6tCNDN\">and a number of other fields were not replicable. Some of the irreproducible </span><span class=\"by_sKAL2jzfkYkDbQmx9\">results, like </span></span><a href=\"https://www.lesswrong.com/tag/priming\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Priming</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> appeared to bear</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> on rationality and </span><span class=\"by_nLbwLhBaQeG6tCNDN\">were</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> referenced </span><span class=\"by_nLbwLhBaQeG6tCNDN\">in early LessWrong posts.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links:</span></strong><br><a href=\"https://retractionwatch.com/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Retraction Watch</span></a><br><a href=\"https://www.gwern.net/Replication\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Replication</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> on gwern.net</span><br><a href=\"https://en.wikipedia.org/wiki/Replication_crisis\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Wikipedia</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/practice-and-philosophy-of-science\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Practice &amp; Philosophy of Science</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/psychology\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Psychology</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/information-cascades\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Information Cascades</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/falsifiability\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Falsifiability</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 54,
    "description": {
      "markdown": "The **Replication Crisis** was the discovery that many fields of so-called science were producing experimental results that could not be replicated, because they were illusions resulting from bad statistical and experimental practices.\n\nThe replication crisis began in the early 2010s when several high-profile irreproducible results inspired mass replication attempts, revealing that the majority of papers checked in psychology and a number of other fields were not replicable. Some of the irreproducible results, like [Priming](https://www.lesswrong.com/tag/priming), appeared to bear on rationality and were referenced in early LessWrong posts.\n\n**External Links:**  \n[Retraction Watch](https://retractionwatch.com/)  \n[Replication](https://www.gwern.net/Replication) on gwern.net  \n[Wikipedia](https://en.wikipedia.org/wiki/Replication_crisis)\n\n**Related Pages:** [Practice & Philosophy of Science](https://www.lesswrong.com/tag/practice-and-philosophy-of-science), [Psychology](https://www.lesswrong.com/tag/psychology), [Information Cascades](https://www.lesswrong.com/tag/information-cascades), [Falsifiability](https://www.lesswrong.com/tag/falsifiability)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XYHzLjwYiqpeqaf4c",
    "name": "Dark Arts",
    "core": null,
    "slug": "dark-arts",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Dark Arts </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a colloquial term for</span><span class=\"by_bfSEPyGaMtQckR9ZW\"> techniques or </span><span class=\"by_qgdGA4ZEyW7zNdK84\">methods which involve deception and/or manipulation</span><span class=\"by_bfSEPyGaMtQckR9ZW\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">others or oneself into believing things for non-truth-seeking reasons. These techniques may prey on human cognitive biases.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Some use the term to refer more narrowly to techniques that work equally well to compel both true and false beliefs, i.e.</span><span class=\"by_ChXHsXmDQFWZH638i\">, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">they are </span></span><a href=\"https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons\"><span class=\"by_qgdGA4ZEyW7zNdK84\">symmetric weapons</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. Some focus more on the Dark Arts </span><span class=\"by_ChXHsXmDQFWZH638i\">as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">applied</span><span class=\"by_ChXHsXmDQFWZH638i\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">oneself (self-deception) vs applied to manipulating others.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">An example from the </span><a href=\"https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">Dark Arts </span><span class=\"by_qgdGA4ZEyW7zNdK84\">of Rationality</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Today, we're going to talk about Dark rationalist techniques: productivity tools which seem incoherent, mad, and downright irrational. These techniques include:</span></p><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Willful Inconsistency</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Intentional Compartmentalization</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Modifying Terminal Goals</span></li></ol></blockquote><h2 id=\"Art_vs__Technology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Art vs. Technology</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Sometimes these arts are further augmented by the use of </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">persuasion technology</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">, such as broadcast advertising or PowerPoint slides. Persuasion technology may prevent the person who is being targeted from carefully deliberating on the intended message, or thinking up an effective response to it in real time.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Such effects can be caused by something as benign as the use of a specialist vocabulary which the target is unfamiliar with, or an institutional vocabulary with high-status connotations: this is one reason why many specialist professions employ ethical codes to regulate their unbalanced power relationship with customers.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The use of such techniques as whiteboards or PowerPoint slides brings additional concerns, since these tend to connote a single party as the one \"in charge\" of the presentation: this makes it even more difficult for the intended audience to raise any effective objection, and encourages them to focus their attention on the content of the whiteboard or slides. Said content is often presented as a list of abrupt \"bullet points\", further connoting it as factual, objective and neutral. One outspoken critic of PowerPoint, management professor David R. Beatty, states: \"It is like a disease. It's the AIDS of management.\" Beatty further states that Powerpoint \"removes subtlety and thinking\".</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Many futurists expect that a technological singularity of even a very mild character will lead to an explosion in the use of radically effective persuasive technology, or \"cognotechnology\"--a term coined by American military researchers at the Lawrence Livermore Laboratories. The collection and distribution of information about people may spiral out beyond any feasible control, perhaps even comprising their inner thought processes; cognitive monitoring may range from non-intrusive body monitoring as seen in a </span><a href=\"https://wiki.lesswrong.com/wiki/polygraph\"><span class=\"by_qgdGA4ZEyW7zNdK84\">polygraph</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> to outright </span><a href=\"https://wiki.lesswrong.com/wiki/brain_emulation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">brain emulation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. In this scenario, persuasion technology may easily blend over into outright mind control. This is clearly a rather paranoiac and dystopian scenario; nevertheless, the fact that it is being seriously discussed has persuasive potential in itself, such as for directing funding for research into guaranteed </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, as opposed to naïvely pursuing expanded funding for neuroscience or artificial intelligence.</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"http://robinhanson.typepad.com/overcomingbias/2009/02/against-propaganda-.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Against Propaganda</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"http://www.overcomingbias.com/2009/03/deceptive-writing-styles.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Lying With Style</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"https://lessestwrong.com/lw/62/defense_against_the_dark_arts_case_study_1/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Defense Against The Dark Arts: Case Study #1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Yvain</span></a></li><li><a href=\"https://lessestwrong.com/lw/yg/informers_and_persuaders/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Informers and Persuaders</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer_Yudkowsky</span></a></li><li><a href=\"https://lessestwrong.com/lw/2v2/the_dark_arts_preamble\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Dark Arts - Preamble</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"http://www.staresattheworld.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aurini</span></a></li><li><a href=\"https://lessestwrong.com/lw/9iw/the_dark_arts_a_beginners_guide/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Dark Arts: A Beginner's Guide</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"http://lesswrong.com/user/faul_sname/overview/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">faul_sname</span></a></li></ul><h2 id=\"Other_Links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Other Links</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/b1/persuasiveness_vs_soundness/789\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Meta-commentary on this terminology</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/anti-epistemology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Anti-epistemology</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Curiosity_stopper\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Curiosity stopper</span></a></li><li><a href=\"https://lessestwrong.com/tag/mind-killer\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind-killer</span></a></li><li><a href=\"https://lessestwrong.com/tag/not-technically-a-lie\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Not technically a lie</span></a></li><li><a href=\"https://lessestwrong.com/tag/inferential-distance\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inferential distance</span></a></li></ul>",
      "sections": [
        {
          "title": "Art vs. Technology",
          "anchor": "Art_vs__Technology",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "Other Links",
          "anchor": "Other_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 44,
    "description": {
      "markdown": "**Dark Arts** is a colloquial term for techniques or methods which involve deception and/or manipulation of others or oneself into believing things for non-truth-seeking reasons. These techniques may prey on human cognitive biases.\n\nSome use the term to refer more narrowly to techniques that work equally well to compel both true and false beliefs, i.e., they are [symmetric weapons](https://www.lesswrong.com/posts/qajfiXo5qRThZQG7s/guided-by-the-beauty-of-our-weapons). Some focus more on the Dark Arts as applied to oneself (self-deception) vs applied to manipulating others.\n\nAn example from the [Dark Arts of Rationality](https://www.lesswrong.com/posts/4DBBQkEQvNEWafkek/dark-arts-of-rationality):\n\n> Today, we're going to talk about Dark rationalist techniques: productivity tools which seem incoherent, mad, and downright irrational. These techniques include:\n> \n> 1.  Willful Inconsistency\n> 2.  Intentional Compartmentalization\n> 3.  Modifying Terminal Goals\n\nArt vs. Technology\n------------------\n\nSometimes these arts are further augmented by the use of **persuasion technology**, such as broadcast advertising or PowerPoint slides. Persuasion technology may prevent the person who is being targeted from carefully deliberating on the intended message, or thinking up an effective response to it in real time.\n\nSuch effects can be caused by something as benign as the use of a specialist vocabulary which the target is unfamiliar with, or an institutional vocabulary with high-status connotations: this is one reason why many specialist professions employ ethical codes to regulate their unbalanced power relationship with customers.\n\nThe use of such techniques as whiteboards or PowerPoint slides brings additional concerns, since these tend to connote a single party as the one \"in charge\" of the presentation: this makes it even more difficult for the intended audience to raise any effective objection, and encourages them to focus their attention on the content of the whiteboard or slides. Said content is often presented as a list of abrupt \"bullet points\", further connoting it as factual, objective and neutral. One outspoken critic of PowerPoint, management professor David R. Beatty, states: \"It is like a disease. It's the AIDS of management.\" Beatty further states that Powerpoint \"removes subtlety and thinking\".\n\nMany futurists expect that a technological singularity of even a very mild character will lead to an explosion in the use of radically effective persuasive technology, or \"cognotechnology\"--a term coined by American military researchers at the Lawrence Livermore Laboratories. The collection and distribution of information about people may spiral out beyond any feasible control, perhaps even comprising their inner thought processes; cognitive monitoring may range from non-intrusive body monitoring as seen in a [polygraph](https://wiki.lesswrong.com/wiki/polygraph) to outright [brain emulation](https://wiki.lesswrong.com/wiki/brain_emulation). In this scenario, persuasion technology may easily blend over into outright mind control. This is clearly a rather paranoiac and dystopian scenario; nevertheless, the fact that it is being seriously discussed has persuasive potential in itself, such as for directing funding for research into guaranteed [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI), as opposed to naïvely pursuing expanded funding for neuroscience or artificial intelligence.\n\nNotable Posts\n-------------\n\n*   [Against Propaganda](http://robinhanson.typepad.com/overcomingbias/2009/02/against-propaganda-.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Lying With Style](http://www.overcomingbias.com/2009/03/deceptive-writing-styles.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Defense Against The Dark Arts: Case Study #1](https://lessestwrong.com/lw/62/defense_against_the_dark_arts_case_study_1/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Informers and Persuaders](https://lessestwrong.com/lw/yg/informers_and_persuaders/) by [Eliezer_Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky)\n*   [The Dark Arts - Preamble](https://lessestwrong.com/lw/2v2/the_dark_arts_preamble) by [Aurini](http://www.staresattheworld.com/)\n*   [The Dark Arts: A Beginner's Guide](https://lessestwrong.com/lw/9iw/the_dark_arts_a_beginners_guide/) by [faul_sname](http://lesswrong.com/user/faul_sname/overview/)\n\nOther Links\n-----------\n\n*   [Meta-commentary on this terminology](https://lessestwrong.com/lw/b1/persuasiveness_vs_soundness/789)\n\nSee Also\n--------\n\n*   [Anti-epistemology](https://lessestwrong.com/tag/anti-epistemology)\n*   [Curiosity stopper](https://wiki.lesswrong.com/wiki/Curiosity_stopper)\n*   [Mind-killer](https://lessestwrong.com/tag/mind-killer)\n*   [Not technically a lie](https://lessestwrong.com/tag/not-technically-a-lie)\n*   [Inferential distance](https://lessestwrong.com/tag/inferential-distance)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YrLoz567b553YouZ2",
    "name": "Willpower",
    "core": null,
    "slug": "willpower",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Willpower</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the ability to overcome urges to do or not some activity– to overcome temptation. Typically there is a sense of coercing oneself to do things despite inner resistance.</span></p><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Willpower is of interest those who wish to increase their productivity or otherwise do more thing that they wish to be done. The question then is \"how does one increase willpower?\"</span></p><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><p><span class=\"by_qgdGA4ZEyW7zNdK84\">There is an argument that the use of willpower is undesirable. The use of willpower my constitute a form of </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">inner violence</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> which is in tension with </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">inner</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><em><span class=\"by_qgdGA4ZEyW7zNdK84\">alignment</span></em><span class=\"by_qgdGA4ZEyW7zNdK84\"> of </span><a href=\"https://www.lessestwrong.com/tag/subagents\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">one's </span><span class=\"by_qxJ28GN72aiJu96iF\">parts</span></span></a><span><span class=\"by_qxJ28GN72aiJu96iF\">–</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> a better path to productivity and wellbeing.</span></span></p><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Related:</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lessestwrong.com/tag/akrasia\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Akrasia</span></a></p><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><h2 id=\"Resources\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Resources</span></h2><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><ul><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The writings on </span><a href=\"http://mindingourway.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Minding Our Way</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> concerning productivity.</span></li><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span></ul><span class=\"by_qgdGA4ZEyW7zNdK84\">\n</span>",
      "sections": [
        {
          "title": "Resources",
          "anchor": "Resources",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 30,
    "description": {
      "markdown": "**Willpower** is the ability to overcome urges to do or not some activity– to overcome temptation. Typically there is a sense of coercing oneself to do things despite inner resistance.   \n  \nWillpower is of interest those who wish to increase their productivity or otherwise do more thing that they wish to be done. The question then is \"how does one increase willpower?\" \n\nThere is an argument that the use of willpower is undesirable. The use of willpower my constitute a form of _inner violence_ which is in tension with _inner_ _alignment_ of [one's parts](https://www.lessestwrong.com/tag/subagents)– a better path to productivity and wellbeing.  \n  \n**Related:** [Akrasia](https://www.lessestwrong.com/tag/akrasia)\n\nResources\n---------\n\n*   The writings on [Minding Our Way](http://mindingourway.com/) concerning productivity."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "r7qAjcbfhj2256EHH",
    "name": "Akrasia",
    "core": null,
    "slug": "akrasia",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Akrasia</span></strong><span><span><span class=\"by_qf77EiaoMw7tH3GSr\"> is the state of acting against one's better judgment. </span><span class=\"by_qgdGA4ZEyW7zNdK84\">A canonical example is procrastination.&nbsp;</span></span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Increasing </span><a href=\"https://www.lessestwrong.com/tag/willpower\"><span class=\"by_qgdGA4ZEyW7zNdK84\">willpower</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is seen by some as a solution</span><span class=\"by_qf77EiaoMw7tH3GSr\"> to </span><span class=\"by_qxJ28GN72aiJu96iF\">akrasia. On the other hand,</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> many favor </span><span class=\"by_qxJ28GN72aiJu96iF\">using tools such as </span></span><a href=\"https://www.lesswrong.com/tag/internal-double-crux?useTagName=true\"><span class=\"by_qxJ28GN72aiJu96iF\">Internal Double Crux</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">resolve internal mental conflicts until one </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">wants</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to do</span><span class=\"by_Mw8rsM7m7E8nnEFEp\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">perform</span><span class=\"by_LoykQRMTxJFxwwdPy\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">reflectively endorsed task. </span><span class=\"by_qxJ28GN72aiJu96iF\">The \"resolve internal conflicts\" approach is often related to viewing the mind in terms of </span></span><a href=\"https://www.lesswrong.com/tag/subagents\"><span class=\"by_qxJ28GN72aiJu96iF\">parts that disagree</span></a><span class=\"by_qxJ28GN72aiJu96iF\"> with each other.</span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/attention\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Attention</span></a></li><li><a href=\"https://www.lesswrong.com/tag/motivations\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivations</span></a></li><li><a href=\"https://www.lesswrong.com/tag/prioritization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prioritization</span></a></li><li><a href=\"https://www.lesswrong.com/tag/procrastination\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Procrastination</span></a></li><li><a href=\"https://www.lesswrong.com/tag/productivity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Productivity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/willpower\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Willpower</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Adaptation executers</span></a></li><li><a href=\"https://www.lesswrong.com/tag/trivial-inconvenience\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Trivial inconvenience</span></a></li><li><a href=\"https://www.lesswrong.com/tag/aversion-ugh-fields\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aversion/Ugh field</span></a></li><li><a href=\"https://www.lesswrong.com/tag/compartmentalization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Compartmentalization</span></a></li><li><a href=\"https://www.lesswrong.com/tag/preference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Preference</span></a></li><li><a href=\"https://www.lesswrong.com/tag/pica\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Pica</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://psychology.wikia.com/wiki/Akrasia\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Akrasia</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> at Psychology Wiki</span></li><li><a href=\"http://plato.stanford.edu/entries/weakness-will/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Weakness of Will</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Stanford Encyclopedia of Philosophy</span></li><li><a href=\"http://beeminder.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Beeminder</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, community-member developed tool for commitment via self-imposed financial penalties</span></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 74,
    "description": {
      "markdown": "**Akrasia** is the state of acting against one's better judgment. A canonical example is procrastination.\n\nIncreasing [willpower](https://www.lessestwrong.com/tag/willpower) is seen by some as a solution to akrasia. On the other hand, many favor using tools such as [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux?useTagName=true) to resolve internal mental conflicts until one *wants* to do the perform the reflectively endorsed task. The \"resolve internal conflicts\" approach is often related to viewing the mind in terms of [parts that disagree](https://www.lesswrong.com/tag/subagents) with each other.\n\nSee also\n--------\n\n*   [Attention](https://www.lesswrong.com/tag/attention)\n*   [Motivations](https://www.lesswrong.com/tag/motivations)\n*   [Prioritization](https://www.lesswrong.com/tag/prioritization)\n*   [Procrastination](https://www.lesswrong.com/tag/procrastination)\n*   [Productivity](https://www.lesswrong.com/tag/productivity)\n*   [Willpower](https://www.lesswrong.com/tag/willpower)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)\n*   [Trivial inconvenience](https://www.lesswrong.com/tag/trivial-inconvenience)\n*   [Aversion/Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)\n*   [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization)\n*   [Preference](https://www.lesswrong.com/tag/preference)\n*   [Pica](https://www.lesswrong.com/tag/pica)\n\nExternal links\n--------------\n\n*   [Akrasia](http://psychology.wikia.com/wiki/Akrasia) at Psychology Wiki\n*   [Weakness of Will](http://plato.stanford.edu/entries/weakness-will/), Stanford Encyclopedia of Philosophy\n*   [Beeminder](http://beeminder.com/), community-member developed tool for commitment via self-imposed financial penalties"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dJ6eJxJrCEget7Wb6",
    "name": "Fallacies",
    "core": null,
    "slug": "fallacies",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">fallacy</span></strong><span><span class=\"by_NRg5Bw8H2DCYTpmHE\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is generally considered to be an error</span><span class=\"by_HoGziwmhpMGqGeWZy\"> in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">reasoning. It refers both to the failure to apply logic to a line of thought, and to the use of problematic arguments. The term can be applied when dealing both with informal and formal logic, although it usual refers to the former.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related:</span></i><span class=\"by_HoGziwmhpMGqGeWZy\"> </span><a href=\"http://lesswrong.com/tag/disagreement\"><span class=\"by_NRg5Bw8H2DCYTpmHE\">Disagreement</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\"><span class=\"by_HoGziwmhpMGqGeWZy\">Heuristics &amp; Biases</span></a></p><h2 id=\"Informal_vs_Formal_Fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Informal vs Formal Fallacy</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">An </span><i><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">informal fallacy</span></strong></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> refers to a flawed argument, where the premises do not support the conclusion. It can, however, have a valid logical format. This type of fallacy is commonly divided in two main groups: </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">material fallacies</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">verbal fallacies</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Material fallacies, concerned with the content of the argument, can be divivided following </span><a href=\"http://en.wikipedia.org/wiki/Aristotle\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aristotle</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">'s taxonomy from his work Organon. One such example is the famous Straw Man fallacy:</span></p><ol><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Person A has position X: We should focus our efforts on </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> research.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Person B distorts position X to something close, but different, Y: So you think we should just give up on webdesign?!</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Person B attacks position Y: That's stupid, websites are such a great way of spreading information!</span></li></ol><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Verbal fallacies, on the other hand, deal with the way the words are used. These include examples such as Equivocation - using words ambiguously or with double meanings - and Proof by Verbosity, where one overwhelms his listener with lots of material in an complicated way.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><i><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">formal fallacy</span></strong></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, contrasting with informal fallacies, refers to a pattern of reasoning which is wrong due to a flaw in the logical structure of the argument. As such, this deductive fallacy does not imply any information about the premises or the conclusion - its their connection that's wrongly stated. Both can be correct and the argument can be wrong because the conclusion doesn't follow from the premises as it is said to.</span></p><h2 id=\"False_Fallacies___Awareness\"><span class=\"by_qgdGA4ZEyW7zNdK84\">False Fallacies &amp; Awareness</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Matters can be further complicated by arguing parties incorrectly claiming that an assertion is false due to a fallacy. For example, if one party was to declare “Albert Einstein has claimed that time and space are relative qualities of the Universe.”, another party might responded by saying that this is an ‘’’argument from authority’’’. However, Albert Einstein’s claims are based on detailed mathematical models that identify him as an expert in this field of inquiry, rather than a casual observer. We are thus facing a kind of meta-fallacy which is wrong by itself.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Recognizing fallacies in everyday arguments can be difficult due to complicated patterns of communication that mask the logical connections between statements. At the same time, informal fallacies can also take advantage of the emotional or psychological weaknesses of the listener. It is thus important to develop the ability to recognize them in arguments, so as to reduce the likelihood of being tricked or cheated. This ability becomes even more important when dealing with today's mass media, where the intention is to influence behavior and change beliefs, from political campaigns to simple local newspapers.</span></p><h2 id=\"Further_Reading___References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Further Reading &amp; References</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Aristotle's </span><a href=\"http://etext.library.adelaide.edu.au/a/aristotle/sophistical/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">On Sophistical Refutations</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Damer, T. Edward (2008). Attacking Faulty Reasoning: A Practical Guide to Fallacy-free Arguments (6 ed.). Cengage Learning. pp. 130. ISBN 978-0-495-09506-4.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">John Woods (2004). The death of argument: fallacies in agent based reasoning. Springer. ISBN 978-1-4020-2663-8.</span></li><li><a href=\"http://www.iep.utm.edu/fallacy/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Logical Fallacies</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> A peer-reviewed academic resource.</span></li><li><a href=\"http://en.wikipedia.org/wiki/Infinite_regress\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Infinite regression</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> Wikipedia entry</span></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/aumann-s-agreement-theorem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Aumann's agreement theorem</span></a></li><li><a href=\"https://lessestwrong.com/tag/disagreement\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Disagreement</span></a></li><li><a href=\"https://lessestwrong.com/tag/information-cascades\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Information cascade</span></a></li></ul>",
      "sections": [
        {
          "title": "Informal vs Formal Fallacy",
          "anchor": "Informal_vs_Formal_Fallacy",
          "level": 1
        },
        {
          "title": "False Fallacies & Awareness",
          "anchor": "False_Fallacies___Awareness",
          "level": 1
        },
        {
          "title": "Further Reading & References",
          "anchor": "Further_Reading___References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 66,
    "description": {
      "markdown": "A **fallacy** is generally considered to be an error in reasoning. It refers both to the failure to apply logic to a line of thought, and to the use of problematic arguments. The term can be applied when dealing both with informal and formal logic, although it usual refers to the former.\n\n*Related:* [Disagreement](http://lesswrong.com/tag/disagreement), [Heuristics & Biases](https://www.lesswrong.com/tag/heuristics-and-biases)\n\nInformal vs Formal Fallacy\n--------------------------\n\nAn ***informal fallacy*** refers to a flawed argument, where the premises do not support the conclusion. It can, however, have a valid logical format. This type of fallacy is commonly divided in two main groups: *material fallacies* and *verbal fallacies*.\n\nMaterial fallacies, concerned with the content of the argument, can be divivided following [Aristotle](http://en.wikipedia.org/wiki/Aristotle)'s taxonomy from his work Organon. One such example is the famous Straw Man fallacy:\n\n1.  Person A has position X: We should focus our efforts on [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI) research.\n2.  Person B distorts position X to something close, but different, Y: So you think we should just give up on webdesign?!\n3.  Person B attacks position Y: That's stupid, websites are such a great way of spreading information!\n\nVerbal fallacies, on the other hand, deal with the way the words are used. These include examples such as Equivocation - using words ambiguously or with double meanings - and Proof by Verbosity, where one overwhelms his listener with lots of material in an complicated way.\n\nA ***formal fallacy***, contrasting with informal fallacies, refers to a pattern of reasoning which is wrong due to a flaw in the logical structure of the argument. As such, this deductive fallacy does not imply any information about the premises or the conclusion - its their connection that's wrongly stated. Both can be correct and the argument can be wrong because the conclusion doesn't follow from the premises as it is said to.\n\nFalse Fallacies & Awareness\n---------------------------\n\nMatters can be further complicated by arguing parties incorrectly claiming that an assertion is false due to a fallacy. For example, if one party was to declare “Albert Einstein has claimed that time and space are relative qualities of the Universe.”, another party might responded by saying that this is an ‘’’argument from authority’’’. However, Albert Einstein’s claims are based on detailed mathematical models that identify him as an expert in this field of inquiry, rather than a casual observer. We are thus facing a kind of meta-fallacy which is wrong by itself.\n\nRecognizing fallacies in everyday arguments can be difficult due to complicated patterns of communication that mask the logical connections between statements. At the same time, informal fallacies can also take advantage of the emotional or psychological weaknesses of the listener. It is thus important to develop the ability to recognize them in arguments, so as to reduce the likelihood of being tricked or cheated. This ability becomes even more important when dealing with today's mass media, where the intention is to influence behavior and change beliefs, from political campaigns to simple local newspapers.\n\nFurther Reading & References\n----------------------------\n\n*   Aristotle's [On Sophistical Refutations](http://etext.library.adelaide.edu.au/a/aristotle/sophistical/)\n*   Damer, T. Edward (2008). Attacking Faulty Reasoning: A Practical Guide to Fallacy-free Arguments (6 ed.). Cengage Learning. pp. 130. ISBN 978-0-495-09506-4.\n*   John Woods (2004). The death of argument: fallacies in agent based reasoning. Springer. ISBN 978-1-4020-2663-8.\n*   [Logical Fallacies](http://www.iep.utm.edu/fallacy/) A peer-reviewed academic resource.\n*   [Infinite regression](http://en.wikipedia.org/wiki/Infinite_regress) Wikipedia entry\n\nSee Also\n--------\n\n*   [Aumann's agreement theorem](https://lessestwrong.com/tag/aumann-s-agreement-theorem)\n*   [Disagreement](https://lessestwrong.com/tag/disagreement)\n*   [Information cascade](https://lessestwrong.com/tag/information-cascades)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "pGqRLe9bFDX2G2kXY",
    "name": "Futurism",
    "core": null,
    "slug": "futurism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Futurism</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is speculation about technologies or social trends that might exist in the near or distant future.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Less Wrong's favorite type of futurism is speculation about </span><a href=\"https://www.lesswrong.com/tag/ai-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">AI risk</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">. Other speculative future technologies include </span><a href=\"http://lesswrong.com/tag/life-extension\"><span class=\"by_HoGziwmhpMGqGeWZy\">life extension</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"http://lesswrong.com/tag/mind-uploading\"><span class=\"by_HoGziwmhpMGqGeWZy\">mind uploading</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"http://lesswrong.com/tag/nanotechnology\"><span class=\"by_HoGziwmhpMGqGeWZy\">nanotechnology</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, and </span><a href=\"https://www.lesswrong.com/tag/space-exploration-and-colonization\"><span class=\"by_HoGziwmhpMGqGeWZy\">space colonization</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">For efforts to predict future trends see </span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_HoGziwmhpMGqGeWZy\">Forecasting &amp; Prediction</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> and </span><a href=\"https://www.lesswrong.com/tag/forecasts-lists-of\"><span class=\"by_HoGziwmhpMGqGeWZy\">Forecasts (Lists of)</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"http://lesswrong.com/tag/transhumanism\"><span class=\"by_HoGziwmhpMGqGeWZy\">Transhumanism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"http://lesswrong.com/tag/fun-theory\"><span class=\"by_HoGziwmhpMGqGeWZy\">Fun Theory</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 81,
    "description": {
      "markdown": "**Futurism** is speculation about technologies or social trends that might exist in the near or distant future.\n\nLess Wrong's favorite type of futurism is speculation about [AI risk](https://www.lesswrong.com/tag/ai-risk). Other speculative future technologies include [life extension](http://lesswrong.com/tag/life-extension), [mind uploading](http://lesswrong.com/tag/mind-uploading), [nanotechnology](http://lesswrong.com/tag/nanotechnology), and [space colonization](https://www.lesswrong.com/tag/space-exploration-and-colonization).\n\nFor efforts to predict future trends see [Forecasting & Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction) and [Forecasts (Lists of)](https://www.lesswrong.com/tag/forecasts-lists-of).\n\nSee also: [Transhumanism](http://lesswrong.com/tag/transhumanism), [Fun Theory](http://lesswrong.com/tag/fun-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "TLrqSmzoGoA3v5tNP",
    "name": "Fact posts",
    "core": false,
    "slug": "fact-posts",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Fact post </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is a piece of writing that attempts to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\". &nbsp;Under this tag, one can find posts that present lots of basic facts about topics. </span><i><span class=\"by_XtphY3uYHwruKqDyG\">This entry requires work.</span></i></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Fact posts were introduced in </span><a href=\"https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Fact Posts: How and Why</span></strong></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by sarahconstantin:</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The most useful thinking skill I've taught myself, which I think should be more widely practiced, is writing what I call \"fact posts.\" &nbsp;I write a bunch of these on my </span><a href=\"https://srconstantin.wordpress.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">blog</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. (I write fact posts about pregnancy and childbirth </span><a href=\"https://parentingwithevidence.wordpress.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">here.</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">To write a fact post, you start with an empirical question, or a general topic. &nbsp;Something like \"How common are hate crimes?\" or \"Are epidurals really dangerous?\" or \"What causes manufacturing job loss?\" &nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">It's okay if this is a topic you know very little about. This is an exercise in original seeing and showing your reasoning, not finding the official last word on a topic or doing the best analysis in the world.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Then you open up a Google doc and start taking notes.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">You look for </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">quantitative&nbsp;data from conventionally reliable sources</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. &nbsp;CDC data for incidences of diseases and other health risks in the US; WHO data for global health issues; Bureau of Labor Statistics data for US employment; and so on. Published scientific journal articles, especially from reputable journals and large randomized studies.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">You explicitly do </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">not </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">look for opinion, even expert opinion. You avoid news, and you're wary of think-tank white papers. You're looking for raw information. You are taking a </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">sola scriptura </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">approach, for better and for worse.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">And then you start letting the data show you things.&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">You see things that are surprising or odd, and you note that. [continues]</span></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "A **Fact post** is a piece of writing that attempts to build an understanding of the world, starting bottom up with empirical facts rather than \"opinions\".  Under this tag, one can find posts that present lots of basic facts about topics. *This entry requires work.*\n\nFact posts were introduced in [**Fact Posts: How and Why**](https://www.lesswrong.com/posts/Sdx6A6yLByRRs8iLY/fact-posts-how-and-why) by sarahconstantin:\n\n> The most useful thinking skill I've taught myself, which I think should be more widely practiced, is writing what I call \"fact posts.\"  I write a bunch of these on my [blog](https://srconstantin.wordpress.com/). (I write fact posts about pregnancy and childbirth [here.](https://parentingwithevidence.wordpress.com/))\n> \n> To write a fact post, you start with an empirical question, or a general topic.  Something like \"How common are hate crimes?\" or \"Are epidurals really dangerous?\" or \"What causes manufacturing job loss?\"  \n> \n> It's okay if this is a topic you know very little about. This is an exercise in original seeing and showing your reasoning, not finding the official last word on a topic or doing the best analysis in the world.\n> \n> Then you open up a Google doc and start taking notes.\n> \n> You look for *quantitative data from conventionally reliable sources*.  CDC data for incidences of diseases and other health risks in the US; WHO data for global health issues; Bureau of Labor Statistics data for US employment; and so on. Published scientific journal articles, especially from reputable journals and large randomized studies.\n> \n> You explicitly do *not* look for opinion, even expert opinion. You avoid news, and you're wary of think-tank white papers. You're looking for raw information. You are taking a *sola scriptura* approach, for better and for worse.\n> \n> And then you start letting the data show you things. \n> \n> You see things that are surprising or odd, and you note that. \\[continues\\]"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "x6evH6MyPK3nxsoff",
    "name": "Identity",
    "core": null,
    "slug": "identity",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Identity </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is an individual's conception of themselves (1). We might conceive of this as the set of </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">I am ___ </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">statements an individual would make about themselves. It seems correct that </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">identity </span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">can</span><span class=\"by_hZzKXBem6W2nCigrR\"> be</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> dangerous for epistemics since the desire to maintain one's identity can interfere with updating correctly or changing actions [</span></span><a href=\"https://www.lesswrong.com/posts/BXQsZmubkovJ76Ldo/the-actionable-version-of-keep-your-identity-small\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">], but at the same time there are potentially useful and safe ways to maintain an identity which even enhances one's rationality [</span><a href=\"https://www.lesswrong.com/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/posts/Zupr296Zy74wpihXT/use-your-identity-carefully\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">].</span><br><br><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">(1) We might consider cases where an external party imposes an identity on someone, but that case has not been the topic of most discussion of Identity on LessWrong.</span></i></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External resources:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"http://www.paulgraham.com/identity.html#f2n\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Keep Your Identity Small</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Paul Graham</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/personal-identity\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Personal Identity</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/self-improvement\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Self Improvement</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 64,
    "description": {
      "markdown": "**Identity** is an individual's conception of themselves (1). We might conceive of this as the set of *I am ___* statements an individual would make about themselves. It seems correct that *identity* can be dangerous for epistemics since the desire to maintain one's identity can interfere with updating correctly or changing actions \\[[1](https://www.lesswrong.com/posts/BXQsZmubkovJ76Ldo/the-actionable-version-of-keep-your-identity-small)\\], but at the same time there are potentially useful and safe ways to maintain an identity which even enhances one's rationality \\[[1](https://www.lesswrong.com/posts/uR8c2NPp4bWHQ5u45/strategic-choice-of-identity), [2](https://www.lesswrong.com/posts/Zupr296Zy74wpihXT/use-your-identity-carefully)\\].  \n  \n  \n*(1) We might consider cases where an external party imposes an identity on someone, but that case has not been the topic of most discussion of Identity on LessWrong.*\n\n**External resources:** [Keep Your Identity Small](http://www.paulgraham.com/identity.html#f2n) by Paul Graham\n\n**Related Pages:** [Personal Identity](https://www.lesswrong.com/tag/personal-identity), [Self Improvement](https://www.lesswrong.com/tag/self-improvement)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LNsEBXoFdAy8yzvbw",
    "name": "Memetic Immune System",
    "core": false,
    "slug": "memetic-immune-system",
    "tableOfContents": {
      "html": "<blockquote><span class=\"by_HoGziwmhpMGqGeWZy\">Intelligent people sometimes do things more stupid than stupid people are capable of. There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes. The trouble is that these antibodies are not logical. On the contrary; these antibodies are often highly </span><em><span class=\"by_HoGziwmhpMGqGeWZy\">illogical</span></em><span class=\"by_HoGziwmhpMGqGeWZy\">. They are the blind spots that let us live with a dangerous meme without being impelled to action by it.</span></blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">-Phil Goetz, </span><a href=\"https://www.lesswrong.com/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder\"><span class=\"by_HoGziwmhpMGqGeWZy\">Reason as memetic immune disorder</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 17,
    "description": {
      "markdown": "> Intelligent people sometimes do things more stupid than stupid people are capable of. There are a variety of reasons for this; but one has to do with the fact that all cultures have dangerous memes circulating in them, and cultural antibodies to those memes. The trouble is that these antibodies are not logical. On the contrary; these antibodies are often highly _illogical_. They are the blind spots that let us live with a dangerous meme without being impelled to action by it.\n\n-Phil Goetz, [Reason as memetic immune disorder](https://www.lesswrong.com/posts/aHaqgTNnFzD7NGLMx/reason-as-memetic-immune-disorder)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZXFpyQWPB5ideFbEG",
    "name": "Conversation (topic)",
    "core": false,
    "slug": "conversation-topic",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">conversation </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is when two people talk or correspond. Most content here is about </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">how to have good conversations.</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_r38pkCm7wF4M44MDQ\">(</span></span><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">This wikitag needs work.</span><span class=\"by_r38pkCm7wF4M44MDQ\">)</span></span></i><br><br><span class=\"by_qgdGA4ZEyW7zNdK84\">For records of conversations, see </span><a href=\"Interviews (1)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Interviews</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Debates,...</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Communication</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Communication Cultures</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Relationshops</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Community</span></li></ul><h2 id=\"Conversation_Halter\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Conversation Halter</span></i></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">This term was introduced on LessWrong by Eliezer in the </span><a href=\"https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters\"><span class=\"by_qgdGA4ZEyW7zNdK84\">eponymous post</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">While working on my book, I found in passing that I'd developed a list of what I started out calling \"stonewalls\", but have since decided to refer to as \"conversation halters\".&nbsp; These tactics of argument are distinguished by their being attempts to cut off the flow of debate - which is rarely the wisest way to think, and should certainly rate an alarm bell.</span></i></p></blockquote>",
      "sections": [
        {
          "title": "Conversation Halter",
          "anchor": "Conversation_Halter",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 107,
    "description": {
      "markdown": "A **conversation** is when two people talk or correspond. Most content here is about *how to have good conversations.* (*This wikitag needs work.)*  \n  \nFor records of conversations, see [Interviews](Interviews (1)), Debates,...\n\nSee also:\n\n*   Communication\n*   Communication Cultures\n*   Relationshops\n*   Community\n\n*Conversation Halter*\n---------------------\n\nThis term was introduced on LessWrong by Eliezer in the [eponymous post](https://www.lesswrong.com/posts/wqmmv6NraYv4Xoeyj/conversation-halters):\n\n> *While working on my book, I found in passing that I'd developed a list of what I started out calling \"stonewalls\", but have since decided to refer to as \"conversation halters\".  These tactics of argument are distinguished by their being attempts to cut off the flow of debate - which is rarely the wisest way to think, and should certainly rate an alarm bell.*"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RE6h98Ziwcfh4EP9T",
    "name": "Steelmanning",
    "core": null,
    "slug": "steelmanning",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Steelmanning </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is</span><span class=\"by_wgKFztEMLyjFRm4ka\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">act of taking</span><span class=\"by_wgKFztEMLyjFRm4ka\"> a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">view,</span><span class=\"by_wgKFztEMLyjFRm4ka\"> or </span><span class=\"by_qgdGA4ZEyW7zNdK84\">opinion,</span><span class=\"by_wgKFztEMLyjFRm4ka\"> or argument </span><span class=\"by_qgdGA4ZEyW7zNdK84\">and constructing the strongest possible version of it. It</span><span class=\"by_wgKFztEMLyjFRm4ka\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the opposite</span><span class=\"by_wgKFztEMLyjFRm4ka\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">strawmanning.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Posts:</span></strong><br><a href=\"https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Against Steelmanning</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by Thing of Things</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"http://lesswrong.com/tag/disagreement\"><span class=\"by_HoGziwmhpMGqGeWZy\">Disagreement</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/ideological-turing-tests\"><span class=\"by_HoGziwmhpMGqGeWZy\">Ideological Turing Tests</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/least-convenient-possible-world\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Least convenient possible world</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 29,
    "description": {
      "markdown": "**Steelmanning** is the act of taking a view, or opinion, or argument and constructing the strongest possible version of it. It is the opposite of strawmanning.\n\n**External Posts:**  \n[Against Steelmanning](https://thingofthings.wordpress.com/2016/08/09/against-steelmanning/) by Thing of Things\n\nSee also: [Disagreement](http://lesswrong.com/tag/disagreement), [Ideological Turing Tests](https://www.lesswrong.com/tag/ideological-turing-tests), [Least convenient possible world](https://lessestwrong.com/tag/least-convenient-possible-world)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mPuSAzJN7CyrMiKrf",
    "name": "Voting Theory",
    "core": null,
    "slug": "voting-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Voting theory</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">, also called social choice theory, is the study of voting mechanisms. In other words, for a given list of candidates and voters, a voting method specifies a set of valid ways to fill out a ballot, and, given a valid ballot from each voter, produces an outcome.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/ZBNBTSMAXbyJwJoKY\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Voting Theory Primer for Rationalists</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Resources: </span></strong><a href=\"https://electowiki.org/wiki/Main_Page\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Electowiki</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> - A wiki focused on voting theory and electoral systems.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">related tags: </span></strong><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Game Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Decision Theory</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&amp;useTagName=true\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Coordination / Cooperation</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 35,
    "description": {
      "markdown": "**Voting theory**, also called social choice theory, is the study of voting mechanisms. In other words, for a given list of candidates and voters, a voting method specifies a set of valid ways to fill out a ballot, and, given a valid ballot from each voter, produces an outcome.\n\n**Related Sequences:** [Voting Theory Primer for Rationalists](https://www.lesswrong.com/s/ZBNBTSMAXbyJwJoKY)\n\n**Resources:** [Electowiki](https://electowiki.org/wiki/Main_Page) \\- A wiki focused on voting theory and electoral systems.\n\n**related tags:** [Game Theory](https://www.lesswrong.com/tag/game-theory), [Decision Theory](https://www.lesswrong.com/tag/decision-theory), [Coordination / Cooperation](https://www.lesswrong.com/tag/coordination-cooperation?showPostCount=true&useTagName=true)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EeSkeTcT4wtW2fWsL",
    "name": "Cause Prioritization",
    "core": false,
    "slug": "cause-prioritization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Cause Prioritization</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is the process of researching which charitable causes offer the most benefit for the marginal investment. Priorities can shift as existing causes reach funding and hiring goals, and new opportunities to do good are discovered. Cause prioritization is an important part of </span><a href=\"http://lesswrong.com/tag/effective-altruism\"><span class=\"by_HoGziwmhpMGqGeWZy\">Effective Altruism</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://causeprioritization.org/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Cause Prioritization Wiki</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 38,
    "description": {
      "markdown": "**Cause Prioritization** is the process of researching which charitable causes offer the most benefit for the marginal investment. Priorities can shift as existing causes reach funding and hiring goals, and new opportunities to do good are discovered. Cause prioritization is an important part of [Effective Altruism](http://lesswrong.com/tag/effective-altruism).\n\n**See also:** [Cause Prioritization Wiki](https://causeprioritization.org/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "t7t9nW6BtJhfGNSR6",
    "name": "Aging",
    "core": false,
    "slug": "aging",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/life-extension\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Life Extension</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 50,
    "description": {
      "markdown": "See also: [Life Extension](https://www.lesswrong.com/tag/life-extension)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "RLQumypPQGPYg9t6G",
    "name": "Gaming (videogames/tabletop)",
    "core": false,
    "slug": "gaming-videogames-tabletop",
    "tableOfContents": {
      "html": "<p><strong id=\"Gaming\"><span class=\"by_HoGziwmhpMGqGeWZy\">Gaming</span></strong></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_HoGziwmhpMGqGeWZy\">Game Theory</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/puzzle-game-index\"><span class=\"by_HoGziwmhpMGqGeWZy\">Puzzle Game Index</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/games-posts-describing\"><span class=\"by_HoGziwmhpMGqGeWZy\">Games (posts describing)</span></a></p>",
      "sections": [
        {
          "title": "Gaming",
          "anchor": "Gaming",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 130,
    "description": {
      "markdown": "**Gaming**\n\nSee also: [Game Theory](https://www.lesswrong.com/tag/game-theory), [Puzzle Game Index](https://www.lesswrong.com/tag/puzzle-game-index), [Games (posts describing)](https://www.lesswrong.com/tag/games-posts-describing)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cRaweRcZcXnb9Qryt",
    "name": "Meta-Honesty",
    "core": null,
    "slug": "meta-honesty",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Meta-Honesty </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is the attempt to be honest about in which situations one will not be honest. It derives from the recognition that an object-level commitment to never lie under any possible circumstance is untenable. A meta-honest person might say something like \"I will lie in circumstances similar to an axe-wielding murderer coming to my door and enquiring after the location of my friend.\"</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "**Meta-Honesty** is the attempt to be honest about in which situations one will not be honest. It derives from the recognition that an object-level commitment to never lie under any possible circumstance is untenable. A meta-honest person might say something like \"I will lie in circumstances similar to an axe-wielding murderer coming to my door and enquiring after the location of my friend.\""
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bY5MaF2EATwDkomvu",
    "name": "History",
    "core": false,
    "slug": "history",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">History</span></strong><span><span class=\"by_nLbwLhBaQeG6tCNDN\">: \"Why should I remember the Wright Brothers’ first flight? I was not there. But as a rationalist, could I dare to not remember, when the event actually happened? Is there so much difference between seeing an event through your eyes—which is actually a causal chain involving reflected photons, not a direct connection—and seeing an event through a history book? Photons and history books both descend by causal chains from the event itself.\"</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> - Eliezer Yudkowsky, </span></span><a href=\"https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available\"><i><span class=\"by_sKAL2jzfkYkDbQmx9\">Making History Available</span></i></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/history-of-rationality\"><span class=\"by_sKAL2jzfkYkDbQmx9\">History of Rationality</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/history-of-less-wrong\"><span class=\"by_sKAL2jzfkYkDbQmx9\">History of Less Wrong</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 193,
    "description": {
      "markdown": "**History**: \"Why should I remember the Wright Brothers’ first flight? I was not there. But as a rationalist, could I dare to not remember, when the event actually happened? Is there so much difference between seeing an event through your eyes—which is actually a causal chain involving reflected photons, not a direct connection—and seeing an event through a history book? Photons and history books both descend by causal chains from the event itself.\" - Eliezer Yudkowsky, [*Making History Available*](https://www.lesswrong.com/posts/TLKPj4GDXetZuPDH5/making-history-available).\n\n**Related Pages:** [History of Rationality](https://www.lesswrong.com/tag/history-of-rationality), [History of Less Wrong](https://www.lesswrong.com/tag/history-of-less-wrong)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sPpZRaxpNNJjw55eu",
    "name": "Progress Studies",
    "core": false,
    "slug": "progress-studies",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Progress Studies</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the study of the causes of </span><span class=\"by_MSy6E9mTc4i3dcf2M\">civilizational</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> progress, e.g., the combination of economic, technological, scientific, and cultural advancements that have transformed human life and raised standards of living over the past couple of centuries.</span></span></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">The bicycle, as we know it today, was not invented until the late 1800s. Yet it was a simple mechanical invention. It would seem to require no brilliant inventive insight, and certainly no scientific background.&nbsp;</span></i><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Why, then, wasn’t it invented much earlier? – </span></i><a href=\"https://www.lesswrong.com/posts/TPytnFcWiD2E4cTrm/why-did-we-wait-so-long-for-the-bicycle\"><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Why did we wait so long for the bicycle?</span></u></i></a></p></blockquote><h2 id=\"See_also_\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/history\"><span class=\"by_qgdGA4ZEyW7zNdK84\">History</span></a></li></ul><h2 id=\"Origin_of_the_Name\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Origin of the Name</span></h2><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Progress Studies </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">was proposed as an academic field by Tyler Cowen and Patrick Collison [</span><a href=\"https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">] after they noticed that there’s no intellectual movement focused on understanding the dynamics of progress, or on trying to speed it up.</span></p><h2 id=\"External_Resources\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Resources</span></h2><ul><li><a href=\"https://rootsofprogress.org/about\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Roots of Progress</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">is a blog by </span><a href=\"https://www.lesswrong.com/users/jasoncrawford\"><span class=\"by_qgdGA4ZEyW7zNdK84\">jasoncrawford</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> that explores the history of technology and industry alongside the philosophical questions of human progress. Many of the blogs posts are crossposted to LessWrong.</span></li></ul>",
      "sections": [
        {
          "title": "See also:",
          "anchor": "See_also_",
          "level": 1
        },
        {
          "title": "Origin of the Name",
          "anchor": "Origin_of_the_Name",
          "level": 1
        },
        {
          "title": "External Resources",
          "anchor": "External_Resources",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 164,
    "description": {
      "markdown": "**Progress Studies** is the study of the causes of civilizational progress, e.g., the combination of economic, technological, scientific, and cultural advancements that have transformed human life and raised standards of living over the past couple of centuries.\n\n> *The bicycle, as we know it today, was not invented until the late 1800s. Yet it was a simple mechanical invention. It would seem to require no brilliant inventive insight, and certainly no scientific background. *  \n> *Why, then, wasn’t it invented much earlier? –* [*Why did we wait so long for the bicycle?*](https://www.lesswrong.com/posts/TPytnFcWiD2E4cTrm/why-did-we-wait-so-long-for-the-bicycle)\n\nSee also:\n---------\n\n*   [History](https://www.lesswrong.com/tag/history)\n\nOrigin of the Name\n------------------\n\n*Progress Studies* was proposed as an academic field by Tyler Cowen and Patrick Collison \\[[1](https://www.theatlantic.com/science/archive/2019/07/we-need-new-science-progress/594946/)\\] after they noticed that there’s no intellectual movement focused on understanding the dynamics of progress, or on trying to speed it up.\n\nExternal Resources\n------------------\n\n*   [*Roots of Progress*](https://rootsofprogress.org/about)  is a blog by [jasoncrawford](https://www.lesswrong.com/users/jasoncrawford) that explores the history of technology and industry alongside the philosophical questions of human progress. Many of the blogs posts are crossposted to LessWrong."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FkzScn5byCs9PxGsA",
    "name": "Politics",
    "core": null,
    "slug": "politics",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 302,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KDpqtN3MxHSmD4vcB",
    "name": "Art",
    "core": false,
    "slug": "art",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Art</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is material created for aesthetic appreciation, including visual art, comics, and music. This tag includes both posts sharing works of art and posts discussing art conceptually.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Some types of art have their own tag category: </span><a href=\"http://lesswrong.com/tag/poetry\"><span class=\"by_HoGziwmhpMGqGeWZy\">Poetry</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">,</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></span><a href=\"http://lesswrong.com/tag/fiction\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Fiction</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/fiction-topic\"><span class=\"by_HoGziwmhpMGqGeWZy\">Fiction(topic)</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/gaming-videogames-tabletop\"><span class=\"by_HoGziwmhpMGqGeWZy\">Games</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Sequences:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/s/WPgA9x5ZvKu9oYvgB\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Drawing Less Wrong</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 62,
    "description": {
      "markdown": "**Art** is material created for aesthetic appreciation, including visual art, comics, and music. This tag includes both posts sharing works of art and posts discussing art conceptually.\n\nSome types of art have their own tag category: [Poetry](http://lesswrong.com/tag/poetry), [Fiction](http://lesswrong.com/tag/fiction), [Fiction(topic)](https://www.lesswrong.com/tag/fiction-topic), [Games](https://www.lesswrong.com/tag/gaming-videogames-tabletop)\n\n**Related Sequences:** [Drawing Less Wrong](https://www.lesswrong.com/s/WPgA9x5ZvKu9oYvgB)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "FtT2T9bRbECCGYxrL",
    "name": "Philosophy of Language",
    "core": false,
    "slug": "philosophy-of-language",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Language</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> is an important part of the way we frame problems, think about the world, and discuss things with others. The study of language is particularly relevant to LessWrong because </span><a href=\"https://www.lesswrong.com/posts/FaJaCgqBKphrDzDSj/37-ways-that-words-can-be-wrong\"><span class=\"by_Xn6ACr6Cua8upALWQ\">many very smart people confuse themselves and others by misusing words.</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The comprehensive introduction to the LessWrong approach to Philosophy of Language is </span><a href=\"https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb\"><span class=\"by_Xn6ACr6Cua8upALWQ\">A Human's Guide To Words</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 120,
    "description": {
      "markdown": "**Language** is an important part of the way we frame problems, think about the world, and discuss things with others. The study of language is particularly relevant to LessWrong because [many very smart people confuse themselves and others by misusing words.](https://www.lesswrong.com/posts/FaJaCgqBKphrDzDSj/37-ways-that-words-can-be-wrong)\n\nThe comprehensive introduction to the LessWrong approach to Philosophy of Language is [A Human's Guide To Words](https://www.lesswrong.com/s/SGB7Y5WERh4skwtnb)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "o9aQASibdsECTfYF6",
    "name": "Moloch",
    "core": null,
    "slug": "moloch",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is </span><span class=\"by_EQNTWXLKMeWMp2FQS\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">personification</span><span class=\"by_TJxpFJEgQzsvaFwhj\"> of</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> the forces that coerce competing individuals to take actions which, although locally optimal, ultimately lead to</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> situations where </span><span class=\"by_qgdGA4ZEyW7zNdK84\">everyone is worse off. Moreover, no individual is able to unilaterally break out of the dynamic. The situation is a bad Nash equilibrium. A trap.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">One example of a Molochian dynamic is a </span><a href=\"https://en.wikipedia.org/wiki/Red_Queen%27s_race\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Red Queen race</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> between scientists</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> who </span><span class=\"by_qgdGA4ZEyW7zNdK84\">must continually spend more time writing grant applications just to keep up with their peers doing</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">same. Through unavoidable competition, they have all lost time while not ending up with any more grant money. And any scientist who unilaterally tried</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">not engage</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> in the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">competition</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> would </span><span class=\"by_qgdGA4ZEyW7zNdK84\">soon be replaced by one who still does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">The topic of Moloch receives a formal treatment in</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">sequence </span></span><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Inadequate Equilibria</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, particularly in the chapter </span><a href=\"https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/moloch-s-toolbox-1-2\"><span><span class=\"by_Csts9WR27sGGEuFHd\">Moloch'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s Toolbox</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h2 id=\"Origin\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Origin</span></h2><p><a href=\"https://www.lesswrong.com/users/yvain?sortedBy=top\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Scott Alexander</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_Csts9WR27sGGEuFHd\">&nbsp;</span><span class=\"by_qgdGA4ZEyW7zNdK84\">linked</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> the name </span><span class=\"by_qgdGA4ZEyW7zNdK84\">to</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">concept in his eponymous post, </span></span><a href=\"https://www.lessestwrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch\"><span><span class=\"by_Sp5wM4aRAhNERd4oY\">Meditations on </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch</span></span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">. </span><span class=\"by_Csts9WR27sGGEuFHd\">&nbsp;</span><span class=\"by_qgdGA4ZEyW7zNdK84\">The post intersperses lines of </span><span class=\"by_EQNTWXLKMeWMp2FQS\">Allan </span><span class=\"by_Csts9WR27sGGEuFHd\">Ginsberg'</span><span class=\"by_EQNTWXLKMeWMp2FQS\">s </span><span class=\"by_qgdGA4ZEyW7zNdK84\">poem,</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span></span><a href=\"https://www.poetryfoundation.org/poems/49303/howl\"><span class=\"by_EQNTWXLKMeWMp2FQS\">Howl</span></a><span><span class=\"by_EQNTWXLKMeWMp2FQS\">, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">with multiples examples </span><span class=\"by_EQNTWXLKMeWMp2FQS\">of the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">dynamic including: the </span><span class=\"by_Csts9WR27sGGEuFHd\">Prisoner'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s Dilemma, dollar auctions, </span></span><a href=\"https://web.archive.org/web/20160928190322/http://raikoth.net/libertarian.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">fish farming story</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">, Malthusian trap, capitalism, two-income trap, agriculture, arms races, races to the bottom, education system, science, and government corruption and corporate welfare.</span><span class=\"by_Csts9WR27sGGEuFHd\">&nbsp;</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">From Allan </span><span class=\"by_Csts9WR27sGGEuFHd\">Ginsberg'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s </span></span><a href=\"https://www.poetryfoundation.org/poems/49303/howl\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Howl</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?</span></i><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!</span></i><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!</span></i><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments!</span></i><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!</span></i><br><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in </span><span class=\"by_EQNTWXLKMeWMp2FQS\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">long streets like endless Jehovahs! Moloch whose factories dream and croak</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> in the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">fog! Moloch whose smoke-stacks and antennae crown the cities!</span></span></i><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Moloch whose love is endless oil and stone! Moloch whose soul is electricity and banks! Moloch whose poverty is the specter of genius! Moloch whose fate is a cloud of sexless hydrogen! Moloch whose name is the Mind!</span></i></p></blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/eldritch-analogies\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Eldritch Analogies</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/game-theory\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Game Theory</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/group-rationality?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Group Rationality</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/social-reality\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Social Reality</span></a></p>",
      "sections": [
        {
          "title": "Origin",
          "anchor": "Origin",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 50,
    "description": {
      "markdown": "**Moloch** is the personification of the forces that coerce competing individuals to take actions which, although locally optimal, ultimately lead to situations where everyone is worse off. Moreover, no individual is able to unilaterally break out of the dynamic. The situation is a bad Nash equilibrium. A trap.\n\nOne example of a Molochian dynamic is a [Red Queen race](https://en.wikipedia.org/wiki/Red_Queen%27s_race) between scientists who must continually spend more time writing grant applications just to keep up with their peers doing the same. Through unavoidable competition, they have all lost time while not ending up with any more grant money. And any scientist who unilaterally tried to not engage in the competition would soon be replaced by one who still does. If they all promised to cap their grant writing time, everyone would face an incentive to defect.\n\nThe topic of Moloch receives a formal treatment in the sequence [Inadequate Equilibria](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d), particularly in the chapter [Moloch's Toolbox](https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/moloch-s-toolbox-1-2).\n\nOrigin\n------\n\n[Scott Alexander](https://www.lesswrong.com/users/yvain?sortedBy=top)  linked the name to the concept in his eponymous post, [Meditations on Moloch](https://www.lessestwrong.com/posts/TxcRbCYHaeL59aY7E/meditations-on-moloch).  The post intersperses lines of Allan Ginsberg's poem, [Howl](https://www.poetryfoundation.org/poems/49303/howl), with multiples examples of the dynamic including: the Prisoner's Dilemma, dollar auctions, [fish farming story](https://web.archive.org/web/20160928190322/http://raikoth.net/libertarian.html), Malthusian trap, capitalism, two-income trap, agriculture, arms races, races to the bottom, education system, science, and government corruption and corporate welfare. \n\nFrom Allan Ginsberg's [Howl](https://www.poetryfoundation.org/poems/49303/howl):\n\n> *What sphinx of cement and aluminum bashed open their skulls and ate up their brains and imagination?*  \n> *Moloch! Solitude! Filth! Ugliness! Ashcans and unobtainable dollars! Children screaming under the stairways! Boys sobbing in armies! Old men weeping in the parks!*  \n> *Moloch! Moloch! Nightmare of Moloch! Moloch the loveless! Mental Moloch! Moloch the heavy judger of men!*  \n> *Moloch the incomprehensible prison! Moloch the crossbone soulless jailhouse and Congress of sorrows! Moloch whose buildings are judgment! Moloch the vast stone of war! Moloch the stunned governments!*  \n> *Moloch whose mind is pure machinery! Moloch whose blood is running money! Moloch whose fingers are ten armies! Moloch whose breast is a cannibal dynamo! Moloch whose ear is a smoking tomb!*  \n> *Moloch whose eyes are a thousand blind windows! Moloch whose skyscrapers stand in the long streets like endless Jehovahs! Moloch whose factories dream and croak in the fog! Moloch whose smoke-stacks and antennae crown the cities!*  \n> *Moloch whose love is endless oil and stone! Moloch whose soul is electricity and banks! Moloch whose poverty is the specter of genius! Moloch whose fate is a cloud of sexless hydrogen! Moloch whose name is the Mind!*\n\nSee also: [Eldritch Analogies](https://www.lesswrong.com/tag/eldritch-analogies), [Game Theory](https://www.lesswrong.com/tag/game-theory), [Group Rationality](https://www.lesswrong.com/tag/group-rationality?showPostCount=true&useTagName=true), [Social Reality](https://www.lesswrong.com/tag/social-reality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "CD6DGZJD4ningyzWF",
    "name": "Trigger-Action Planning",
    "core": null,
    "slug": "trigger-action-planning",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Trigger-Action Planning (</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">TAP), sometimes </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Trigger-Action Patterns</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">, and formerly </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Implementation Intentions </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">are techniques for getting oneself to successfully enact desired actions (or inactions) by training something like a \"stimulus-response\" pair. The technique was spread by CFAR which initially drew upon the psychology literature of Implementation Intentions.&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">After it was clear that TAPs should be heavily applied to cognitive motions/thought patterns, some decided that the 'P' should stand for 'Pattern' rather than 'Plan'.</span></p><h2 id=\"Resources\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Resources</span></strong></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The CFAR Participant Handbook [1] contains a chapter on TAPs.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Brienne Yudkowsky's writings on the topic of Noticing, found mostly at her blog [</span><a href=\"https://agentyduck.blogspot.com/search?q=noticing\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">], are particularly good material related to training TAPs.</span></li></ul>",
      "sections": [
        {
          "title": "Resources",
          "anchor": "Resources",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 27,
    "description": {
      "markdown": "**Trigger-Action Planning (**TAP), sometimes **Trigger-Action Patterns**, and formerly **Implementation Intentions** are techniques for getting oneself to successfully enact desired actions (or inactions) by training something like a \"stimulus-response\" pair. The technique was spread by CFAR which initially drew upon the psychology literature of Implementation Intentions. \n\nAfter it was clear that TAPs should be heavily applied to cognitive motions/thought patterns, some decided that the 'P' should stand for 'Pattern' rather than 'Plan'.\n\n**Resources**\n-------------\n\n*   The CFAR Participant Handbook \\[1\\] contains a chapter on TAPs.\n*   Brienne Yudkowsky's writings on the topic of Noticing, found mostly at her blog \\[[1](https://agentyduck.blogspot.com/search?q=noticing)\\], are particularly good material related to training TAPs."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8uNFGxejo5hykCEez",
    "name": "Virtues",
    "core": null,
    "slug": "virtues",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Virtues</span></strong><span><span class=\"by_XtphY3uYHwruKqDyG\"> </span><span class=\"by_QBvPFLFyZyuHcBwFm\">are traits that one </span></span><i><span class=\"by_QBvPFLFyZyuHcBwFm\">ought</span></i><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> to possess, for</span><span class=\"by_XtphY3uYHwruKqDyG\"> the </span><span class=\"by_QBvPFLFyZyuHcBwFm\">benefit of the world or </span><span class=\"by_PdzQ73mN7S4SvRMhu\">oneself.</span></span></p><p><span><span class=\"by_QBvPFLFyZyuHcBwFm\">On </span><span class=\"by_PdzQ73mN7S4SvRMhu\">LessWrong</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> the </span><span class=\"by_PdzQ73mN7S4SvRMhu\">focus</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> is often </span><span class=\"by_PdzQ73mN7S4SvRMhu\">on epistemic virtues, as in Eliezer Yudowsky's essay </span></span><a href=\"https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality\"><strong><span><span class=\"by_XtphY3uYHwruKqDyG\">Twelve Virtues of </span><span class=\"by_QBvPFLFyZyuHcBwFm\">Rationality</span></span></strong></a><span><span class=\"by_PdzQ73mN7S4SvRMhu\"> which offers this list</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> of </span><span class=\"by_PdzQ73mN7S4SvRMhu\">virtues</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> (roughly summarized):</span></span></p><ul><li><a href=\"https://www.lesswrong.com/tag/curiosity\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Curiosity</span></strong></a><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - the</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_XtphY3uYHwruKqDyG\">burning </span><span class=\"by_QBvPFLFyZyuHcBwFm\">desire</span><span class=\"by_XtphY3uYHwruKqDyG\"> to pursue </span><span class=\"by_QBvPFLFyZyuHcBwFm\">truth;</span></span></li><li><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Relinquishment</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\"> - not being attached to mistaken beliefs;</span></li><li><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Lightness</span></strong><span class=\"by_QBvPFLFyZyuHcBwFm\"> - updating your beliefs with ease;</span></li><li><a href=\"https://www.lesswrong.com/tag/evenness\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Evenness</span></strong></a><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - not privileging particular hypotheses in</span><span class=\"by_XtphY3uYHwruKqDyG\"> the </span><span class=\"by_QBvPFLFyZyuHcBwFm\">pursuit</span><span class=\"by_XtphY3uYHwruKqDyG\"> of </span><span class=\"by_QBvPFLFyZyuHcBwFm\">truth;</span></span></li><li><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Argument</span></strong><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - the will to let </span><span class=\"by_sKAL2jzfkYkDbQmx9\">one'</span><span class=\"by_QBvPFLFyZyuHcBwFm\">s beliefs</span><span class=\"by_XtphY3uYHwruKqDyG\"> be </span><span class=\"by_QBvPFLFyZyuHcBwFm\">challenged;</span></span></li><li><a href=\"https://www.lesswrong.com/tag/empiricism\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Empiricism</span></strong></a><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - grounding oneself in observation</span><span class=\"by_XtphY3uYHwruKqDyG\"> and </span><span class=\"by_QBvPFLFyZyuHcBwFm\">prediction;</span></span></li><li><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Simplicity</span></strong></a><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - elimination of unnecessary detail</span><span class=\"by_XtphY3uYHwruKqDyG\"> in </span><span class=\"by_QBvPFLFyZyuHcBwFm\">modeling the world;</span></span></li><li><a href=\"https://www.lesswrong.com/tag/humility\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Humility</span></strong></a><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - recognition of </span><span class=\"by_sKAL2jzfkYkDbQmx9\">one'</span><span class=\"by_QBvPFLFyZyuHcBwFm\">s fallibility;</span></span></li><li><a href=\"https://www.lesswrong.com/tag/perfectionism\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Perfectionism</span></strong></a><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - seeking perfection even</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_XtphY3uYHwruKqDyG\">if </span><span class=\"by_sKAL2jzfkYkDbQmx9\">it'</span><span class=\"by_QBvPFLFyZyuHcBwFm\">s</span><span class=\"by_XtphY3uYHwruKqDyG\"> not </span><span class=\"by_QBvPFLFyZyuHcBwFm\">attainable;</span></span></li><li><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Precision</span></strong><span><span class=\"by_QBvPFLFyZyuHcBwFm\"> - seeking narrower statements</span><span class=\"by_XtphY3uYHwruKqDyG\"> and </span><span class=\"by_QBvPFLFyZyuHcBwFm\">not overcorrect;</span></span></li><li><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">Scholarship</span></strong></a><span class=\"by_QBvPFLFyZyuHcBwFm\"> - the study of multiple domains and perspectives;</span></li><li><a href=\"https://www.lesswrong.com/tag/twelfth-virtue\"><strong><span class=\"by_QBvPFLFyZyuHcBwFm\">The nameless virtue</span></strong></a><span class=\"by_QBvPFLFyZyuHcBwFm\"> - seeking truth and not the virtues for themselves.</span></li></ul><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See Also: </span></strong><a href=\"https://www.lesswrong.com/tag/courage\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Courage</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/trust\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Trust</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/honesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Honesty</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/agency\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Agency</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/altruism\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Altruism</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/ambition\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Ambition</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/stoicism-letting-go-making-peace\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Stoicism / Letting Go / Making Peace</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/attention\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Attention</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/gratitude\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Gratitude</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Sequences:</span></strong><br><a href=\"https://www.lesswrong.com/s/xqgwpmwDYsn8osoje\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Notes On Virtues</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by </span><a href=\"https://www.lesswrong.com/users/david_gross\"><span class=\"by_sKAL2jzfkYkDbQmx9\">David_Gross</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 92,
    "description": {
      "markdown": "**Virtues** are traits that one *ought* to possess, for the benefit of the world or oneself.\n\nOn LessWrong the focus is often on epistemic virtues, as in Eliezer Yudowsky's essay [**Twelve Virtues of Rationality**](https://www.lesswrong.com/posts/7ZqGiPHTpiDMwqMN2/twelve-virtues-of-rationality) which offers this list of virtues (roughly summarized):\n\n*   [**Curiosity**](https://www.lesswrong.com/tag/curiosity) \\- the burning desire to pursue truth;\n*   **Relinquishment** \\- not being attached to mistaken beliefs;\n*   **Lightness** \\- updating your beliefs with ease;\n*   [**Evenness**](https://www.lesswrong.com/tag/evenness) \\- not privileging particular hypotheses in the pursuit of truth;\n*   **Argument** \\- the will to let one's beliefs be challenged;\n*   [**Empiricism**](https://www.lesswrong.com/tag/empiricism) \\- grounding oneself in observation and prediction;\n*   [**Simplicity**](https://www.lesswrong.com/tag/occam-s-razor) \\- elimination of unnecessary detail in modeling the world;\n*   [**Humility**](https://www.lesswrong.com/tag/humility) \\- recognition of one's fallibility;\n*   [**Perfectionism**](https://www.lesswrong.com/tag/perfectionism) \\- seeking perfection even if it's not attainable;\n*   **Precision** \\- seeking narrower statements and not overcorrect;\n*   [**Scholarship**](https://www.lesswrong.com/tag/scholarship-and-learning) \\- the study of multiple domains and perspectives;\n*   [**The nameless virtue**](https://www.lesswrong.com/tag/twelfth-virtue) \\- seeking truth and not the virtues for themselves.\n\n**See Also:** [Courage](https://www.lesswrong.com/tag/courage), [Trust](https://www.lesswrong.com/tag/trust), [Honesty](https://www.lesswrong.com/tag/honesty), [Agency](https://www.lesswrong.com/tag/agency), [Altruism](https://www.lesswrong.com/tag/altruism), [Ambition](https://www.lesswrong.com/tag/ambition), [Stoicism / Letting Go / Making Peace](https://www.lesswrong.com/tag/stoicism-letting-go-making-peace), [Attention](https://www.lesswrong.com/tag/attention), [Gratitude](https://www.lesswrong.com/tag/gratitude)\n\n**Sequences:**  \n[Notes On Virtues](https://www.lesswrong.com/s/xqgwpmwDYsn8osoje) by [David_Gross](https://www.lesswrong.com/users/david_gross)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZzxvopS4BwLuQy42n",
    "name": "Rationalization",
    "core": null,
    "slug": "rationalization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is the act of finding reasons to believe what one has already decided they want to believe. It is a decidedly terrible way to arrive at true beliefs.</span></p><blockquote><p><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">“Rationalization.” What a</span><span class=\"by_nmk3nLpQE89dMRzzN\"> curious </span><span class=\"by_qgdGA4ZEyW7zNdK84\">term. I would call it a wrong word.</span><span class=\"by_nmk3nLpQE89dMRzzN\"> You cannot </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“rationalize”</span><span class=\"by_nmk3nLpQE89dMRzzN\"> what is not already rational.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> It is as if “lying” were called “truthization.” – </span></span></i><a href=\"https://www.lessestwrong.com/posts/SFZoEBpLo9frSJGkc/rationalization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization</span></a></p></blockquote><p><span class=\"by_nmk3nLpQE89dMRzzN\">Rationality starts from evidence, and then crunches forward through belief updates, in order to output a probable conclusion. \"Rationalization\" starts from a conclusion, and then works backward to arrive at arguments apparently favoring that conclusion. Rationalization argues for a side already selected; rationality tries to choose between sides.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Rationalization can be conscious or unconscious. It can take on a blatant, conscious form, in which you are aware that you want a particular side to be correct and you deliberately compose arguments for only that side, looking over the evidence and consciously filtering which facts will be presented. Or it can occur at perceptual speeds, without conscious intent or conscious awareness.</span></p><p><span class=\"by_nmk3nLpQE89dMRzzN\">Defeating rationalization - or even </span><i><span class=\"by_nmk3nLpQE89dMRzzN\">discovering</span></i><span class=\"by_nmk3nLpQE89dMRzzN\"> rationalizations - is a lifelong battle for the aspiring rationalist.</span></p><h2 id=\"See_Also\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/motivated-skepticism\"><span><span class=\"by_nmk3nLpQE89dMRzzN\">Motivated </span><span class=\"by_qf77EiaoMw7tH3GSr\">skepticism</span></span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/motivated-reasoning\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">motivated</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">cognition</span></span></a></li><li><a href=\"https://lessestwrong.com/tag/filtered-evidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Filtered evidence</span></a></li><li><a href=\"https://lessestwrong.com/tag/fake-simplicity\"><span class=\"by_qf77EiaoMw7tH3GSr\">Fake simplicity</span></a></li><li><a href=\"https://lessestwrong.com/tag/self-deception\"><span class=\"by_LoykQRMTxJFxwwdPy\">Self-deception</span></a></li><li><a href=\"https://lessestwrong.com/tag/litany-of-gendlin\"><span class=\"by_LoykQRMTxJFxwwdPy\">Litany of Gendlin</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Occam's_Imaginary_Razor\"><span class=\"by_LoykQRMTxJFxwwdPy\">Occam's Imaginary Razor</span></a></li><li><a href=\"https://lessestwrong.com/tag/hope\"><span class=\"by_qf77EiaoMw7tH3GSr\">Hope</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/oops\"><span class=\"by_LoykQRMTxJFxwwdPy\">oops</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 61,
    "description": {
      "markdown": "**Rationalization** is the act of finding reasons to believe what one has already decided they want to believe. It is a decidedly terrible way to arrive at true beliefs.\n\n> *“Rationalization.” What a curious term. I would call it a wrong word. You cannot “rationalize” what is not already rational. It is as if “lying” were called “truthization.” –* [Rationalization](https://www.lessestwrong.com/posts/SFZoEBpLo9frSJGkc/rationalization)\n\nRationality starts from evidence, and then crunches forward through belief updates, in order to output a probable conclusion. \"Rationalization\" starts from a conclusion, and then works backward to arrive at arguments apparently favoring that conclusion. Rationalization argues for a side already selected; rationality tries to choose between sides.\n\nRationalization can be conscious or unconscious. It can take on a blatant, conscious form, in which you are aware that you want a particular side to be correct and you deliberately compose arguments for only that side, looking over the evidence and consciously filtering which facts will be presented. Or it can occur at perceptual speeds, without conscious intent or conscious awareness.\n\nDefeating rationalization - or even *discovering* rationalizations - is a lifelong battle for the aspiring rationalist.\n\nSee Also\n--------\n\n*   [Motivated skepticism](https://lessestwrong.com/tag/motivated-skepticism), [motivated cognition](https://lessestwrong.com/tag/motivated-reasoning)\n*   [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)\n*   [Fake simplicity](https://lessestwrong.com/tag/fake-simplicity)\n*   [Self-deception](https://lessestwrong.com/tag/self-deception)\n*   [Litany of Gendlin](https://lessestwrong.com/tag/litany-of-gendlin)\n*   [Occam's Imaginary Razor](https://wiki.lesswrong.com/wiki/Occam's_Imaginary_Razor)\n*   [Hope](https://lessestwrong.com/tag/hope), [oops](https://lessestwrong.com/tag/oops)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LDTSbmXtokYAsEq8e",
    "name": "Motivated Reasoning",
    "core": false,
    "slug": "motivated-reasoning",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Motivated Reasoning</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a label for various mental processes that lead to desired conclusions regardless of </span><span class=\"by_HoGziwmhpMGqGeWZy\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">veracity of those conclusions.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span></span><a href=\"https://www.lesswrong.com/tag/confirmation-bias\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Confirmation </span><span class=\"by_HoGziwmhpMGqGeWZy\">Bias</span></span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/rationalization\"><span class=\"by_HoGziwmhpMGqGeWZy\">Rationalization</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/self-deception\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Self-deception</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/it/semantic_stopsigns/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Semantic Stopsigns</span></a></li><li><a href=\"https://lessestwrong.com/lw/j2/explainworshipignore/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Explain/Worship/Ignore?</span></a></li><li><a href=\"https://lessestwrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Avoiding Your Belief's Real Weak Points</span></a></li><li><a href=\"https://lessestwrong.com/lw/km/motivated_stopping_and_motivated_continuation/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivated Stopping and Motivated Continuation</span></a></li><li><a href=\"https://lesswrong.com/lw/js/the_bottom_line/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Bottom Line</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/rationalization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization</span></a></li><li><a href=\"https://lessestwrong.com/tag/motivated-skepticism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivated skepticism</span></a></li><li><a href=\"https://lessestwrong.com/tag/filtered-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Filtered evidence</span></a></li><li><a href=\"https://lessestwrong.com/tag/confirmation-bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Positive bias</span></a></li><li><a href=\"https://lessestwrong.com/tag/aversion-ugh-fields\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ugh field</span></a></li></ul>",
      "sections": [
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 57,
    "description": {
      "markdown": "**Motivated Reasoning** is a label for various mental processes that lead to desired conclusions regardless of the veracity of those conclusions.\n\n*Related*: [Confirmation Bias](https://www.lesswrong.com/tag/confirmation-bias), [Rationalization](https://www.lesswrong.com/tag/rationalization), [Self-deception](https://www.lesswrong.com/tag/self-deception) \n\nNotable Posts\n-------------\n\n*   [Semantic Stopsigns](https://lessestwrong.com/lw/it/semantic_stopsigns/)\n*   [Explain/Worship/Ignore?](https://lessestwrong.com/lw/j2/explainworshipignore/)\n*   [Avoiding Your Belief's Real Weak Points](https://lessestwrong.com/lw/jy/avoiding_your_beliefs_real_weak_points/)\n*   [Motivated Stopping and Motivated Continuation](https://lessestwrong.com/lw/km/motivated_stopping_and_motivated_continuation/)\n*   [The Bottom Line](https://lesswrong.com/lw/js/the_bottom_line/)\n\nSee also\n--------\n\n*   [Rationalization](https://lessestwrong.com/tag/rationalization)\n*   [Motivated skepticism](https://lessestwrong.com/tag/motivated-skepticism)\n*   [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)\n*   [Positive bias](https://lessestwrong.com/tag/confirmation-bias)\n*   [Ugh field](https://lessestwrong.com/tag/aversion-ugh-fields)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mf8wHrMrJR73uyDLQ",
    "name": "Moral Mazes",
    "core": null,
    "slug": "moral-mazes",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Moral Mazes </span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">is a term for businesses where middle managers spend most of their time and energy on internal status competitions rather than improving the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">company'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s products. The phrase comes from </span></span><a href=\"https://www.amazon.com/Moral-Mazes-World-Corporate-Managers/dp/0199729883\"><span class=\"by_HoGziwmhpMGqGeWZy\">the book of the same name</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by Robert Jackall.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">See also: </span><a href=\"https://www.lesswrong.com/tag/moloch\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Moloch</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> tag, and the </span><a href=\"https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Immoral Mazes sequence</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 39,
    "description": {
      "markdown": "**Moral Mazes** is a term for businesses where middle managers spend most of their time and energy on internal status competitions rather than improving the company's products. The phrase comes from [the book of the same name](https://www.amazon.com/Moral-Mazes-World-Corporate-Managers/dp/0199729883) by Robert Jackall.\n\nSee also: [Moloch](https://www.lesswrong.com/tag/moloch) tag, and the [Immoral Mazes sequence](https://www.lesswrong.com/s/kNANcHLNtJt5qeuSS)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vmvTYnmaKA73fYDe5",
    "name": "Life Extension",
    "core": false,
    "slug": "life-extension",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Life Extension </span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">is the theory / practice of extending human lifespans – for decades, centuries. or much longer. This includes advice that applies to individuals, research projects that might extend human lifespans as a whole, or philosophical discussion of the concept.&nbsp;</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">See also </span><a href=\"https://www.lesswrong.com/tag/aging\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Aging</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, and </span><a href=\"https://www.lessestwrong.com/tag/cryonics\"><span class=\"by_r38pkCm7wF4M44MDQ\">Cryonics</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\"> -</span><span class=\"by_r38pkCm7wF4M44MDQ\"> a particular life extension technique that has received a lot of discussion on LessWrong.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 58,
    "description": {
      "markdown": "**Life Extension** is the theory / practice of extending human lifespans – for decades, centuries. or much longer. This includes advice that applies to individuals, research projects that might extend human lifespans as a whole, or philosophical discussion of the concept. \n\nSee also [Aging](https://www.lesswrong.com/tag/aging), and [Cryonics](https://www.lessestwrong.com/tag/cryonics) \\- a particular life extension technique that has received a lot of discussion on LessWrong."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zcvsZQWJBFK6SxK4K",
    "name": "Postmortems & Retrospectives",
    "core": null,
    "slug": "postmortems-and-retrospectives",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Postmortem </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Retrospective </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is a reflection on past actions with an eye to what went well, what </span><span class=\"by_sKAL2jzfkYkDbQmx9\">didn'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">t, and the cause of any failures. Retrospectives are crucial for improving </span><span class=\"by_sKAL2jzfkYkDbQmx9\">one'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s rationality: they are the opportunity to the grade both </span><span class=\"by_sKAL2jzfkYkDbQmx9\">one'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s direct decisions as well as the decision-procedure and epistemic algorithms that one was employing. Sharing such accounts publicly is prosocial and allows others to learn from </span><span class=\"by_sKAL2jzfkYkDbQmx9\">one'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s experience too.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">One can ask </span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"could</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> I have done better had I reasoned better with the information available?</span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Often the answer is </span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"yes\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">, and one can apply lessons learnt going forward. It can feel painful to reflect on </span><span class=\"by_sKAL2jzfkYkDbQmx9\">one'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s mistakes, but doing so is how one grows.</span><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></span></p><p><span><span class=\"by_qxJ28GN72aiJu96iF\">This tag is specifically reporting actions and outcomes together with an evaluation of the choices/thinking patterns used. If a post focuses on changes in general beliefs about the world, without reflecting on specific actions, then it is a good fit for</span><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span><span class=\"by_qxJ28GN72aiJu96iF\">the </span></span><a href=\"https://www.lesswrong.com/tag/updated-beliefs-examples-of\"><span class=\"by_qxJ28GN72aiJu96iF\">Updated Beliefs</span></a><span><span class=\"by_qxJ28GN72aiJu96iF\"> tag. A central example of a Postmortems &amp; Retrospectives post is </span><span class=\"by_sKAL2jzfkYkDbQmx9\">“</span></span><a href=\"https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem\"><span class=\"by_qxJ28GN72aiJu96iF\">Arbital Postmortem</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">”</span><span class=\"by_qxJ28GN72aiJu96iF\">; in contrast, central examples of Updated Beliefs posts are </span><span class=\"by_sKAL2jzfkYkDbQmx9\">“</span></span><a href=\"https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">'</span><span class=\"by_qxJ28GN72aiJu96iF\">Other people are </span><span class=\"by_sKAL2jzfkYkDbQmx9\">wrong'</span><span class=\"by_qxJ28GN72aiJu96iF\"> vs </span><span class=\"by_sKAL2jzfkYkDbQmx9\">'I</span><span class=\"by_qxJ28GN72aiJu96iF\"> am </span><span class=\"by_sKAL2jzfkYkDbQmx9\">right'</span></span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">”</span><span class=\"by_qxJ28GN72aiJu96iF\"> and </span><span class=\"by_sKAL2jzfkYkDbQmx9\">“</span></span><a href=\"https://www.lesswrong.com/posts/MgFDzAfCku9MSDLuw/six-economics-misconceptions-of-mine-which-i-ve-resolved\"><span class=\"by_qxJ28GN72aiJu96iF\">Six Economics Misconceptions</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">”</span><span class=\"by_qxJ28GN72aiJu96iF\">.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/updated-beliefs-examples-of\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Updated Beliefs (examples of)</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/growth-stories\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Growth Stories</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/progress-studies\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Progress Studies</span></a></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://en.wikipedia.org/wiki/Pre-mortem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Premortem</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 135,
    "description": {
      "markdown": "A **Postmortem** or **Retrospective** is a reflection on past actions with an eye to what went well, what didn't, and the cause of any failures. Retrospectives are crucial for improving one's rationality: they are the opportunity to the grade both one's direct decisions as well as the decision-procedure and epistemic algorithms that one was employing. Sharing such accounts publicly is prosocial and allows others to learn from one's experience too.\n\nOne can ask \"could I have done better had I reasoned better with the information available?\" Often the answer is \"yes\", and one can apply lessons learnt going forward. It can feel painful to reflect on one's mistakes, but doing so is how one grows. \n\nThis tag is specifically reporting actions and outcomes together with an evaluation of the choices/thinking patterns used. If a post focuses on changes in general beliefs about the world, without reflecting on specific actions, then it is a good fit for the [Updated Beliefs](https://www.lesswrong.com/tag/updated-beliefs-examples-of) tag. A central example of a Postmortems & Retrospectives post is “[Arbital Postmortem](https://www.lesswrong.com/posts/kAgJJa3HLSZxsuSrf/arbital-postmortem)”; in contrast, central examples of Updated Beliefs posts are “['Other people are wrong' vs 'I am right'](https://www.lesswrong.com/posts/4QemtxDFaGXyGSrGD/other-people-are-wrong-vs-i-am-right)” and “[Six Economics Misconceptions](https://www.lesswrong.com/posts/MgFDzAfCku9MSDLuw/six-economics-misconceptions-of-mine-which-i-ve-resolved)”.\n\n**Related pages:** [Updated Beliefs (examples of)](https://www.lesswrong.com/tag/updated-beliefs-examples-of), [Growth Stories](https://www.lesswrong.com/tag/growth-stories), [Progress Studies](https://www.lesswrong.com/tag/progress-studies)\n\n**See also:** [Premortem](https://en.wikipedia.org/wiki/Pre-mortem)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HFou6RHqFagkyrKkW",
    "name": "Programming",
    "core": null,
    "slug": "programming",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 130,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hXTqT62YDTTiqJfxG",
    "name": "Ritual",
    "core": false,
    "slug": "ritual",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Rituals</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> are symbolic actions. In the context of LessWrong, it's significant that many rituals have some impact on your cognition, which makes them appropriate to be careful with. Nonetheless, some LessWrongers have worked to explore the space of ritual through a rationalist lens.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">It's a bit tricky to define. The book </span><a href=\"https://www.amazon.com/Secular-Wholeness-Skeptics-Paths-Richer/dp/155369175X\"><span class=\"by_r38pkCm7wF4M44MDQ\">Secular Wholeness</span></a><span class=\"by_r38pkCm7wF4M44MDQ\"> notes:</span></p><blockquote><p><i><span><span class=\"by_r38pkCm7wF4M44MDQ\">There’s a hazy boundary between the words “ritual,” “habit,” and “custom.” I think the difference between a ritual act and a habitual one lies </span><span class=\"by_sKAL2jzfkYkDbQmx9\">in awareness</span><span class=\"by_r38pkCm7wF4M44MDQ\"> and assent. An act becomes a ritual for you when you perform it with conscious awareness of its symbolic and emotional meaning, and with willing assent to those meanings. Unless you act with both awareness and assent, your act is merely a habit (if it is unique to you) or a custom (if you share it with others).</span></span></i></p></blockquote><p><span class=\"by_r38pkCm7wF4M44MDQ\">Two key questions relating to ritual and rationality are:</span></p><ul><li><span class=\"by_r38pkCm7wF4M44MDQ\">How can we capture the value of ritual, without incurring epistemic risk?</span></li><li><span class=\"by_r38pkCm7wF4M44MDQ\">Can rituals be actively helpful for rationality?</span></li></ul><p><strong id=\"Sequences_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Sequences:</span></strong></p><ul><li><a href=\"https://www.lesswrong.com/s/3bbvzoRA8n6ZgbiyK\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Rational Ritual</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by </span><a href=\"https://www.lesswrong.com/users/raemon\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Raemon</span></a></li></ul><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/secular-solstice\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Secular Solstice</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/petrov-day\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Petrov Day</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/grieving\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Grieving</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/marriage\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Marriage</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/religion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Religion</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/art\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Art</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/music\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Music</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/poetry\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Poetry</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/meditation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Meditation</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/circling\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Circling</span></a></p>",
      "sections": [
        {
          "title": "Sequences:",
          "anchor": "Sequences_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 67,
    "description": {
      "markdown": "**Rituals** are symbolic actions. In the context of LessWrong, it's significant that many rituals have some impact on your cognition, which makes them appropriate to be careful with. Nonetheless, some LessWrongers have worked to explore the space of ritual through a rationalist lens.\n\nIt's a bit tricky to define. The book [Secular Wholeness](https://www.amazon.com/Secular-Wholeness-Skeptics-Paths-Richer/dp/155369175X) notes:\n\n> *There’s a hazy boundary between the words “ritual,” “habit,” and “custom.” I think the difference between a ritual act and a habitual one lies in awareness and assent. An act becomes a ritual for you when you perform it with conscious awareness of its symbolic and emotional meaning, and with willing assent to those meanings. Unless you act with both awareness and assent, your act is merely a habit (if it is unique to you) or a custom (if you share it with others).*\n\nTwo key questions relating to ritual and rationality are:\n\n*   How can we capture the value of ritual, without incurring epistemic risk?\n*   Can rituals be actively helpful for rationality?\n\n**Sequences:**\n\n*   [Rational Ritual](https://www.lesswrong.com/s/3bbvzoRA8n6ZgbiyK) by [Raemon](https://www.lesswrong.com/users/raemon)\n\n**Related Pages:** [Secular Solstice](https://www.lesswrong.com/tag/secular-solstice), [Petrov Day](https://www.lesswrong.com/tag/petrov-day), [Grieving](https://www.lesswrong.com/tag/grieving), [Marriage](https://www.lesswrong.com/tag/marriage), [Religion](https://www.lesswrong.com/tag/religion), [Art](https://www.lesswrong.com/tag/art), [Music](https://www.lesswrong.com/tag/music), [Poetry](https://www.lesswrong.com/tag/poetry), [Meditation](https://www.lesswrong.com/tag/meditation), [Circling](https://www.lesswrong.com/tag/circling)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "AodfCFefLAuwDyj7Z",
    "name": "Self Experimentation",
    "core": false,
    "slug": "self-experimentation",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 52,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZnHkaTkxukegSrZqE",
    "name": "Cryonics",
    "core": null,
    "slug": "cryonics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nmk3nLpQE89dMRzzN\">Cryonics</span></strong><span><span class=\"by_nmk3nLpQE89dMRzzN\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the practice of preserving people who are dying in liquid nitrogen soon after their heart stops. The idea is that most of your brain's information content is still intact right after you've \"died\", i.e. medical death or legal death. If humans invent molecular nanotechnology or</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">brain</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">emulation techniques,</span><span class=\"by_mcKSiwq2TBrTMZS6X\"> it may be possible </span><span class=\"by_qgdGA4ZEyW7zNdK84\">to reconstruct the consciousness of cryopreserved patients.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span><span class=\"by_r38pkCm7wF4M44MDQ\"> </span></span><a href=\"https://www.lessestwrong.com/tag/life-extension\"><span class=\"by_r38pkCm7wF4M44MDQ\">Life Extension</span></a><span><span class=\"by_r38pkCm7wF4M44MDQ\">, a more general tag about ways to avoid </span><span class=\"by_qgdGA4ZEyW7zNdK84\">death.</span></span></p><h2 id=\"Cryonics_associated_issues_commonly_raised_on_LessWrong\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cryonics-associated issues commonly raised on LessWrong</span></h2><p><strong id=\"Pro_cryonics_points\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pro-cryonics points</span></strong></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Advanced reductionism/physicalism (because of the issues associated with </span><a href=\"https://lessestwrong.com/tag/personal-identity\"><span class=\"by_qgdGA4ZEyW7zNdK84\">identifying a person</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> with continuity of brain information).</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Whether an extended healthy lifespan is worthwhile (relates to </span><a href=\"https://lessestwrong.com/tag/fun-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fun Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, religious rationalizations for 70-year lifespans, \"sour grapes\" rationalizations for why death is actually a good thing).</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The \"</span><a href=\"https://lessestwrong.com/tag/shut-up-and-multiply\"><span class=\"by_qgdGA4ZEyW7zNdK84\">shut up and multiply</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">\" aspect of spending $300/year (as Eliezer Yudkowsky quotes his costs for Cryonics Institute membership ($125/year) plus term life insurance ($180/year)) for a probability (how large being widely disputed) of obtaining many more years of lifespan. For this reason, cryonics advocates regard it as an </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">extreme case</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> of failure at rationality - a low-hanging fruit by which millions of deaths per year could be prevented at low cost.</span></li></ul><p><strong id=\"Anti_cryonics_points\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Anti-cryonics points</span></strong></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Cognitive biases contributing to emotional prejudice in favor of cryonics (optimistic bias, motivated cognition).</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><a href=\"https://lessestwrong.com/tag/conjunction-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">multiply chained nature</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> of the probabilities involved in cryonics, and whether the final expected utility is worth the cost.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Money spent on cryonics could, arguably, be better spent on </span><a href=\"https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">efficient charity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li></ul><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2008/12/we-agree-get-froze.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">We Agree: Get Froze</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. \"My co-blogger Eliezer and I may disagree on AI fooms, but we agree on something quite contrarian and, we think, huge: More likely than not, most folks who die today didn't have to die! ... It seems far more people read this blog daily than have ever signed up for cryonics. While it is hard to justify most medical procedures using standard health economics calculations, such calculations say that at today's prices cryonics seems a good deal even if you think there's only a 5% chance it'll work.\"</span></li><li><a href=\"https://lessestwrong.com/lw/wq/you_only_live_twice/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">You Only Live Twice</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. \"My co-blogger Robin and I may disagree on how fast an AI can improve itself, but we agree on an issue that seems much simpler to us than that: At the point where the current legal and medical system gives up on a patient, they aren't really dead.\"</span></li><li><a href=\"https://lessestwrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Pascal's Wager Fallacy Fallacy</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - the fallacy of Pascal's Wager combines a high payoff with a </span><a href=\"https://lessestwrong.com/tag/privileging-the-hypothesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">privileged hypothesis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, one with low prior probability and no particular reason to believe it. Perceptually seeing an instance of \"Pascal's Wager\" </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">just</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> from the high payoff, even when the probability is not small, is the Pascal's Wager Fallacy Fallacy.</span></li><li><a href=\"https://lessestwrong.com/lw/1mc/normal_cryonics/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Normal Cryonics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - On the shift of perspective that came from attending a gathering of normal-seeming young cryonicists.</span></li><li><a href=\"https://lessestwrong.com/lw/1mh/that_magical_click/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">That Magical Click</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - What is the unexplained process whereby some people get cryonics, or other frequently-derailed chains of thought, in a very short time?</span></li><li><a href=\"https://lessestwrong.com/lw/r9/quantum_mechanics_and_personal_identity/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Quantum Mechanics and Personal Identity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. A shortened index into the </span><a href=\"http://www.overcomingbias.com/2008/06/the-quantum-phy.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Quantum Physics Sequence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> describing only the prerequisite knowledge to understand the statement that \"science can rule out a notion of personal identity that depends on your being composed of the same atoms - because modern physics has taken the concept of 'same atom' and thrown it out the window. There </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">are</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> no little billiard balls with individual identities. It's experimentally ruled out.\" The key post in this sequence is </span><a href=\"http://www.overcomingbias.com/2008/06/timeless-identi.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Timeless Identity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, in which \"Having used physics to completely trash all naive theories of identity, we reassemble a conception of persons and experiences from what is left\" but this finale might make little sense without the prior discussion.</span></li><li><a href=\"http://www.overcomingbias.com/2009/03/break-cryonics-down.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Break Cryonics Down</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - tries to identify some of the chained probabilities involved in cryonics.</span></li><li><a href=\"https://lessestwrong.com/lw/hv/third_alternatives_for_afterlifeism/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Third Alternatives for Afterlife-ism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - explains why cryonics is a </span><a href=\"https://lessestwrong.com/tag/third-option\"><span class=\"by_qgdGA4ZEyW7zNdK84\">third option</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> in the dilemma about whether we should tell </span><a href=\"https://wiki.lesswrong.com/wiki/noble_lie\"><span class=\"by_qgdGA4ZEyW7zNdK84\">noble lies</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> about an afterlife, to prevent people from getting depressed by not believing in an afterlife.</span></li><li><a href=\"https://lessestwrong.com/lw/1r0/a_survey_of_anticryonics_writing/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A survey of anti-cryonics writing</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/ciphergoth\"><span class=\"by_qgdGA4ZEyW7zNdK84\">ciphergoth</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - an attempt to find quality criticism of cryonics, with a surprising result that \"there is not one person who has ever taken the time to read and understand cryonics claims in any detail, still considers it pseudoscience, and has written a paper, article or even a blog post to rebut anything that cryonics advocates actually say\".</span></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://waitbutwhy.com/2016/03/cryonics.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why Croynics Makes Sense, WaitButWhy</span></a></li><li><a href=\"http://www.benbest.com/cryonics/CryoFAQ.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cryonics Institute FAQ</span></a></li><li><a href=\"http://www.alcor.org/FAQs/index.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Alcor Life Extension Foundation FAQ</span></a></li><li><a href=\"http://www.alcor.org/sciencefaq.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Alcor FAQ for scientists</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/exploratory-engineering\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Exploratory engineering</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/absurdity-heuristic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Absurdity heuristic</span></a></li><li><a href=\"https://lessestwrong.com/tag/status-quo-bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Status quo bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/reversal-test\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Reversal test</span></a></li><li><a href=\"https://lessestwrong.com/tag/signaling\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Signaling</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/near-far-thinking\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Near/far thinking</span></a></li><li><a href=\"https://lessestwrong.com/tag/death\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Death</span></a></li></ul>",
      "sections": [
        {
          "title": "Cryonics-associated issues commonly raised on LessWrong",
          "anchor": "Cryonics_associated_issues_commonly_raised_on_LessWrong",
          "level": 1
        },
        {
          "title": "Pro-cryonics points",
          "anchor": "Pro_cryonics_points",
          "level": 2
        },
        {
          "title": "Anti-cryonics points",
          "anchor": "Anti_cryonics_points",
          "level": 2
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 118,
    "description": {
      "markdown": "**Cryonics** is the practice of preserving people who are dying in liquid nitrogen soon after their heart stops. The idea is that most of your brain's information content is still intact right after you've \"died\", i.e. medical death or legal death. If humans invent molecular nanotechnology or brain emulation techniques, it may be possible to reconstruct the consciousness of cryopreserved patients.\n\n*Related*: [Life Extension](https://www.lessestwrong.com/tag/life-extension), a more general tag about ways to avoid death.\n\nCryonics-associated issues commonly raised on LessWrong\n-------------------------------------------------------\n\n**Pro-cryonics points**\n\n*   Advanced reductionism/physicalism (because of the issues associated with [identifying a person](https://lessestwrong.com/tag/personal-identity) with continuity of brain information).\n*   Whether an extended healthy lifespan is worthwhile (relates to [Fun Theory](https://lessestwrong.com/tag/fun-theory), religious rationalizations for 70-year lifespans, \"sour grapes\" rationalizations for why death is actually a good thing).\n*   The \"[shut up and multiply](https://lessestwrong.com/tag/shut-up-and-multiply)\" aspect of spending $300/year (as Eliezer Yudkowsky quotes his costs for Cryonics Institute membership ($125/year) plus term life insurance ($180/year)) for a probability (how large being widely disputed) of obtaining many more years of lifespan. For this reason, cryonics advocates regard it as an *extreme case* of failure at rationality - a low-hanging fruit by which millions of deaths per year could be prevented at low cost.\n\n**Anti-cryonics points**\n\n*   Cognitive biases contributing to emotional prejudice in favor of cryonics (optimistic bias, motivated cognition).\n*   The [multiply chained nature](https://lessestwrong.com/tag/conjunction-fallacy) of the probabilities involved in cryonics, and whether the final expected utility is worth the cost.\n*   Money spent on cryonics could, arguably, be better spent on [efficient charity](https://lessestwrong.com/lw/3gj/efficient_charity_do_unto_others/).\n\nNotable Posts\n-------------\n\n*   [We Agree: Get Froze](http://www.overcomingbias.com/2008/12/we-agree-get-froze.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson). \"My co-blogger Eliezer and I may disagree on AI fooms, but we agree on something quite contrarian and, we think, huge: More likely than not, most folks who die today didn't have to die! ... It seems far more people read this blog daily than have ever signed up for cryonics. While it is hard to justify most medical procedures using standard health economics calculations, such calculations say that at today's prices cryonics seems a good deal even if you think there's only a 5% chance it'll work.\"\n*   [You Only Live Twice](https://lessestwrong.com/lw/wq/you_only_live_twice/) by [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky). \"My co-blogger Robin and I may disagree on how fast an AI can improve itself, but we agree on an issue that seems much simpler to us than that: At the point where the current legal and medical system gives up on a patient, they aren't really dead.\"\n*   [The Pascal's Wager Fallacy Fallacy](https://lessestwrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/) \\- the fallacy of Pascal's Wager combines a high payoff with a [privileged hypothesis](https://lessestwrong.com/tag/privileging-the-hypothesis), one with low prior probability and no particular reason to believe it. Perceptually seeing an instance of \"Pascal's Wager\" *just* from the high payoff, even when the probability is not small, is the Pascal's Wager Fallacy Fallacy.\n*   [Normal Cryonics](https://lessestwrong.com/lw/1mc/normal_cryonics/) \\- On the shift of perspective that came from attending a gathering of normal-seeming young cryonicists.\n*   [That Magical Click](https://lessestwrong.com/lw/1mh/that_magical_click/) \\- What is the unexplained process whereby some people get cryonics, or other frequently-derailed chains of thought, in a very short time?\n*   [Quantum Mechanics and Personal Identity](https://lessestwrong.com/lw/r9/quantum_mechanics_and_personal_identity/) by [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky). A shortened index into the [Quantum Physics Sequence](http://www.overcomingbias.com/2008/06/the-quantum-phy.html) describing only the prerequisite knowledge to understand the statement that \"science can rule out a notion of personal identity that depends on your being composed of the same atoms - because modern physics has taken the concept of 'same atom' and thrown it out the window. There *are* no little billiard balls with individual identities. It's experimentally ruled out.\" The key post in this sequence is [Timeless Identity](http://www.overcomingbias.com/2008/06/timeless-identi.html), in which \"Having used physics to completely trash all naive theories of identity, we reassemble a conception of persons and experiences from what is left\" but this finale might make little sense without the prior discussion.\n*   [Break Cryonics Down](http://www.overcomingbias.com/2009/03/break-cryonics-down.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson) \\- tries to identify some of the chained probabilities involved in cryonics.\n*   [Third Alternatives for Afterlife-ism](https://lessestwrong.com/lw/hv/third_alternatives_for_afterlifeism/) by [Eliezer Yudkowsky](https://lessestwrong.com/tag/eliezer-yudkowsky) \\- explains why cryonics is a [third option](https://lessestwrong.com/tag/third-option) in the dilemma about whether we should tell [noble lies](https://wiki.lesswrong.com/wiki/noble_lie) about an afterlife, to prevent people from getting depressed by not believing in an afterlife.\n*   [A survey of anti-cryonics writing](https://lessestwrong.com/lw/1r0/a_survey_of_anticryonics_writing/) by [ciphergoth](https://lessestwrong.com/tag/ciphergoth) \\- an attempt to find quality criticism of cryonics, with a surprising result that \"there is not one person who has ever taken the time to read and understand cryonics claims in any detail, still considers it pseudoscience, and has written a paper, article or even a blog post to rebut anything that cryonics advocates actually say\".\n\nExternal links\n--------------\n\n*   [Why Croynics Makes Sense, WaitButWhy](http://waitbutwhy.com/2016/03/cryonics.html)\n*   [Cryonics Institute FAQ](http://www.benbest.com/cryonics/CryoFAQ.html)\n*   [Alcor Life Extension Foundation FAQ](http://www.alcor.org/FAQs/index.html)\n*   [Alcor FAQ for scientists](http://www.alcor.org/sciencefaq.htm)\n\nSee also\n--------\n\n*   [Exploratory engineering](https://lessestwrong.com/tag/exploratory-engineering), [Absurdity heuristic](https://lessestwrong.com/tag/absurdity-heuristic)\n*   [Status quo bias](https://lessestwrong.com/tag/status-quo-bias), [Reversal test](https://lessestwrong.com/tag/reversal-test)\n*   [Signaling](https://lessestwrong.com/tag/signaling), [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Death](https://lessestwrong.com/tag/death)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GpcY5Q226TTy4Cv8N",
    "name": "Decoupling vs Contextualizing",
    "core": false,
    "slug": "decoupling-vs-contextualizing",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ogWsaHQKwa6ddidRC",
    "name": "Conflict vs Mistake",
    "core": false,
    "slug": "conflict-vs-mistake",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Conflict vs Mistake</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a framework for analyzing disagreements about policy.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Mistake theorists think problems in society are caused by people being bad at achieving common goals. Conflict theorists think problems in society are caused by adversaries with incompatible goals.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Scott Alexander </span><a href=\"https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/\"><span class=\"by_HoGziwmhpMGqGeWZy\">attributed</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> the conflict vs mistake framework to </span><a href=\"https://www.reddit.com/r/slatestarcodex/comments/74vpwm/socialism_communism_and_marxism_pt_1_on_trust_and/\"><span class=\"by_HoGziwmhpMGqGeWZy\">a post on reddit by user no_bear_so_low</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">A </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">conflict theorist</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> thinks problems are primarily due to the conflicting interests of different players. If someone is suffering, someone else must be making money off of it. Karl Marx was a conflict theorist; he blamed the ills of society on class conflict.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">A </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">mistake theorist</span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> thinks problems are primarily due to mistakes. If only we knew how to run society better, there would be less problems. Jeremy Bentham was more of a mistake theorist: he thought producing a formula by which we could calculate the quality of social interventions would help improve society.</span></p><p><a href=\"https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Humans are not automatically strategic</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is a mistake theory of human (ir)rationality. Things are hard. If people are doing something dumb, it's probably because they don't know better.</span></p><p><a href=\"https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">The Elephant in the Brain</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is more like a conflict theory of human (ir)rationality. Apparent irrationality is attributed mainly to humans not actually wanting what they think they want. </span></p><p><strong><a href=\"https://en.m.wikipedia.org/wiki/Hanlon%27s_razor\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Hanlon's Razor</span></a></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\"> says: </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">Never attribute to malice what is adequately explained by stupidity.</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\"> This is a clear bias toward mistake theory.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">On the other hand, economics, evolutionary psychology, and some other fields are based on </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">rational choice theory</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\">, IE, an assumption that behavior can be explained by rational decision-making. </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">(Economic rationality assumes that individuals choose rationally to maximize economic value, based on the incentives of the current situation. Evolutionary psychology instead assumes that human and animal behaviors will be optimal solutions to the problems they faced in evolutionary history. Bruce Bueno de Mesquita assumes that politicians act rationally so as to maximize their tenure in positions of power. The ACT-R theory of cognition assumes that individual cognitive mechanisms are designed to optimally perform their individual cognitive tasks, such as retrieving memories which are useful in expectation, even if the whole brain is not perfectly rational.)</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\"> This assumption of rationality lends itself more naturally to conflict theories.</span></p><h2 id=\"Game_Theoretic_Connections\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Game-Theoretic Connections</span></h2><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In game theory, assuming that people can make mistakes (a so-called </span><a href=\"https://en.m.wikipedia.org/wiki/Trembling_hand_perfect_equilibrium\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">trembling hand</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">) can complicate cooperative strategies. </span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">For example, in iterated </span><a href=\"https://www.lesswrong.com/tag/prisoner-s-dilemma\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">prisoner's dilemma</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">tit for tat </span></strong><span class=\"by_Q7NW4XaWQmfPfdcFj\">is a cooperative equilibrium (that is to say, it is pareto-optimal, and it is a Nash equilibrium). The tit-for-tat strategy is: cooperate on the first round; then, copy the other person's move from the previous round. This enforces cooperation, because if I defect, I expect my partner to defect on the next round (which is bad for me). This is effectively eye-for-an-eye morality.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">However, if people make mistakes (the trembling-hand assumption), then tit-for-tat only results in cooperation for an initial period before anyone makes a mistake. If both mistakes are equally probable, then in the long run we'll average only 50% cooperation. We can see this as an interminable family feud where both sides see the other as having done more wrong. \"An eye for an eye makes everyone blind.\"</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">We need to recognize that people make mistakes sometimes -- we can't punish everything eye-for-an-eye.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">Therefore, some form of </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">forgiving</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\"> tit-for-tat does better. For example, copy cooperation 100% of the time, but copy defection 90% of the time. This can still work to enforce rational cooperation (depending on the exact payouts and time-discounting of the players), but without everlasting feuds. See also </span><a href=\"https://www.lesswrong.com/posts/2meuc3kPRkBcRpj3R/contrite-strategies-and-the-need-for-standards\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Contrite Strategies and the Need for Standards</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">In this framing, a conflict theorist thinks people are actually defecting on purpose. They </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">know what they're doing</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\">, and therefore, </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">would respond to incentives.</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\"> Punishing them is prosocial and helps to encourage more cooperation overall.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">A mistake theorist thinks people </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">are defecting accidentally, </span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\">and therefore, </span><em><span class=\"by_Q7NW4XaWQmfPfdcFj\">would not respond to incentives</span></em><span class=\"by_Q7NW4XaWQmfPfdcFj\">. Punishing them is pointless and counterproductive; it could even result in a continuing feud, making things much worse for everyone.</span></p>",
      "sections": [
        {
          "title": "Game-Theoretic Connections",
          "anchor": "Game_Theoretic_Connections",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 17,
    "description": {
      "markdown": "**Conflict vs Mistake** is a framework for analyzing disagreements about policy.\n\nMistake theorists think problems in society are caused by people being bad at achieving common goals. Conflict theorists think problems in society are caused by adversaries with incompatible goals.\n\nScott Alexander [attributed](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/) the conflict vs mistake framework to [a post on reddit by user no\\_bear\\_so_low](https://www.reddit.com/r/slatestarcodex/comments/74vpwm/socialism_communism_and_marxism_pt_1_on_trust_and/).\n\nA **conflict theorist** thinks problems are primarily due to the conflicting interests of different players. If someone is suffering, someone else must be making money off of it. Karl Marx was a conflict theorist; he blamed the ills of society on class conflict.\n\nA **mistake theorist** thinks problems are primarily due to mistakes. If only we knew how to run society better, there would be less problems. Jeremy Bentham was more of a mistake theorist: he thought producing a formula by which we could calculate the quality of social interventions would help improve society.\n\n[Humans are not automatically strategic](https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic) is a mistake theory of human (ir)rationality. Things are hard. If people are doing something dumb, it's probably because they don't know better.\n\n[The Elephant in the Brain](https://www.lesswrong.com/posts/BgBrXpByCSmCLjpwr/book-review-the-elephant-in-the-brain) is more like a conflict theory of human (ir)rationality. Apparent irrationality is attributed mainly to humans not actually wanting what they think they want.\n\n**[Hanlon's Razor](https://en.m.wikipedia.org/wiki/Hanlon%27s_razor)** says: _Never attribute to malice what is adequately explained by stupidity._ This is a clear bias toward mistake theory.\n\nOn the other hand, economics, evolutionary psychology, and some other fields are based on _rational choice theory_, IE, an assumption that behavior can be explained by rational decision-making. _(Economic rationality assumes that individuals choose rationally to maximize economic value, based on the incentives of the current situation. Evolutionary psychology instead assumes that human and animal behaviors will be optimal solutions to the problems they faced in evolutionary history. Bruce Bueno de Mesquita assumes that politicians act rationally so as to maximize their tenure in positions of power. The ACT-R theory of cognition assumes that individual cognitive mechanisms are designed to optimally perform their individual cognitive tasks, such as retrieving memories which are useful in expectation, even if the whole brain is not perfectly rational.)_ This assumption of rationality lends itself more naturally to conflict theories.\n\nGame-Theoretic Connections\n--------------------------\n\nIn game theory, assuming that people can make mistakes (a so-called [trembling hand](https://en.m.wikipedia.org/wiki/Trembling_hand_perfect_equilibrium)) can complicate cooperative strategies.\n\nFor example, in iterated [prisoner's dilemma](https://www.lesswrong.com/tag/prisoner-s-dilemma), **tit for tat** is a cooperative equilibrium (that is to say, it is pareto-optimal, and it is a Nash equilibrium). The tit-for-tat strategy is: cooperate on the first round; then, copy the other person's move from the previous round. This enforces cooperation, because if I defect, I expect my partner to defect on the next round (which is bad for me). This is effectively eye-for-an-eye morality.\n\nHowever, if people make mistakes (the trembling-hand assumption), then tit-for-tat only results in cooperation for an initial period before anyone makes a mistake. If both mistakes are equally probable, then in the long run we'll average only 50% cooperation. We can see this as an interminable family feud where both sides see the other as having done more wrong. \"An eye for an eye makes everyone blind.\"\n\nWe need to recognize that people make mistakes sometimes -- we can't punish everything eye-for-an-eye.\n\nTherefore, some form of _forgiving_ tit-for-tat does better. For example, copy cooperation 100% of the time, but copy defection 90% of the time. This can still work to enforce rational cooperation (depending on the exact payouts and time-discounting of the players), but without everlasting feuds. See also [Contrite Strategies and the Need for Standards](https://www.lesswrong.com/posts/2meuc3kPRkBcRpj3R/contrite-strategies-and-the-need-for-standards).\n\nIn this framing, a conflict theorist thinks people are actually defecting on purpose. They _know what they're doing_, and therefore, _would respond to incentives._ Punishing them is prosocial and helps to encourage more cooperation overall.\n\nA mistake theorist thinks people _are defecting accidentally,_ and therefore, _would not respond to incentives_. Punishing them is pointless and counterproductive; it could even result in a continuing feud, making things much worse for everyone."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BtQRRKTPxagBH6KrG",
    "name": "Blues & Greens (metaphor)",
    "core": false,
    "slug": "blues-and-greens-metaphor",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Blues</span><span class=\"by_9c2mQkLQq6gQSksMs\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Greens\"</span></span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a term</span><span class=\"by_9c2mQkLQq6gQSksMs\"> used to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">metaphorically </span><span class=\"by_9c2mQkLQq6gQSksMs\">refer to opposing political factions.</span></span><br><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"/tag/tribalism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Tribalism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/mind-killer\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind-killer</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/arguments-as-soldiers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Arguments as soldiers</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/false-dilemma\"><span class=\"by_qgdGA4ZEyW7zNdK84\">False dilemma</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/in-group-bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">In-group bias</span></a></p><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">The </span><span class=\"by_qgdGA4ZEyW7zNdK84\">term</span><span class=\"by_qf77EiaoMw7tH3GSr\"> come from the names of chariot racing teams, that differed in nothing but the team colors, but</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> rivalry of whose fans sometimes reached the level of gang wars.</span></span><a href=\"https://www.lesswrong.com/tag/blues-and-greens-metaphor?revision=0.0.21&amp;lw_source=import_sheet#fn1\"><sup><span class=\"by_qf77EiaoMw7tH3GSr\">1</span></sup></a><span class=\"by_ChXHsXmDQFWZH638i\"> By definition, politics also deals with matters that people physically fight over in the real world -- or at least, matters that are to be enforced by the government's monopoly on violence.</span></p><p><span class=\"by_ChXHsXmDQFWZH638i\">Politics commonly involves an </span><a href=\"https://wiki.lesswrong.com/wiki/adversarial_process\"><span class=\"by_ChXHsXmDQFWZH638i\">adversarial process</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\">, where</span><span class=\"by_qf77EiaoMw7tH3GSr\"> factions usually identify with political positions, and use </span></span><a href=\"https://www.lesswrong.com/tag/arguments-as-soldiers\"><span class=\"by_qf77EiaoMw7tH3GSr\">arguments as soldiers</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> to defend their side. </span><span class=\"by_ChXHsXmDQFWZH638i\">When tempered by appropriate </span></span><a href=\"https://www.lesswrong.com/tag/standard-of-evidence\"><span class=\"by_ChXHsXmDQFWZH638i\">standards of evidence</span></a><span class=\"by_ChXHsXmDQFWZH638i\">, </span><a href=\"https://wiki.lesswrong.com/wiki/rules_of_order\"><span class=\"by_ChXHsXmDQFWZH638i\">rules of order</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\"> and other safeguards, such a process may be the only way of introducing a modicum of deliberative </span><span class=\"by_qgdGA4ZEyW7zNdK84\">truth-</span><span class=\"by_ChXHsXmDQFWZH638i\">seeking and other </span></span><a href=\"https://www.lesswrong.com/tag/virtues-of-rationality\"><span class=\"by_ChXHsXmDQFWZH638i\">virtues of rationality</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\"> into an inherently violent domain. </span><span class=\"by_qf77EiaoMw7tH3GSr\">However, </span><span class=\"by_ChXHsXmDQFWZH638i\">the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> dichotomies</span><span class=\"by_ChXHsXmDQFWZH638i\"> presented by the opposing sides</span><span class=\"by_qf77EiaoMw7tH3GSr\"> are often </span></span><a href=\"https://www.lesswrong.com/tag/false-dilemma\"><span class=\"by_qf77EiaoMw7tH3GSr\">false dilemmas</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, which can be shown by presenting </span><a href=\"https://www.lesswrong.com/tag/third-option\"><span class=\"by_qf77EiaoMw7tH3GSr\">third options</span></a><span class=\"by_ChXHsXmDQFWZH638i\">.</span></p><p><span><span class=\"by_ChXHsXmDQFWZH638i\">For a </span><span class=\"by_Q2oaNonArzibx5cQN\">variety</span><span class=\"by_ChXHsXmDQFWZH638i\"> of reasons, Less Wrong tries to avoid political disputes: see </span></span><a href=\"https://www.lesswrong.com/tag/mind-killer\"><span class=\"by_ChXHsXmDQFWZH638i\">Mind-killer</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blog posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/lt/the_robbers_cave_experiment/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Robbers Cave Experiment</span></a></li><li><a href=\"https://www.lesswrong.com/lw/mg/the_twoparty_swindle/\"><span class=\"by_qf77EiaoMw7tH3GSr\">The Two-Party Swindle</span></a></li><li><a href=\"https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/\"><span class=\"by_9c2mQkLQq6gQSksMs\">A Fable of Science and Politics</span></a></li><li><a href=\"https://www.lesswrong.com/lw/h2/blue_or_green_on_regulation/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Blue or Green on Regulation?</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_9c2mQkLQq6gQSksMs\">-</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://wiki.lesswrong.com/wiki/Burch's_law\"><span class=\"by_qf77EiaoMw7tH3GSr\">Burch's law</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> isn't a </span><a href=\"https://www.lesswrong.com/tag/scales-of-justice-fallacy\"><span class=\"by_qf77EiaoMw7tH3GSr\">soldier-argument</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> for regulation; estimating the appropriate level of regulation in each particular case is a superior </span><a href=\"https://www.lesswrong.com/tag/third-option\"><span class=\"by_qf77EiaoMw7tH3GSr\">third option</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></li></ul><h2 id=\"Footnotes\"><span class=\"by_9c2mQkLQq6gQSksMs\">Footnotes</span></h2><ol><li><a href=\"https://en.wikipedia.org/wiki/Chariot_racing#Byzantine_era\"><span class=\"by_qf77EiaoMw7tH3GSr\">Wikipedia:Chariot racing#Byzantine era</span></a><a href=\"https://www.lesswrong.com/tag/blues-and-greens-metaphor?revision=0.0.21&amp;lw_source=import_sheet#fnref1\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">↩</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></span></a></li></ol>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "Footnotes",
          "anchor": "Footnotes",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 13,
    "description": {
      "markdown": "**\"Blues and Greens\"** is a term used to metaphorically refer to opposing political factions.  \n  \n*See also*: [Tribalism](/tag/tribalism), [Mind-killer](https://www.lesswrong.com/tag/mind-killer), [Arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers), [False dilemma](https://www.lesswrong.com/tag/false-dilemma), [In-group bias](https://www.lesswrong.com/tag/in-group-bias)\n\nThe term come from the names of chariot racing teams, that differed in nothing but the team colors, but the rivalry of whose fans sometimes reached the level of gang wars.[^1^](https://www.lesswrong.com/tag/blues-and-greens-metaphor?revision=0.0.21&lw_source=import_sheet#fn1) By definition, politics also deals with matters that people physically fight over in the real world -- or at least, matters that are to be enforced by the government's monopoly on violence.\n\nPolitics commonly involves an [adversarial process](https://wiki.lesswrong.com/wiki/adversarial_process), where factions usually identify with political positions, and use [arguments as soldiers](https://www.lesswrong.com/tag/arguments-as-soldiers) to defend their side. When tempered by appropriate [standards of evidence](https://www.lesswrong.com/tag/standard-of-evidence), [rules of order](https://wiki.lesswrong.com/wiki/rules_of_order) and other safeguards, such a process may be the only way of introducing a modicum of deliberative truth-seeking and other [virtues of rationality](https://www.lesswrong.com/tag/virtues-of-rationality) into an inherently violent domain. However, the dichotomies presented by the opposing sides are often [false dilemmas](https://www.lesswrong.com/tag/false-dilemma), which can be shown by presenting [third options](https://www.lesswrong.com/tag/third-option).\n\nFor a variety of reasons, Less Wrong tries to avoid political disputes: see [Mind-killer](https://www.lesswrong.com/tag/mind-killer).\n\nBlog posts\n----------\n\n*   [The Robbers Cave Experiment](https://www.lesswrong.com/lw/lt/the_robbers_cave_experiment/)\n*   [The Two-Party Swindle](https://www.lesswrong.com/lw/mg/the_twoparty_swindle/)\n*   [A Fable of Science and Politics](https://www.lesswrong.com/lw/gt/a_fable_of_science_and_politics/)\n*   [Blue or Green on Regulation?](https://www.lesswrong.com/lw/h2/blue_or_green_on_regulation/) \\- [Burch's law](https://wiki.lesswrong.com/wiki/Burch's_law) isn't a [soldier-argument](https://www.lesswrong.com/tag/scales-of-justice-fallacy) for regulation; estimating the appropriate level of regulation in each particular case is a superior [third option](https://www.lesswrong.com/tag/third-option).\n\nFootnotes\n---------\n\n1.  [Wikipedia:Chariot racing#Byzantine era](https://en.wikipedia.org/wiki/Chariot_racing#Byzantine_era)[↩ ](https://www.lesswrong.com/tag/blues-and-greens-metaphor?revision=0.0.21&lw_source=import_sheet#fnref1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q6P8jLn8hH7kbuXRr",
    "name": "Signaling",
    "core": null,
    "slug": "signaling",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_9c2mQkLQq6gQSksMs\">Signaling</span></strong><span class=\"by_9c2mQkLQq6gQSksMs\"> is </span><a href=\"https://lessestwrong.com/lw/did/what_is_signaling_really/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">defined</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Yvain</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> as \"a method of conveying information among not-necessarily-trustworthy parties by performing an action which</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">more likely or less costly if the information is true than if it is not true\". Some signaling is performed exclusively </span><span class=\"by_HoGziwmhpMGqGeWZy\">to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">impress</span><span class=\"by_HoGziwmhpMGqGeWZy\"> others </span><span class=\"by_qgdGA4ZEyW7zNdK84\">(to improve your </span></span><a href=\"https://lessestwrong.com/tag/social-status\"><span class=\"by_qgdGA4ZEyW7zNdK84\">status</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">), and in</span><span class=\"by_HoGziwmhpMGqGeWZy\"> some </span><span class=\"by_qgdGA4ZEyW7zNdK84\">cases </span></span><a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">isn't even worth that</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. In other cases, signaling is a side-effect of an otherwise useful activity.</span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">For example, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">if doing something is easy for one type of person and hard for another type of person, you might do that thing just to get people to think you're the former type of person, even if the thing isn't in itself worth doing. This could explain many facets of human behavior, and reveal opportunities for reducing waste.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Not all signaling is about abilities. Signaling can also be about personality, current emotional state, beliefs, loyalty to </span><span class=\"by_HoGziwmhpMGqGeWZy\">a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">particular group, status within a group, etc.</span></span></p><p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Countersignaling</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is signaling</span><span class=\"by_HoGziwmhpMGqGeWZy\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a naive observer might take to mean that one</span><span class=\"by_73WJbnX59kE4afuuY\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">opposite</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of X, when in fact, one is X, used as a means to signal that one is, in fact, X. For example, aristocrats (\"old money\") may </span><span class=\"by_t7fcYsg2WsKYKT6ix\">forgo</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> gaudy bling in order to signal that</span><span class=\"by_HoGziwmhpMGqGeWZy\"> they are </span><span class=\"by_qgdGA4ZEyW7zNdK84\">not </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">nouveau riche</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> (new money), which may lead some people</span><span class=\"by_HoGziwmhpMGqGeWZy\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">incorrectly assume that</span><span class=\"by_AbHRQKDfSbarpLsqT\"> they are </span><span class=\"by_qgdGA4ZEyW7zNdK84\">not rich.</span></span></p><h2 id=\"Blog_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></p><ul><li><a href=\"http://www.overcomingbias.com/2006/12/do_helping_prof.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Do Helping Professions Help More?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"http://www.overcomingbias.com/2006/12/gifts_hurt.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Gifts Hurt</span></a></li><li><a href=\"http://www.overcomingbias.com/2007/01/excess_signalin.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Excess Signaling Example</span></a></li><li><a href=\"http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A Tale Of Two Tradeoffs</span></a></li><li><a href=\"http://www.overcomingbias.com/2009/06/why-signals-are-shallow.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why Signals Are Shallow</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> - \"We all want</span><span class=\"by_AbHRQKDfSbarpLsqT\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">affiliate with high status people, but since status is about common distant perceptions of quality, we often care more about what distant observers would think about our associates than about how we privately evaluate them.\"</span></span></li><li><a href=\"http://www.overcomingbias.com/2009/06/signals-are-forever.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Signals Are Forever</span></a></li><li><a href=\"https://lessestwrong.com/lw/g7/least_signaling_activities/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Least Signaling Activities?</span></a></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">by others</span></p><ul><li><a href=\"https://lessestwrong.com/lw/did/what_is_signaling_really/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">What Is Signaling, Really?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Yvain\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Yvain</span></a></li><li><a href=\"https://lessestwrong.com/lw/1y3/think_before_you_speak_and_signal_it/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Think Before You Speak (And Signal It)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"http://weidai.com/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wei Dai</span></a></li><li><a href=\"https://lessestwrong.com/lw/b2/declare_your_signaling_and_hidden_agendas/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Declare Your Signaling and Hidden Agendas</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Kaj_Sotala\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Kaj Sotala</span></a></li><li><a href=\"https://lessestwrong.com/lw/8ev/modularity_signaling_and_belief_in_belief/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Modularity, Signaling, and Belief in Belief</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Kaj Sotala</span></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/social-status\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Status</span></a></li><li><a href=\"https://lessestwrong.com/tag/near-far-thinking\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Near/far thinking</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Adaptation executers</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/superstimuli\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Superstimulus</span></a></li><li><a href=\"https://lessestwrong.com/tag/goodhart-s-law\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart's law</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://www.econtalk.org/archives/2008/05/hanson_on_signa.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson on Signaling (Econtalk Podcast)</span></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 69,
    "description": {
      "markdown": "**Signaling** is [defined](https://lessestwrong.com/lw/did/what_is_signaling_really/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain) as \"a method of conveying information among not-necessarily-trustworthy parties by performing an action which is more likely or less costly if the information is true than if it is not true\". Some signaling is performed exclusively to impress others (to improve your [status](https://lessestwrong.com/tag/social-status)), and in some cases [isn't even worth that](http://www.overcomingbias.com/2007/01/excess_signalin.html). In other cases, signaling is a side-effect of an otherwise useful activity.\n\nFor example, if doing something is easy for one type of person and hard for another type of person, you might do that thing just to get people to think you're the former type of person, even if the thing isn't in itself worth doing. This could explain many facets of human behavior, and reveal opportunities for reducing waste.\n\nNot all signaling is about abilities. Signaling can also be about personality, current emotional state, beliefs, loyalty to a particular group, status within a group, etc.\n\n**Countersignaling** is signaling that a naive observer might take to mean that one is the *opposite* of X, when in fact, one is X, used as a means to signal that one is, in fact, X. For example, aristocrats (\"old money\") may forgo gaudy bling in order to signal that they are not *nouveau riche* (new money), which may lead some people to incorrectly assume that they are not rich.\n\nBlog posts\n----------\n\nby [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n\n*   [Do Helping Professions Help More?](http://www.overcomingbias.com/2006/12/do_helping_prof.html) and [Gifts Hurt](http://www.overcomingbias.com/2006/12/gifts_hurt.html)\n*   [Excess Signaling Example](http://www.overcomingbias.com/2007/01/excess_signalin.html)\n*   [A Tale Of Two Tradeoffs](http://www.overcomingbias.com/2009/01/a-tale-of-two-tradeoffs.html)\n*   [Why Signals Are Shallow](http://www.overcomingbias.com/2009/06/why-signals-are-shallow.html) \\- \"We all want to affiliate with high status people, but since status is about common distant perceptions of quality, we often care more about what distant observers would think about our associates than about how we privately evaluate them.\"\n*   [Signals Are Forever](http://www.overcomingbias.com/2009/06/signals-are-forever.html)\n*   [Least Signaling Activities?](https://lessestwrong.com/lw/g7/least_signaling_activities/)\n\nby others\n\n*   [What Is Signaling, Really?](https://lessestwrong.com/lw/did/what_is_signaling_really/) by [Yvain](https://wiki.lesswrong.com/wiki/Yvain)\n*   [Think Before You Speak (And Signal It)](https://lessestwrong.com/lw/1y3/think_before_you_speak_and_signal_it/) by [Wei Dai](http://weidai.com/)\n*   [Declare Your Signaling and Hidden Agendas](https://lessestwrong.com/lw/b2/declare_your_signaling_and_hidden_agendas/) by [Kaj Sotala](https://wiki.lesswrong.com/wiki/Kaj_Sotala)\n*   [Modularity, Signaling, and Belief in Belief](https://lessestwrong.com/lw/8ev/modularity_signaling_and_belief_in_belief/) by Kaj Sotala\n\nSee also\n--------\n\n*   [Status](https://lessestwrong.com/tag/social-status)\n*   [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers), [Superstimulus](https://lessestwrong.com/tag/superstimuli)\n*   [Goodhart's law](https://lessestwrong.com/tag/goodhart-s-law)\n\nExternal links\n--------------\n\n*   [Robin Hanson on Signaling (Econtalk Podcast)](http://www.econtalk.org/archives/2008/05/hanson_on_signa.html)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ZTRNmvQGgoYiymYnq",
    "name": "Consequentialism",
    "core": false,
    "slug": "consequentialism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qxJ28GN72aiJu96iF\">Consequentialism</span></strong><span><span class=\"by_qxJ28GN72aiJu96iF\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the ethical theory that people should choose their actions</span><span class=\"by_JCcoaax5G6Py3ytrs\"> based on </span><span class=\"by_KgzPEGnYWvKDmWuNY\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">outcomes they expect will result. How to judge outcomes is not specified, but there are many types</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">consequentialism that specify how </span><span class=\"by_JCcoaax5G6Py3ytrs\">outcomes </span><span class=\"by_qgdGA4ZEyW7zNdK84\">should be judged. For example, </span></span><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utilitarianism</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> holds that</span><span class=\"by_HoGziwmhpMGqGeWZy\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">best outcome is that which maximizes the total welfare</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">all people, and ethical egoism holds that the best outcome is that which maximizes their own</span><span class=\"by_HoGziwmhpMGqGeWZy\"> personal </span><span class=\"by_qgdGA4ZEyW7zNdK84\">interests. Consequentialism</span><span class=\"by_HoGziwmhpMGqGeWZy\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">one of three main strands of ethical thought, along with deontology,</span><span class=\"by_HoGziwmhpMGqGeWZy\"> which </span><span class=\"by_qgdGA4ZEyW7zNdK84\">holds that people should choose actions which conform</span><span class=\"by_KgzPEGnYWvKDmWuNY\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a prescribed list of moral rules, and virtue ethics, which holds that people should be judged by how virtuous they are, instead of by what actions they take.</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Related:</span><span class=\"by_qxJ28GN72aiJu96iF\"> </span></span><a href=\"https://www.lesswrong.com/tag/ethics-and-morality\"><span class=\"by_HoGziwmhpMGqGeWZy\">Ethics &amp; Morality</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span></span><a href=\"http://lesswrong.com/tag/deontology\"><span class=\"by_LoykQRMTxJFxwwdPy\">Deontology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"/tag/moral-uncertainty\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Moral Uncertainty</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utilitarianism</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Consequentialism is often associated with maximizing the </span><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">expected value</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> of a </span><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utility function</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. However, it has been argued that consequentialism is not the same thing as having a utility function because it is possible to evaluate actions based on their consequences without obeying the </span><a href=\"http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">von Neuman-Morgenstern axioms</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> necessary for having a utility function, and because utility functions can also be used to implement moral theories similar to deontology.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></h2><ul><li><a href=\"https://www.lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ends Don't Justify Means (Among Humans)</span></a></li><li><a href=\"https://www.lesswrong.com/lw/kn/torture_vs_dust_specks/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Torture vs. Dust Specks</span></a></li><li><a href=\"https://www.lesswrong.com/lw/1og/deontology_for_consequentialists/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Deontology for Consequentialists</span></a></li><li><a href=\"https://www.lesswrong.com/lw/2aa/virtue_ethics_for_consequentialists/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Virtue Ethics for Consequentialists</span></a></li><li><a href=\"https://www.lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consequentialism Need Not Be Shortsighted</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://plato.stanford.edu/archives/win2011/entries/consequentialism/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consequentialism entry on Stanford Encyclopedia of Philosophy</span></a></li><li><a href=\"http://www.raikoth.net/consequentialism.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Consequentialism FAQ</span></a></li><li><a href=\"http://people.howstuffworks.com/trolley-problem.htm\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Description and discussion about trolley problems</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/utilitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utilitarianism</span></a></li><li><a href=\"https://www.lesswrong.com/tag/utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/utility-functions\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utility function</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/expected-utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">expected utility</span></a></li><li><a href=\"https://www.lesswrong.com/tag/metaethics-sequence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Metaethics sequence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/ethical-injunction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ethical injunction</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Shut up and multiply</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hedons\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hedons</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/utils\"><span class=\"by_qgdGA4ZEyW7zNdK84\">utils</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/fuzzies\"><span class=\"by_qgdGA4ZEyW7zNdK84\">fuzzies</span></a></li></ul><h2 id=\"References\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">References</span></h2><ul><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Jeremy Bentham (1907). </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">An Introduction to the Principles of Morals and Legislation</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Library of Economics and Liberty.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Perter Fishburn (1970). </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Utility Theory for Decision Making</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Huntington, NY.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Walter Sinnot-Armstrong (2011). \"</span><a href=\"http://plato.stanford.edu/archives/win2011/entries/consequentialism/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Consequentialism</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">\". </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Stanford Encyclopedia of Philosophy (Winter 2011 Edition)</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">.</span></li><li><span class=\"by_6Fx2vQtkYSZkaCvAg\">Judith Jarvis Thonson (1975). \"Killing, Letting Die, and the Trolley Problem\". </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Monist</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> </span><strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">59</span></strong><span class=\"by_6Fx2vQtkYSZkaCvAg\">: 204-217.</span></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 59,
    "description": {
      "markdown": "**Consequentialism** is the ethical theory that people should choose their actions based on the outcomes they expect will result. How to judge outcomes is not specified, but there are many types of consequentialism that specify how outcomes should be judged. For example, [utilitarianism](https://www.lesswrong.com/tag/utilitarianism) holds that the best outcome is that which maximizes the total welfare of all people, and ethical egoism holds that the best outcome is that which maximizes their own personal interests. Consequentialism is one of three main strands of ethical thought, along with deontology, which holds that people should choose actions which conform to a prescribed list of moral rules, and virtue ethics, which holds that people should be judged by how virtuous they are, instead of by what actions they take.\n\nRelated: [Ethics & Morality](https://www.lesswrong.com/tag/ethics-and-morality), [Deontology](http://lesswrong.com/tag/deontology), [Moral Uncertainty](/tag/moral-uncertainty), [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n\nConsequentialism is often associated with maximizing the [expected value](https://www.lesswrong.com/tag/expected-utility) of a [utility function](https://www.lesswrong.com/tag/utility-functions). However, it has been argued that consequentialism is not the same thing as having a utility function because it is possible to evaluate actions based on their consequences without obeying the [von Neuman-Morgenstern axioms](http://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem) necessary for having a utility function, and because utility functions can also be used to implement moral theories similar to deontology.\n\nBlog posts\n----------\n\n*   [Ends Don't Justify Means (Among Humans)](https://www.lesswrong.com/lw/uv/ends_dont_justify_means_among_humans/)\n*   [Torture vs. Dust Specks](https://www.lesswrong.com/lw/kn/torture_vs_dust_specks/)\n*   [Deontology for Consequentialists](https://www.lesswrong.com/lw/1og/deontology_for_consequentialists/)\n*   [Virtue Ethics for Consequentialists](https://www.lesswrong.com/lw/2aa/virtue_ethics_for_consequentialists/)\n*   [Consequentialism Need Not Be Shortsighted](https://www.lesswrong.com/lw/778/consequentialism_need_not_be_nearsighted/)\n\nExternal links\n--------------\n\n*   [Consequentialism entry on Stanford Encyclopedia of Philosophy](http://plato.stanford.edu/archives/win2011/entries/consequentialism/)\n*   [Consequentialism FAQ](http://www.raikoth.net/consequentialism.html)\n*   [Description and discussion about trolley problems](http://people.howstuffworks.com/trolley-problem.htm)\n\nSee also\n--------\n\n*   [Utilitarianism](https://www.lesswrong.com/tag/utilitarianism)\n*   [Utility](https://www.lesswrong.com/tag/utility), [utility function](https://www.lesswrong.com/tag/utility-functions), [expected utility](https://www.lesswrong.com/tag/expected-utility)\n*   [Metaethics sequence](https://www.lesswrong.com/tag/metaethics-sequence)\n*   [Ethical injunction](https://www.lesswrong.com/tag/ethical-injunction)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply)\n*   [Hedons](https://wiki.lesswrong.com/wiki/Hedons), [utils](https://wiki.lesswrong.com/wiki/utils), [fuzzies](https://www.lesswrong.com/tag/fuzzies)\n\nReferences\n----------\n\n*   Jeremy Bentham (1907). *An Introduction to the Principles of Morals and Legislation*. Library of Economics and Liberty.\n*   Perter Fishburn (1970). *Utility Theory for Decision Making*. Huntington, NY.\n*   Walter Sinnot-Armstrong (2011). \"[Consequentialism](http://plato.stanford.edu/archives/win2011/entries/consequentialism/)\". *The Stanford Encyclopedia of Philosophy (Winter 2011 Edition)*.\n*   Judith Jarvis Thonson (1975). \"Killing, Letting Die, and the Trolley Problem\". *The Monist* **59**: 204-217."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Qs5KwojJdJitjisD4",
    "name": "Commitment Mechanisms",
    "core": null,
    "slug": "commitment-mechanisms",
    "tableOfContents": {
      "html": "<p><span class=\"by_sKAL2jzfkYkDbQmx9\">A </span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Commitment Mechanism </span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\">is a tool or technique that lets people </span><a href=\"https://www.lesswrong.com/tag/pre-commitment\"><span class=\"by_sKAL2jzfkYkDbQmx9\">pre-commit</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> to something. an </span><a href=\"https://www.lesswrong.com/tag/assurance-contracts\"><span class=\"by_sKAL2jzfkYkDbQmx9\">assurance contract </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">is a Pre-Commitment done between several people which is conditional on other people also pre-committing.</span></p><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Many commitment mechanisms incentivize following through on a commitment by penalizing failures to do so. For example, having to pay some amount of money, or automatically posting an announcement to your social media that you failed.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">See also:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.beeminder.com/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">beeminder.com</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 5,
    "description": {
      "markdown": "A **Commitment Mechanism** is a tool or technique that lets people [pre-commit](https://www.lesswrong.com/tag/pre-commitment) to something. an [assurance contract](https://www.lesswrong.com/tag/assurance-contracts) is a Pre-Commitment done between several people which is conditional on other people also pre-committing.\n\nMany commitment mechanisms incentivize following through on a commitment by penalizing failures to do so. For example, having to pay some amount of money, or automatically posting an announcement to your social media that you failed.\n\n**See also:** [beeminder.com](https://www.beeminder.com/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NSMKfa8emSbGNXRKD",
    "name": "Religion",
    "core": null,
    "slug": "religion",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_ikCc3sp9Zguwdqipb\">Religion</span></strong><span><span class=\"by_ikCc3sp9Zguwdqipb\"> is a complex group of human activities — involving </span><span class=\"by_WmpMDJM6PNFaZQh7z\">commitment to higher power,</span><span class=\"by_ikCc3sp9Zguwdqipb\"> </span></span><a href=\"https://www.lesswrong.com/tag/belief-in-belief\"><span class=\"by_ikCc3sp9Zguwdqipb\">belief in belief</span></a><span class=\"by_ikCc3sp9Zguwdqipb\">, and a range of shared group practices such as worship meetings, rites of passage, etc.</span></p><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/groupthink\"><span class=\"by_qf77EiaoMw7tH3GSr\">Groupthink</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://www.lesswrong.com/tag/affective-death-spiral\"><span class=\"by_qf77EiaoMw7tH3GSr\">Affective death spiral</span></a></li><li><a href=\"https://www.lesswrong.com/tag/belief-in-belief\"><span class=\"by_LoykQRMTxJFxwwdPy\">Belief in belief</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/improper-belief\"><span class=\"by_LoykQRMTxJFxwwdPy\">improper belief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/epistemic-hygiene\"><span class=\"by_LoykQRMTxJFxwwdPy\">Epistemic hygiene</span></a></li><li><a href=\"https://www.lesswrong.com/tag/truth-semantics-and-meaning\"><span class=\"by_qf77EiaoMw7tH3GSr\">Truth</span></a></li><li><a href=\"https://www.lesswrong.com/tag/rationalization\"><span class=\"by_LoykQRMTxJFxwwdPy\">Rationalization</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/self-deception\"><span class=\"by_LoykQRMTxJFxwwdPy\">self-deception</span></a></li><li><a href=\"https://www.lesswrong.com/tag/magic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Magic</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/fake-simplicity\"><span class=\"by_LoykQRMTxJFxwwdPy\">fake simplicity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span class=\"by_LoykQRMTxJFxwwdPy\">Absurdity heuristic</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Third_alternative\"><span class=\"by_LoykQRMTxJFxwwdPy\">Third alternative</span></a></li><li><a href=\"https://www.lesswrong.com/tag/mysterious-answers-to-mysterious-questions\"><span class=\"by_LoykQRMTxJFxwwdPy\">Mysterious Answers to Mysterious Questions</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://bloggingheads.tv/diavlogs/18501\"><span class=\"by_qgdGA4ZEyW7zNdK84\">BHTV: Yudkowsky &amp; Adam Frank on \"religious experience\"</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 130,
    "description": {
      "markdown": "**Religion** is a complex group of human activities — involving commitment to higher power, [belief in belief](https://www.lesswrong.com/tag/belief-in-belief), and a range of shared group practices such as worship meetings, rites of passage, etc.\n\nSee also\n--------\n\n*   [Groupthink](https://www.lesswrong.com/tag/groupthink), [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)\n*   [Belief in belief](https://www.lesswrong.com/tag/belief-in-belief), [improper belief](https://www.lesswrong.com/tag/improper-belief)\n*   [Epistemic hygiene](https://www.lesswrong.com/tag/epistemic-hygiene)\n*   [Truth](https://www.lesswrong.com/tag/truth-semantics-and-meaning)\n*   [Rationalization](https://www.lesswrong.com/tag/rationalization), [self-deception](https://www.lesswrong.com/tag/self-deception)\n*   [Magic](https://www.lesswrong.com/tag/magic), [fake simplicity](https://www.lesswrong.com/tag/fake-simplicity)\n*   [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Third alternative](https://wiki.lesswrong.com/wiki/Third_alternative)\n*   [Mysterious Answers to Mysterious Questions](https://www.lesswrong.com/tag/mysterious-answers-to-mysterious-questions)\n\nExternal links\n--------------\n\n*   [BHTV: Yudkowsky & Adam Frank on \"religious experience\"](http://bloggingheads.tv/diavlogs/18501)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "P64rmDCvTBAehmkoi",
    "name": "Filtered Evidence",
    "core": false,
    "slug": "filtered-evidence",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Filtered </span><span class=\"by_qgdGA4ZEyW7zNdK84\">evidence</span></span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">is </span></span><a href=\"https://lessestwrong.com/tag/evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">evidence</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">was selected for the </span><span class=\"by_E3pLSZ6ePw4DjE2jm\">fact that it supports (or opposes)</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">hypothesis. Filtered evidence may be highly misleading, but still it can be useful, if considered with care.</span></span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/conservation-of-expected-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Conservation of expected evidence</span></a></li><li><a href=\"https://lessestwrong.com/tag/rational-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rational evidence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/standard-of-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Standard of evidence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Adversarial_process\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Adversarial process</span></a></li><li><a href=\"https://lessestwrong.com/tag/epistemic-hygiene\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic hygiene</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Availability_bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Availability bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/dangerous-knowledge\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Dangerous knowledge</span></a></li><li><a href=\"https://lessestwrong.com/tag/dark-arts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Dark arts</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/arguments-as-soldiers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Arguments as soldiers</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/rationalization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization</span></a></li><li><a href=\"https://lessestwrong.com/tag/not-technically-a-lie\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Not technically a lie</span></a></li><li><a href=\"https://www.lesswrong.com/tag/anthropics\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Anthropics</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 12,
    "description": {
      "markdown": "**Filtered evidence** is [evidence](https://lessestwrong.com/tag/evidence) that was selected for the fact that it supports (or opposes) a hypothesis. Filtered evidence may be highly misleading, but still it can be useful, if considered with care.\n\nSee also\n--------\n\n*   [Conservation of expected evidence](https://lessestwrong.com/tag/conservation-of-expected-evidence)\n*   [Rational evidence](https://lessestwrong.com/tag/rational-evidence), [Standard of evidence](https://lessestwrong.com/tag/standard-of-evidence), [Adversarial process](https://wiki.lesswrong.com/wiki/Adversarial_process)\n*   [Epistemic hygiene](https://lessestwrong.com/tag/epistemic-hygiene)\n*   [Availability bias](https://wiki.lesswrong.com/wiki/Availability_bias), [Dangerous knowledge](https://lessestwrong.com/tag/dangerous-knowledge)\n*   [Dark arts](https://lessestwrong.com/tag/dark-arts), [Arguments as soldiers](https://lessestwrong.com/tag/arguments-as-soldiers), [Rationalization](https://lessestwrong.com/tag/rationalization)\n*   [Not technically a lie](https://lessestwrong.com/tag/not-technically-a-lie)\n*   [Anthropics](https://www.lesswrong.com/tag/anthropics)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "fihKHQuS5WZBJgkRm",
    "name": "Newcomb's Problem",
    "core": false,
    "slug": "newcomb-s-problem",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb'</span><span class=\"by_baGAQoNAH4hXaC6qf\">s </span><span class=\"by_HoGziwmhpMGqGeWZy\">Problem</span></span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is</span><span class=\"by_baGAQoNAH4hXaC6qf\"> </span><span class=\"by_PZwuAu7D3thM5LmWc\">a </span><span class=\"by_HoGziwmhpMGqGeWZy\">thought experiment in decision theory exploring problems posed by having other agents in</span><span class=\"by_baGAQoNAH4hXaC6qf\"> the </span><span class=\"by_HoGziwmhpMGqGeWZy\">environment</span><span class=\"by_baGAQoNAH4hXaC6qf\"> who</span><span class=\"by_mLnNK3xEMczLs8ind\"> can predict </span><span class=\"by_HoGziwmhpMGqGeWZy\">your actions.</span></span></p><h2 id=\"The_Problem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Problem</span></h2><p><span class=\"by_HoGziwmhpMGqGeWZy\">From </span><a href=\"https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb'</span><span class=\"by_baGAQoNAH4hXaC6qf\">s Problem and Regret of Rationality</span></span></a><span class=\"by_HoGziwmhpMGqGeWZy\">:</span></p><blockquote><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">A superintelligence from another galaxy, whom we shall call Omega, comes to Earth</span><span class=\"by_qf77EiaoMw7tH3GSr\"> and </span><span class=\"by_HoGziwmhpMGqGeWZy\">sets about playing a strange little game. In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.</span></span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Box A is transparent and contains a thousand dollars.</span><br><span class=\"by_HoGziwmhpMGqGeWZy\">Box B is opaque, and contains either a million dollars, or nothing.</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">You can take both boxes, or take only box B.</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars. (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Before you make your choice, Omega has flown off and moved on to its next game. Box B is already empty or already full.</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Omega drops two boxes on the ground in front of you and flies off.</span></p></blockquote><blockquote><p><span class=\"by_HoGziwmhpMGqGeWZy\">Do you take both boxes, or only box B?</span></p></blockquote><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">One line of reasoning about the problem says that because Omega has already left, the boxes are set and you </span><span class=\"by_qgdGA4ZEyW7zNdK84\">can'</span><span class=\"by_HoGziwmhpMGqGeWZy\">t change them. And if you look</span><span class=\"by_qf77EiaoMw7tH3GSr\"> at </span><span class=\"by_HoGziwmhpMGqGeWZy\">the payoff matrix, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you'</span><span class=\"by_HoGziwmhpMGqGeWZy\">ll see that whatever decision Omega has already made, you get $1000 more for taking both boxes. This makes taking two boxes </span><span class=\"by_ibW3cznxxg96857o3\">(\"two-boxing\") </span><span class=\"by_HoGziwmhpMGqGeWZy\">a dominant strategy and therefore the correct choice. Agents who reason this way do not make very much money playing this game.</span><span class=\"by_ibW3cznxxg96857o3\"> This is because this line of reasoning ignores the connection between the agent and Omega's prediction: two-boxing only makes $1000 more than one-boxing if Omega's prediction is the same in both cases, while the problem states Omega is extremely accurate in its predictions. Switching from one-boxing to two-boxing doesn't give the agent a $1000 more, it results in a loss of $999,000.</span></span></p><p><span class=\"by_ibW3cznxxg96857o3\">Because the agent's decision in this problem can't causally affect Omega's prediction (which happened in the past), </span><a href=\"https://www.lesswrong.com/tag/causal-decision-theory\"><span class=\"by_ibW3cznxxg96857o3\">Causal Decision Theory</span></a><span class=\"by_ibW3cznxxg96857o3\"> two-boxes. One-boxing is correlated with getting a million dollars, whereas two-boxing is correlated with getting only $1000; therefore, </span><a href=\"https://www.lesswrong.com/tag/evidential-decision-theory\"><span class=\"by_ibW3cznxxg96857o3\">Evidential Decision Theory</span></a><span class=\"by_ibW3cznxxg96857o3\"> one-boxes. </span><a href=\"https://www.lesswrong.com/tag/functional-decision-theory\"><span class=\"by_ibW3cznxxg96857o3\">Functional Decision Theory</span></a><span class=\"by_ibW3cznxxg96857o3\"> (FDT) also one-boxes, but for a completely different reason: FDT reasons that Omega must have had a model of the agent's decision procedure in order to make the prediction. Therefore, your decision procedure is run not only by you, but also (in the past) by Omega; whatever you decide, Omega's model must have decided the same. Either both you and Omega's model two-box, or both you and Omega's model one-box; of these two options, the latter is preferable, so FDT one-boxes.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">The general class of decision problems that involve other agents predicting your actions are called Newcomblike Problems.</span></p><h2 id=\"Irrelevance_of_Omega_s_Physical_Impossibility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Irrelevance of Omega's Physical Impossibility</span></h2><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Sometimes people dismiss Newcomb's problem because </span><span class=\"by_XLwKyCK7JmC292ZCC\">of the physical impossibility of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a being </span><span class=\"by_XLwKyCK7JmC292ZCC\">like Omega. However, Newcomb's problem does not actually depend on</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> the possibility of Omega </span><span class=\"by_XLwKyCK7JmC292ZCC\">in order to </span><span class=\"by_HoGziwmhpMGqGeWZy\">be </span><span class=\"by_XLwKyCK7JmC292ZCC\">relevant. Similar issues arise if we imagine</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> a skilled human psychologist </span><span class=\"by_XLwKyCK7JmC292ZCC\">who</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> can predict other </span><span class=\"by_XLwKyCK7JmC292ZCC\">people's</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> actions </span><span class=\"by_XLwKyCK7JmC292ZCC\">with</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> 65% accuracy.</span></span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's Problem and Regret of Rationality</span></a></li><li><a href=\"https://lessestwrong.com/lw/7v/formalizing_newcombs/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Formalizing Newcomb's</span></a></li><li><a href=\"https://lessestwrong.com/lw/90/newcombs_problem_standard_positions/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's Problem standard positions</span></a></li><li><a href=\"https://lessestwrong.com/lw/6r/newcombs_problem_vs_oneshot_prisoners_dilemma/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's Problem vs. One-Shot Prisoner's Dilemma</span></a></li><li><a href=\"https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory: Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory</span></a></li><li><a href=\"https://lessestwrong.com/tag/counterfactual-mugging\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Counterfactual mugging</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Parfit's hitchhiker</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Smoker's lesion</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Absentminded driver</span></a></li><li><a href=\"https://lessestwrong.com/tag/sleeping-beauty-paradox\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sleeping Beauty problem</span></a></li><li><a href=\"https://lessestwrong.com/tag/prisoner-s-dilemma\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner's dilemma</span></a></li></ul>",
      "sections": [
        {
          "title": "The Problem",
          "anchor": "The_Problem",
          "level": 1
        },
        {
          "title": "Irrelevance of Omega's Physical Impossibility",
          "anchor": "Irrelevance_of_Omega_s_Physical_Impossibility",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 47,
    "description": {
      "markdown": "**Newcomb's Problem** is a thought experiment in decision theory exploring problems posed by having other agents in the environment who can predict your actions.\n\nThe Problem\n-----------\n\nFrom [Newcomb's Problem and Regret of Rationality](https://www.lesswrong.com/posts/6ddcsdA2c2XpNpE5x/newcomb-s-problem-and-regret-of-rationality):\n\n> A superintelligence from another galaxy, whom we shall call Omega, comes to Earth and sets about playing a strange little game. In this game, Omega selects a human being, sets down two boxes in front of them, and flies away.\n\n> Box A is transparent and contains a thousand dollars.  \n> Box B is opaque, and contains either a million dollars, or nothing.\n\n> You can take both boxes, or take only box B.\n\n> And the twist is that Omega has put a million dollars in box B iff Omega has predicted that you will take only box B.\n\n> Omega has been correct on each of 100 observed occasions so far - everyone who took both boxes has found box B empty and received only a thousand dollars; everyone who took only box B has found B containing a million dollars. (We assume that box A vanishes in a puff of smoke if you take only box B; no one else can take box A afterward.)\n\n> Before you make your choice, Omega has flown off and moved on to its next game. Box B is already empty or already full.\n\n> Omega drops two boxes on the ground in front of you and flies off.\n\n> Do you take both boxes, or only box B?\n\nOne line of reasoning about the problem says that because Omega has already left, the boxes are set and you can't change them. And if you look at the payoff matrix, you'll see that whatever decision Omega has already made, you get $1000 more for taking both boxes. This makes taking two boxes (\"two-boxing\") a dominant strategy and therefore the correct choice. Agents who reason this way do not make very much money playing this game. This is because this line of reasoning ignores the connection between the agent and Omega's prediction: two-boxing only makes $1000 more than one-boxing if Omega's prediction is the same in both cases, while the problem states Omega is extremely accurate in its predictions. Switching from one-boxing to two-boxing doesn't give the agent a $1000 more, it results in a loss of $999,000.\n\nBecause the agent's decision in this problem can't causally affect Omega's prediction (which happened in the past), [Causal Decision Theory](https://www.lesswrong.com/tag/causal-decision-theory) two-boxes. One-boxing is correlated with getting a million dollars, whereas two-boxing is correlated with getting only $1000; therefore, [Evidential Decision Theory](https://www.lesswrong.com/tag/evidential-decision-theory) one-boxes. [Functional Decision Theory](https://www.lesswrong.com/tag/functional-decision-theory) (FDT) also one-boxes, but for a completely different reason: FDT reasons that Omega must have had a model of the agent's decision procedure in order to make the prediction. Therefore, your decision procedure is run not only by you, but also (in the past) by Omega; whatever you decide, Omega's model must have decided the same. Either both you and Omega's model two-box, or both you and Omega's model one-box; of these two options, the latter is preferable, so FDT one-boxes.\n\nThe general class of decision problems that involve other agents predicting your actions are called Newcomblike Problems.\n\nIrrelevance of Omega's Physical Impossibility\n---------------------------------------------\n\nSometimes people dismiss Newcomb's problem because of the physical impossibility of a being like Omega. However, Newcomb's problem does not actually depend on the possibility of Omega in order to be relevant. Similar issues arise if we imagine a skilled human psychologist who can predict other people's actions with 65% accuracy.\n\nNotable Posts\n-------------\n\n*   [Newcomb's Problem and Regret of Rationality](https://lessestwrong.com/lw/nc/newcombs_problem_and_regret_of_rationality/)\n*   [Formalizing Newcomb's](https://lessestwrong.com/lw/7v/formalizing_newcombs/)\n*   [Newcomb's Problem standard positions](https://lessestwrong.com/lw/90/newcombs_problem_standard_positions/)\n*   [Newcomb's Problem vs. One-Shot Prisoner's Dilemma](https://lessestwrong.com/lw/6r/newcombs_problem_vs_oneshot_prisoners_dilemma/)\n*   [Decision theory: Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives](https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/)\n\nSee Also\n--------\n\n*   [Decision theory](https://lessestwrong.com/tag/decision-theory)\n*   [Counterfactual mugging](https://lessestwrong.com/tag/counterfactual-mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker's_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Sleeping Beauty problem](https://lessestwrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://lessestwrong.com/tag/prisoner-s-dilemma)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NZ67PZ8CkeS6xn27h",
    "name": "Mesa-Optimization",
    "core": null,
    "slug": "mesa-optimization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Mesa-Optimization</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the situation that occurs when a learned model (such as a neural network) is itself an optimizer. </span><span class=\"by_2aoRX3ookcCozcb3m\">In this situation, a</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">base optimizer</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> creates a </span><span class=\"by_2aoRX3ookcCozcb3m\">second optimizer, called a </span></span><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">mesa-</span><span class=\"by_2aoRX3ookcCozcb3m\">optimizer</span></span></i><span><span class=\"by_2aoRX3ookcCozcb3m\">. The primary reference</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> work </span><span class=\"by_2aoRX3ookcCozcb3m\">for</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> this concept </span><span class=\"by_2aoRX3ookcCozcb3m\">is Hubinger et al.'s \"</span></span><a href=\"https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction\"><span><span class=\"by_2aoRX3ookcCozcb3m\">Risks from Learned </span><span class=\"by_j8TwwtYJusmkqvGfh\">Optimization </span><span class=\"by_2aoRX3ookcCozcb3m\">in Advanced Machine Learning Systems</span></span></a><span class=\"by_2aoRX3ookcCozcb3m\">\".</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">Example: </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Natural</span><span class=\"by_j8TwwtYJusmkqvGfh\"> selection is an optimization process </span><span class=\"by_2aoRX3ookcCozcb3m\">that</span><span class=\"by_j8TwwtYJusmkqvGfh\"> optimizes for reproductive </span><span class=\"by_2aoRX3ookcCozcb3m\">fitness. Natural selection</span><span class=\"by_j8TwwtYJusmkqvGfh\"> produced </span><span class=\"by_2aoRX3ookcCozcb3m\">humans, who</span><span class=\"by_j8TwwtYJusmkqvGfh\"> are </span><span class=\"by_2aoRX3ookcCozcb3m\">themselves optimizers. Humans</span><span class=\"by_j8TwwtYJusmkqvGfh\"> are </span><span class=\"by_2aoRX3ookcCozcb3m\">therefore mesa-optimizers</span><span class=\"by_j8TwwtYJusmkqvGfh\"> of natural selection.</span></span></p><p><span><span class=\"by_j8TwwtYJusmkqvGfh\">In the context of AI </span><span class=\"by_sKAL2jzfkYkDbQmx9\">alignment,</span><span class=\"by_j8TwwtYJusmkqvGfh\"> the concern is that </span><span class=\"by_2aoRX3ookcCozcb3m\">a base optimizer (e.g., a gradient descent process)</span><span class=\"by_j8TwwtYJusmkqvGfh\"> may produce </span><span class=\"by_2aoRX3ookcCozcb3m\">a learned model</span><span class=\"by_j8TwwtYJusmkqvGfh\"> that </span><span class=\"by_2aoRX3ookcCozcb3m\">is itself an optimizer, and that has unexpected and undesirable properties. Even if the gradient descent process is in some sense \"trying\" to do exactly what human developers want, the resultant mesa-optimizer will not typically be trying to do the exact same thing.</span></span><a href=\"https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn1\"><sup><span class=\"by_j8TwwtYJusmkqvGfh\">1</span></sup></a></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"History\"><span class=\"by_j8TwwtYJusmkqvGfh\">History</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Previously work under this concept was called </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Inner Optimizer </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Optimization Daemons.</span></i></p><p><a href=\"https://www.lesswrong.com/users/wei_dai\"><span class=\"by_j8TwwtYJusmkqvGfh\">Wei Dai</span></a><span class=\"by_j8TwwtYJusmkqvGfh\"> brings up a similar idea in an SL4 thread.</span><a href=\"https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn2\"><sup><span class=\"by_j8TwwtYJusmkqvGfh\">2</span></sup></a></p><p><span class=\"by_j8TwwtYJusmkqvGfh\">The optimization daemons article on </span><a href=\"https://arbital.com/\"><span class=\"by_j8TwwtYJusmkqvGfh\">Arbital</span></a><span class=\"by_j8TwwtYJusmkqvGfh\"> was published probably in 2016.</span><a href=\"https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn3\"><sup><span class=\"by_j8TwwtYJusmkqvGfh\">3</span></sup></a></p><p><a href=\"https://www.lesswrong.com/users/jessica-liu-taylor\"><span class=\"by_j8TwwtYJusmkqvGfh\">Jessica Taylor</span></a><span class=\"by_j8TwwtYJusmkqvGfh\"> wrote two posts about daemons while at </span><a href=\"https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri\"><span class=\"by_j8TwwtYJusmkqvGfh\">MIRI</span></a><span class=\"by_j8TwwtYJusmkqvGfh\">:</span></p><ul><li><a href=\"https://agentfoundations.org/item?id=1281\"><span class=\"by_j8TwwtYJusmkqvGfh\">\"Are daemons a problem for ideal agents?\"</span></a><span class=\"by_j8TwwtYJusmkqvGfh\"> (2017-02-11)</span></li><li><a href=\"https://agentfoundations.org/item?id=1290\"><span class=\"by_j8TwwtYJusmkqvGfh\">\"Maximally efficient agents will probably have an anti-daemon immune system\"</span></a><span class=\"by_j8TwwtYJusmkqvGfh\"> (2017-02-23)</span></li></ul><h2><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></h2><h2 id=\"See_also\"><span class=\"by_j8TwwtYJusmkqvGfh\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/inner-alignment\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Inner Alignment</span></a></li><li><a href=\"https://lessestwrong.com/tag/complexity-of-value\"><span class=\"by_j8TwwtYJusmkqvGfh\">Complexity of value</span></a></li><li><a href=\"https://lessestwrong.com/lw/l3/thou_art_godshatter/\"><span class=\"by_2aoRX3ookcCozcb3m\">Thou Art Godshatter</span></a></li></ul><h2><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></h2><h2 id=\"References\"><span class=\"by_j8TwwtYJusmkqvGfh\">References</span></h2><ol><li><a href=\"https://arbital.com/p/daemons/\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Optimization daemons\"</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Arbital.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Wei Dai. </span><a href=\"http://sl4.org/archive/0312/7421.html\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">'\"friendly\" humans?'</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> December 31, 2003.</span></li></ol><h2><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></h2><h2 id=\"External_links\"><span class=\"by_j8TwwtYJusmkqvGfh\">External links</span></h2><p><a href=\"https://www.youtube.com/watch?v=bJLcIBixGj8\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Video by Robert Miles</span></a></p><p><span class=\"by_j8TwwtYJusmkqvGfh\">Some posts that reference optimization daemons:</span></p><ul><li><a href=\"http://effective-altruism.com/ea/1k4/draft_cause_prioritization_for_downsidefocused/\"><span class=\"by_j8TwwtYJusmkqvGfh\">\"Cause prioritization for downside-focused value systems\"</span></a><span class=\"by_j8TwwtYJusmkqvGfh\">: \"Alternatively, perhaps goal preservation becomes more difficult the more capable AI systems become, in which case the future might be controlled by unstable goal functions taking turns over the steering wheel\"</span></li><li><a href=\"https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99\"><span class=\"by_j8TwwtYJusmkqvGfh\">\"Techniques for optimizing worst-case performance\"</span></a><span class=\"by_j8TwwtYJusmkqvGfh\">: \"The difficulty of optimizing worst-case performance is one of the most likely reasons that I think prosaic AI alignment might turn out to be impossible (if combined with an unlucky empirical situation).\" (the phrase \"unlucky empirical situation\" links to the optimization daemons page on </span><a href=\"https://arbital.com/\"><span class=\"by_j8TwwtYJusmkqvGfh\">Arbital</span></a><span class=\"by_j8TwwtYJusmkqvGfh\">)</span></li></ul>",
      "sections": [
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 68,
    "description": {
      "markdown": "**Mesa-Optimization** is the situation that occurs when a learned model (such as a neural network) is itself an optimizer. In this situation, a *base optimizer* creates a second optimizer, called a *mesa-optimizer*. The primary reference work for this concept is Hubinger et al.'s \"[Risks from Learned Optimization in Advanced Machine Learning Systems](https://www.alignmentforum.org/posts/FkgsxrGf3QxhfLWHG/risks-from-learned-optimization-introduction)\".\n\nExample: Natural selection is an optimization process that optimizes for reproductive fitness. Natural selection produced humans, who are themselves optimizers. Humans are therefore mesa-optimizers of natural selection.\n\nIn the context of AI alignment, the concern is that a base optimizer (e.g., a gradient descent process) may produce a learned model that is itself an optimizer, and that has unexpected and undesirable properties. Even if the gradient descent process is in some sense \"trying\" to do exactly what human developers want, the resultant mesa-optimizer will not typically be trying to do the exact same thing.[^1^](https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn1)\n\nHistory\n-------\n\nPreviously work under this concept was called *Inner Optimizer* or *Optimization Daemons.*\n\n[Wei Dai](https://www.lesswrong.com/users/wei_dai) brings up a similar idea in an SL4 thread.[^2^](https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn2)\n\nThe optimization daemons article on [Arbital](https://arbital.com/) was published probably in 2016.[^3^](https://lessestwrong.com/tag/mesa-optimization?revision=0.0.3#fn3)\n\n[Jessica Taylor](https://www.lesswrong.com/users/jessica-liu-taylor) wrote two posts about daemons while at [MIRI](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri):\n\n*   [\"Are daemons a problem for ideal agents?\"](https://agentfoundations.org/item?id=1281) (2017-02-11)\n*   [\"Maximally efficient agents will probably have an anti-daemon immune system\"](https://agentfoundations.org/item?id=1290) (2017-02-23)\n\nSee also\n--------\n\n*   [Inner Alignment](https://www.lesswrong.com/tag/inner-alignment)\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Thou Art Godshatter](https://lessestwrong.com/lw/l3/thou_art_godshatter/)\n\nReferences\n----------\n\n1.  [\"Optimization daemons\"](https://arbital.com/p/daemons/). Arbital.\n2.  Wei Dai. ['\"friendly\" humans?'](http://sl4.org/archive/0312/7421.html) December 31, 2003.\n\nExternal links\n--------------\n\n[Video by Robert Miles](https://www.youtube.com/watch?v=bJLcIBixGj8)\n\nSome posts that reference optimization daemons:\n\n*   [\"Cause prioritization for downside-focused value systems\"](http://effective-altruism.com/ea/1k4/draft_cause_prioritization_for_downsidefocused/): \"Alternatively, perhaps goal preservation becomes more difficult the more capable AI systems become, in which case the future might be controlled by unstable goal functions taking turns over the steering wheel\"\n*   [\"Techniques for optimizing worst-case performance\"](https://ai-alignment.com/techniques-for-optimizing-worst-case-performance-39eafec74b99): \"The difficulty of optimizing worst-case performance is one of the most likely reasons that I think prosaic AI alignment might turn out to be impossible (if combined with an unlucky empirical situation).\" (the phrase \"unlucky empirical situation\" links to the optimization daemons page on [Arbital](https://arbital.com/))"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "7oXfRFCR7N22MnuY5",
    "name": "Circling",
    "core": null,
    "slug": "circling",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Circling </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is a group \"meditative\", \"relational\" practice. Typically, a group of people sit in a circle and deliberately focus their attention the emotions and experiences of each participant in the group. Communication is usually restricted to the topic of what the individuals in the Circle are experiencing in the present moment, particular their attitudes, feelings, and reactions to others in the group.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Circling may offer benefits in greater awareness of oneself, others, and the interpersonal dynamics between the two. Since social relations are so key to human wellbeing and at the heart of so many psychological challenges, Circling can be of key interest to anyone trying optimize themselves. It may also foster better relationships and cooperation with others.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">However, Circling originated outside the LessWrong community and many feel that the practice does not have sufficient evidence behind it for it to so widely admired within the Rationalist community.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Circling** is a group \"meditative\", \"relational\" practice. Typically, a group of people sit in a circle and deliberately focus their attention the emotions and experiences of each participant in the group. Communication is usually restricted to the topic of what the individuals in the Circle are experiencing in the present moment, particular their attitudes, feelings, and reactions to others in the group.\n\nCircling may offer benefits in greater awareness of oneself, others, and the interpersonal dynamics between the two. Since social relations are so key to human wellbeing and at the heart of so many psychological challenges, Circling can be of key interest to anyone trying optimize themselves. It may also foster better relationships and cooperation with others.\n\nHowever, Circling originated outside the LessWrong community and many feel that the practice does not have sufficient evidence behind it for it to so widely admired within the Rationalist community."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "nZCb9BSnmXZXSNA2u",
    "name": "Evolution",
    "core": false,
    "slug": "evolution",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Evolution</span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"</span></span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">change in the heritable characteristics of biological populations over successive generations</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> (</span></span><a href=\"https://en.wikipedia.org/wiki/Evolution\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Wikipedia</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">). For posts about machine learning look </span><a href=\"https://www.lesswrong.com/tag/machine-learning?showPostCount=false&amp;useTagName=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">here</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related: </span></i><a href=\"https://www.lesswrong.com/tag/biology?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Biology</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Evolutionary Psychology</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">,</span></p><p><span><span class=\"by_nmk3nLpQE89dMRzzN\">The </span><span class=\"by_Xn6ACr6Cua8upALWQ\">sequence,</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span></span><a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8\"><span class=\"by_Xn6ACr6Cua8upALWQ\">The Simple Math of Evolution</span></a><span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_Xn6ACr6Cua8upALWQ\">provides a good introduction to LessWrong thinking about evolution.</span></span></p><h1 id=\"Why_be_interested_in_evolution_\"><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Why </span><span class=\"by_qgdGA4ZEyW7zNdK84\">be interested in evolution?</span></span></h1><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Firstly,</span><span class=\"by_nmk3nLpQE89dMRzzN\"> evolution</span><span class=\"by_qf77EiaoMw7tH3GSr\"> is </span><span class=\"by_Xn6ACr6Cua8upALWQ\">a useful case study of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">humans'</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> ability (or inability) to model the real world. This is because</span><span class=\"by_nmk3nLpQE89dMRzzN\"> it </span><span class=\"by_Xn6ACr6Cua8upALWQ\">has a single clear criterion </span><span class=\"by_qgdGA4ZEyW7zNdK84\">(\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">relative reproductive </span><span class=\"by_qgdGA4ZEyW7zNdK84\">fitness\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">) which is selected (optimized) for:</span></span></p><blockquote><p><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">If we </span><span class=\"by_qgdGA4ZEyW7zNdK84\">can'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t see clearly the result of a single monotone optimization </span><span class=\"by_qgdGA4ZEyW7zNdK84\">criterion—</span><span class=\"by_Xn6ACr6Cua8upALWQ\">if we </span><span class=\"by_qgdGA4ZEyW7zNdK84\">can'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t even train ourselves</span><span class=\"by_nmk3nLpQE89dMRzzN\"> to </span><span class=\"by_Xn6ACr6Cua8upALWQ\">hear a single pure </span><span class=\"by_qgdGA4ZEyW7zNdK84\">note—</span><span class=\"by_Xn6ACr6Cua8upALWQ\">then how will we listen</span><span class=\"by_nmk3nLpQE89dMRzzN\"> to </span><span class=\"by_Xn6ACr6Cua8upALWQ\">an orchestra? How will we see that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Always</span><span class=\"by_nmk3nLpQE89dMRzzN\"> be </span><span class=\"by_qgdGA4ZEyW7zNdK84\">selfish\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> or </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Always</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> obey</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">government\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> are poor guiding principles for human beings to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">adopt—</span><span class=\"by_Xn6ACr6Cua8upALWQ\">if we think</span><span class=\"by_nmk3nLpQE89dMRzzN\"> that </span><span class=\"by_Xn6ACr6Cua8upALWQ\">even optimizing genes for inclusive fitness will yield organisms which sacrifice reproductive opportunities</span><span class=\"by_nmk3nLpQE89dMRzzN\"> in the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">name</span><span class=\"by_nmk3nLpQE89dMRzzN\"> of </span><span class=\"by_Xn6ACr6Cua8upALWQ\">social resource conservation?</span></span></i></p></blockquote><blockquote><p><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">To train ourselves to see clearly, we need simple practice </span><span class=\"by_qgdGA4ZEyW7zNdK84\">cases\" -- </span></span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">Eliezer Yudkowsky</span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span></i><a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/i6fKszWY6gLZSX2Ey\"><i><span class=\"by_Xn6ACr6Cua8upALWQ\">Fake Optimisation Criteria</span></i></a></p></blockquote><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Secondly, much of rationality necessarily revolves around the human brain (</span><a href=\"https://www.lesswrong.com/tag/transhumanism?usePostCount=false&amp;useTagName=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">for</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span><a href=\"https://www.lesswrong.com/tag/mind-uploading?showPostCount=false&amp;useTagName=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">now</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\">). An understanding of how it came into being can </span><span class=\"by_nmk3nLpQE89dMRzzN\">be </span><span class=\"by_Xn6ACr6Cua8upALWQ\">very helpful both for understanding </span><span class=\"by_qgdGA4ZEyW7zNdK84\">'bugs'</span><span class=\"by_nmk3nLpQE89dMRzzN\"> in </span><span class=\"by_Xn6ACr6Cua8upALWQ\">the system (like superstimuli), and for explaining </span></span><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Complexity of Value</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, among others.</span></p><blockquote><p><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">A candy bar is</span><span class=\"by_nmk3nLpQE89dMRzzN\"> a </span><span class=\"by_Xn6ACr6Cua8upALWQ\">superstimulus: it contains more concentrated sugar, salt, and fat than anything that exists in</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">ancestral environment.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\">A candy bar matches taste buds</span><span class=\"by_nmk3nLpQE89dMRzzN\"> that </span><span class=\"by_Xn6ACr6Cua8upALWQ\">evolved</span><span class=\"by_nmk3nLpQE89dMRzzN\"> in a </span><span class=\"by_Xn6ACr6Cua8upALWQ\">hunter-gatherer environment, but it matches those</span><span class=\"by_nmk3nLpQE89dMRzzN\"> taste buds </span><span class=\"by_Xn6ACr6Cua8upALWQ\">much more strongly than anything</span><span class=\"by_nmk3nLpQE89dMRzzN\"> that </span><span class=\"by_Xn6ACr6Cua8upALWQ\">actually existed</span><span class=\"by_nmk3nLpQE89dMRzzN\"> in the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">hunter-gatherer environment.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> The signal that once reliably correlated</span><span class=\"by_nmk3nLpQE89dMRzzN\"> to </span><span class=\"by_Xn6ACr6Cua8upALWQ\">healthy food has been hijacked, blotted out with a point in tastespace</span><span class=\"by_nmk3nLpQE89dMRzzN\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wasn'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t in</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">training dataset</span><span class=\"by_nmk3nLpQE89dMRzzN\"> - </span><span class=\"by_Xn6ACr6Cua8upALWQ\">an impossibly distant outlier on</span><span class=\"by_nmk3nLpQE89dMRzzN\"> the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">old ancestral graphs.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></span></i><br><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">-</span><span class=\"by_nmk3nLpQE89dMRzzN\">- </span></span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">Eliezer Yudkowsky, </span><a href=\"https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/Jq73GozjsuhdwMLEG\"><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Superstimuli</span><span class=\"by_nmk3nLpQE89dMRzzN\"> and the </span><span class=\"by_Xn6ACr6Cua8upALWQ\">Collapse of Western Civilisation</span></span></a></p></blockquote><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/evolution-as-alien-god\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolution as alien god</span></a></li><li><a href=\"https://lessestwrong.com/tag/slowness-of-evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Slowness of evolution</span></a></li><li><a href=\"https://lessestwrong.com/tag/stupidity-of-evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Stupidity of evolution</span></a></li><li><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolutionary psychology</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><a href=\"http://dl.dropbox.com/u/33627365/Scholarship/Selfish%20Gene%20-%20Dawkins.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Richard Dawkins - The Selfish Gene</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (PDF)</span></li></ul><h2 id=\"Summaries_of_Sequence_s_Posts_on_Evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Summaries of Sequence's Posts on Evolution</span></h2><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">The following are summaries of posts concerning evolution in the Eliezer's sequences:</span></i></p><ul><li><a href=\"https://lessestwrong.com/lw/kr/an_alien_god/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">An Alien God</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Evolution is awesomely powerful, unbelievably stupid, incredibly slow, monomaniacally singleminded, irrevocably splintered in focus, blindly shortsighted, and itself a completely accidental process. If evolution were a god, it would not be Jehovah, but H. P. Lovecraft's Azathoth, the blind idiot God burbling chaotically at the center of everything.</span></li><li><a href=\"https://lessestwrong.com/lw/ks/the_wonder_of_evolution/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Wonder of Evolution</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - The wonder of the first replicator was not how amazingly well it replicated, but that a first replicator could arise, at all, by pure accident, in the primordial seas of Earth. That first replicator would undoubtedly be devoured in an instant by a sophisticated modern bacterium. Likewise, the wonder of evolution itself is not how </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">well</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> it works, but that a </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">brainless, accidentally occurring</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://lessestwrong.com/tag/optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">optimization process</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> can work </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">at all</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. If you praise evolution for being such a wonderfully intelligent Creator, you're entirely missing the wonderful thing about it.</span></li><li><a href=\"https://lessestwrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolutions Are Stupid (But Work Anyway)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Modern evolutionary theory gives us a definite picture of evolution's capabilities. If you praise evolution one millimeter higher than this, you are not scoring points against creationists, you are just being factually inaccurate. In particular we can calculate the probability and time for advantageous genes to rise to fixation. For example, a mutation conferring a 3% advantage would have only a 6% probability of surviving, and if it did so, would take 875 generations to rise to fixation in a population of 500,000 (on average).</span></li><li><a href=\"https://lessestwrong.com/tag/speed-limit-and-complexity-bound-for-evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Speed limit and complexity bound for evolution</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - It is widely understood that there is a limit on how fast evolution can accumulate information in a gene pool, and an upper bound on how much genetic information can be sustained against the degenerative pressure of copying errors. (But Yudkowsky's attempt to calculate an actual bound failed mathematically, so see the referenced summary of the discussion instead of the original blog post.)</span></li><li><a href=\"https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Adaptation-Executers, not Fitness-Maximizers</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - A central principle of evolutionary biology in general, and </span><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">evolutionary psychology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> in particular. If we regarded human taste buds as trying to </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">maximize fitness</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">executing adaptation</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> - they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor.</span></li><li><a href=\"https://lessestwrong.com/lw/l6/no_evolutions_for_corporations_or_nanodevices/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">No Evolutions for Corporations or Nanodevices</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Price's Equation describes quantitatively how the change in a average trait, in each generation, is equal to the covariance between that trait and fitness. Such covariance requires substantial variation in traits, substantial variation in fitness, and substantial correlation between the two - and then, to get large </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">cumulative</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> selection pressures, the correlation must have persisted over </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">many</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> generations with </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">high-fidelity</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> inheritance, continuing sources of new variation, and frequent birth of a significant fraction of the population. People think of \"evolution\" as something that automatically gets invoked where \"reproduction\" exists, but these other conditions may not be fulfilled - which is why corporations haven't evolved, and nanodevices probably won't.</span></li><li><a href=\"https://lessestwrong.com/lw/l5/evolving_to_extinction/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolving to Extinction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Contrary to a naive view that evolution works for the good of a species, evolution says that genes which outreproduce their alternative alleles increase in frequency within a gene pool. It is entirely possible for genes which \"harm\" the species to outcompete their alternatives in this way - indeed, it is entirely possible for a species to </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">evolve to extinction</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><a href=\"https://lessestwrong.com/lw/kw/the_tragedy_of_group_selectionism/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Tragedy of Group Selectionism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Describes a key case where some pre-1960s evolutionary biologists went wrong by </span><a href=\"https://wiki.lesswrong.com/wiki/anthropomorphizing\"><span class=\"by_qgdGA4ZEyW7zNdK84\">anthropomorphizing</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> evolution - in particular, Wynne-Edwards, Allee, and Brereton among others believed that predators would voluntarily restrain their breeding to avoid overpopulating their habitat. Since evolution does not usually do this sort of thing, their rationale was </span><a href=\"https://lessestwrong.com/tag/group-selection\"><span class=\"by_qgdGA4ZEyW7zNdK84\">group selection</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - populations that did this would survive better. But group selection is extremely difficult to make work mathematically, and an experiment under sufficiently extreme conditions to permit group selection, had rather different results.</span></li><li><a href=\"https://lessestwrong.com/lw/kz/fake_optimization_criteria/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fake Optimization Criteria</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - Why study evolution? For one thing - it lets us see an alien </span><a href=\"https://lessestwrong.com/tag/optimization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">optimization process</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> up close - lets us see the </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">real</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> consequence of optimizing </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">strictly</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> for an alien optimization criterion like inclusive genetic fitness. Humans, who try to persuade other humans to do things their way, think that this policy criterion ought to require predators to </span><a href=\"https://lessestwrong.com/tag/group-selection\"><span class=\"by_qgdGA4ZEyW7zNdK84\">restrain their breeding</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> to live in harmony with prey; the true result is something that humans find less aesthetic.</span></li><li><a href=\"https://lessestwrong.com/lw/kv/beware_of_stephen_j_gould/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Beware of Stephen J. Gould</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - A lot of people have gotten their grasp of evolutionary theory from Stephen J. Gould, a man who committed the moral equivalent of fraud in a way that is difficult to explain. At any rate, he severely misrepresented what evolutionary biologists believe, in the course of pretending to attack certain beliefs. One needs to clear from memory, as much as possible, not just everything that Gould positively stated but everything he seemed to imply the mainstream theory believed.</span></li><li><a href=\"https://lessestwrong.com/lw/l8/conjuring_an_evolution_to_serve_you/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Conjuring An Evolution To Serve You</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> - If you take the hens who lay the most eggs in each generation, and breed from them, you should get hens who lay more and more eggs. Sounds logical, right? But this selection may actually favor the most </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">dominant</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> hen, that pecked its way to the top of the pecking order at the expense of other hens. Such breeding programs produce hens that must be housed in individual cages, or they will peck each other to death. Jeff Skilling of Enron fancied himself an evolution-conjurer - summoning </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">the awesome power of evolution</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> to work for him - and so, every year, every Enron employee's performance would be evaluated, and the bottom 10% would get fired, and the top performers would get huge raises and bonuses.</span></li></ul>",
      "sections": [
        {
          "title": "Why be interested in evolution?",
          "anchor": "Why_be_interested_in_evolution_",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 2
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 2
        },
        {
          "title": "Summaries of Sequence's Posts on Evolution",
          "anchor": "Summaries_of_Sequence_s_Posts_on_Evolution",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 103,
    "description": {
      "markdown": "**Evolution** is \"*change in the heritable characteristics of biological populations over successive generations*\" ([Wikipedia](https://en.wikipedia.org/wiki/Evolution)). For posts about machine learning look [here](https://www.lesswrong.com/tag/machine-learning?showPostCount=false&useTagName=false).\n\n*Related:* [Biology](https://www.lesswrong.com/tag/biology?showPostCount=true&useTagName=true), [Evolutionary Psychology](https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&useTagName=true),\n\nThe sequence, [The Simple Math of Evolution](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8) provides a good introduction to LessWrong thinking about evolution.\n\nWhy be interested in evolution?\n===============================\n\nFirstly, evolution is a useful case study of humans' ability (or inability) to model the real world. This is because it has a single clear criterion (\"relative reproductive fitness\") which is selected (optimized) for:\n\n> *\"If we can't see clearly the result of a single monotone optimization criterion—if we can't even train ourselves to hear a single pure note—then how will we listen to an orchestra? How will we see that \"Always be selfish\" or \"Always obey the government\" are poor guiding principles for human beings to adopt—if we think that even optimizing genes for inclusive fitness will yield organisms which sacrifice reproductive opportunities in the name of social resource conservation?*\n\n> *To train ourselves to see clearly, we need simple practice cases\" --* Eliezer Yudkowsky*,* [*Fake Optimisation Criteria*](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/i6fKszWY6gLZSX2Ey)\n\nSecondly, much of rationality necessarily revolves around the human brain ([for](https://www.lesswrong.com/tag/transhumanism?usePostCount=false&useTagName=false) [now](https://www.lesswrong.com/tag/mind-uploading?showPostCount=false&useTagName=false)). An understanding of how it came into being can be very helpful both for understanding 'bugs' in the system (like superstimuli), and for explaining [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true), among others.\n\n> *A candy bar is a superstimulus: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.   A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment.  The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs. *  \n> *\\-\\-* Eliezer Yudkowsky, [Superstimuli and the Collapse of Western Civilisation](https://www.lesswrong.com/s/MH2b8NfWv22dBtrs8/p/Jq73GozjsuhdwMLEG)\n\nSee also\n--------\n\n*   [Evolution as alien god](https://lessestwrong.com/tag/evolution-as-alien-god)\n*   [Slowness of evolution](https://lessestwrong.com/tag/slowness-of-evolution)\n*   [Stupidity of evolution](https://lessestwrong.com/tag/stupidity-of-evolution)\n*   [Evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology)\n\nExternal links\n--------------\n\n*   [Richard Dawkins - The Selfish Gene](http://dl.dropbox.com/u/33627365/Scholarship/Selfish%20Gene%20-%20Dawkins.pdf) (PDF)\n\nSummaries of Sequence's Posts on Evolution\n------------------------------------------\n\n*The following are summaries of posts concerning evolution in the Eliezer's sequences:*\n\n*   [An Alien God](https://lessestwrong.com/lw/kr/an_alien_god/) \\- Evolution is awesomely powerful, unbelievably stupid, incredibly slow, monomaniacally singleminded, irrevocably splintered in focus, blindly shortsighted, and itself a completely accidental process. If evolution were a god, it would not be Jehovah, but H. P. Lovecraft's Azathoth, the blind idiot God burbling chaotically at the center of everything.\n*   [The Wonder of Evolution](https://lessestwrong.com/lw/ks/the_wonder_of_evolution/) \\- The wonder of the first replicator was not how amazingly well it replicated, but that a first replicator could arise, at all, by pure accident, in the primordial seas of Earth. That first replicator would undoubtedly be devoured in an instant by a sophisticated modern bacterium. Likewise, the wonder of evolution itself is not how *well* it works, but that a *brainless, accidentally occurring* [optimization process](https://lessestwrong.com/tag/optimization) can work *at all*. If you praise evolution for being such a wonderfully intelligent Creator, you're entirely missing the wonderful thing about it.\n*   [Evolutions Are Stupid (But Work Anyway)](https://lessestwrong.com/lw/kt/evolutions_are_stupid_but_work_anyway/) \\- Modern evolutionary theory gives us a definite picture of evolution's capabilities. If you praise evolution one millimeter higher than this, you are not scoring points against creationists, you are just being factually inaccurate. In particular we can calculate the probability and time for advantageous genes to rise to fixation. For example, a mutation conferring a 3% advantage would have only a 6% probability of surviving, and if it did so, would take 875 generations to rise to fixation in a population of 500,000 (on average).\n*   [Speed limit and complexity bound for evolution](https://lessestwrong.com/tag/speed-limit-and-complexity-bound-for-evolution) \\- It is widely understood that there is a limit on how fast evolution can accumulate information in a gene pool, and an upper bound on how much genetic information can be sustained against the degenerative pressure of copying errors. (But Yudkowsky's attempt to calculate an actual bound failed mathematically, so see the referenced summary of the discussion instead of the original blog post.)\n*   [Adaptation-Executers, not Fitness-Maximizers](https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/) \\- A central principle of evolutionary biology in general, and [evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology) in particular. If we regarded human taste buds as trying to *maximize fitness*, we might expect that, say, humans fed a diet too high in calories and too low in micronutrients, would begin to find lettuce delicious, and cheeseburgers distasteful. But it is better to regard taste buds as an *executing adaptation* \\- they are adapted to an ancestral environment in which calories, not micronutrients, were the limiting factor.\n*   [No Evolutions for Corporations or Nanodevices](https://lessestwrong.com/lw/l6/no_evolutions_for_corporations_or_nanodevices/) \\- Price's Equation describes quantitatively how the change in a average trait, in each generation, is equal to the covariance between that trait and fitness. Such covariance requires substantial variation in traits, substantial variation in fitness, and substantial correlation between the two - and then, to get large *cumulative* selection pressures, the correlation must have persisted over *many* generations with *high-fidelity* inheritance, continuing sources of new variation, and frequent birth of a significant fraction of the population. People think of \"evolution\" as something that automatically gets invoked where \"reproduction\" exists, but these other conditions may not be fulfilled - which is why corporations haven't evolved, and nanodevices probably won't.\n*   [Evolving to Extinction](https://lessestwrong.com/lw/l5/evolving_to_extinction/) \\- Contrary to a naive view that evolution works for the good of a species, evolution says that genes which outreproduce their alternative alleles increase in frequency within a gene pool. It is entirely possible for genes which \"harm\" the species to outcompete their alternatives in this way - indeed, it is entirely possible for a species to *evolve to extinction*.\n*   [The Tragedy of Group Selectionism](https://lessestwrong.com/lw/kw/the_tragedy_of_group_selectionism/) \\- Describes a key case where some pre-1960s evolutionary biologists went wrong by [anthropomorphizing](https://wiki.lesswrong.com/wiki/anthropomorphizing) evolution - in particular, Wynne-Edwards, Allee, and Brereton among others believed that predators would voluntarily restrain their breeding to avoid overpopulating their habitat. Since evolution does not usually do this sort of thing, their rationale was [group selection](https://lessestwrong.com/tag/group-selection) \\- populations that did this would survive better. But group selection is extremely difficult to make work mathematically, and an experiment under sufficiently extreme conditions to permit group selection, had rather different results.\n*   [Fake Optimization Criteria](https://lessestwrong.com/lw/kz/fake_optimization_criteria/) \\- Why study evolution? For one thing - it lets us see an alien [optimization process](https://lessestwrong.com/tag/optimization) up close - lets us see the *real* consequence of optimizing *strictly* for an alien optimization criterion like inclusive genetic fitness. Humans, who try to persuade other humans to do things their way, think that this policy criterion ought to require predators to [restrain their breeding](https://lessestwrong.com/tag/group-selection) to live in harmony with prey; the true result is something that humans find less aesthetic.\n*   [Beware of Stephen J. Gould](https://lessestwrong.com/lw/kv/beware_of_stephen_j_gould/) \\- A lot of people have gotten their grasp of evolutionary theory from Stephen J. Gould, a man who committed the moral equivalent of fraud in a way that is difficult to explain. At any rate, he severely misrepresented what evolutionary biologists believe, in the course of pretending to attack certain beliefs. One needs to clear from memory, as much as possible, not just everything that Gould positively stated but everything he seemed to imply the mainstream theory believed.\n*   [Conjuring An Evolution To Serve You](https://lessestwrong.com/lw/l8/conjuring_an_evolution_to_serve_you/) \\- If you take the hens who lay the most eggs in each generation, and breed from them, you should get hens who lay more and more eggs. Sounds logical, right? But this selection may actually favor the most *dominant* hen, that pecked its way to the top of the pecking order at the expense of other hens. Such breeding programs produce hens that must be housed in individual cages, or they will peck each other to death. Jeff Skilling of Enron fancied himself an evolution-conjurer - summoning *the awesome power of evolution* to work for him - and so, every year, every Enron employee's performance would be evaluated, and the bottom 10% would get fired, and the top performers would get huge raises and bonuses."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4R8JYu4QF2FqzJxE5",
    "name": "Heuristics & Biases",
    "core": null,
    "slug": "heuristics-and-biases",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Heuristics </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">and</span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> Biases</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> are the</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_nLbwLhBaQeG6tCNDN\">ways human reasoning differs from a theoretical ideal agent, due to reasoning shortcuts that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">don'</span><span class=\"by_nLbwLhBaQeG6tCNDN\">t always work (heuristics)</span><span class=\"by_nmk3nLpQE89dMRzzN\"> and </span><span class=\"by_nLbwLhBaQeG6tCNDN\">systematic errors (biases)</span><span class=\"by_nmk3nLpQE89dMRzzN\">.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">: </span><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Affect Heuristic</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/confirmation-bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Confirmation Bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/fallacies\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fallacies</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Predictably Wrong</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/rationality?showPostCount=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationality</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Your Intuitions Are Not Magic</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bias</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/heuristic\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Heuristic</span></a></p><h1 id=\"Basics\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Basics</span></h1><p><a href=\"https://www.lesswrong.com/posts/jnZbHi873v9vcpGpZ/what-s-a-bias-again\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">“</span><span class=\"by_Xn6ACr6Cua8upALWQ\">Cognitive </span><span class=\"by_qgdGA4ZEyW7zNdK84\">biases”</span></span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by </span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">the shape of our own mental machinery</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">. For example, our mental processes might be evolutionarily adapted to specifically believe some things that arent true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.</span></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Our brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“(</span><span class=\"by_Xn6ACr6Cua8upALWQ\">cognitive) bias.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> Not to be confused with the colloquial </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“that</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> person is biased,</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> which just means </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“that</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> person has a skewed or prejudiced attitude toward something.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">”</span></span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">A bias is an obstacle to our goal of obtaining truth, and thus </span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">in our way</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">We are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">we'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">re curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them </span><span class=\"by_qgdGA4ZEyW7zNdK84\">“biases”</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> or not.</span></span></p><p><a href=\"https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">It'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s also useful to know the kinds of faults human brains are prone to, in the same way </span><span class=\"by_qgdGA4ZEyW7zNdK84\">it'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s useful to know that your </span><span class=\"by_qgdGA4ZEyW7zNdK84\">car'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s brakes are a little gummy (so you </span><span class=\"by_qgdGA4ZEyW7zNdK84\">don'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t sail through a red light and into an 18-wheeler).</span></span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The Sequence, </span><a href=\"https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Predictably Wrong</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> offers an excellent introduction to the topic for those who are not familiar.</span></span></p><h1 id=\"Wait_a_minute____fallacies__biases__heuristics____what_s_the_difference__\"><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Wait a minute... fallacies, biases, heuristics... </span><span class=\"by_qgdGA4ZEyW7zNdK84\">what'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s the difference??</span></span></h1><p><span class=\"by_Xn6ACr6Cua8upALWQ\">While a </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">bias</span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> is always wrong, a </span><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">heuristic </span></strong><span><span class=\"by_Xn6ACr6Cua8upALWQ\">is just a shortcut which may or may not give you an accurate answer. Just because you know complex mathematical methods for precisely calculating the flight of objects through space </span><span class=\"by_qgdGA4ZEyW7zNdK84\">doesn'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t mean you should be using them to play volleyball. Which is to say, heuristics are necessary for actually getting anything done. But because they are just approximations they frequently </span></span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">produce </span></i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">biases, which is where the problem lies. </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Fallacy\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> is often used to mean a very similar thing as bias on LessWrong. [Needs better clarification]</span></span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">A good example of a heuristic is the </span><a href=\"https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">affect heuristic</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">-- people tend to guess unknown traits about people or things </span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">based on</span></i><span class=\"by_Xn6ACr6Cua8upALWQ\"> the perceived goodness of badness of known traits. In some circumstances this is a useful shortcut-- you may like to assume, for instance, that people who are good singers are more likely to be good dancers, too. However, it also frequently produces (unconscious) biases-- a bias towards believing that people who are tall and good looking have better moral character, for instance.</span></p><h1 id=\"So_if_I_learn_all_the_biases__I_can_conquer_the_world_with_my_superior_intellect_\"><span class=\"by_Xn6ACr6Cua8upALWQ\">So if I learn all the biases, I can conquer the world with my superior intellect?</span></h1><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Well, no. If it were that easy we </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wouldn'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t need a community initially dedicated to overcoming bias (the name of </span></span><a href=\"https://www.overcomingbias.com/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">the blog which this website grew out of</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">). Unfortunately, </span><a href=\"https://www.iejme.com/article/university-students-knowledge-and-biases-in-conditional-probability-reasoning\"><span><span class=\"by_Xn6ACr6Cua8upALWQ\">learning about a bias alone </span><span class=\"by_qgdGA4ZEyW7zNdK84\">doesn'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t seem to improve your ability to avoid it in real life</span></span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\">. </span><span class=\"by_qgdGA4ZEyW7zNdK84\">There'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s also the (major) issue that </span></span><a href=\"https://www.lesswrong.com/posts/AdYdLP2sRqPMoe8fb/knowing-about-biases-can-hurt-people\"><span class=\"by_Xn6ACr6Cua8upALWQ\">knowing about biases can hurt people</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">. Instead of being purely focused on removing negative habits, there is now a major focus at LessWrong to implementing </span><a href=\"https://www.lesswrong.com/tag/techniques?showPostCount=false&amp;useTagName=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">positive habits</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">. These are skills such as how to update (change your mind) the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more.</span></p>",
      "sections": [
        {
          "title": "Basics",
          "anchor": "Basics",
          "level": 1
        },
        {
          "title": "Wait a minute... fallacies, biases, heuristics... what's the difference??",
          "anchor": "Wait_a_minute____fallacies__biases__heuristics____what_s_the_difference__",
          "level": 1
        },
        {
          "title": "So if I learn all the biases, I can conquer the world with my superior intellect?",
          "anchor": "So_if_I_learn_all_the_biases__I_can_conquer_the_world_with_my_superior_intellect_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 186,
    "description": {
      "markdown": "**Heuristics** and **Biases** are the ways human reasoning differs from a theoretical ideal agent, due to reasoning shortcuts that don't always work (heuristics) and systematic errors (biases).\n\n*See also*: [Affect Heuristic](https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&useTagName=true), [Confirmation Bias](https://www.lesswrong.com/tag/confirmation-bias), [Fallacies](https://www.lesswrong.com/tag/fallacies), [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM), [Rationality](https://www.lesswrong.com/tag/rationality?showPostCount=true), [Your Intuitions Are Not Magic](https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic), [Bias](https://lessestwrong.com/tag/bias), [Heuristic](https://lessestwrong.com/tag/heuristic)\n\nBasics\n======\n\n[“Cognitive biases”](https://www.lesswrong.com/posts/jnZbHi873v9vcpGpZ/what-s-a-bias-again) are those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by *the shape of our own mental machinery*. For example, our mental processes might be evolutionarily adapted to specifically believe some things that arent true, so that we could win political arguments in a tribal context. Or the mental machinery might be adapted not to particularly care whether something is true, such as when we feel the urge to believe what others believe to get along socially. Or the bias may be a side-effect of a useful reasoning heuristic. The availability heuristic is not itself a bias, but it gives rise to them; the machinery uses an algorithm (give things more evidential weight if they come to mind more readily) that does some good cognitive work but also produces systematic errors.\n\nOur brains are doing something wrong, and after a lot of experimentation and/or heavy thinking, someone identifies the problem verbally and concretely; then we call it a “(cognitive) bias.” Not to be confused with the colloquial “that person is biased,” which just means “that person has a skewed or prejudiced attitude toward something.”\n\nA bias is an obstacle to our goal of obtaining truth, and thus *in our way*.\n\nWe are here to pursue the great human quest for truth: for we have desperate need of the knowledge, and besides, we're curious. To this end let us strive to overcome whatever obstacles lie in our way, whether we call them “biases” or not.\n\n[It's also useful to know the kinds of faults human brains are prone to, in the same way it's useful to know that your car's brakes are a little gummy (so you don't sail through a red light and into an 18-wheeler).](https://www.lesswrong.com/posts/Psp8ZpYLCDJjshpRb/your-intuitions-are-not-magic)\n\nThe Sequence, [Predictably Wrong](https://www.lesswrong.com/s/5g5TkQTe9rmPS5vvM), offers an excellent introduction to the topic for those who are not familiar.\n\nWait a minute... fallacies, biases, heuristics... what's the difference??\n=========================================================================\n\nWhile a **bias** is always wrong, a **heuristic** is just a shortcut which may or may not give you an accurate answer. Just because you know complex mathematical methods for precisely calculating the flight of objects through space doesn't mean you should be using them to play volleyball. Which is to say, heuristics are necessary for actually getting anything done. But because they are just approximations they frequently *produce* biases, which is where the problem lies. \"Fallacy\" is often used to mean a very similar thing as bias on LessWrong. \\[Needs better clarification\\]\n\nA good example of a heuristic is the [affect heuristic](https://www.lesswrong.com/tag/affect-heuristic?showPostCount=true&useTagName=true)\\-\\- people tend to guess unknown traits about people or things *based on* the perceived goodness of badness of known traits. In some circumstances this is a useful shortcut-- you may like to assume, for instance, that people who are good singers are more likely to be good dancers, too. However, it also frequently produces (unconscious) biases-- a bias towards believing that people who are tall and good looking have better moral character, for instance.\n\nSo if I learn all the biases, I can conquer the world with my superior intellect?\n=================================================================================\n\nWell, no. If it were that easy we wouldn't need a community initially dedicated to overcoming bias (the name of [the blog which this website grew out of](https://www.overcomingbias.com/)). Unfortunately, [learning about a bias alone doesn't seem to improve your ability to avoid it in real life](https://www.iejme.com/article/university-students-knowledge-and-biases-in-conditional-probability-reasoning). There's also the (major) issue that [knowing about biases can hurt people](https://www.lesswrong.com/posts/AdYdLP2sRqPMoe8fb/knowing-about-biases-can-hurt-people). Instead of being purely focused on removing negative habits, there is now a major focus at LessWrong to implementing [positive habits](https://www.lesswrong.com/tag/techniques?showPostCount=false&useTagName=false). These are skills such as how to update (change your mind) the correct amount in response to evidence, how to resolve disagreements with others, how to introspect, and many more."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "L3NcKBNTvQaFXwv9u",
    "name": "Paradoxes",
    "core": false,
    "slug": "paradoxes",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Paradoxes</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are circumstances in mathematics, philosophy, or other domains that present a seeming contradiction, which may be difficult to resolve.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related: </span></strong><a href=\"https://www.lesswrong.com/tag/sleeping-beauty-paradox\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Sleeping Beauty Paradox</span></a><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span></strong><a href=\"https://www.lesswrong.com/tag/great-filter\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Great Filter</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/newcomb-s-problem\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Newcomb's Problem</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span></strong><a href=\"https://www.lesswrong.com/tag/exercises-problem-sets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Exercises / Problem-Sets</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/free-will\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Free Will</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 48,
    "description": {
      "markdown": "**Paradoxes** are circumstances in mathematics, philosophy, or other domains that present a seeming contradiction, which may be difficult to resolve.\n\n**Related:** [Sleeping Beauty Paradox](https://www.lesswrong.com/tag/sleeping-beauty-paradox)**,** [Great Filter](https://www.lesswrong.com/tag/great-filter), [Newcomb's Problem](https://www.lesswrong.com/tag/newcomb-s-problem),  [Exercises / Problem-Sets](https://www.lesswrong.com/tag/exercises-problem-sets), [Free Will](https://www.lesswrong.com/tag/free-will)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cpBfacd22cJsm5fuL",
    "name": "Hypocrisy",
    "core": false,
    "slug": "hypocrisy",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">Formally, </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">hypocrisy</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the act of claiming to motives, morals and standards one does not possess. Informally, it refers to not living up to the standards that one espouses, whether or not one sincerely believes those standards.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></h2><ul><li><a href=\"http://www.overcomingbias.com/2006/12/resolving_your_.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Resolving Your Hypocrisy</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"https://lessestwrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-deception: </span><span class=\"by_HoGziwmhpMGqGeWZy\">Hypocrisy</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">or Akrasia?</span></span></a></li></ul><h2 id=\"See_also\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/self-deception\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-deception</span></a></li><li><a href=\"http://lesswrong.com/tag/motivated-reasoning\"><span class=\"by_HoGziwmhpMGqGeWZy\">Motivated Reasoning</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 17,
    "description": {
      "markdown": "Formally, **hypocrisy** is the act of claiming to motives, morals and standards one does not possess. Informally, it refers to not living up to the standards that one espouses, whether or not one sincerely believes those standards.\n\nBlog posts\n----------\n\n*   [Resolving Your Hypocrisy](http://www.overcomingbias.com/2006/12/resolving_your_.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Self-deception: Hypocrisy or Akrasia?](https://lessestwrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/)\n\nSee also\n--------\n\n*   [Self-deception](https://lessestwrong.com/tag/self-deception)\n*   [Motivated Reasoning](http://lesswrong.com/tag/motivated-reasoning)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "4Kcm4etxAJjmeDkHP",
    "name": "Book Reviews",
    "core": null,
    "slug": "book-reviews",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_EQNTWXLKMeWMp2FQS\">Book Reviews</span></strong><span class=\"by_EQNTWXLKMeWMp2FQS\"> on LessWrong are different from normal book reviews; they summarize and respond to a book's core ideas first, and judge whether you should read it second. A good book review sometimes distills the book's ideas so well that you no longer need to read the book.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Reviews engage with the perspective of the author, someone who has put in the effort to record their understanding of the world. Some of the best essays on LessWrong are reviews that teach us about </span><a href=\"https://www.lesswrong.com/tag/history\"><span class=\"by_EQNTWXLKMeWMp2FQS\">history</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, </span><a href=\"https://www.lesswrong.com/tag/replication-crisis\"><span class=\"by_EQNTWXLKMeWMp2FQS\">psychology</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, </span><a href=\"https://www.lesswrong.com/tag/biology\"><span class=\"by_EQNTWXLKMeWMp2FQS\">biology</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, or some other area where the author has developed a detailed understanding of a phenomena that few others have ever reached, and the essay writer engages with that perspective from our rationalist perspective.</span></p><p><span class=\"by_EQNTWXLKMeWMp2FQS\">Good book reviews embody the virtues of </span><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><u><span class=\"by_EQNTWXLKMeWMp2FQS\">scholarship</span></u></a><span class=\"by_EQNTWXLKMeWMp2FQS\">, </span><a href=\"https://www.lesswrong.com/tag/curiosity\"><span class=\"by_EQNTWXLKMeWMp2FQS\">curiosity</span></a><span class=\"by_EQNTWXLKMeWMp2FQS\"> and perspective-taking.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/epistemic-review\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Epistemic Review</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/summaries\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Summaries</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/literature-reviews\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Literature Reviews</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/lesswrong-review\"><span class=\"by_sKAL2jzfkYkDbQmx9\">LessWrong Review</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 300,
    "description": {
      "markdown": "**Book Reviews** on LessWrong are different from normal book reviews; they summarize and respond to a book's core ideas first, and judge whether you should read it second. A good book review sometimes distills the book's ideas so well that you no longer need to read the book.\n\nReviews engage with the perspective of the author, someone who has put in the effort to record their understanding of the world. Some of the best essays on LessWrong are reviews that teach us about [history](https://www.lesswrong.com/tag/history), [psychology](https://www.lesswrong.com/tag/replication-crisis), [biology](https://www.lesswrong.com/tag/biology), or some other area where the author has developed a detailed understanding of a phenomena that few others have ever reached, and the essay writer engages with that perspective from our rationalist perspective.\n\nGood book reviews embody the virtues of [scholarship](https://www.lesswrong.com/tag/scholarship-and-learning), [curiosity](https://www.lesswrong.com/tag/curiosity) and perspective-taking.\n\n**Related Pages:** [Epistemic Review](https://www.lesswrong.com/tag/epistemic-review), [Summaries](https://www.lesswrong.com/tag/summaries), [Literature Reviews](https://www.lesswrong.com/tag/literature-reviews), [LessWrong Review](https://www.lesswrong.com/tag/lesswrong-review)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "KWFhr6A2dHEb6wmWJ",
    "name": "Compartmentalization",
    "core": null,
    "slug": "compartmentalization",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_LoykQRMTxJFxwwdPy\">Compartmentalization</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is keeping information and processes within your mind segregated, especially in ways that keep knowledge possessed by some of your reasoning processes being accessed by other processes.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">From an alternative angle, one can think of compartmentalizing one's different activities or domains from each other. when one couple the skills or habits from one to another, e.g.</span><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the religious scientist who does not apply</span><span class=\"by_LoykQRMTxJFxwwdPy\"> scientific </span><span class=\"by_qgdGA4ZEyW7zNdK84\">thinking outside</span><span class=\"by_LoykQRMTxJFxwwdPy\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">lab.&nbsp;</span></span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">One might even have</span><span class=\"by_LoykQRMTxJFxwwdPy\"> excellent epistemological performance in one domain and terrible performance in others.</span></span></p><h2 id=\"See_also\"><span class=\"by_LoykQRMTxJFxwwdPy\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/semantic-stopsign\"><span class=\"by_LoykQRMTxJFxwwdPy\">Semantic stopsign</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/anti-epistemology\"><span class=\"by_LoykQRMTxJFxwwdPy\">Anti-epistemology</span></a></li><li><a href=\"https://www.lesswrong.com/tag/cached-thought\"><span class=\"by_LoykQRMTxJFxwwdPy\">Cached thought</span></a></li><li><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_LoykQRMTxJFxwwdPy\">Shut up and multiply</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/bite-the-bullet\"><span class=\"by_LoykQRMTxJFxwwdPy\">Bite the bullet</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/absurdity-heuristic\"><span class=\"by_LoykQRMTxJFxwwdPy\">Absurdity heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/dangerous-knowledge\"><span class=\"by_LoykQRMTxJFxwwdPy\">Dangerous knowledge</span></a></li><li><a href=\"https://www.lesswrong.com/tag/general-knowledge\"><span class=\"by_LoykQRMTxJFxwwdPy\">General knowledge</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/understanding\"><span class=\"by_LoykQRMTxJFxwwdPy\">Understanding</span></a></li><li><a href=\"https://www.lesswrong.com/tag/alief\"><span class=\"by_LoykQRMTxJFxwwdPy\">Alief</span></a></li><li><a href=\"https://www.lesswrong.com/tag/aversion-ugh-fields\"><span class=\"by_LoykQRMTxJFxwwdPy\">Ugh field</span></a></li><li><a href=\"https://www.lesswrong.com/tag/distinctions\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Distinctions</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 15,
    "description": {
      "markdown": "**Compartmentalization** is keeping information and processes within your mind segregated, especially in ways that keep knowledge possessed by some of your reasoning processes being accessed by other processes.\n\nFrom an alternative angle, one can think of compartmentalizing one's different activities or domains from each other. when one couple the skills or habits from one to another, e.g., the religious scientist who does not apply scientific thinking outside the lab. \n\nOne might even have excellent epistemological performance in one domain and terrible performance in others.\n\nSee also\n--------\n\n*   [Semantic stopsign](https://www.lesswrong.com/tag/semantic-stopsign), [Anti-epistemology](https://www.lesswrong.com/tag/anti-epistemology)\n*   [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), [Bite the bullet](https://www.lesswrong.com/tag/bite-the-bullet), [Absurdity heuristic](https://www.lesswrong.com/tag/absurdity-heuristic)\n*   [Dangerous knowledge](https://www.lesswrong.com/tag/dangerous-knowledge)\n*   [General knowledge](https://www.lesswrong.com/tag/general-knowledge), [Understanding](https://www.lesswrong.com/tag/understanding)\n*   [Alief](https://www.lesswrong.com/tag/alief)\n*   [Ugh field](https://www.lesswrong.com/tag/aversion-ugh-fields)\n*   [Distinctions](https://www.lesswrong.com/tag/distinctions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yEs5Tdwfw5Zw8yGWC",
    "name": "Wireheading",
    "core": false,
    "slug": "wireheading",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XgYW5s8njaYrtyP7q\">Wireheading</span></strong><span><span class=\"by_XgYW5s8njaYrtyP7q\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">is </span><span class=\"by_woC2b5rav5sGrAo3E\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">artificial stimulation</span><span class=\"by_woC2b5rav5sGrAo3E\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the brain to experience pleasure, usually through the direct stimulation of an individual's brain's reward or pleasure center with electrical current. It can also be used in a more expanded sense, to refer to any kind of method that produces a form of </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">counterfeit utility</span></i><span><span class=\"by_woC2b5rav5sGrAo3E\"> by directly </span><span class=\"by_qgdGA4ZEyW7zNdK84\">maximizing a good feeling, but that fails to realize what we value.</span></span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related pages:</span></strong><i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></i><a href=\"https://www.lesswrong.com/tag/complexity-of-value\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Complexity of Value</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">,</span><i><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span></i><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s Law</span></span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/inner-alignment\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Inner Alignment</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In both thought experiments and </span><a href=\"http://www.mindhacks.com/blog/2008/09/erotic_selfstimulat.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">laboratory experiments</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> direct stimulation of the brain’s reward center makes the individual feel happy. In theory, wireheading with a powerful enough current would be the most pleasurable experience imaginable. There is some evidence that </span><a href=\"https://lessestwrong.com/lw/1lb/are_wireheads_happy/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">reward is distinct from pleasure</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">and that most currently hypothesized forms of wireheading just motivate a person to continue the wirehead experience, not to feel happy. However, there seems to be no reason to believe that a different form of wireheading which does create subjective pleasure could not be found. The possibility of wireheading raises difficult ethical questions for </span></span><a href=\"https://lessestwrong.com/tag/hedonism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">those who believe that morality is based on human happiness</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. A civilization of wireheads \"blissing out\" all day while being fed and maintained by robots would be a state of maximum happiness, but such a civilization would have no art, love, scientific discovery, or any of the other things humans find valuable.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">If we take wireheading as a more general form of producing counterfeit utility, there are many examples of ways of directly stimulating of the reward and pleasure centers of the brain, without actually engaging in valuable experiences. Cocaine, heroin, cigarettes and gambling are all examples of current methods of directly achieving pleasure or reward, but can be seen by many as lacking much of what we value and are potentially extremely detrimental. </span><a href=\"https://en.wikipedia.org/wiki/Steve_Omohundro\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Steve Omohundro</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> argues</span><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> that: “An important class of vulnerabilities arises when the subsystems for measuring utility become corrupted. Human pleasure may be thought of as the experiential correlate of an assessment of high utility. But pleasure is mediated by neurochemicals and these are subject to manipulation.”</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Wireheading is also an illustration of the complexities of creating a </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Friendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Any AGI naively programmed to increase human happiness could devote its energies to wireheading people, possibly without their consent, in preference to any other goals. Equivalent problems arise for any simple attempt to create AGIs who care directly about human feelings (\"love\", \"compassion\", \"excitement\", etc). An AGI could wirehead people to feel in love all the time, but this wouldn’t correctly realize what we value when we say love is a virtue. For Omohundro, because exploiting those vulnerabilities in our subsystems for measuring utility is much easier than truly realizing our values, a wrongly designed AGI would most certainly prefer to wirehead humanity instead of pursuing human values. In addition, an AGI itself could be vulnerable to wirehead and would need to implement “police forces” or “immune systems” to ensure its measuring system doesn’t become corrupted by trying to produce counterfeit utility.</span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Steve Omohundro's paper, section 4 deals with vulnerabilities to counterfeit utility and wireheading</span></a></li><li><a href=\"https://lessestwrong.com/tag/hedonism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hedonism</span></a></li><li><a href=\"https://lessestwrong.com/tag/complexity-of-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of value</span></a></li><li><a href=\"https://lessestwrong.com/tag/wanting-and-liking\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Wanting and liking</span></a></li><li><a href=\"https://lessestwrong.com/tag/near-far-thinking\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Near/far thinking</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Hedonium\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hedonium</span></a></li><li><a href=\"https://lessestwrong.com/tag/abolitionism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Abolitionism</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_Yw87LwbkMwGpB6hFh\">External links</span></h2><ul><li><a href=\"https://www.hedweb.com/wirehead/index.html\"><span class=\"by_Yw87LwbkMwGpB6hFh\">Wirehead Hedonism versus paradise engineering</span></a><span class=\"by_Yw87LwbkMwGpB6hFh\"> by David Pearce</span></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 27,
    "description": {
      "markdown": "**Wireheading** is the artificial stimulation of the brain to experience pleasure, usually through the direct stimulation of an individual's brain's reward or pleasure center with electrical current. It can also be used in a more expanded sense, to refer to any kind of method that produces a form of *counterfeit utility* by directly maximizing a good feeling, but that fails to realize what we value.\n\n**Related pages:**  [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value),  [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law), [Inner Alignment](https://www.lesswrong.com/tag/inner-alignment)\n\nIn both thought experiments and [laboratory experiments](http://www.mindhacks.com/blog/2008/09/erotic_selfstimulat.html) direct stimulation of the brain’s reward center makes the individual feel happy. In theory, wireheading with a powerful enough current would be the most pleasurable experience imaginable. There is some evidence that [reward is distinct from pleasure](https://lessestwrong.com/lw/1lb/are_wireheads_happy/), and that most currently hypothesized forms of wireheading just motivate a person to continue the wirehead experience, not to feel happy. However, there seems to be no reason to believe that a different form of wireheading which does create subjective pleasure could not be found. The possibility of wireheading raises difficult ethical questions for [those who believe that morality is based on human happiness](https://lessestwrong.com/tag/hedonism). A civilization of wireheads \"blissing out\" all day while being fed and maintained by robots would be a state of maximum happiness, but such a civilization would have no art, love, scientific discovery, or any of the other things humans find valuable.\n\nIf we take wireheading as a more general form of producing counterfeit utility, there are many examples of ways of directly stimulating of the reward and pleasure centers of the brain, without actually engaging in valuable experiences. Cocaine, heroin, cigarettes and gambling are all examples of current methods of directly achieving pleasure or reward, but can be seen by many as lacking much of what we value and are potentially extremely detrimental. [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro) argues[1](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf) that: “An important class of vulnerabilities arises when the subsystems for measuring utility become corrupted. Human pleasure may be thought of as the experiential correlate of an assessment of high utility. But pleasure is mediated by neurochemicals and these are subject to manipulation.”\n\nWireheading is also an illustration of the complexities of creating a [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI). Any AGI naively programmed to increase human happiness could devote its energies to wireheading people, possibly without their consent, in preference to any other goals. Equivalent problems arise for any simple attempt to create AGIs who care directly about human feelings (\"love\", \"compassion\", \"excitement\", etc). An AGI could wirehead people to feel in love all the time, but this wouldn’t correctly realize what we value when we say love is a virtue. For Omohundro, because exploiting those vulnerabilities in our subsystems for measuring utility is much easier than truly realizing our values, a wrongly designed AGI would most certainly prefer to wirehead humanity instead of pursuing human values. In addition, an AGI itself could be vulnerable to wirehead and would need to implement “police forces” or “immune systems” to ensure its measuring system doesn’t become corrupted by trying to produce counterfeit utility.\n\nSee also\n--------\n\n*   [Steve Omohundro's paper, section 4 deals with vulnerabilities to counterfeit utility and wireheading](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)\n*   [Hedonism](https://lessestwrong.com/tag/hedonism)\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Wanting and liking](https://lessestwrong.com/tag/wanting-and-liking)\n*   [Near/far thinking](https://lessestwrong.com/tag/near-far-thinking)\n*   [Hedonium](https://wiki.lesswrong.com/wiki/Hedonium)\n*   [Abolitionism](https://lessestwrong.com/tag/abolitionism)\n\nExternal links\n--------------\n\n*   [Wirehead Hedonism versus paradise engineering](https://www.hedweb.com/wirehead/index.html) by David Pearce"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Q9ASuEEoJWxT3RLMT",
    "name": "Animal Welfare",
    "core": false,
    "slug": "animal-welfare",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 44,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "dPPATLhRmhdJtJM2t",
    "name": "Decision Theory",
    "core": null,
    "slug": "decision-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Decision theory</span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\"> is </span><span class=\"by_mPipmBTniuABY5PQy\">the study of</span><span class=\"by_qf77EiaoMw7tH3GSr\"> principles and algorithms for making correct </span><span class=\"by_qgdGA4ZEyW7zNdK84\">decisions—</span><span class=\"by_qf77EiaoMw7tH3GSr\">that </span><span class=\"by_cn4SiEmqWbu7K9em5\">is,</span><span class=\"by_qf77EiaoMw7tH3GSr\"> decisions that allow </span><span class=\"by_cn4SiEmqWbu7K9em5\">an agent </span><span class=\"by_qf77EiaoMw7tH3GSr\">to achieve better outcomes </span><span class=\"by_mPipmBTniuABY5PQy\">with respect</span><span class=\"by_qf77EiaoMw7tH3GSr\"> to</span><span class=\"by_cn4SiEmqWbu7K9em5\"> its</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">goals. Every action at least implicitly represents a decision under uncertainty: in a state of partial knowledge, something has to be done, even if that something turns out to be nothing (call it \"the null action\"). Even if you don't know how you make decisions, decisions do get made, and so there has to be some underlying mechanism. What is it? And how can it be done better? Decision theory has the answers.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Note: this page needs to be updated with content regarding Functional Decision Theory, the latest theory from MIRI.</span></i></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related: </span></i><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Game Theory</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/robust-agents?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Robust Agents</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Utility Functions</span></a></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A core idea in decision theory is that of </span><a href=\"https://lessestwrong.com/tag/expected-utility\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">expected utility</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\"> maximization</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, usually intractable to directly calculate in practice, but an invaluable theoretical concept. An agent assigns utility to every possible outcome: a real number representing the goodness or desirability of that outcome. The mapping of outcomes to utilities is called the agent's </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">utility function</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. (The utility function is said to be invariant under affine transformations: that is, the utilities can be scaled or translated by a constant while resulting in all the same decisions.) For every action that the agent could take, sum over the utilities of the various possible outcomes weighted by their probability: this is the </span><a href=\"https://lessestwrong.com/tag/expected-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">expected</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> utility of the action, and the action with the highest expected utility is to be chosen.</span></p><h2 id=\"Thought_experiments\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Thought experiments</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The limitations and pathologies of decision theories can be analyzed by considering the decisions they suggest in the certain idealized situations that stretch the limits of decision theory's applicability. Some of the thought experiments more frequently discussed on </span><a href=\"https://wiki.lesswrong.com/wiki/LW\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LW</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> include:</span></p><ul><li><a href=\"https://lessestwrong.com/tag/newcomb-s-problem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Newcomb's problem</span></a></li><li><a href=\"https://lessestwrong.com/tag/counterfactual-mugging\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Counterfactual mugging</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Parfit's hitchhiker</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Smoker's_lesion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Smoker's lesion</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Absentminded_driver\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Absentminded driver</span></a></li><li><a href=\"https://lessestwrong.com/tag/sleeping-beauty-paradox\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sleeping Beauty problem</span></a></li><li><a href=\"https://lessestwrong.com/tag/prisoner-s-dilemma\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prisoner's dilemma</span></a></li><li><a href=\"https://lessestwrong.com/tag/pascal-s-mugging\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pascal's mugging</span></a></li></ul><h2 id=\"Commonly_discussed_decision_theories\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Commonly discussed decision theories</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Standard theories well-known in academia:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">CDT, </span><a href=\"http://en.wikipedia.org/wiki/Causal_decision_theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Causal Decision Theory</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">EDT, </span><a href=\"http://en.wikipedia.org/wiki/Evidential_decision_theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evidential Decision Theory</span></a></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Theories invented by researchers associated with </span><a href=\"https://wiki.lesswrong.com/wiki/MIRI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">MIRI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and LW:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">FDT: </span><a href=\"https://intelligence.org/2017/10/22/fdt/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Functional Decision Theory</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">TDT, </span><a href=\"https://lessestwrong.com/tag/timeless-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Timeless Decision Theory</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">UDT, </span><a href=\"https://lessestwrong.com/tag/updateless-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Updateless Decision Theory</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">ADT: </span><a href=\"https://lessestwrong.com/tag/ambient-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Ambient Decision Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (a variant of UDT)</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">FDT: </span><a href=\"https://intelligence.org/files/DeathInDamascus.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cheating Death in Damascus</span></a></li></ul><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Other</span><span class=\"by_mPipmBTniuABY5PQy\"> decision </span><span class=\"by_qgdGA4ZEyW7zNdK84\">theories are listed in </span></span><a href=\"https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A comprehensive list of decision theories</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Blog posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/l4/terminal_values_and_instrumental_values/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Terminal Values and Instrumental Values</span></a></li><li><a href=\"https://lessestwrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theories: A Less Wrong Primer</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by orthonormal</span></li><li><a href=\"https://lessestwrong.com/lw/gu1/decision_theory_faq/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory FAQ</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by lukeprog and crazy88</span></li></ul><h2 id=\"Sequence_by_AnnaSalamon\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sequence by </span><a href=\"https://wiki.lesswrong.com/wiki/AnnaSalamon\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AnnaSalamon</span></a></h2><ul><li><a href=\"https://lessestwrong.com/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision theory: An outline of some upcoming posts</span></a></li><li><a href=\"https://lessestwrong.com/lw/16i/confusion_about_newcomb_is_confusion_about/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Confusion about Newcomb is confusion about counterfactuals</span></a></li><li><a href=\"https://lessestwrong.com/lw/174/decision_theory_why_we_need_to_reduce_could_would/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why we need to reduce “could”, “would”, “should”</span></a></li><li><a href=\"https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives</span></a></li></ul><h2 id=\"Sequence_by_orthonormal__Decision_Theories__A_Semi_Formal_Analysis_\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Sequence by </span><a href=\"http://lesswrong.com/user/orthonormal/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">orthonormal</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (Decision Theories: A Semi-Formal Analysis)</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/aq9/decision_theories_a_less_wrong_primer/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Part 0: Decision Theories: A Less Wrong Primer</span></a></li><li><a href=\"https://lessestwrong.com/lw/axl/decision_theories_a_semiformal_analysis_part_i/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Part I: The Problem with Naive Decision Theory</span></a></li><li><a href=\"https://lessestwrong.com/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Part II: Causal Decision Theory and Substitution</span></a></li><li><a href=\"https://lessestwrong.com/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Part III: Formalizing Timeless Decision Theory</span></a></li></ul><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Instrumental_rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental rationality</span></a></li><li><a href=\"https://lessestwrong.com/tag/causality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Causality</span></a></li><li><a href=\"https://lessestwrong.com/tag/expected-utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Expected utility</span></a></li><li><a href=\"https://lessestwrong.com/tag/evidential-decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evidential Decision Theory</span></a></li><li><a href=\"https://lessestwrong.com/tag/timeless-decision-theory\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Timeless</span><span class=\"by_qf77EiaoMw7tH3GSr\"> decision </span><span class=\"by_qgdGA4ZEyW7zNdK84\">theory</span></span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/updateless-decision-theory\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Updateless</span><span class=\"by_qxJ28GN72aiJu96iF\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">decision theory</span></span></a></li><li><a href=\"https://lessestwrong.com/tag/aixi\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI</span></a></li></ul>",
      "sections": [
        {
          "title": "Thought experiments",
          "anchor": "Thought_experiments",
          "level": 1
        },
        {
          "title": "Commonly discussed decision theories",
          "anchor": "Commonly_discussed_decision_theories",
          "level": 1
        },
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "Sequence by AnnaSalamon",
          "anchor": "Sequence_by_AnnaSalamon",
          "level": 1
        },
        {
          "title": "Sequence by orthonormal (Decision Theories: A Semi-Formal Analysis)",
          "anchor": "Sequence_by_orthonormal__Decision_Theories__A_Semi_Formal_Analysis_",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 276,
    "description": {
      "markdown": "**Decision theory** is the study of principles and algorithms for making correct decisions—that is, decisions that allow an agent to achieve better outcomes with respect to its goals. Every action at least implicitly represents a decision under uncertainty: in a state of partial knowledge, something has to be done, even if that something turns out to be nothing (call it \"the null action\"). Even if you don't know how you make decisions, decisions do get made, and so there has to be some underlying mechanism. What is it? And how can it be done better? Decision theory has the answers.\n\n*Note: this page needs to be updated with content regarding Functional Decision Theory, the latest theory from MIRI.*\n\n*Related:* [Game Theory](https://www.lesswrong.com/tag/game-theory?showPostCount=true&useTagName=true), [Robust Agents](https://www.lesswrong.com/tag/robust-agents?showPostCount=true&useTagName=true), [Utility Functions](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nA core idea in decision theory is that of [*expected utility*](https://lessestwrong.com/tag/expected-utility) *maximization*, usually intractable to directly calculate in practice, but an invaluable theoretical concept. An agent assigns utility to every possible outcome: a real number representing the goodness or desirability of that outcome. The mapping of outcomes to utilities is called the agent's *utility function*. (The utility function is said to be invariant under affine transformations: that is, the utilities can be scaled or translated by a constant while resulting in all the same decisions.) For every action that the agent could take, sum over the utilities of the various possible outcomes weighted by their probability: this is the [expected](https://lessestwrong.com/tag/expected-value) utility of the action, and the action with the highest expected utility is to be chosen.\n\nThought experiments\n-------------------\n\nThe limitations and pathologies of decision theories can be analyzed by considering the decisions they suggest in the certain idealized situations that stretch the limits of decision theory's applicability. Some of the thought experiments more frequently discussed on [LW](https://wiki.lesswrong.com/wiki/LW) include:\n\n*   [Newcomb's problem](https://lessestwrong.com/tag/newcomb-s-problem)\n*   [Counterfactual mugging](https://lessestwrong.com/tag/counterfactual-mugging)\n*   [Parfit's hitchhiker](https://wiki.lesswrong.com/wiki/Parfit's_hitchhiker)\n*   [Smoker's lesion](https://wiki.lesswrong.com/wiki/Smoker's_lesion)\n*   [Absentminded driver](https://wiki.lesswrong.com/wiki/Absentminded_driver)\n*   [Sleeping Beauty problem](https://lessestwrong.com/tag/sleeping-beauty-paradox)\n*   [Prisoner's dilemma](https://lessestwrong.com/tag/prisoner-s-dilemma)\n*   [Pascal's mugging](https://lessestwrong.com/tag/pascal-s-mugging)\n\nCommonly discussed decision theories\n------------------------------------\n\nStandard theories well-known in academia:\n\n*   CDT, [Causal Decision Theory](http://en.wikipedia.org/wiki/Causal_decision_theory)\n*   EDT, [Evidential Decision Theory](http://en.wikipedia.org/wiki/Evidential_decision_theory)\n\nTheories invented by researchers associated with [MIRI](https://wiki.lesswrong.com/wiki/MIRI) and LW:\n\n*   FDT: [Functional Decision Theory](https://intelligence.org/2017/10/22/fdt/)\n*   TDT, [Timeless Decision Theory](https://lessestwrong.com/tag/timeless-decision-theory)\n*   UDT, [Updateless Decision Theory](https://lessestwrong.com/tag/updateless-decision-theory)\n*   ADT: [Ambient Decision Theory](https://lessestwrong.com/tag/ambient-decision-theory) (a variant of UDT)\n*   FDT: [Cheating Death in Damascus](https://intelligence.org/files/DeathInDamascus.pdf)\n\nOther decision theories are listed in [A comprehensive list of decision theories](https://casparoesterheld.com/a-comprehensive-list-of-decision-theories/).\n\nBlog posts\n----------\n\n*   [Terminal Values and Instrumental Values](https://lessestwrong.com/lw/l4/terminal_values_and_instrumental_values/)\n*   [Decision Theories: A Less Wrong Primer](https://lessestwrong.com/lw/aq9/decision_theories_a_less_wrong_primer/) by orthonormal\n*   [Decision Theory FAQ](https://lessestwrong.com/lw/gu1/decision_theory_faq/) by lukeprog and crazy88\n\nSequence by [AnnaSalamon](https://wiki.lesswrong.com/wiki/AnnaSalamon)\n----------------------------------------------------------------------\n\n*   [Decision theory: An outline of some upcoming posts](https://lessestwrong.com/lw/16f/decision_theory_an_outline_of_some_upcoming_posts/)\n*   [Confusion about Newcomb is confusion about counterfactuals](https://lessestwrong.com/lw/16i/confusion_about_newcomb_is_confusion_about/)\n*   [Why we need to reduce “could”, “would”, “should”](https://lessestwrong.com/lw/174/decision_theory_why_we_need_to_reduce_could_would/)\n*   [Why Pearl helps reduce “could” and “would”, but still leaves us with at least three alternatives](https://lessestwrong.com/lw/17b/decision_theory_why_pearl_helps_reduce_could_and/)\n\nSequence by [orthonormal](http://lesswrong.com/user/orthonormal/) (Decision Theories: A Semi-Formal Analysis)\n-------------------------------------------------------------------------------------------------------------\n\n*   [Part 0: Decision Theories: A Less Wrong Primer](https://lessestwrong.com/lw/aq9/decision_theories_a_less_wrong_primer/)\n*   [Part I: The Problem with Naive Decision Theory](https://lessestwrong.com/lw/axl/decision_theories_a_semiformal_analysis_part_i/)\n*   [Part II: Causal Decision Theory and Substitution](https://lessestwrong.com/lw/az6/decision_theories_a_semiformal_analysis_part_ii/)\n*   [Part III: Formalizing Timeless Decision Theory](https://lessestwrong.com/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/)\n\nSee also\n--------\n\n*   [Instrumental rationality](https://wiki.lesswrong.com/wiki/Instrumental_rationality)\n*   [Causality](https://lessestwrong.com/tag/causality)\n*   [Expected utility](https://lessestwrong.com/tag/expected-utility)\n*   [Evidential Decision Theory](https://lessestwrong.com/tag/evidential-decision-theory)\n*   [Timeless decision theory](https://lessestwrong.com/tag/timeless-decision-theory), [Updateless decision theory](https://lessestwrong.com/tag/updateless-decision-theory)\n*   [AIXI](https://lessestwrong.com/tag/aixi)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "NLwTnsH9RSotqXYLw",
    "name": "Value Learning",
    "core": null,
    "slug": "value-learning",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_woC2b5rav5sGrAo3E\">Value learning</span></strong><span class=\"by_woC2b5rav5sGrAo3E\"> is a proposed method for incorporating human values in an </span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_woC2b5rav5sGrAo3E\">AGI</span></a><span class=\"by_woC2b5rav5sGrAo3E\">. It involves the creation of an artificial learner whose actions consider many possible set of values and preferences, weighed by their likelihood. Value learning could prevent an AGI of having goals detrimental to human values, hence helping in the creation of </span><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_woC2b5rav5sGrAo3E\">Friendly AI</span></a><span class=\"by_woC2b5rav5sGrAo3E\">.</span></p><p><span class=\"by_woC2b5rav5sGrAo3E\">Although there are many ways to incorporate human values in an AGI (e.g.: </span><a href=\"https://lessestwrong.com/tag/coherent-extrapolated-volition\"><span class=\"by_woC2b5rav5sGrAo3E\">Coherent Extrapolated Volition</span></a><span class=\"by_woC2b5rav5sGrAo3E\">, </span><a href=\"https://lessestwrong.com/tag/coherent-aggregated-volition\"><span class=\"by_woC2b5rav5sGrAo3E\">Coherent Aggregated Volition</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> and </span><a href=\"https://lessestwrong.com/tag/coherent-blended-volition\"><span class=\"by_woC2b5rav5sGrAo3E\">Coherent Blended Volition</span></a><span><span class=\"by_dRGmZYGDzf5LFNjtz\">)</span><span class=\"by_woC2b5rav5sGrAo3E\">, this method is directly mentioned and developed in </span></span><a href=\"http://www.futuretech.ox.ac.uk/daniel-dewey\"><span class=\"by_woC2b5rav5sGrAo3E\">Daniel Dewey’s</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> paper </span><a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">‘Learning What to Value’</span></a><span class=\"by_woC2b5rav5sGrAo3E\">. Like most authors, he assumes that human’s goals would not naturally occur in an artificial agent and should be enforced in it. First, Dewey argues against the use of a simple use of </span><a href=\"https://lessestwrong.com/tag/reinforcement-learning\"><span class=\"by_woC2b5rav5sGrAo3E\">reinforcement learning</span></a><span><span class=\"by_woC2b5rav5sGrAo3E\"> to solve this problem, on the basis that this lead to the maximization of specific rewards that can diverge </span><span class=\"by_dRGmZYGDzf5LFNjtz\">from</span><span class=\"by_woC2b5rav5sGrAo3E\"> value maximization. For example, even if we forcefully engineer the agent to maximize those rewards that also maximize human values, the agent could alter its environment to more easily produce those same rewards without the trouble of also maximizing human </span><span class=\"by_dRGmZYGDzf5LFNjtz\">values (i.</span><span class=\"by_woC2b5rav5sGrAo3E\">e.: if the reward was human happiness it could alter the human mind so it became happy with anything).</span></span></p><p><span class=\"by_woC2b5rav5sGrAo3E\">To solve all these problems, Dewey proposes a </span><a href=\"https://lessestwrong.com/tag/utility-functions\"><span class=\"by_woC2b5rav5sGrAo3E\">utility function</span></a><span class=\"by_woC2b5rav5sGrAo3E\"> maximizer, who considers all possible utility functions weighted by their probabilities: \"[W]e propose uncertainty over utility functions. Instead of providing an agent one utility function up front, we provide an agent with a pool of possible utility functions and a probability distribution P such that each utility function can be assigned probability P(Ujyxm) given a particular interaction history [yxm]. An agent can then calculate an expected value over possible utility functions given a particular interaction history\" He concludes saying that although it solves many of the mentioned problems, this method still leaves many open questions. However it should provide a direction for future work.</span></p><h2 id=\"References\"><span class=\"by_woC2b5rav5sGrAo3E\">References</span></h2><ul><li><a href=\"http://www.danieldewey.net/learning-what-to-value.pdf\"><span class=\"by_woC2b5rav5sGrAo3E\">Dewey’s paper</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_woC2b5rav5sGrAo3E\">See Also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Friendly_AI\"><span class=\"by_woC2b5rav5sGrAo3E\">Friendly AI</span></a></li><li><a href=\"https://lessestwrong.com/tag/reinforcement-learning\"><span class=\"by_woC2b5rav5sGrAo3E\">Reinforcement learning</span></a></li><li><a href=\"https://lessestwrong.com/tag/value-extrapolation\"><span class=\"by_woC2b5rav5sGrAo3E\">Value extrapolation</span></a></li><li><a href=\"https://lessestwrong.com/tag/complexity-of-value\"><span class=\"by_woC2b5rav5sGrAo3E\">Complexity of value</span></a></li><li><a href=\"https://lessestwrong.com/tag/coherent-extrapolated-volition\"><span class=\"by_woC2b5rav5sGrAo3E\">Coherent Extrapolated Volition</span></a></li><li><a href=\"https://lessestwrong.com/tag/coherent-aggregated-volition\"><span class=\"by_woC2b5rav5sGrAo3E\">Coherent Aggregated Volition</span></a></li><li><a href=\"https://lessestwrong.com/tag/coherent-blended-volition\"><span class=\"by_woC2b5rav5sGrAo3E\">Coherent Blended Volition</span></a></li></ul>",
      "sections": [
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 120,
    "description": {
      "markdown": "**Value learning** is a proposed method for incorporating human values in an [AGI](https://wiki.lesswrong.com/wiki/AGI). It involves the creation of an artificial learner whose actions consider many possible set of values and preferences, weighed by their likelihood. Value learning could prevent an AGI of having goals detrimental to human values, hence helping in the creation of [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI).\n\nAlthough there are many ways to incorporate human values in an AGI (e.g.: [Coherent Extrapolated Volition](https://lessestwrong.com/tag/coherent-extrapolated-volition), [Coherent Aggregated Volition](https://lessestwrong.com/tag/coherent-aggregated-volition) and [Coherent Blended Volition](https://lessestwrong.com/tag/coherent-blended-volition)), this method is directly mentioned and developed in [Daniel Dewey’s](http://www.futuretech.ox.ac.uk/daniel-dewey) paper [‘Learning What to Value’](http://www.danieldewey.net/learning-what-to-value.pdf). Like most authors, he assumes that human’s goals would not naturally occur in an artificial agent and should be enforced in it. First, Dewey argues against the use of a simple use of [reinforcement learning](https://lessestwrong.com/tag/reinforcement-learning) to solve this problem, on the basis that this lead to the maximization of specific rewards that can diverge from value maximization. For example, even if we forcefully engineer the agent to maximize those rewards that also maximize human values, the agent could alter its environment to more easily produce those same rewards without the trouble of also maximizing human values (i.e.: if the reward was human happiness it could alter the human mind so it became happy with anything).\n\nTo solve all these problems, Dewey proposes a [utility function](https://lessestwrong.com/tag/utility-functions) maximizer, who considers all possible utility functions weighted by their probabilities: \"\\[W\\]e propose uncertainty over utility functions. Instead of providing an agent one utility function up front, we provide an agent with a pool of possible utility functions and a probability distribution P such that each utility function can be assigned probability P(Ujyxm) given a particular interaction history \\[yxm\\]. An agent can then calculate an expected value over possible utility functions given a particular interaction history\" He concludes saying that although it solves many of the mentioned problems, this method still leaves many open questions. However it should provide a direction for future work.\n\nReferences\n----------\n\n*   [Dewey’s paper](http://www.danieldewey.net/learning-what-to-value.pdf)\n\nSee Also\n--------\n\n*   [Friendly AI](https://wiki.lesswrong.com/wiki/Friendly_AI)\n*   [Reinforcement learning](https://lessestwrong.com/tag/reinforcement-learning)\n*   [Value extrapolation](https://lessestwrong.com/tag/value-extrapolation)\n*   [Complexity of value](https://lessestwrong.com/tag/complexity-of-value)\n*   [Coherent Extrapolated Volition](https://lessestwrong.com/tag/coherent-extrapolated-volition)\n*   [Coherent Aggregated Volition](https://lessestwrong.com/tag/coherent-aggregated-volition)\n*   [Coherent Blended Volition](https://lessestwrong.com/tag/coherent-blended-volition)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "cq69M9ceLNA35ShTR",
    "name": "Causality",
    "core": null,
    "slug": "causality",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Causality </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">is the </span><span class=\"by_sKAL2jzfkYkDbQmx9\">intuitive</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> notion that some events happening \"result\" in other events happening. What's going on with that? What does it mean for A to cause B? How do we figure out the causal relationship between things?</span></span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/free-will\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Free will</span></a></li><li><a href=\"https://www.lesswrong.com/tag/teleology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Teleology</span></a></li><li><a href=\"https://www.lesswrong.com/tag/beliefs-require-observations\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Beliefs require observations</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qf77EiaoMw7tH3GSr\">External links</span></h2><ul><li><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/philip_dawids_t.html\"><span class=\"by_qf77EiaoMw7tH3GSr\">Philip Dawid's explication of Pearl's model, and two ways of thinking about nonrandom sampling</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by </span><a href=\"https://en.wikipedia.org/wiki/Philip_Dawid\"><span class=\"by_qf77EiaoMw7tH3GSr\">Philip Dawid</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> and </span><a href=\"http://andrewgelman.com/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Andrew Gelman</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> - Causal inference as \"the task of using data collected under one regime to infer about the properties of another\".</span></li><li><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/disputes_about.html\"><span class=\"by_qf77EiaoMw7tH3GSr\">Resolving disputes between J. Pearl and D. Rubin on causal inference</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> and </span><a href=\"http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/more_on_pearlru.html\"><span class=\"by_qf77EiaoMw7tH3GSr\">More on Pearl's and Rubin's frameworks for causal inference</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> by Andrew Gelman</span></li><li><a href=\"http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/\"><span class=\"by_LoykQRMTxJFxwwdPy\">If correlation doesn't imply causation, then what does?</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by </span><a href=\"https://en.wikipedia.org/wiki/Michael_Nielsen\"><span class=\"by_LoykQRMTxJFxwwdPy\">Michael Nielsen</span></a></li><li><a href=\"http://oyhus.no/CorrelationAndCausation.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">Correlation is Evidence of Causation</span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Kim Øyhus</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Judea Pearls's works: </span><a href=\"http://bayes.cs.ucla.edu/BOOK-2K/\"><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Causality: Models, Reasoning, and Inference</span></u></i></a><i><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.goodreads.com/book/show/36204378-the-book-of-why\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Book of Why, A Primer on Causality</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 87,
    "description": {
      "markdown": "**Causality** is the intuitive notion that some events happening \"result\" in other events happening. What's going on with that? What does it mean for A to cause B? How do we figure out the causal relationship between things?\n\nSee also\n--------\n\n*   [Free will](https://www.lesswrong.com/tag/free-will)\n*   [Teleology](https://www.lesswrong.com/tag/teleology)\n*   [Beliefs require observations](https://www.lesswrong.com/tag/beliefs-require-observations)\n\nExternal links\n--------------\n\n*   [Philip Dawid's explication of Pearl's model, and two ways of thinking about nonrandom sampling](http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/philip_dawids_t.html) by [Philip Dawid](https://en.wikipedia.org/wiki/Philip_Dawid) and [Andrew Gelman](http://andrewgelman.com/) \\- Causal inference as \"the task of using data collected under one regime to infer about the properties of another\".\n*   [Resolving disputes between J. Pearl and D. Rubin on causal inference](http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/disputes_about.html) and [More on Pearl's and Rubin's frameworks for causal inference](http://www.stat.columbia.edu/~cook/movabletype/archives/2009/07/more_on_pearlru.html) by Andrew Gelman\n*   [If correlation doesn't imply causation, then what does?](http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/) by [Michael Nielsen](https://en.wikipedia.org/wiki/Michael_Nielsen)\n*   [Correlation is Evidence of Causation](http://oyhus.no/CorrelationAndCausation.html) by Kim Øyhus\n*   Judea Pearls's works: [*Causality: Models, Reasoning, and Inference*](http://bayes.cs.ucla.edu/BOOK-2K/)*,* [Book of Why, A Primer on Causality](https://www.goodreads.com/book/show/36204378-the-book-of-why)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "33BrBRSrRQS4jEHdk",
    "name": "Forecasts (Specific Predictions)",
    "core": false,
    "slug": "forecasts-specific-predictions",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasts</span><span class=\"by_r38pkCm7wF4M44MDQ\"> (Specific Predictions)</span></span></strong><span class=\"by_r38pkCm7wF4M44MDQ\">.</span><strong><span class=\"by_r38pkCm7wF4M44MDQ\"> </span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\">This tag is for specific</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> predictions about unknown </span><span class=\"by_r38pkCm7wF4M44MDQ\">facts</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><span class=\"by_r38pkCm7wF4M44MDQ\">future events.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Discussion of the practice of making forecasts can be found in </span></span><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasting and Prediction</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Related: </span><a href=\"https://www.lesswrong.com/tag/betting\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Betting</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. &nbsp;– </span></i><a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Making Beliefs Pay Rent</span></i></a></p></blockquote>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 83,
    "description": {
      "markdown": "**Forecasts (Specific Predictions)**.  This tag is for specific predictions about unknown facts or future events. Discussion of the practice of making forecasts can be found in [Forecasting and Prediction](https://www.lesswrong.com/tag/forecasting-and-prediction). Related: [Betting](https://www.lesswrong.com/tag/betting).\n\n> *Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.  –* [*Making Beliefs Pay Rent*](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "jaf5zfcGgCB2REXGw",
    "name": "Biology",
    "core": null,
    "slug": "biology",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 109,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "sHbKQDqrSinRPcnBv",
    "name": "Information Cascades",
    "core": false,
    "slug": "information-cascades",
    "tableOfContents": {
      "html": "<p><span class=\"by_nLbwLhBaQeG6tCNDN\">An </span><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">information cascade</span></strong><span><span class=\"by_nLbwLhBaQeG6tCNDN\"> occurs when people </span><span class=\"by_E3pLSZ6ePw4DjE2jm\">update</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> on other people's </span><span class=\"by_E3pLSZ6ePw4DjE2jm\">beliefs, which may individually be a </span></span><a href=\"https://www.lesswrong.com/tag/aumann-s-agreement-theorem\"><span class=\"by_E3pLSZ6ePw4DjE2jm\">rational decision</span></a><span><span class=\"by_E3pLSZ6ePw4DjE2jm\"> but may still result</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> in a self-reinforcing </span><span class=\"by_qgdGA4ZEyW7zNdK84\">community</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> opinion that does not necessarily reflect reality.</span></span></p><h2 id=\"See_Also\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">See </span><span class=\"by_nLbwLhBaQeG6tCNDN\">Also</span></span></h2><ul><li><a href=\"/tag/groupthink\"><span class=\"by_fmTiLqp6mmXeLjwfN\">Groupthink</span></a></li><li><a href=\"https://lessestwrong.com/tag/egalitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Egalitarianism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/modesty-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Modesty argument</span></a></li><li><a href=\"https://lessestwrong.com/tag/epistemic-luck\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemic luck</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/privileging-the-hypothesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Privileging the hypothesis</span></a></li><li><a href=\"https://lessestwrong.com/tag/free-floating-belief\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Free-floating belief</span></a></li><li><a href=\"https://lessestwrong.com/tag/groupthink\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Groupthink</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/affective-death-spiral\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Affective death spiral</span></a></li><li><a href=\"https://lessestwrong.com/tag/goodhart-s-law\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart's law</span></a></li><li><a href=\"https://lessestwrong.com/tag/religion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Religion</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 16,
    "description": {
      "markdown": "An **information cascade** occurs when people update on other people's beliefs, which may individually be a [rational decision](https://www.lesswrong.com/tag/aumann-s-agreement-theorem) but may still result in a self-reinforcing community opinion that does not necessarily reflect reality.\n\nSee Also\n--------\n\n*   [Groupthink](/tag/groupthink)\n*   [Egalitarianism](https://lessestwrong.com/tag/egalitarianism), [Modesty argument](https://lessestwrong.com/tag/modesty-argument)\n*   [Epistemic luck](https://lessestwrong.com/tag/epistemic-luck), [Privileging the hypothesis](https://lessestwrong.com/tag/privileging-the-hypothesis)\n*   [Free-floating belief](https://lessestwrong.com/tag/free-floating-belief)\n*   [Groupthink](https://lessestwrong.com/tag/groupthink), [Affective death spiral](https://lessestwrong.com/tag/affective-death-spiral)\n*   [Goodhart's law](https://lessestwrong.com/tag/goodhart-s-law)\n*   [Religion](https://lessestwrong.com/tag/religion)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "mip7tdAN87Jarkcew",
    "name": "Relationships (Interpersonal)",
    "core": false,
    "slug": "relationships-interpersonal",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Interpersonal Relationships </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">includes all forms of sustained interaction between people. This topic includes any discussion relating to friendship, romantic relationships, family relationships, business relationships, and so on.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Related:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Communication</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Communication Cultures</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Circling</span></li></ul>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 138,
    "description": {
      "markdown": "**Interpersonal Relationships** includes all forms of sustained interaction between people. This topic includes any discussion relating to friendship, romantic relationships, family relationships, business relationships, and so on.\n\nRelated:\n\n*   Communication\n*   Communication Cultures\n*   Circling"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8daMDi9NEShyLqxth",
    "name": "Forecasting & Prediction",
    "core": null,
    "slug": "forecasting-and-prediction",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasting </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Predicting </span></strong><span><span class=\"by_9c2mQkLQq6gQSksMs\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the act of making statements about what</span><span class=\"by_9c2mQkLQq6gQSksMs\"> will </span><span class=\"by_qgdGA4ZEyW7zNdK84\">happen</span><span class=\"by_9c2mQkLQq6gQSksMs\"> in the future </span><span class=\"by_qgdGA4ZEyW7zNdK84\">(and </span><span class=\"by_9c2mQkLQq6gQSksMs\">in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">some cases, the past)</span><span class=\"by_cJnvyeYrotgZgfG8W\"> and then scoring the predictions. Posts market with this</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> tag is for discussion of the practice, skill, and methodology of forecasting. Posts exclusively containing object-level lists of forecasts and predictions are in</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span></span><a href=\"https://www.lesswrong.com/tag/forecasts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasts</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. &nbsp;Related: </span><a href=\"https://www.lesswrong.com/tag/betting\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Betting</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><blockquote><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry. &nbsp;– </span></i><a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Making Beliefs Pay Rent</span></i></a></p></blockquote><p><span class=\"by_cJnvyeYrotgZgfG8W\">Forecasting allows individuals and institutions to test their internal models of reality. A good forecaster can have confidence in future predictions and hence actions in the same area as they have a good track record in. Organisations with decision-makers with good track records can likewise be more confident in their choices.</span></p><h2 id=\"How_to_get_started_in_forecasting__many_ways_\"><span class=\"by_cJnvyeYrotgZgfG8W\">How to get started in forecasting (many ways)</span></h2><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Just do it - go on </span><a href=\"https://www.metaculus.com/questions/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Metaculus</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> and forecast for 30 minutes. Look back in a month and see how things have changed</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Read a book - </span><a href=\"https://www.amazon.com/Superforecasting-audiobook/dp/B0131HGPQQ/ref=sr_1_1?crid=EIN4B0NE83W2&amp;keywords=superforecasting&amp;qid=1660197471&amp;sprefix=superforecastin%2Caps%2C389&amp;sr=8-1\"><span class=\"by_cJnvyeYrotgZgfG8W\">Superforecasting</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> by Phil Tetlock, one of the founding books in forecasting</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Watch a video - Alex Lawson's </span><a href=\"https://www.youtube.com/watch?v=e6Q7Ez3PkOw\"><span class=\"by_cJnvyeYrotgZgfG8W\">Intro to Forecasting</span></a><span class=\"by_cJnvyeYrotgZgfG8W\"> videos</span></li></ul><h2 id=\"Basic_forecasting_tips\"><span class=\"by_cJnvyeYrotgZgfG8W\">Basic forecasting tips</span></h2><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">5-second forecast - Just go with your gut. You'll probably be badly calibrated but you will learn than</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Longer forecast</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Base rates</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Adjust up or down</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Look at the median</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Adjust again</span></li></ul></li></ul><h2 id=\"Bottlenecks\"><span class=\"by_cJnvyeYrotgZgfG8W\">Bottlenecks</span></h2><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Understanding key decisions that decision-makers wants decisions about</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Writing high-quality questions and getting them published as forecasting question or prediction markets</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Getting enough forecasts</span><ul><li><span class=\"by_cJnvyeYrotgZgfG8W\">Getting enough forecasters on forecasting questions</span></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Getting enough money in prediction markets</span></li></ul></li><li><span class=\"by_cJnvyeYrotgZgfG8W\">Getting forecasts seen by relevant decisionmakers</span></li></ul><h2 id=\"Organisations_in_the_space\"><span class=\"by_cJnvyeYrotgZgfG8W\">Organisations in the space</span></h2><p><span class=\"by_cJnvyeYrotgZgfG8W\">Prediction markets/aggregators</span></p><ul><li><a href=\"https://www.metaculus.com/questions/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Metaculus</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, a prediction aggregator</span></li><li><a href=\"https://manifold.markets/home\"><span class=\"by_cJnvyeYrotgZgfG8W\">Manifold Markets</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, a fake-money prediction market where anyone can create a market</span></li><li><a href=\"http://predictionbook.com/\"><span class=\"by_cJnvyeYrotgZgfG8W\">PredictionBook</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, a website that keeps track of predictions and calibration levels</span></li><li><a href=\"https://kalshi.com/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Kalshi</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, the only US prediction market which can run policy markets over $500 dollars of total exposure</span></li><li><a href=\"https://polymarket.com/\"><span class=\"by_cJnvyeYrotgZgfG8W\">Polymarket</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">, a large crypo prediction market</span></li></ul><p><span class=\"by_cJnvyeYrotgZgfG8W\">Meta forecasting orgs&nbsp;</span></p><ul><li><a href=\"https://www.lesswrong.com/tag/quri\"><span class=\"by_cJnvyeYrotgZgfG8W\">QURI</span></a><span class=\"by_cJnvyeYrotgZgfG8W\">,&nbsp;</span></li></ul><h2 id=\"Individuals_publicly_attached_to_the_space\"><span class=\"by_cJnvyeYrotgZgfG8W\">Individuals publicly attached to the space</span></h2><p><span class=\"by_cJnvyeYrotgZgfG8W\">[not sure how to list]&nbsp;</span></p><h2 id=\"See_also\"><span class=\"by_9c2mQkLQq6gQSksMs\">See also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/antiprediction\"><span class=\"by_9c2mQkLQq6gQSksMs\">Antiprediction</span></a></li><li><a href=\"https://lessestwrong.com/tag/making-beliefs-pay-rent\"><span class=\"by_LoykQRMTxJFxwwdPy\">Making beliefs pay rent</span></a></li><li><a href=\"https://lessestwrong.com/tag/prediction-markets\"><span class=\"by_LoykQRMTxJFxwwdPy\">Prediction market</span></a></li><li><a href=\"https://lessestwrong.com/tag/forecast\"><span class=\"by_LoykQRMTxJFxwwdPy\">Forecast</span></a></li><li><a href=\"https://lessestwrong.com/tag/calibration\"><span class=\"by_LoykQRMTxJFxwwdPy\">Calibration</span></a></li><li><a href=\"https://lessestwrong.com/tag/black-swans\"><span class=\"by_LoykQRMTxJFxwwdPy\">Black swan</span></a></li></ul>",
      "sections": [
        {
          "title": "How to get started in forecasting (many ways)",
          "anchor": "How_to_get_started_in_forecasting__many_ways_",
          "level": 1
        },
        {
          "title": "Basic forecasting tips",
          "anchor": "Basic_forecasting_tips",
          "level": 1
        },
        {
          "title": "Bottlenecks",
          "anchor": "Bottlenecks",
          "level": 1
        },
        {
          "title": "Organisations in the space",
          "anchor": "Organisations_in_the_space",
          "level": 1
        },
        {
          "title": "Individuals publicly attached to the space",
          "anchor": "Individuals_publicly_attached_to_the_space",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 236,
    "description": {
      "markdown": "**Forecasting** or **Predicting** is the act of making statements about what will happen in the future (and in some cases, the past) and then scoring the predictions. Posts market with this tag is for discussion of the practice, skill, and methodology of forecasting. Posts exclusively containing object-level lists of forecasts and predictions are in [Forecasts](https://www.lesswrong.com/tag/forecasts).  Related: [Betting](https://www.lesswrong.com/tag/betting).\n\n> *Above all, don’t ask what to believe—ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.  –* [*Making Beliefs Pay Rent*](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences)\n\nForecasting allows individuals and institutions to test their internal models of reality. A good forecaster can have confidence in future predictions and hence actions in the same area as they have a good track record in. Organisations with decision-makers with good track records can likewise be more confident in their choices.\n\nHow to get started in forecasting (many ways)\n---------------------------------------------\n\n*   Just do it - go on [Metaculus](https://www.metaculus.com/questions/) and forecast for 30 minutes. Look back in a month and see how things have changed\n*   Read a book - [Superforecasting](https://www.amazon.com/Superforecasting-audiobook/dp/B0131HGPQQ/ref=sr_1_1?crid=EIN4B0NE83W2&keywords=superforecasting&qid=1660197471&sprefix=superforecastin%2Caps%2C389&sr=8-1) by Phil Tetlock, one of the founding books in forecasting\n*   Watch a video - Alex Lawson's [Intro to Forecasting](https://www.youtube.com/watch?v=e6Q7Ez3PkOw) videos\n\nBasic forecasting tips\n----------------------\n\n*   5-second forecast - Just go with your gut. You'll probably be badly calibrated but you will learn than\n*   Longer forecast\n    *   Base rates\n    *   Adjust up or down\n    *   Look at the median\n    *   Adjust again\n\nBottlenecks\n-----------\n\n*   Understanding key decisions that decision-makers wants decisions about\n*   Writing high-quality questions and getting them published as forecasting question or prediction markets\n*   Getting enough forecasts\n    *   Getting enough forecasters on forecasting questions\n    *   Getting enough money in prediction markets\n*   Getting forecasts seen by relevant decisionmakers\n\nOrganisations in the space\n--------------------------\n\nPrediction markets/aggregators\n\n*   [Metaculus](https://www.metaculus.com/questions/), a prediction aggregator\n*   [Manifold Markets](https://manifold.markets/home), a fake-money prediction market where anyone can create a market\n*   [PredictionBook](http://predictionbook.com/), a website that keeps track of predictions and calibration levels\n*   [Kalshi](https://kalshi.com/), the only US prediction market which can run policy markets over $500 dollars of total exposure\n*   [Polymarket](https://polymarket.com/), a large crypo prediction market\n\nMeta forecasting orgs \n\n*   [QURI](https://www.lesswrong.com/tag/quri), \n\nIndividuals publicly attached to the space\n------------------------------------------\n\n\\[not sure how to list\\] \n\nSee also\n--------\n\n*   [Antiprediction](https://lessestwrong.com/tag/antiprediction)\n*   [Making beliefs pay rent](https://lessestwrong.com/tag/making-beliefs-pay-rent)\n*   [Prediction market](https://lessestwrong.com/tag/prediction-markets)\n*   [Forecast](https://lessestwrong.com/tag/forecast)\n*   [Calibration](https://lessestwrong.com/tag/calibration)\n*   [Black swan](https://lessestwrong.com/tag/black-swans)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5SPDtxJT6y6ZTXHBJ",
    "name": "Simulacrum Levels",
    "core": false,
    "slug": "simulacrum-levels",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Simulacrum Levels</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are a framework for analyzing different motivations people can have for making statements.</span></p><ul><li><span class=\"by_HoGziwmhpMGqGeWZy\">Simulacrum Level 1: Attempt to describe the world accurately.</span></li><li><span class=\"by_HoGziwmhpMGqGeWZy\">Simulacrum Level 2: Choose what to say based on what your statement will cause other people to do or believe.</span></li><li><span class=\"by_HoGziwmhpMGqGeWZy\">Simulacrum Level 3: Say things that </span><a href=\"https://www.lesswrong.com/tag/signaling\"><span class=\"by_HoGziwmhpMGqGeWZy\">signal</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> membership to your ingroup.</span></li><li><span class=\"by_HoGziwmhpMGqGeWZy\">Simulacrum Level 4: Choose which group to signal membership to based on what the benefit would be for you.</span></li></ul><h2 id=\"More_descriptions_of_the_four_levels_\"><span class=\"by_sKAL2jzfkYkDbQmx9\">More descriptions of the four levels:</span></h2><p><span class=\"by_sKAL2jzfkYkDbQmx9\">By </span><a href=\"https://www.lesswrong.com/posts/fEX7G2N7CtmZQ3eB5/simulacra-and-subjectivity?commentId=FgajiMrSpY9MxTS8b\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Strawperson</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span></p><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 1: “There’s a lion across the river.” = There’s a lion across the river.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 2: “There’s a lion across the river.” = I don’t want to go (or have other people go) across the river.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 3: “There’s a lion across the river.” = I’m with the popular kids who are too cool to go across the river.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 4: “There’s a lion across the river.” = A firm stance against trans-river expansionism focus grouped well with undecided voters in my constituency.</span></li></ul><p><span class=\"by_sKAL2jzfkYkDbQmx9\">By </span><a href=\"https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Zvi</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">:</span></p><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 1: Symbols describe reality.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 2: Symbols pretend to describe reality.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 3: Symbols pretend to pretend to describe reality.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 4: Symbols need not pretend to describe reality.</span></li></ul><p><span class=\"by_sKAL2jzfkYkDbQmx9\">A concrete example of the above from Michael Vassar:</span></p><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 1: A court reflects justice.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 2: A corrupt judge distorts justice.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 3: A Soviet show trial conceals the absence of real Soviet courts.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 4: A trial by ordeal or trial by combat lacks and denies the concept of justice entirely.</span></li></ul><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Zvi </span><a href=\"https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels\"><span class=\"by_sKAL2jzfkYkDbQmx9\">describes</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> the four children of the Seder (Passover) as the four (and one extra) simulacrum levels:</span></p><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 1: The wise child.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 2: The wicked child.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 3: The simple child.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 4: The one who does not know how to ask.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Level 5: The one who is not there.</span></li></ul><blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">\"The wicked understand, acknowledge and value the Wise—they depend on the Wise for their own cynical gain. The simple don’t see the point of wisdom. Those who do not know how to ask don’t even know wisdom is a thing.\" - </span><a href=\"https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels\"><span class=\"by_sKAL2jzfkYkDbQmx9\">The Four Children of the Seder as the Simulacra Levels</span></a></p></blockquote><hr><p><span class=\"by_sKAL2jzfkYkDbQmx9\">The origin of this framework is in </span><a href=\"https://en.wikipedia.org/wiki/Simulacra_and_Simulation\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Simulacra and Simulation</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> by sociologist </span><a href=\"https://en.wikipedia.org/wiki/Jean_Baudrillard\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Jean Baudrillard</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p>",
      "sections": [
        {
          "title": "More descriptions of the four levels:",
          "anchor": "More_descriptions_of_the_four_levels_",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 26,
    "description": {
      "markdown": "**Simulacrum Levels** are a framework for analyzing different motivations people can have for making statements.\n\n*   Simulacrum Level 1: Attempt to describe the world accurately.\n*   Simulacrum Level 2: Choose what to say based on what your statement will cause other people to do or believe.\n*   Simulacrum Level 3: Say things that [signal](https://www.lesswrong.com/tag/signaling) membership to your ingroup.\n*   Simulacrum Level 4: Choose which group to signal membership to based on what the benefit would be for you.\n\nMore descriptions of the four levels:\n-------------------------------------\n\nBy [Strawperson](https://www.lesswrong.com/posts/fEX7G2N7CtmZQ3eB5/simulacra-and-subjectivity?commentId=FgajiMrSpY9MxTS8b):\n\n*   Level 1: “There’s a lion across the river.” = There’s a lion across the river.\n*   Level 2: “There’s a lion across the river.” = I don’t want to go (or have other people go) across the river.\n*   Level 3: “There’s a lion across the river.” = I’m with the popular kids who are too cool to go across the river.\n*   Level 4: “There’s a lion across the river.” = A firm stance against trans-river expansionism focus grouped well with undecided voters in my constituency.\n\nBy [Zvi](https://www.lesswrong.com/posts/QdppEcbhLTZqDDtDa/unifying-the-simulacra-definitions):\n\n*   Level 1: Symbols describe reality.\n*   Level 2: Symbols pretend to describe reality.\n*   Level 3: Symbols pretend to pretend to describe reality.\n*   Level 4: Symbols need not pretend to describe reality.\n\nA concrete example of the above from Michael Vassar:\n\n*   Level 1: A court reflects justice.\n*   Level 2: A corrupt judge distorts justice.\n*   Level 3: A Soviet show trial conceals the absence of real Soviet courts.\n*   Level 4: A trial by ordeal or trial by combat lacks and denies the concept of justice entirely.\n\nZvi [describes](https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels) the four children of the Seder (Passover) as the four (and one extra) simulacrum levels:\n\n*   Level 1: The wise child.\n*   Level 2: The wicked child.\n*   Level 3: The simple child.\n*   Level 4: The one who does not know how to ask.\n*   Level 5: The one who is not there.\n\n> \"The wicked understand, acknowledge and value the Wise—they depend on the Wise for their own cynical gain. The simple don’t see the point of wisdom. Those who do not know how to ask don’t even know wisdom is a thing.\" - [The Four Children of the Seder as the Simulacra Levels](https://www.lesswrong.com/posts/v6NBpQc2AzWjzxJ4S/the-four-children-of-the-seder-as-the-simulacra-levels)\n\n* * *\n\nThe origin of this framework is in [Simulacra and Simulation](https://en.wikipedia.org/wiki/Simulacra_and_Simulation) by sociologist [Jean Baudrillard](https://en.wikipedia.org/wiki/Jean_Baudrillard)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2i3w84KCkqZzpnQ4d",
    "name": "Petrov Day",
    "core": null,
    "slug": "petrov-day",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Petrov Day</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a tradition celebrating Soviet military officer Stanislav Petrov, who played a key role in preventing a nuclear attack during a false alarm incident on September 26, 1983.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_HoGziwmhpMGqGeWZy\">Existential Risk</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/ritual\"><span class=\"by_HoGziwmhpMGqGeWZy\">Ritual</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 35,
    "description": {
      "markdown": "**Petrov Day** is a tradition celebrating Soviet military officer Stanislav Petrov, who played a key role in preventing a nuclear attack during a false alarm incident on September 26, 1983.\n\nSee also: [Existential Risk](https://www.lesswrong.com/tag/existential-risk), [Ritual](https://www.lesswrong.com/tag/ritual)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "vtozKm5BZ8gf6zd45",
    "name": "Secular Solstice",
    "core": null,
    "slug": "secular-solstice",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Secular Solstice</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is a holiday tradition invented by Less Wrong user Raymond Arnold. It is celebrated </span><span class=\"by_r38pkCm7wF4M44MDQ\">on-or-near</span><span class=\"by_HoGziwmhpMGqGeWZy\"> December 21st, the winter solstice in the northern hemisphere. It is typically celebrated in large groups with a ceremony involving singing and storytelling.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 52,
    "description": {
      "markdown": "The **Secular Solstice** is a holiday tradition invented by Less Wrong user Raymond Arnold. It is celebrated on-or-near December 21st, the winter solstice in the northern hemisphere. It is typically celebrated in large groups with a ceremony involving singing and storytelling."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "zCYXpx33wq8chGyEz",
    "name": "AI Boxing (Containment)",
    "core": null,
    "slug": "ai-boxing-containment",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_Ge36tWtFQERudEYhR\">AI </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Boxing </span></span></strong><span><span class=\"by_Ge36tWtFQERudEYhR\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">attempts, experiments, or proposals to isolate (\"box\")</span><span class=\"by_Ge36tWtFQERudEYhR\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">a powerful AI (~AGI) where it can't</span><span class=\"by_Ge36tWtFQERudEYhR\"> interact with the world </span><span class=\"by_qgdGA4ZEyW7zNdK84\">at large, save for limited communication with its human liaison. It is often proposed that so long as the AI is physically isolated</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">restricted, or \"boxed\", it will be harmless even if it is an</span><span class=\"by_XtphY3uYHwruKqDyG\"> </span></span><a href=\"https://www.lesswrong.com/tag/unfriendly-artificial-intelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">unfriendly artificial intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (UAI).</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Challenges are: 1)</span><span class=\"by_Ge36tWtFQERudEYhR\"> can </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you successively prevent</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> </span><span class=\"by_Ge36tWtFQERudEYhR\">it</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> from </span><span class=\"by_qgdGA4ZEyW7zNdK84\">interacting with the world? And 2) can you prevent</span><span class=\"by_Ge36tWtFQERudEYhR\"> it </span><span class=\"by_qgdGA4ZEyW7zNdK84\">from convincing</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> you to let </span><span class=\"by_qgdGA4ZEyW7zNdK84\">it out?</span></span></p><p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">See </span><span class=\"by_XtphY3uYHwruKqDyG\">also:</span></span></strong><span class=\"by_XtphY3uYHwruKqDyG\"> </span><a href=\"https://www.lesswrong.com/tag/ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/AGI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AGI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/oracle-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Oracle AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/tool-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Tool AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Unfriendly AI</span></a></p><h1 id=\"Escaping_the_box\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Escaping the box</span></h1><p><span class=\"by_qgdGA4ZEyW7zNdK84\">It is not regarded as likely that an AGI can be boxed in the long term. Since the AGI might be a </span><a href=\"https://www.lesswrong.com/tag/superintelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">superintelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, it could persuade someone (the human liaison, most likely) to free it from its box and thus, human control. Some practical ways of achieving this goal include:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Offering enormous wealth, power and intelligence to its liberator</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Claiming that only it can prevent an </span><a href=\"https://www.lesswrong.com/tag/existential-risk\"><span class=\"by_qgdGA4ZEyW7zNdK84\">existential risk</span></a></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Claiming it needs outside resources to cure all diseases</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Predicting a real-world disaster (which then occurs), then claiming it could have been prevented had it been let out</span></li></ul><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Other, more speculative ways include: threatening to torture millions of conscious copies of you</span><span class=\"by_HoGziwmhpMGqGeWZy\"> for </span><span class=\"by_qgdGA4ZEyW7zNdK84\">thousands of years, starting in exactly</span><span class=\"by_HoGziwmhpMGqGeWZy\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">same situation as in such a way that it seems overwhelmingly likely that </span></span><a href=\"https://www.lesswrong.com/tag/simulation-argument\"><span class=\"by_qgdGA4ZEyW7zNdK84\">you are a simulation</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, or it might discover and exploit unknown physics to free itself.</span></p><h1 id=\"Containing_the_AGI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Containing the AGI</span></h1><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Attempts to box an AGI may add some degree of safety to the development of a </span><a href=\"https://wiki.lesswrong.com/wiki/FAI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">friendly artificial intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (FAI). A number of strategies for keeping an AGI in its box are discussed in </span><a href=\"http://www.aleph.se/papers/oracleAI.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Thinking inside the box</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Leakproofing the Singularity</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Among them are:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Physically isolating the AGI and permitting it zero control of any machinery</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Limiting the AGI’s outputs and inputs with regards to humans</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Programming the AGI with deliberately convoluted logic or </span><a href=\"http://en.wikipedia.org/wiki/Homomorphic_encryption\"><span class=\"by_qgdGA4ZEyW7zNdK84\">homomorphically encrypting</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> portions of it</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Periodic resets of the AGI's memory</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">A virtual world between the real world and the AI, where its unfriendly intentions would be first revealed</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivational control using a variety of techniques</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Creating an </span><a href=\"https://www.lesswrong.com/tag/oracle-ai\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">Oracle</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> AI</span></span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">:</span><span class=\"by_HoGziwmhpMGqGeWZy\"> an AI that only answers questions and isn't designed to interact with the world in any other way. But even the act of the AI putting strings of text in front of humans poses some risk.</span></span></li></ul><h1 id=\"Simulations___Experiments\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Simulations / Experiments</span></h1><p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">AI Box Experiment</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a game meant to explore the possible pitfalls of AI boxing. It is played over text chat, with one human roleplaying as an AI in a box, and another human roleplaying as a gatekeeper with the ability to let the AI out of the box. The AI player wins if they successfully convince the gatekeeper to let them out of the box, and the gatekeeper wins if the AI player has not been freed after a certain period of time.&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Both Eliezer Yudkowsky and Justin Corwin have ran simulations, pretending to be a </span><a href=\"https://www.lesswrong.com/tag/superintelligence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">superintelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, and been able to convince a human playing a guard to let them out on many - but not all - occasions. Eliezer's five experiments required the guard to listen for at least two hours with participants who had approached him, while Corwin's 26 experiments had no time limit and subjects he approached.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The text of Eliezer's experiments have not been made public.</span></p><h2 id=\"List_of_experiments\"><span class=\"by_qgdGA4ZEyW7zNdK84\">List of experiments</span></h2><ul><li><a href=\"http://yudkowsky.net/singularity/aibox/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The AI-Box Experiment</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/eliezer-yudkowsky\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Eliezer Yudkowsky's</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> original two tests</span></li><li><a href=\"https://www.lesswrong.com/lw/up/shut_up_and_do_the_impossible/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Shut up and do the impossible!</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, three other experiments Eliezer ran</span></li><li><a href=\"http://www.sl4.org/archive/0207/4935.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Boxing</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, 26 trials ran by Justin Corwin</span></li><li><a href=\"https://www.lesswrong.com/lw/9ld/ai_box_log/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Box Log</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, a log of a trial between MileyCyrus and Dorikka</span></li></ul><h1 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h1><ul><li><a href=\"http://www.aleph.se/papers/oracleAI.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Thinking inside the box: using and controlling an Oracle AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Stuart Armstrong, Anders Sandberg, and Nick Bostrom</span></li><li><a href=\"http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Leakproofing the Singularity: Artificial Intelligence Confinement Problem</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Roman V. Yampolskiy</span></li><li><a href=\"http://ordinaryideas.wordpress.com/2012/04/27/on-the-difficulty-of-ai-boxing/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">On the Difficulty of AI Boxing</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Paul Christiano</span></li><li><a href=\"https://www.lesswrong.com/lw/3cz/cryptographic_boxes_for_unfriendly_ai/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cryptographic Boxes for Unfriendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Paul Christiano</span></li><li><a href=\"https://www.lesswrong.com/r/lesswrong/lw/12s/the_strangest_thing_an_ai_could_tell_you/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Strangest Thing An AI Could Tell You</span></a></li><li><a href=\"https://www.lesswrong.com/lw/1pz/ai_in_box_boxes_you/\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">The AI </span><span class=\"by_qgdGA4ZEyW7zNdK84\">in a box boxes you</span></span></a></li></ul>",
      "sections": [
        {
          "title": "Escaping the box",
          "anchor": "Escaping_the_box",
          "level": 1
        },
        {
          "title": "Containing the AGI",
          "anchor": "Containing_the_AGI",
          "level": 1
        },
        {
          "title": "Simulations / Experiments",
          "anchor": "Simulations___Experiments",
          "level": 1
        },
        {
          "title": "List of experiments",
          "anchor": "List_of_experiments",
          "level": 2
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 6
    },
    "postCount": 48,
    "description": {
      "markdown": "**AI Boxing** is attempts, experiments, or proposals to isolate (\"box\") a powerful AI (~AGI) where it can't interact with the world at large, save for limited communication with its human liaison. It is often proposed that so long as the AI is physically isolated and restricted, or \"boxed\", it will be harmless even if it is an [unfriendly artificial intelligence](https://www.lesswrong.com/tag/unfriendly-artificial-intelligence) (UAI).\n\nChallenges are: 1) can you successively prevent it from interacting with the world? And 2) can you prevent it from convincing you to let it out?\n\n**See also:** [AI](https://www.lesswrong.com/tag/ai), [AGI](https://wiki.lesswrong.com/wiki/AGI), [Oracle AI](https://www.lesswrong.com/tag/oracle-ai), [Tool AI](https://www.lesswrong.com/tag/tool-ai), [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI)\n\nEscaping the box\n================\n\nIt is not regarded as likely that an AGI can be boxed in the long term. Since the AGI might be a [superintelligence](https://www.lesswrong.com/tag/superintelligence), it could persuade someone (the human liaison, most likely) to free it from its box and thus, human control. Some practical ways of achieving this goal include:\n\n*   Offering enormous wealth, power and intelligence to its liberator\n*   Claiming that only it can prevent an [existential risk](https://www.lesswrong.com/tag/existential-risk)\n*   Claiming it needs outside resources to cure all diseases\n*   Predicting a real-world disaster (which then occurs), then claiming it could have been prevented had it been let out\n\nOther, more speculative ways include: threatening to torture millions of conscious copies of you for thousands of years, starting in exactly the same situation as in such a way that it seems overwhelmingly likely that [you are a simulation](https://www.lesswrong.com/tag/simulation-argument), or it might discover and exploit unknown physics to free itself.\n\nContaining the AGI\n==================\n\nAttempts to box an AGI may add some degree of safety to the development of a [friendly artificial intelligence](https://wiki.lesswrong.com/wiki/FAI) (FAI). A number of strategies for keeping an AGI in its box are discussed in [Thinking inside the box](http://www.aleph.se/papers/oracleAI.pdf) and [Leakproofing the Singularity](http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf). Among them are:\n\n*   Physically isolating the AGI and permitting it zero control of any machinery\n*   Limiting the AGI’s outputs and inputs with regards to humans\n*   Programming the AGI with deliberately convoluted logic or [homomorphically encrypting](http://en.wikipedia.org/wiki/Homomorphic_encryption) portions of it\n*   Periodic resets of the AGI's memory\n*   A virtual world between the real world and the AI, where its unfriendly intentions would be first revealed\n*   Motivational control using a variety of techniques\n*   Creating an [Oracle AI](https://www.lesswrong.com/tag/oracle-ai): an AI that only answers questions and isn't designed to interact with the world in any other way. But even the act of the AI putting strings of text in front of humans poses some risk.\n\nSimulations / Experiments\n=========================\n\nThe **AI Box Experiment** is a game meant to explore the possible pitfalls of AI boxing. It is played over text chat, with one human roleplaying as an AI in a box, and another human roleplaying as a gatekeeper with the ability to let the AI out of the box. The AI player wins if they successfully convince the gatekeeper to let them out of the box, and the gatekeeper wins if the AI player has not been freed after a certain period of time. \n\nBoth Eliezer Yudkowsky and Justin Corwin have ran simulations, pretending to be a [superintelligence](https://www.lesswrong.com/tag/superintelligence), and been able to convince a human playing a guard to let them out on many - but not all - occasions. Eliezer's five experiments required the guard to listen for at least two hours with participants who had approached him, while Corwin's 26 experiments had no time limit and subjects he approached.\n\nThe text of Eliezer's experiments have not been made public.\n\nList of experiments\n-------------------\n\n*   [The AI-Box Experiment](http://yudkowsky.net/singularity/aibox/) [Eliezer Yudkowsky's](https://www.lesswrong.com/tag/eliezer-yudkowsky) original two tests\n*   [Shut up and do the impossible!](https://www.lesswrong.com/lw/up/shut_up_and_do_the_impossible/), three other experiments Eliezer ran\n*   [AI Boxing](http://www.sl4.org/archive/0207/4935.html), 26 trials ran by Justin Corwin\n*   [AI Box Log](https://www.lesswrong.com/lw/9ld/ai_box_log/), a log of a trial between MileyCyrus and Dorikka\n\nReferences\n==========\n\n*   [Thinking inside the box: using and controlling an Oracle AI](http://www.aleph.se/papers/oracleAI.pdf) by Stuart Armstrong, Anders Sandberg, and Nick Bostrom\n*   [Leakproofing the Singularity: Artificial Intelligence Confinement Problem](http://dl.dropbox.com/u/5317066/2012-yampolskiy.pdf) by Roman V. Yampolskiy\n*   [On the Difficulty of AI Boxing](http://ordinaryideas.wordpress.com/2012/04/27/on-the-difficulty-of-ai-boxing/) by Paul Christiano\n*   [Cryptographic Boxes for Unfriendly AI](https://www.lesswrong.com/lw/3cz/cryptographic_boxes_for_unfriendly_ai/) by Paul Christiano\n*   [The Strangest Thing An AI Could Tell You](https://www.lesswrong.com/r/lesswrong/lw/12s/the_strangest_thing_an_ai_could_tell_you/)\n*   [The AI in a box boxes you](https://www.lesswrong.com/lw/1pz/ai_in_box_boxes_you/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "kEX5CzbfiAzGn4q8B",
    "name": "Superstimuli",
    "core": false,
    "slug": "superstimuli",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_HoGziwmhpMGqGeWZy\">Humans evolved various desires that promoted survival</span><span class=\"by_9c2mQkLQq6gQSksMs\"> and </span><span class=\"by_HoGziwmhpMGqGeWZy\">reproductive success in </span><span class=\"by_9c2mQkLQq6gQSksMs\">the </span></span><a href=\"https://www.psychologytoday.com/intl/blog/darwins-subterranean-world/201806/3-things-we-know-about-the-ancestral-environment\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">ancestral </span><span class=\"by_qmJFRN7jitjPsuF3f\">environment</span></span></a><span><span class=\"by_qmJFRN7jitjPsuF3f\">.</span><span class=\"by_HoGziwmhpMGqGeWZy\"> </span></span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Superstimuli</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> are modern inventions that satisfy desires better than anything in the ancestral environment </span><span class=\"by_qmJFRN7jitjPsuF3f\">could</span><span class=\"by_HoGziwmhpMGqGeWZy\"> but are detrimental to survival, reproduction, or other </span><span class=\"by_qmJFRN7jitjPsuF3f\">high-</span><span class=\"by_HoGziwmhpMGqGeWZy\">level goals.</span></span></p><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">See also:</span></i><a href=\"https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\"> Evolutionary Psychology</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&amp;useTagName=true\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s Law</span></span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/wireheading?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Wireheading</span></a></p><blockquote><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">A candy bar is a </span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">superstimulus</span><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp; &nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\">A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment. The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">wasn'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">t in the training dataset - an impossibly distant outlier on the old ancestral graphs.</span><span class=\"by_qgdGA4ZEyW7zNdK84\">&nbsp;</span></span></i></p></blockquote><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">-- </span></i><span class=\"by_Xn6ACr6Cua8upALWQ\">Eliezer Yudkowsky, </span><a href=\"https://www.lesswrong.com/posts/Jq73GozjsuhdwMLEG/superstimuli-and-the-collapse-of-western-civilization\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Superstimuli and the Collapse of Western Civilisation</span></a></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/h3/superstimuli_and_the_collapse_of_western/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Superstimuli and the Collapse of Western Civilization</span></a></li><li><a href=\"https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Adaptation-Executers, not Fitness-Maximizers</span></a></li></ul><h2 id=\"External_Links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Links</span></h2><ul><li><a href=\"http://www.ted.com/talks/dan_dennett_cute_sexy_sweet_funny.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cute, Sexy, Sweet, Funny</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://en.wikipedia.org/wiki/Daniel_Dennett\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Daniel Dennett</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> at TED</span></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/akrasia\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Akrasia</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Adaptation executers</span></a></li><li><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Evolutionary psychology</span></a></li></ul>",
      "sections": [
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 16,
    "description": {
      "markdown": "Humans evolved various desires that promoted survival and reproductive success in the [ancestral environment](https://www.psychologytoday.com/intl/blog/darwins-subterranean-world/201806/3-things-we-know-about-the-ancestral-environment). **Superstimuli** are modern inventions that satisfy desires better than anything in the ancestral environment could but are detrimental to survival, reproduction, or other high-level goals.\n\n*See also:* [Evolutionary Psychology](https://www.lesswrong.com/tag/evolutionary-psychology?showPostCount=true&useTagName=true), [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law?showPostCount=true&useTagName=true), [Wireheading](https://www.lesswrong.com/tag/wireheading?showPostCount=true&useTagName=true)\n\n> *A candy bar is a* superstimulus*: it contains more concentrated sugar, salt, and fat than anything that exists in the ancestral environment.   A candy bar matches taste buds that evolved in a hunter-gatherer environment, but it matches those taste buds much more strongly than anything that actually existed in the hunter-gatherer environment. The signal that once reliably correlated to healthy food has been hijacked, blotted out with a point in tastespace that wasn't in the training dataset - an impossibly distant outlier on the old ancestral graphs. *\n\n*\\-\\-* Eliezer Yudkowsky, [Superstimuli and the Collapse of Western Civilisation](https://www.lesswrong.com/posts/Jq73GozjsuhdwMLEG/superstimuli-and-the-collapse-of-western-civilization)\n\nNotable Posts\n-------------\n\n*   [Superstimuli and the Collapse of Western Civilization](https://lessestwrong.com/lw/h3/superstimuli_and_the_collapse_of_western/)\n*   [Adaptation-Executers, not Fitness-Maximizers](https://lessestwrong.com/lw/l0/adaptationexecuters_not_fitnessmaximizers/)\n\nExternal Links\n--------------\n\n*   [Cute, Sexy, Sweet, Funny](http://www.ted.com/talks/dan_dennett_cute_sexy_sweet_funny.html) by [Daniel Dennett](https://en.wikipedia.org/wiki/Daniel_Dennett) at TED\n\nSee Also\n--------\n\n*   [Akrasia](https://lessestwrong.com/tag/akrasia)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers)\n*   [Evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Kj9q8FXoauL7mQDWt",
    "name": "Affect Heuristic",
    "core": false,
    "slug": "affect-heuristic",
    "tableOfContents": {
      "html": "<p><span class=\"by_XzXbiS2zWYNdZdLW8\">The </span><strong><span class=\"by_XtphY3uYHwruKqDyG\">affect heuristic</span></strong><span><span class=\"by_XzXbiS2zWYNdZdLW8\"> is </span><span class=\"by_XtphY3uYHwruKqDyG\">a principle for making fast, perceptual judgments based on subjective impressions of goodness/badness. It can cause people</span><span class=\"by_HoGziwmhpMGqGeWZy\"> to </span><span class=\"by_XtphY3uYHwruKqDyG\">use one positive (or negative) fact about an object/action to judge the likely positivity or negativity</span><span class=\"by_HoGziwmhpMGqGeWZy\"> of other </span><span class=\"by_XtphY3uYHwruKqDyG\">facts about that object/action.</span></span></p><h2 id=\"See_also\"><span class=\"by_XtphY3uYHwruKqDyG\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/halo-effect\"><span class=\"by_XtphY3uYHwruKqDyG\">Halo effect</span></a></li><li><a href=\"https://www.lesswrong.com/tag/priming\"><span class=\"by_XtphY3uYHwruKqDyG\">Priming</span></a></li><li><a href=\"https://www.lesswrong.com/tag/connotation\"><span class=\"by_XtphY3uYHwruKqDyG\">Connotation</span></a></li><li><a href=\"https://www.lesswrong.com/tag/affective-death-spiral\"><span class=\"by_XtphY3uYHwruKqDyG\">Affective death spiral</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 15,
    "description": {
      "markdown": "The **affect heuristic** is a principle for making fast, perceptual judgments based on subjective impressions of goodness/badness. It can cause people to use one positive (or negative) fact about an object/action to judge the likely positivity or negativity of other facts about that object/action.\n\nSee also\n--------\n\n*   [Halo effect](https://www.lesswrong.com/tag/halo-effect)\n*   [Priming](https://www.lesswrong.com/tag/priming)\n*   [Connotation](https://www.lesswrong.com/tag/connotation)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "H2q58pKG6xFrv8bPz",
    "name": "Spaced Repetition",
    "core": null,
    "slug": "spaced-repetition",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Spaced </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Repetition </span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">is a</span><span class=\"by_8GnKujYLZ2ZZLs5zk\"> technique </span><span class=\"by_mcKSiwq2TBrTMZS6X\">for long-term </span><span class=\"by_qgdGA4ZEyW7zNdK84\">retention of learned </span><span class=\"by_Xn6ACr6Cua8upALWQ\">material where instead</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of </span><span class=\"by_Xn6ACr6Cua8upALWQ\">attempting to memorize by </span><span class=\"by_qgdGA4ZEyW7zNdK84\">‘cramming’</span><span class=\"by_Xn6ACr6Cua8upALWQ\">, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software.</span></span><br><br><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also:</span></i><strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span></strong><a href=\"https://www.lesswrong.com/tag/scholarship-and-learning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Scholarship &amp; Learning</span></a></p><h1 id=\"The_case_for_Spaced_Repetition\"><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">The case for Spaced Repetition</span></strong></h1><p><span class=\"by_Xn6ACr6Cua8upALWQ\">A good place to learn more about Spaced Repetition is </span><a href=\"https://www.gwern.net/Spaced-repetition\"><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Spaced Repetition for Efficient Learning</span></strong></a><strong><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span></strong><span class=\"by_Xn6ACr6Cua8upALWQ\">by </span><a href=\"https://www.lesswrong.com/users/gwern\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Gwern</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">:</span></p><blockquote><p><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Spaced repetition is a centuries-old psychological technique for efficient memorization &amp; practice of skills where instead of attempting to memorize by </span><span class=\"by_qgdGA4ZEyW7zNdK84\">‘cramming’</span><span class=\"by_Xn6ACr6Cua8upALWQ\">, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp; medical studies.</span></span></i></p></blockquote><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">The key insight for why spaced repetition should be effective is that you forget things approximately hyperbolically-- reviewing things very soon (as in cramming-style learning) is ineffective because you have not forgotten much yet when you come to a review. In comparison, Spaced Repetition allows you to renew your knowledge precisely as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">re about to forget a given fact, giving the review the maximum return-on-investment possible and (over time) flattening the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">'forgetting curve'</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> so that the interval between successive reviews gets progressively larger for a given fact.</span></span></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Obviously, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">it'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">s not possible to remind </span></span><i><span class=\"by_Xn6ACr6Cua8upALWQ\">yourself </span></i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">of something precisely when </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you'</span><span class=\"by_Xn6ACr6Cua8upALWQ\">re about to forget it. Enter Spaced Repetition Software (SRS)! By using the forgetting curve, SRS is able to plan when you need to review each item. You can either create decks yourself, or (for some topics) download from databases. </span></span><a href=\"https://apps.ankiweb.net/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Anki</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> and </span><a href=\"https://mnemosyne-proj.org/\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Mnemnosyne</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> are two popular free options, and </span><a href=\"https://www.supermemo.com/en\"><span class=\"by_Xn6ACr6Cua8upALWQ\">SuperMemo</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> is a subscription-based choice.</span></p><h1 id=\"Criticisms\"><strong><span class=\"by_Xn6ACr6Cua8upALWQ\">Criticisms</span></strong></h1><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">Criticisms of Spaced Repetition primarily revolve around the fact that, for it to be effective, knowledge has to be broken down into individual </span><span class=\"by_qgdGA4ZEyW7zNdK84\">'pieces'</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> to go onto cards for testing. This is difficult or impossible for some types of knowledge, and may not promote an integrated view, where the structure or hierarchy of the knowledge is clear, as well as other methods. More can be found in the post </span></span><a href=\"https://www.lesswrong.com/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition\"><span class=\"by_Xn6ACr6Cua8upALWQ\">A Vote Against Spaced Repetition</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><h1 id=\"Resources\"><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Resources</span></strong></h1><h2 id=\"Supermemo_material\"><span class=\"by_HoGziwmhpMGqGeWZy\">Supermemo material</span></h2><ul><li><a href=\"http://supermemo.com/english/contents.htm#Articles\"><span class=\"by_HoGziwmhpMGqGeWZy\">Many articles on assorted related topics</span></a></li><li><a href=\"http://supermemo.com/english/contents.htm#Research\"><span class=\"by_HoGziwmhpMGqGeWZy\">Research on memory and learning</span></a></li><li><a href=\"http://supermemo.com/help/faq/index.htm\"><span class=\"by_HoGziwmhpMGqGeWZy\">Frequently asked questions about various aspects of spaced repetition</span></a></li></ul><h2 id=\"Spaced_Repetition_Decks\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Spaced Repetition Decks</span></h2><p><span class=\"by_HoGziwmhpMGqGeWZy\">Decks (links, or for Anki, the names of a deck in the Anki collection) relevant to LW.</span></p><ul><li><a href=\"http://www.stafforini.com/blog/anki-decks-by-lesswrong-users/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Anki decks by LW users</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by Pablo_Stafforini. Comprehensive and up-to-date (as of 2019) list.</span></li><li><a href=\"https://www.lesswrong.com/lw/3px/anki_deck_for_biases_and_fallacies/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Anki deck for biases and fallacies</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by phob</span></li><li><a href=\"https://www.lesswrong.com/r/discussion/lw/74o/anki_deck_for_cognitive_science_in_one_lesson/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Deck for Cognitive Science in One Lesson</span></a></li><li><a href=\"https://www.lesswrong.com/r/discussion/lw/ee6/lesswrong_wiki_as_anki_deck\"><span class=\"by_HoGziwmhpMGqGeWZy\">LessWrong Wiki as an Anki deck</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by mapnoterritory</span></li></ul><h2 id=\"SR_cards_for_LessWrong_Sequences\"><span class=\"by_HoGziwmhpMGqGeWZy\">SR cards for </span><a href=\"https://www.lesswrong.com/tag/sequences\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWrong </span><span class=\"by_HoGziwmhpMGqGeWZy\">Sequences</span></span></a></h2><ul><li><a href=\"https://www.lesswrong.com/lw/2e6/spaced_repetition_database_for_the_mysterious/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Spaced Repetition Database for the Mysterious Answers to Mysterious Questions Sequence</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by divia</span></li><li><a href=\"https://www.lesswrong.com/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">Spaced Repetition Database for A </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Human'</span><span class=\"by_HoGziwmhpMGqGeWZy\">s Guide to Words</span></span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> by divia</span></li></ul><h2 id=\"Other_Spaced_Repetition_Software\"><span><span class=\"by_HoGziwmhpMGqGeWZy\">Other </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Spaced Repetition Software</span></span></h2><ul><li><a href=\"https://vocapp.com/\"><span class=\"by_HoGziwmhpMGqGeWZy\">VocApp.com</span></a></li></ul>",
      "sections": [
        {
          "title": "The case for Spaced Repetition",
          "anchor": "The_case_for_Spaced_Repetition",
          "level": 1
        },
        {
          "title": "Criticisms",
          "anchor": "Criticisms",
          "level": 1
        },
        {
          "title": "Resources",
          "anchor": "Resources",
          "level": 1
        },
        {
          "title": "Supermemo material",
          "anchor": "Supermemo_material",
          "level": 2
        },
        {
          "title": "Spaced Repetition Decks",
          "anchor": "Spaced_Repetition_Decks",
          "level": 2
        },
        {
          "title": "SR cards for LessWrong Sequences",
          "anchor": "SR_cards_for_LessWrong_Sequences",
          "level": 2
        },
        {
          "title": "Other Spaced Repetition Software",
          "anchor": "Other_Spaced_Repetition_Software",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 8
    },
    "postCount": 61,
    "description": {
      "markdown": "**Spaced Repetition** is a technique for long-term retention of learned material where instead of attempting to memorize by ‘cramming’, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software.  \n  \n*See Also:*  [Scholarship & Learning](https://www.lesswrong.com/tag/scholarship-and-learning)\n\n**The case for Spaced Repetition**\n==================================\n\nA good place to learn more about Spaced Repetition is [**Spaced Repetition for Efficient Learning**](https://www.gwern.net/Spaced-repetition)  by [Gwern](https://www.lesswrong.com/users/gwern):\n\n> *Spaced repetition is a centuries-old psychological technique for efficient memorization & practice of skills where instead of attempting to memorize by ‘cramming’, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages & medical studies.*\n\nThe key insight for why spaced repetition should be effective is that you forget things approximately hyperbolically-- reviewing things very soon (as in cramming-style learning) is ineffective because you have not forgotten much yet when you come to a review. In comparison, Spaced Repetition allows you to renew your knowledge precisely as you're about to forget a given fact, giving the review the maximum return-on-investment possible and (over time) flattening the 'forgetting curve' so that the interval between successive reviews gets progressively larger for a given fact.\n\nObviously, it's not possible to remind *yourself* of something precisely when you're about to forget it. Enter Spaced Repetition Software (SRS)! By using the forgetting curve, SRS is able to plan when you need to review each item. You can either create decks yourself, or (for some topics) download from databases. [Anki](https://apps.ankiweb.net/) and [Mnemnosyne](https://mnemosyne-proj.org/) are two popular free options, and [SuperMemo](https://www.supermemo.com/en) is a subscription-based choice.\n\n**Criticisms**\n==============\n\nCriticisms of Spaced Repetition primarily revolve around the fact that, for it to be effective, knowledge has to be broken down into individual 'pieces' to go onto cards for testing. This is difficult or impossible for some types of knowledge, and may not promote an integrated view, where the structure or hierarchy of the knowledge is clear, as well as other methods. More can be found in the post [A Vote Against Spaced Repetition](https://www.lesswrong.com/posts/As9E3HfgED2zkTAfB/a-vote-against-spaced-repetition).\n\n**Resources**\n=============\n\nSupermemo material\n------------------\n\n*   [Many articles on assorted related topics](http://supermemo.com/english/contents.htm#Articles)\n*   [Research on memory and learning](http://supermemo.com/english/contents.htm#Research)\n*   [Frequently asked questions about various aspects of spaced repetition](http://supermemo.com/help/faq/index.htm)\n\nSpaced Repetition Decks\n-----------------------\n\nDecks (links, or for Anki, the names of a deck in the Anki collection) relevant to LW.\n\n*   [Anki decks by LW users](http://www.stafforini.com/blog/anki-decks-by-lesswrong-users/) by Pablo_Stafforini. Comprehensive and up-to-date (as of 2019) list.\n*   [Anki deck for biases and fallacies](https://www.lesswrong.com/lw/3px/anki_deck_for_biases_and_fallacies/) by phob\n*   [Deck for Cognitive Science in One Lesson](https://www.lesswrong.com/r/discussion/lw/74o/anki_deck_for_cognitive_science_in_one_lesson/)\n*   [LessWrong Wiki as an Anki deck](https://www.lesswrong.com/r/discussion/lw/ee6/lesswrong_wiki_as_anki_deck) by mapnoterritory\n\nSR cards for [LessWrong Sequences](https://www.lesswrong.com/tag/sequences)\n---------------------------------------------------------------------------\n\n*   [Spaced Repetition Database for the Mysterious Answers to Mysterious Questions Sequence](https://www.lesswrong.com/lw/2e6/spaced_repetition_database_for_the_mysterious/) by divia\n*   [Spaced Repetition Database for A Human's Guide to Words](https://www.lesswrong.com/lw/3oq/spaced_repetition_database_for_a_humans_guide_to/) by divia\n\nOther Spaced Repetition Software\n--------------------------------\n\n*   [VocApp.com](https://vocapp.com/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YPZCAs9Axp9PtrF22",
    "name": "Humility",
    "core": false,
    "slug": "humility",
    "tableOfContents": {
      "html": "<p><span class=\"by_2aoRX3ookcCozcb3m\">Outside of LessWrong, \"humility\" usually </span><a href=\"https://www.google.com/search?q=humility\"><span class=\"by_2aoRX3ookcCozcb3m\">refers to</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> \"a modest </span><span class=\"by_sKAL2jzfkYkDbQmx9\">or</span><span class=\"by_2aoRX3ookcCozcb3m\"> low view of one's own importance\". </span><span class=\"by_sKAL2jzfkYkDbQmx9\">In common parlance, to</span><span class=\"by_2aoRX3ookcCozcb3m\"> be </span></span><a href=\"https://ahdictionary.com/word/search.html?q=%20HUMBLE\"><span class=\"by_2aoRX3ookcCozcb3m\">humble</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> is to be meek, deferential, submissive, or unpretentious, \"not arrogant or prideful\". Thus, in ordinary English \"humility\" and </span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span></span><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_sKAL2jzfkYkDbQmx9\">modesty</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">\"</span><span class=\"by_2aoRX3ookcCozcb3m\"> have pretty similar connotations.</span></span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">On LessWrong, Eliezer Yudkowsky has proposed</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> that we </span><span class=\"by_2aoRX3ookcCozcb3m\">instead draw a sharp distinction between two kinds of \"humility\" — social modesty, versus \"</span></span><strong><span class=\"by_2aoRX3ookcCozcb3m\">scientific humility</span></strong><span class=\"by_2aoRX3ookcCozcb3m\">\".</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">In </span><a href=\"https://www.lesswrong.com/posts/GrDqnMjhqoxiqpQPw/the-proper-use-of-humility\"><span class=\"by_2aoRX3ookcCozcb3m\">The Proper Use of Humility</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> (2006), Yudkowsky writes:</span></p><blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">You suggest studying harder, and the student replies: “No, it wouldn’t work for me; I’m not one of the smart kids like you; nay, one so lowly as myself can hope for no better lot.”</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">This is social modesty, not humility. It has to do with regulating status</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> in </span><span class=\"by_2aoRX3ookcCozcb3m\">the tribe, rather than scientific process.</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">If you ask someone to “be more humble,” by default they’ll associate the words to social modesty—which is an intuitive, everyday, ancestrally relevant concept. Scientific humility is a more recent and rarefied invention, and it is not inherently social. Scientific humility is something you would practice even if you were alone in a spacesuit, light years from Earth with no one watching. Or even if you received an absolute guarantee that no one would ever criticize you again, no matter what you said or thought of yourself. You’d still double-check your calculations if you were wise.</span></p></blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">On LW, then, we tend to follow the convention of using \"humility\" as a term of art for an important part of reasoning: combating </span><a href=\"https://www.lesswrong.com/tag/overconfidence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">overconfidence</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_2aoRX3ookcCozcb3m\"> recognizing and improving on your </span></span><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\"><span class=\"by_2aoRX3ookcCozcb3m\">weaknesses</span></a><span class=\"by_2aoRX3ookcCozcb3m\">, anticipating and preparing for likely errors you'll make, etc.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">In contrast, \"</span><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_2aoRX3ookcCozcb3m\">modesty</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" here refers to the bad habit of letting your behavior and </span><a href=\"https://www.lesswrong.com/tag/epistemology\"><span class=\"by_2aoRX3ookcCozcb3m\">epistemics</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> be </span><span class=\"by_2aoRX3ookcCozcb3m\">ruled by not wanting to look arrogant or conceited. Yudkowsky argues in </span></span><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d\"><i><span class=\"by_2aoRX3ookcCozcb3m\">Inadequate Equilibria</span></i></a><span class=\"by_2aoRX3ookcCozcb3m\"> (2017) that psychological impulses like \"</span><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/o28fkhcZsBhhgfGjx\"><span class=\"by_2aoRX3ookcCozcb3m\">status regulation and anxious underconfidence</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" have caused many people in the effective altruism and rationality communities to adopt a \"</span><a href=\"https://www.lesswrong.com/tag/modest-epistemology\"><span class=\"by_2aoRX3ookcCozcb3m\">modest epistemology</span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" that involves rationalizing various false world-models and invalid reasoning heuristics.</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">LW tries to create a social environment where social reward and punishment is generally </span><span class=\"by_Xn6ACr6Cua8upALWQ\">less </span><span class=\"by_2aoRX3ookcCozcb3m\">salient, and where (to the extent it persists) it incentivizes honesty and truth-seeking as much as possible. LW doesn't always </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">succeed</span></i><span><span class=\"by_2aoRX3ookcCozcb3m\"> in this goal, but this</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> is </span><span class=\"by_2aoRX3ookcCozcb3m\">nonetheless the goal.</span></span></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">The </span><span class=\"by_2aoRX3ookcCozcb3m\">most commonly cited explanation of scientific/epistemic humility on LW</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> is </span><span class=\"by_2aoRX3ookcCozcb3m\">found in Yudkowsky's \"</span></span><a href=\"https://www.yudkowsky.net/rational/virtues\"><span><span class=\"by_2aoRX3ookcCozcb3m\">Twelve Virtues</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> of </span><span class=\"by_2aoRX3ookcCozcb3m\">Rationality</span></span></a><span class=\"by_2aoRX3ookcCozcb3m\">\" (2006):</span></p><blockquote><p><span class=\"by_Xn6ACr6Cua8upALWQ\">The eighth virtue is humility.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">To be humble is to take specific actions in anticipation of your own errors.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.</span></p></blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Humility_versus_Modest_Epistemology\"><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">Humility </span><span class=\"by_2aoRX3ookcCozcb3m\">versus Modest Epistemology</span></span></h2><p><span><span class=\"by_Q7NW4XaWQmfPfdcFj\">While </span><span class=\"by_2aoRX3ookcCozcb3m\">humility</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> is</span><span class=\"by_2aoRX3ookcCozcb3m\"> based on</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> the general idea that you </span><span class=\"by_2aoRX3ookcCozcb3m\">are fallible (and </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">should </span><span class=\"by_2aoRX3ookcCozcb3m\">try</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> to be </span><span class=\"by_2aoRX3ookcCozcb3m\">calibrated and realistic about this), modest epistemology makes stronger claims such as:</span></span></p><ul><li><span><span class=\"by_2aoRX3ookcCozcb3m\">Given</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> your fallibility, you should rely heavily on </span><span class=\"by_2aoRX3ookcCozcb3m\">various techniques associated with \"</span></span><a href=\"https://www.lesswrong.com/tag/inside-outside-view\"><span><span class=\"by_2aoRX3ookcCozcb3m\">the outside </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">view</span></span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">\", and try to avoid using \"inside views\"</span><span class=\"by_Q7NW4XaWQmfPfdcFj\">.</span></span></li><li><span><span class=\"by_2aoRX3ookcCozcb3m\">Given the human tendency to rationalize and self-deceive,</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> you should trust average </span><span class=\"by_2aoRX3ookcCozcb3m\">opinions, or the average opinion of authoritative-sounding sources,</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> more than your own </span><span class=\"by_2aoRX3ookcCozcb3m\">opinions (including</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> your own </span><span class=\"by_2aoRX3ookcCozcb3m\">fine-grained opinions about which authorities have good epistemics on which topics).</span></span></li><li><span><span class=\"by_2aoRX3ookcCozcb3m\">Given the risks</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> and </span><span class=\"by_2aoRX3ookcCozcb3m\">commonness of </span></span><a href=\"https://www.lesswrong.com/tag/overconfidence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">overconfidence</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">,</span><span class=\"by_2aoRX3ookcCozcb3m\"> you should worry much more about overconfidence, and worry little (or not at all) about </span></span><a href=\"https://www.lesswrong.com/s/pvim9PZJ6qHRTMqD3/p/pkFazhcTErMw7TFtT\"><span class=\"by_2aoRX3ookcCozcb3m\">underconfidence</span></a><span class=\"by_2aoRX3ookcCozcb3m\">.</span></li></ul><p><span class=\"by_2aoRX3ookcCozcb3m\">In contrast, Yudkowsky has </span><a href=\"https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/o28fkhcZsBhhgfGjx\"><span class=\"by_2aoRX3ookcCozcb3m\">argued</span></a><span class=\"by_2aoRX3ookcCozcb3m\">:</span></p><blockquote><p><span class=\"by_2aoRX3ookcCozcb3m\">I try to be careful to distinguish the virtue of avoiding overconfidence, which I sometimes call “</span><a href=\"https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/\"><span class=\"by_2aoRX3ookcCozcb3m\">humility</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\">,” from the phenomenon I’m calling “modest epistemology.” But even so, when overconfidence is such a terrible scourge according to the cognitive bias literature, can it ever be wise to caution people </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">against </span></span><i><span class=\"by_2aoRX3ookcCozcb3m\">under</span></i><span class=\"by_2aoRX3ookcCozcb3m\">confidence?</span></p><p><span><span class=\"by_2aoRX3ookcCozcb3m\">Yes. First of all, overcompensation after being warned about a cognitive bias is also a recognized problem in the literature; and the literature on that talks about how bad people often are at determining whether they’re undercorrecting or overcorrecting. Second, my own experience has been that while, yes, commenters on the Internet are often overconfident, it’s very different when I’m talking to people in person. My </span><span class=\"by_Q7NW4XaWQmfPfdcFj\">more </span><span class=\"by_2aoRX3ookcCozcb3m\">recent experience seems more like 90% telling people to</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> be </span><span class=\"by_2aoRX3ookcCozcb3m\">less underconfident, to reach higher, to be more ambitious, to test themselves, and maybe 10% cautioning people against overconfidence. And yes, this ratio applies to men as well as women and nonbinary people, and to people considered high-status as well as people considered low-status.</span></span></p></blockquote><p><a href=\"https://www.lesswrong.com/posts/pkFazhcTErMw7TFtT/the-sin-of-underconfidence\"><span class=\"by_2aoRX3ookcCozcb3m\">The Sin of Underconfidence</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> (2009) argues that underconfidence is one of the \"three great besetting sins of rationalists\" (the </span><a href=\"https://www.lesswrong.com/posts/yffPyiu7hRLyc7r23/final-words\"><span class=\"by_2aoRX3ookcCozcb3m\">others</span></a><span><span class=\"by_2aoRX3ookcCozcb3m\"> being motivated reasoning /</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> </span></span><a href=\"https://www.lesswrong.com/tag/motivated-skepticism\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">motivated skepticism</span></a><span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> </span><span class=\"by_2aoRX3ookcCozcb3m\">and \"cleverness\"</span><span class=\"by_Q7NW4XaWQmfPfdcFj\">).</span></span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">In </span><a href=\"https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view\"><span class=\"by_2aoRX3ookcCozcb3m\">Taboo \"Outside View\"</span></a><span class=\"by_2aoRX3ookcCozcb3m\"> (2021), Daniel Kokotajlo notes that the original meaning of \"outside view\" (</span><a href=\"https://en.wikipedia.org/wiki/Reference_class_forecasting\"><u><span class=\"by_2aoRX3ookcCozcb3m\">reference class forecasting</span></u></a><span class=\"by_2aoRX3ookcCozcb3m\">) has become eroded as EAs have begun using \"outside view\" to refer to everything from reasoning by analogy, to trend extrapolation, to foxy aggregation, to bias correction, to \"deference to wisdom of the many\", to \"anti-weirdness heuristics\", to priors, etc.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Additionally, proponents of outside-viewing often behave as though there is a single obvious reference class to use -- \"</span><i><span class=\"by_2aoRX3ookcCozcb3m\">the</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> outside view\", as opposed to \"</span><i><span class=\"by_2aoRX3ookcCozcb3m\">an</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> outside view\" -- and tend to neglect the role of detailed model-building in helping us figure out which reference classes are relevant.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">The lesson of this isn't \"it's bad to ever use reference class forecasting, trend extrapolation, etc.\", but rather that these tools are part and parcel of building good world-models and deriving good predictions from them, rather than being a robust replacement for world-modeling.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">Likewise, the lesson isn't \"it's bad to ever worry about overconfidence\", but rather that overconfidence and underconfidence are </span><i><span class=\"by_2aoRX3ookcCozcb3m\">both</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> problems, neither is </span><i><span class=\"by_2aoRX3ookcCozcb3m\">a priori</span></i><span class=\"by_2aoRX3ookcCozcb3m\"> worse than the other, and fixing them requires doing a lot of legwork and model-building about your own capabilities -- again, there isn't a royal road to 'getting the right answer without having to figure things out'.</span></p><p><span class=\"by_2aoRX3ookcCozcb3m\">&nbsp;</span></p><h2 id=\"Related_pages\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">Related </span><span class=\"by_2aoRX3ookcCozcb3m\">pages</span></span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/calibration\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Calibration</span></a></li><li><a href=\"https://www.lesswrong.com/tag/chesterton-s-fence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Chesterton's Fence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/underconfidence\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Underconfidence</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> and </span><a href=\"https://www.lesswrong.com/tag/overconfidence\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Overconfidence</span></a></li><li><a href=\"https://www.lesswrong.com/tag/modest-epistemology\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Modest Epistemology</span></a></li><li><a href=\"https://www.lesswrong.com/tag/modesty\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Modesty</span></a></li><li><a href=\"https://www.lesswrong.com/tag/fallacy-of-gray\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">Fallacy of Gray</span></a></li></ul>",
      "sections": [
        {
          "title": "Humility versus Modest Epistemology",
          "anchor": "Humility_versus_Modest_Epistemology",
          "level": 1
        },
        {
          "title": "Related pages",
          "anchor": "Related_pages",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 25,
    "description": {
      "markdown": "Outside of LessWrong, \"humility\" usually [refers to](https://www.google.com/search?q=humility) \"a modest or low view of one's own importance\". In common parlance, to be [humble](https://ahdictionary.com/word/search.html?q=%20HUMBLE) is to be meek, deferential, submissive, or unpretentious, \"not arrogant or prideful\". Thus, in ordinary English \"humility\" and \"[modesty](https://www.lesswrong.com/tag/modesty)\" have pretty similar connotations.\n\nOn LessWrong, Eliezer Yudkowsky has proposed that we instead draw a sharp distinction between two kinds of \"humility\" — social modesty, versus \"**scientific humility**\".\n\nIn [The Proper Use of Humility](https://www.lesswrong.com/posts/GrDqnMjhqoxiqpQPw/the-proper-use-of-humility) (2006), Yudkowsky writes:\n\n> You suggest studying harder, and the student replies: “No, it wouldn’t work for me; I’m not one of the smart kids like you; nay, one so lowly as myself can hope for no better lot.”\n> \n> This is social modesty, not humility. It has to do with regulating status in the tribe, rather than scientific process.\n> \n> If you ask someone to “be more humble,” by default they’ll associate the words to social modesty—which is an intuitive, everyday, ancestrally relevant concept. Scientific humility is a more recent and rarefied invention, and it is not inherently social. Scientific humility is something you would practice even if you were alone in a spacesuit, light years from Earth with no one watching. Or even if you received an absolute guarantee that no one would ever criticize you again, no matter what you said or thought of yourself. You’d still double-check your calculations if you were wise.\n\nOn LW, then, we tend to follow the convention of using \"humility\" as a term of art for an important part of reasoning: combating [overconfidence](https://www.lesswrong.com/tag/overconfidence), recognizing and improving on your [weaknesses](https://www.lesswrong.com/tag/heuristics-and-biases), anticipating and preparing for likely errors you'll make, etc.\n\nIn contrast, \"[modesty](https://www.lesswrong.com/tag/modesty)\" here refers to the bad habit of letting your behavior and [epistemics](https://www.lesswrong.com/tag/epistemology) be ruled by not wanting to look arrogant or conceited. Yudkowsky argues in [*Inadequate Equilibria*](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d) (2017) that psychological impulses like \"[status regulation and anxious underconfidence](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/o28fkhcZsBhhgfGjx)\" have caused many people in the effective altruism and rationality communities to adopt a \"[modest epistemology](https://www.lesswrong.com/tag/modest-epistemology)\" that involves rationalizing various false world-models and invalid reasoning heuristics.\n\nLW tries to create a social environment where social reward and punishment is generally less salient, and where (to the extent it persists) it incentivizes honesty and truth-seeking as much as possible. LW doesn't always *succeed* in this goal, but this is nonetheless the goal.\n\nThe most commonly cited explanation of scientific/epistemic humility on LW is found in Yudkowsky's \"[Twelve Virtues of Rationality](https://www.yudkowsky.net/rational/virtues)\" (2006):\n\n> The eighth virtue is humility.\n> \n> To be humble is to take specific actions in anticipation of your own errors.\n> \n> To confess your fallibility and then do nothing about it is not humble; it is boasting of your modesty.\n> \n> Who are most humble? Those who most skillfully prepare for the deepest and most catastrophic errors in their own beliefs and plans.\n> \n> Because this world contains many whose grasp of rationality is abysmal, beginning students of rationality win arguments and acquire an exaggerated view of their own abilities. But it is useless to be superior: Life is not graded on a curve. The best physicist in ancient Greece could not calculate the path of a falling apple. There is no guarantee that adequacy is possible given your hardest effort; therefore spare no thought for whether others are doing worse. If you compare yourself to others you will not see the biases that all humans share. To be human is to make ten thousand errors. No one in this world achieves perfection.\n\nHumility versus Modest Epistemology\n-----------------------------------\n\nWhile humility is based on the general idea that you are fallible (and should try to be calibrated and realistic about this), modest epistemology makes stronger claims such as:\n\n*   Given your fallibility, you should rely heavily on various techniques associated with \"[the outside view](https://www.lesswrong.com/tag/inside-outside-view)\", and try to avoid using \"inside views\".\n*   Given the human tendency to rationalize and self-deceive, you should trust average opinions, or the average opinion of authoritative-sounding sources, more than your own opinions (including your own fine-grained opinions about which authorities have good epistemics on which topics).\n*   Given the risks and commonness of [overconfidence](https://www.lesswrong.com/tag/overconfidence), you should worry much more about overconfidence, and worry little (or not at all) about [underconfidence](https://www.lesswrong.com/s/pvim9PZJ6qHRTMqD3/p/pkFazhcTErMw7TFtT).\n\nIn contrast, Yudkowsky has [argued](https://www.lesswrong.com/s/oLGCcbnvabyibnG9d/p/o28fkhcZsBhhgfGjx):\n\n> I try to be careful to distinguish the virtue of avoiding overconfidence, which I sometimes call “[humility](https://www.lesswrong.com/lw/gq/the_proper_use_of_humility/),” from the phenomenon I’m calling “modest epistemology.” But even so, when overconfidence is such a terrible scourge according to the cognitive bias literature, can it ever be wise to caution people against *under*confidence?\n> \n> Yes. First of all, overcompensation after being warned about a cognitive bias is also a recognized problem in the literature; and the literature on that talks about how bad people often are at determining whether they’re undercorrecting or overcorrecting. Second, my own experience has been that while, yes, commenters on the Internet are often overconfident, it’s very different when I’m talking to people in person. My more recent experience seems more like 90% telling people to be less underconfident, to reach higher, to be more ambitious, to test themselves, and maybe 10% cautioning people against overconfidence. And yes, this ratio applies to men as well as women and nonbinary people, and to people considered high-status as well as people considered low-status.\n\n[The Sin of Underconfidence](https://www.lesswrong.com/posts/pkFazhcTErMw7TFtT/the-sin-of-underconfidence) (2009) argues that underconfidence is one of the \"three great besetting sins of rationalists\" (the [others](https://www.lesswrong.com/posts/yffPyiu7hRLyc7r23/final-words) being motivated reasoning / [motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism) and \"cleverness\").\n\nIn [Taboo \"Outside View\"](https://www.lesswrong.com/posts/BcYfsi7vmhDvzQGiF/taboo-outside-view) (2021), Daniel Kokotajlo notes that the original meaning of \"outside view\" ([reference class forecasting](https://en.wikipedia.org/wiki/Reference_class_forecasting)) has become eroded as EAs have begun using \"outside view\" to refer to everything from reasoning by analogy, to trend extrapolation, to foxy aggregation, to bias correction, to \"deference to wisdom of the many\", to \"anti-weirdness heuristics\", to priors, etc.\n\nAdditionally, proponents of outside-viewing often behave as though there is a single obvious reference class to use -- \"*the* outside view\", as opposed to \"*an* outside view\" -- and tend to neglect the role of detailed model-building in helping us figure out which reference classes are relevant.\n\nThe lesson of this isn't \"it's bad to ever use reference class forecasting, trend extrapolation, etc.\", but rather that these tools are part and parcel of building good world-models and deriving good predictions from them, rather than being a robust replacement for world-modeling.\n\nLikewise, the lesson isn't \"it's bad to ever worry about overconfidence\", but rather that overconfidence and underconfidence are *both* problems, neither is *a priori* worse than the other, and fixing them requires doing a lot of legwork and model-building about your own capabilities -- again, there isn't a royal road to 'getting the right answer without having to figure things out'.\n\nRelated pages\n-------------\n\n*   [Calibration](https://www.lesswrong.com/tag/calibration)\n*   [Chesterton's Fence](https://www.lesswrong.com/tag/chesterton-s-fence)\n*   [Underconfidence](https://www.lesswrong.com/tag/underconfidence) and [Overconfidence](https://www.lesswrong.com/tag/overconfidence)\n*   [Modest Epistemology](https://www.lesswrong.com/tag/modest-epistemology)\n*   [Modesty](https://www.lesswrong.com/tag/modesty)\n*   [Fallacy of Gray](https://www.lesswrong.com/tag/fallacy-of-gray)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ai87fPyyT6mWb4YkT",
    "name": "Eldritch Analogies",
    "core": null,
    "slug": "eldritch-analogies",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Eldritch Analogies</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> are the association of impersonal social or physical dynamics with fictional or mythological deities. The most well known is Moloch, who is associated with coordination problems and competition.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": {
      "markdown": "**Eldritch Analogies** are the association of impersonal social or physical dynamics with fictional or mythological deities. The most well known is Moloch, who is associated with coordination problems and competition."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "DbMQGrxbhLxtNkmca",
    "name": "Value of Information",
    "core": false,
    "slug": "value-of-information",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Value of Information</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> (VoI) is a concept from </span><a href=\"https://www.lesswrong.com/lw/8xr/decision_analysis_sequence/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">decision analysis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">: how much answering a question allows a decision-maker to improve its decision.</span></p><p><em><span class=\"by_Xn6ACr6Cua8upALWQ\">See also: </span><a href=\"https://www.lesswrong.com/tag/bayes-theorem-bayesianism\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Bayes theorem</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_Xn6ACr6Cua8upALWQ\">Decision Theory</span></a></em></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Value of Information** (VoI) is a concept from [decision analysis](https://www.lesswrong.com/lw/8xr/decision_analysis_sequence/): how much answering a question allows a decision-maker to improve its decision.\n\n_See also: [Bayes theorem](https://www.lesswrong.com/tag/bayes-theorem-bayesianism), [Decision Theory](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)_"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "36RkM85iDocrnaypb",
    "name": "Aesthetics",
    "core": null,
    "slug": "aesthetics",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_XtphY3uYHwruKqDyG\">Aesthetics</span></strong><span class=\"by_XtphY3uYHwruKqDyG\">:</span><strong><span class=\"by_r38pkCm7wF4M44MDQ\">&nbsp;</span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\">\"Imagine</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> if we could talk about why things seem beautiful and appealing, or ugly and unappealing.&nbsp; Where do these preferences come from, in a causal sense? Do we still endorse them when we know their origins?&nbsp; What happens when we bring tacit things into consciousness, when we talk carefully about what aesthetics evoke in us, and how that might be the same or different from person to person?\"&nbsp; </span><span class=\"by_EQNTWXLKMeWMp2FQS\">–</span><span class=\"by_nLbwLhBaQeG6tCNDN\"> </span></span><a href=\"/posts/4ZwGqkMTyAvANYEDw/naming-the-nameless\"><span><span class=\"by_nLbwLhBaQeG6tCNDN\">Naming the </span><span class=\"by_XtphY3uYHwruKqDyG\">Nameless, Sarah Constantin</span></span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 27,
    "description": {
      "markdown": "**Aesthetics**:\"Imagine if we could talk about why things seem beautiful and appealing, or ugly and unappealing.  Where do these preferences come from, in a causal sense? Do we still endorse them when we know their origins?  What happens when we bring tacit things into consciousness, when we talk carefully about what aesthetics evoke in us, and how that might be the same or different from person to person?\"  – [Naming the Nameless, Sarah Constantin](/posts/4ZwGqkMTyAvANYEDw/naming-the-nameless)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xHTXnyp65X8YX6ahT",
    "name": "Instrumental Convergence",
    "core": false,
    "slug": "instrumental-convergence",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental convergence</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">or </span></span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">convergent instrumental values </span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\">is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the theorized tendency for</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> most sufficiently </span><span class=\"by_qgdGA4ZEyW7zNdK84\">intelligent agents to pursue potentially unbounded</span><span class=\"by_2aoRX3ookcCozcb3m\"> instrumental goals </span><span class=\"by_woC2b5rav5sGrAo3E\">such as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">self-preservation</span><span class=\"by_woC2b5rav5sGrAo3E\"> and </span><span class=\"by_qgdGA4ZEyW7zNdK84\">resource acquisition [</span></span><a href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]. This concept has also been discussed under the term </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">basic drives.</span></i></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The idea was first explored by </span><a href=\"https://en.wikipedia.org/wiki/Steve_Omohundro\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Steve Omohundro</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. He argued that sufficiently advanced AI systems would all naturally discover similar instrumental subgoals. The view that there are important basic AI drives was subsequently defended by </span><a href=\"https://lessestwrong.com/tag/nick-bostrom\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Nick Bostrom</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> as the</span><i><span class=\"by_qgdGA4ZEyW7zNdK84\"> instrumental convergence thesis</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, or the </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">convergent instrumental goals thesis</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">. On this view, a few goals are </span><a href=\"https://lessestwrong.com/tag/instrumental-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">instrumental</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> to almost all possible </span><a href=\"https://lessestwrong.com/tag/terminal-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">final</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> goals. Therefore, all advanced AIs will pursue these instrumental goals. Omohundro uses microeconomic theory by von Neumann to support this idea.</span></p><h2 id=\"Omohundro_s_Drives\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Omohundro’s Drives</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Omohundro presents two sets of values, one for self-improving artificial intelligences </span><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and another he says will emerge in any sufficiently advanced AGI system </span><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. The former set is composed of four main drives:</span></p><ul><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-preservation</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: A sufficiently advanced AI will probably be the best entity to achieve its goals. Therefore it must continue existing in order to maximize goal fulfillment. Similarly, if its goal system were modified, then it would likely begin pursuing different ends. Since this is not desirable to the current AI, it will act to preserve the content of its goal system.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Efficiency</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: At any time, the AI will have finite resources of time, space, matter, energy and computational power. Using these more efficiently will increase its utility. This will lead the AI to do things like implement more efficient algorithms, physical embodiments, and particular mechanisms. It will also lead the AI to replace desired physical events with computational simulations as much as possible, to expend fewer resources.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Acquisition</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: Resources like matter and energy are indispensable for action. The more resources the AI can control, the more actions it can perform to achieve its goals. The AI's physical capabilities are determined by its level of technology. For instance, if the AI could invent nanotechnology, it would vastly increase the actions it could take to achieve its goals.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Creativity</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: The AI's operations will depend on its ability to come up with new, more efficient ideas. It will be driven to acquire more computational power for raw searching ability, and it will also be driven to search for better search algorithms. Omohundro argues that the drive for creativity is critical for the AI to display the richness and diversity that is valued by humanity. He discusses </span><a href=\"https://lessestwrong.com/tag/signaling\"><span class=\"by_qgdGA4ZEyW7zNdK84\">signaling</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> goals as particularly rich sources of creativity.</span></li></ul><h2 id=\"Bostrom_s_Drives\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom’s Drives</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom argues for an </span><a href=\"https://lessestwrong.com/tag/orthogonality-thesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">orthogonality thesis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">: But he also argues that, despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a particular set of instrumental values that are useful for achieving any kind of </span><a href=\"https://lessestwrong.com/tag/terminal-value\"><span class=\"by_qgdGA4ZEyW7zNdK84\">terminal value</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> On his view, those values are:</span></p><ul><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-preservation</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: A superintelligence will value its continuing existence as a means to to continuing to take actions that promote its values.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Goal-content integrity</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: The superintelligence will value retaining the same preferences over time. Modifications to its future values through swapping memories, downloading skills, and altering its cognitive architecture and personalities would result in its transformation into an agent that no longer optimizes for the same things.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Cognitive enhancement</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: Improvements in cognitive capacity, intelligence and rationality will help the superintelligence make better decisions, furthering its goals more in the long run.</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Technological perfection</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: Increases in hardware power and algorithm efficiency will deliver increases in its cognitive capacities. Also, better engineering will enable the creation of a wider set of physical structures using fewer resources (e.g., </span><a href=\"https://lessestwrong.com/tag/nanotechnology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">nanotechnology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">).</span></li><li><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Resource acquisition</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">: In addition to guaranteeing the superintelligence's continued existence, basic resources such as time, space, matter and free energy could be processed to serve almost any goal, in the form of extended hardware, backups and protection.</span></li></ul><h2 id=\"Relevance\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Relevance</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Both Bostrom and Omohundro argue these values should be used in trying to predict a superintelligence's behavior, since they are likely to be the only set of values shared by most superintelligences. They also note that these values are consistent with safe and beneficial AIs as well as unsafe ones.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom emphasizes, however, that our ability to predict a superintelligence's behavior may be very limited even if it shares most intelligences' instrumental goals.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Yudkowsky echoes Omohundro's point that the convergence thesis is consistent with the possibility of Friendly AI. However, he also notes that the convergence thesis implies that most AIs will be extremely dangerous, merely by being indifferent to one or more human values:</span><a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">4</span></a></p><h2 id=\"Pathological_Cases\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pathological Cases</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In some rarer cases, AIs may not pursue these goals. For instance, if there are two AIs with the same goals, the less capable AI may determine that it should destroy itself to allow the stronger AI to control the universe. Or an AI may have the goal of using as few resources as possible, or of being as unintelligent as possible. These relatively specific goals will limit the growth and power of the AI.</span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://arbital.com/p/convergent_strategies/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Convergent instrumental strategies</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (</span><a href=\"https://wiki.lesswrong.com/wiki/Arbital\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Arbital</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></li><li><a href=\"https://arbital.com/p/instrumental_convergence/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental convergence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (</span><a href=\"https://wiki.lesswrong.com/wiki/Arbital\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Arbital</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></li><li><a href=\"https://lessestwrong.com/tag/orthogonality-thesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Orthogonality thesis</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Cox's_theorem\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cox's theorem</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Unfriendly_AI\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Unfriendly AI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/paperclip-maximizer\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Paperclip maximizer</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/oracle-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Oracle AI</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Instrumental_values\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental values</span></a></li></ul><h2 id=\"References\"><span class=\"by_qgdGA4ZEyW7zNdK84\">References</span></h2><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Omohundro, S. (2007). </span><a href=\"http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf\"><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">The Nature of Self-Improving Artificial Intelligence</span></u></i></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Omohundro, S. (2008). \"</span><a href=\"http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">The Basic AI Drives</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">\". </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Proceedings of the First AGI Conference</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Omohundro, S. (2012). </span><a href=\"http://selfawaresystems.files.wordpress.com/2012/03/rational_ai_greater_good.pdf\"><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Rational Artificial Intelligence for the Greater Good</span></u></i></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Bostrom, N. (2012). \"</span><a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><u><span class=\"by_qgdGA4ZEyW7zNdK84\">The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents</span></u></a><span class=\"by_qgdGA4ZEyW7zNdK84\">\". </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Minds and Machines</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Shulman, C. (2010). </span><a href=\"http://intelligence.org/files/BasicAIDrives.pdf\"><i><u><span class=\"by_qgdGA4ZEyW7zNdK84\">Omohundro's \"Basic AI Drives\" and Catastrophic Risks</span></u></i></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></li></ul>",
      "sections": [
        {
          "title": "Omohundro’s Drives",
          "anchor": "Omohundro_s_Drives",
          "level": 1
        },
        {
          "title": "Bostrom’s Drives",
          "anchor": "Bostrom_s_Drives",
          "level": 1
        },
        {
          "title": "Relevance",
          "anchor": "Relevance",
          "level": 1
        },
        {
          "title": "Pathological Cases",
          "anchor": "Pathological_Cases",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 7
    },
    "postCount": 53,
    "description": {
      "markdown": "**Instrumental convergence** or **convergent instrumental values** is the theorized tendency for most sufficiently intelligent agents to pursue potentially unbounded instrumental goals such as self-preservation and resource acquisition \\[[1](https://en.wikipedia.org/wiki/Instrumental_convergence)\\]. This concept has also been discussed under the term *basic drives.*\n\nThe idea was first explored by [Steve Omohundro](https://en.wikipedia.org/wiki/Steve_Omohundro). He argued that sufficiently advanced AI systems would all naturally discover similar instrumental subgoals. The view that there are important basic AI drives was subsequently defended by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) as the *instrumental convergence thesis*, or the *convergent instrumental goals thesis*. On this view, a few goals are [instrumental](https://lessestwrong.com/tag/instrumental-value) to almost all possible [final](https://lessestwrong.com/tag/terminal-value) goals. Therefore, all advanced AIs will pursue these instrumental goals. Omohundro uses microeconomic theory by von Neumann to support this idea.\n\nOmohundro’s Drives\n------------------\n\nOmohundro presents two sets of values, one for self-improving artificial intelligences [1](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf) and another he says will emerge in any sufficiently advanced AGI system [2](http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf). The former set is composed of four main drives:\n\n*   **Self-preservation**: A sufficiently advanced AI will probably be the best entity to achieve its goals. Therefore it must continue existing in order to maximize goal fulfillment. Similarly, if its goal system were modified, then it would likely begin pursuing different ends. Since this is not desirable to the current AI, it will act to preserve the content of its goal system.\n*   **Efficiency**: At any time, the AI will have finite resources of time, space, matter, energy and computational power. Using these more efficiently will increase its utility. This will lead the AI to do things like implement more efficient algorithms, physical embodiments, and particular mechanisms. It will also lead the AI to replace desired physical events with computational simulations as much as possible, to expend fewer resources.\n*   **Acquisition**: Resources like matter and energy are indispensable for action. The more resources the AI can control, the more actions it can perform to achieve its goals. The AI's physical capabilities are determined by its level of technology. For instance, if the AI could invent nanotechnology, it would vastly increase the actions it could take to achieve its goals.\n*   **Creativity**: The AI's operations will depend on its ability to come up with new, more efficient ideas. It will be driven to acquire more computational power for raw searching ability, and it will also be driven to search for better search algorithms. Omohundro argues that the drive for creativity is critical for the AI to display the richness and diversity that is valued by humanity. He discusses [signaling](https://lessestwrong.com/tag/signaling) goals as particularly rich sources of creativity.\n\nBostrom’s Drives\n----------------\n\nBostrom argues for an [orthogonality thesis](https://lessestwrong.com/tag/orthogonality-thesis): But he also argues that, despite the fact that values and intelligence are independent, any recursively self-improving intelligence would likely possess a particular set of instrumental values that are useful for achieving any kind of [terminal value](https://lessestwrong.com/tag/terminal-value).[3](http://www.nickbostrom.com/superintelligentwill.pdf) On his view, those values are:\n\n*   **Self-preservation**: A superintelligence will value its continuing existence as a means to to continuing to take actions that promote its values.\n*   **Goal-content integrity**: The superintelligence will value retaining the same preferences over time. Modifications to its future values through swapping memories, downloading skills, and altering its cognitive architecture and personalities would result in its transformation into an agent that no longer optimizes for the same things.\n*   **Cognitive enhancement**: Improvements in cognitive capacity, intelligence and rationality will help the superintelligence make better decisions, furthering its goals more in the long run.\n*   **Technological perfection**: Increases in hardware power and algorithm efficiency will deliver increases in its cognitive capacities. Also, better engineering will enable the creation of a wider set of physical structures using fewer resources (e.g., [nanotechnology](https://lessestwrong.com/tag/nanotechnology)).\n*   **Resource acquisition**: In addition to guaranteeing the superintelligence's continued existence, basic resources such as time, space, matter and free energy could be processed to serve almost any goal, in the form of extended hardware, backups and protection.\n\nRelevance\n---------\n\nBoth Bostrom and Omohundro argue these values should be used in trying to predict a superintelligence's behavior, since they are likely to be the only set of values shared by most superintelligences. They also note that these values are consistent with safe and beneficial AIs as well as unsafe ones.\n\nBostrom emphasizes, however, that our ability to predict a superintelligence's behavior may be very limited even if it shares most intelligences' instrumental goals.\n\nYudkowsky echoes Omohundro's point that the convergence thesis is consistent with the possibility of Friendly AI. However, he also notes that the convergence thesis implies that most AIs will be extremely dangerous, merely by being indifferent to one or more human values:[4](http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/)\n\nPathological Cases\n------------------\n\nIn some rarer cases, AIs may not pursue these goals. For instance, if there are two AIs with the same goals, the less capable AI may determine that it should destroy itself to allow the stronger AI to control the universe. Or an AI may have the goal of using as few resources as possible, or of being as unintelligent as possible. These relatively specific goals will limit the growth and power of the AI.\n\nSee Also\n--------\n\n*   [Convergent instrumental strategies](https://arbital.com/p/convergent_strategies/) ([Arbital](https://wiki.lesswrong.com/wiki/Arbital))\n*   [Instrumental convergence](https://arbital.com/p/instrumental_convergence/) ([Arbital](https://wiki.lesswrong.com/wiki/Arbital))\n*   [Orthogonality thesis](https://lessestwrong.com/tag/orthogonality-thesis)\n*   [Cox's theorem](https://wiki.lesswrong.com/wiki/Cox's_theorem)\n*   [Unfriendly AI](https://wiki.lesswrong.com/wiki/Unfriendly_AI), [Paperclip maximizer](https://lessestwrong.com/tag/paperclip-maximizer), [Oracle AI](https://lessestwrong.com/tag/oracle-ai)\n*   [Instrumental values](https://wiki.lesswrong.com/wiki/Instrumental_values)\n\nReferences\n----------\n\n*   Omohundro, S. (2007). [*The Nature of Self-Improving Artificial Intelligence*](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf).\n*   Omohundro, S. (2008). \"[The Basic AI Drives](http://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/)\". *Proceedings of the First AGI Conference*.\n*   Omohundro, S. (2012). [*Rational Artificial Intelligence for the Greater Good*](http://selfawaresystems.files.wordpress.com/2012/03/rational_ai_greater_good.pdf).\n*   Bostrom, N. (2012). \"[The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents](http://www.nickbostrom.com/superintelligentwill.pdf)\". *Minds and Machines*.\n*   Shulman, C. (2010). [*Omohundro's \"Basic AI Drives\" and Catastrophic Risks*](http://intelligence.org/files/BasicAIDrives.pdf)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JsJPrdgRGRqnci8cZ",
    "name": "Altruism",
    "core": null,
    "slug": "altruism",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_8bJycwyZpC4XbwPCF\">Altruism</span></strong><span><span class=\"by_8bJycwyZpC4XbwPCF\"> refers to</span><span class=\"by_ezbRa3dntKWQ5995r\"> </span><span class=\"by_8bJycwyZpC4XbwPCF\">actions</span><span class=\"by_cn4SiEmqWbu7K9em5\"> undertaken for the </span><span class=\"by_LDxtSTvkdPFJJ28nX\">concern and </span><span class=\"by_cn4SiEmqWbu7K9em5\">benefit of </span><span class=\"by_LDxtSTvkdPFJJ28nX\">others at ones own expense.</span></span></p><span class=\"by_ezbRa3dntKWQ5995r\">\n</span><p><em><span class=\"by_8bJycwyZpC4XbwPCF\">Related tags and wikis:</span></em><span class=\"by_8bJycwyZpC4XbwPCF\"> </span><a href=\"https://www.lesswrong.com/tag/shut-up-and-multiply\"><span class=\"by_qf77EiaoMw7tH3GSr\">Shut up and multiply</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/fuzzies\"><span class=\"by_qf77EiaoMw7tH3GSr\">Fuzzies</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/world-optimization\"><span class=\"by_8bJycwyZpC4XbwPCF\">World Optimization</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/effective-altruism\"><span class=\"by_8bJycwyZpC4XbwPCF\">Effective Altruism</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/cause-prioritization\"><span class=\"by_8bJycwyZpC4XbwPCF\">Cause Prioritization</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/motivations\"><span class=\"by_8bJycwyZpC4XbwPCF\">Motivations</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/psychology-of-altruism\"><span class=\"by_8bJycwyZpC4XbwPCF\">Psychology of Altruism</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/ethics-and-morality\"><span class=\"by_8bJycwyZpC4XbwPCF\">Ethics and Morality</span></a></p><span class=\"by_8bJycwyZpC4XbwPCF\">\n</span>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 69,
    "description": {
      "markdown": "**Altruism** refers to actions undertaken for the concern and benefit of others at ones own expense.\n\n_Related tags and wikis:_ [Shut up and multiply](https://www.lesswrong.com/tag/shut-up-and-multiply), [Fuzzies](https://www.lesswrong.com/tag/fuzzies), [World Optimization](https://www.lesswrong.com/tag/world-optimization), [Effective Altruism](https://www.lesswrong.com/tag/effective-altruism), [Cause Prioritization](https://www.lesswrong.com/tag/cause-prioritization), [Motivations](https://www.lesswrong.com/tag/motivations), [Psychology of Altruism](https://www.lesswrong.com/tag/psychology-of-altruism), [Ethics and Morality](https://www.lesswrong.com/tag/ethics-and-morality)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yeJFqsWrP2pjYfNEr",
    "name": "Deontology",
    "core": false,
    "slug": "deontology",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Deontology</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> is a theory of morality based around obeying moral rules.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 21,
    "description": {
      "markdown": "**Deontology** is a theory of morality based around obeying moral rules."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "2q2cK4FdnSeohTEaJ",
    "name": "Blackmail / Extortion",
    "core": false,
    "slug": "blackmail-extortion",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Blackmail</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> is an act of </span><a href=\"https://en.wikipedia.org/wiki/Coercion\"><span class=\"by_sKAL2jzfkYkDbQmx9\">coercion</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> using the </span><a href=\"https://en.wikipedia.org/wiki/Threat\"><span class=\"by_sKAL2jzfkYkDbQmx9\">threat</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> of revealing or publicizing either </span><a href=\"https://en.wikipedia.org/wiki/Substantial_truth\"><span class=\"by_sKAL2jzfkYkDbQmx9\">substantially true</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> or </span><a href=\"https://en.wikipedia.org/wiki/False_information\"><span class=\"by_sKAL2jzfkYkDbQmx9\">false information</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> about a person or people unless certain demands are met (</span><a href=\"https://en.wikipedia.org/wiki/Blackmail\"><span class=\"by_sKAL2jzfkYkDbQmx9\">from Wikipedia</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">)</span><span class=\"by_r38pkCm7wF4M44MDQ\">, </span></span><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Extortion</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is the more general concept of threatening someone to get them to give you something. They are often discussed together as a reference class.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Blackmail** is an act of [coercion](https://en.wikipedia.org/wiki/Coercion) using the [threat](https://en.wikipedia.org/wiki/Threat) of revealing or publicizing either [substantially true](https://en.wikipedia.org/wiki/Substantial_truth) or [false information](https://en.wikipedia.org/wiki/False_information) about a person or people unless certain demands are met ([from Wikipedia](https://en.wikipedia.org/wiki/Blackmail)), **Extortion** is the more general concept of threatening someone to get them to give you something. They are often discussed together as a reference class."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YTCrHWYHAsAD74EHo",
    "name": "Self-Deception",
    "core": false,
    "slug": "self-deception",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Self-deception</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a state of preserving a wrong </span><a href=\"https://lessestwrong.com/tag/belief\"><span class=\"by_qgdGA4ZEyW7zNdK84\">belief</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, often facilitated by denying or </span><a href=\"https://lessestwrong.com/tag/rationalization\"><span class=\"by_qgdGA4ZEyW7zNdK84\">rationalizing away</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> the relevance, significance, or importance of opposing </span><a href=\"https://lessestwrong.com/tag/evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">evidence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and logical arguments. Beliefs supported by self-deception are often chosen for reasons other than how closely those beliefs approximate </span><a href=\"https://lessestwrong.com/tag/truth-semantics-and-meaning\"><span class=\"by_qgdGA4ZEyW7zNdK84\">truth</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related:</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Anticipated Experiences</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Motivated Reasoning</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/rationalization?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Rationalization</span></a></p><p><span><span class=\"by_Xn6ACr6Cua8upALWQ\">On LessWrong, a common distinction</span><span class=\"by_XzXbiS2zWYNdZdLW8\"> is </span><span class=\"by_Xn6ACr6Cua8upALWQ\">between</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&amp;useTagName=false\"><span class=\"by_Xn6ACr6Cua8upALWQ\">beliefs as expectation-controllers</span></a><span class=\"by_XzXbiS2zWYNdZdLW8\"> and </span><a href=\"https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights\"><span class=\"by_qf77EiaoMw7tH3GSr\">other</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><a href=\"https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password\"><span class=\"by_Xn6ACr6Cua8upALWQ\">things</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span><a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/CqyJzDZWvGhhFJ7dY\"><span class=\"by_Xn6ACr6Cua8upALWQ\">people</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span><a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/RmCjazjupRGcHSm5N\"><span class=\"by_Xn6ACr6Cua8upALWQ\">commonly</span></a><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span><a href=\"https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/nYkMLFpx77Rz3uo9c\"><span class=\"by_Xn6ACr6Cua8upALWQ\">label</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\"> as beliefs. When these different things conflict, a person </span><span class=\"by_qf77EiaoMw7tH3GSr\">is </span><span class=\"by_Xn6ACr6Cua8upALWQ\">said to have </span><span class=\"by_qgdGA4ZEyW7zNdK84\">engaged in self-deception.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Deceiving yourself is </span><a href=\"https://lessestwrong.com/lw/s/belief_in_selfdeception/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">harder than it seems</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. What looks like a successively adopted false belief may actually be just a </span><a href=\"https://lessestwrong.com/tag/belief-in-belief\"><span class=\"by_qgdGA4ZEyW7zNdK84\">belief in false belief</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_Xn6ACr6Cua8upALWQ\">An example from </span><a href=\"https://www.lesswrong.com/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">No, Really, </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">I'</span><span class=\"by_qf77EiaoMw7tH3GSr\">ve Deceived Myself</span></span></a><span class=\"by_Xn6ACr6Cua8upALWQ\">:</span></p><blockquote><p><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">When this woman was</span><span class=\"by_qf77EiaoMw7tH3GSr\"> in </span><span class=\"by_Xn6ACr6Cua8upALWQ\">high school, she thought she was an atheist.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> But she decided, at that time, that she should act as if she believed in God.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> And </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">then—</span><span class=\"by_Xn6ACr6Cua8upALWQ\">she told me </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">earnestly—</span><span class=\"by_Xn6ACr6Cua8upALWQ\">over time, she came to really believe in God.</span></span></i></p></blockquote><blockquote><p><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">So far as I can tell, she is completely wrong about that.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> Always throughout our conversation, she said, over and over, </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"I</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> believe in </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">God\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">, never once, </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"There</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> is a God.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> When I asked her why she was religious, she never once talked about the consequences of God existing, only about the consequences of believing in God.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> Never, </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"God</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> will help </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">me\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">, always, </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"my</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> belief in God helps </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">me\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\">.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> When I put to her, </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"Someone</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> who just wanted the truth and looked at our universe would not even invent God as a hypothesis,</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">\"</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> she agreed outright.</span></span></i></p></blockquote><blockquote><p><i><span><span class=\"by_Xn6ACr6Cua8upALWQ\">She </span><span class=\"by_6Fx2vQtkYSZkaCvAg\">hasn'</span><span class=\"by_qf77EiaoMw7tH3GSr\">t </span><span class=\"by_Xn6ACr6Cua8upALWQ\">actually deceived herself into believing that God exists or that the Jewish religion is true.</span><span class=\"by_6Fx2vQtkYSZkaCvAg\">&nbsp;</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> Not even close, so far as I can tell.</span></span></i></p></blockquote><blockquote><p><i><span class=\"by_Xn6ACr6Cua8upALWQ\">On the other hand, I think she really does believe she has deceived herself.</span></i></p></blockquote><h2 id=\"Blog_posts\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Self-deception: Hypocrisy or Akrasia?</span></u></a></li></ul><h2 id=\"Sequence_by_Eliezer_Yudkowsky\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Sequence by </span><a href=\"https://wiki.lesswrong.com/wiki/Eliezer_Yudkowsky\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Eliezer Yudkowsky</span></u></a></h2><p><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Part of </span></i><a href=\"https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind\"><i><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">How To Actually Change Your Mind</span></u></i></a><i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> sequence</span></i></p><ul><li><a href=\"http://lesswrong.com/lw/r/no_really_ive_deceived_myself/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">No, Really, I've Deceived Myself</span></u></a></li><li><a href=\"http://lesswrong.com/lw/s/belief_in_selfdeception\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Belief in Self Deception</span></u></a></li><li><a href=\"http://lesswrong.com/lw/1f/moores_paradox\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Moore's Paradox</span></u></a></li><li><a href=\"http://lesswrong.com/lw/1o/dont_believe_youll_selfdeceive\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Don't Believe You'll Self-Deceive</span></u></a></li><li><a href=\"http://lesswrong.com/lw/1r/striving_to_accept\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Striving to Accept</span></u></a></li></ul><h2 id=\"Other_resources\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Other resources</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Robin_Hanson\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Robin Hanson</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> (2009). \"Enhancing Our Truth Orientation\". in Larissa Behrendt, Nick Bostrom. </span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">Human Enhancement</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\">. Oxford University Press. (</span><a href=\"http://hanson.gmu.edu/moretrue.pdf\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">PDF</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">)</span></li></ul><h2 id=\"See_also\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Anti-epistemology\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Anti-epistemology</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Belief_in_belief\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Belief in belief</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Semantic_stopsign\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Semantic stopsign</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Compartmentalization\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Compartmentalization</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Motivated_skepticism\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Motivated skepticism</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Improper_belief\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Improper belief</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">, </span><a href=\"https://wiki.lesswrong.com/wiki/Truth\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Truth</span></u></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "Sequence by Eliezer Yudkowsky",
          "anchor": "Sequence_by_Eliezer_Yudkowsky",
          "level": 1
        },
        {
          "title": "Other resources",
          "anchor": "Other_resources",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 55,
    "description": {
      "markdown": "**Self-deception** is a state of preserving a wrong [belief](https://lessestwrong.com/tag/belief), often facilitated by denying or [rationalizing away](https://lessestwrong.com/tag/rationalization) the relevance, significance, or importance of opposing [evidence](https://lessestwrong.com/tag/evidence) and logical arguments. Beliefs supported by self-deception are often chosen for reasons other than how closely those beliefs approximate [truth](https://lessestwrong.com/tag/truth-semantics-and-meaning).\n\n*Related:* [Anticipated Experiences](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=true&useTagName=true), [Motivated Reasoning](https://www.lesswrong.com/tag/motivated-reasoning?showPostCount=true&useTagName=true), [Rationalization](https://www.lesswrong.com/tag/rationalization?showPostCount=true&useTagName=true)\n\nOn LessWrong, a common distinction is between [beliefs as expectation-controllers](https://www.lesswrong.com/tag/anticipated-experiences?showPostCount=false&useTagName=false) and [other](https://www.lesswrong.com/posts/dLbkrPu5STNCBLRjr/applause-lights) [things](https://www.lesswrong.com/posts/NMoLJuDJEms7Ku9XS/guessing-the-teacher-s-password) [people](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/CqyJzDZWvGhhFJ7dY) [commonly](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/RmCjazjupRGcHSm5N) [label](https://www.lesswrong.com/s/7gRSERQZbqTuLX5re/p/nYkMLFpx77Rz3uo9c) as beliefs. When these different things conflict, a person is said to have engaged in self-deception.\n\nDeceiving yourself is [harder than it seems](https://lessestwrong.com/lw/s/belief_in_selfdeception/). What looks like a successively adopted false belief may actually be just a [belief in false belief](https://lessestwrong.com/tag/belief-in-belief).\n\nAn example from [No, Really, I've Deceived Myself](https://www.lesswrong.com/posts/rZX4WuufAPbN6wQTv/no-really-i-ve-deceived-myself):\n\n> *When this woman was in high school, she thought she was an atheist.  But she decided, at that time, that she should act as if she believed in God.  And then—she told me earnestly—over time, she came to really believe in God.*\n\n> *So far as I can tell, she is completely wrong about that.  Always throughout our conversation, she said, over and over, \"I believe in God\", never once, \"There is a God.\"  When I asked her why she was religious, she never once talked about the consequences of God existing, only about the consequences of believing in God.  Never, \"God will help me\", always, \"my belief in God helps me\".  When I put to her, \"Someone who just wanted the truth and looked at our universe would not even invent God as a hypothesis,\" she agreed outright.*\n\n> *She hasn't actually deceived herself into believing that God exists or that the Jewish religion is true.  Not even close, so far as I can tell.*\n\n> *On the other hand, I think she really does believe she has deceived herself.*\n\nBlog posts\n----------\n\n*   [Self-deception: Hypocrisy or Akrasia?](http://lesswrong.com/lw/h7/selfdeception_hypocrisy_or_akrasia/)\n\nSequence by [Eliezer Yudkowsky](https://wiki.lesswrong.com/wiki/Eliezer_Yudkowsky)\n----------------------------------------------------------------------------------\n\n*Part of* [*How To Actually Change Your Mind*](https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind) *sequence*\n\n*   [No, Really, I've Deceived Myself](http://lesswrong.com/lw/r/no_really_ive_deceived_myself/)\n*   [Belief in Self Deception](http://lesswrong.com/lw/s/belief_in_selfdeception)\n*   [Moore's Paradox](http://lesswrong.com/lw/1f/moores_paradox)\n*   [Don't Believe You'll Self-Deceive](http://lesswrong.com/lw/1o/dont_believe_youll_selfdeceive)\n*   [Striving to Accept](http://lesswrong.com/lw/1r/striving_to_accept)\n\nOther resources\n---------------\n\n*   [Robin Hanson](https://wiki.lesswrong.com/wiki/Robin_Hanson) (2009). \"Enhancing Our Truth Orientation\". in Larissa Behrendt, Nick Bostrom. *Human Enhancement*. Oxford University Press. ([PDF](http://hanson.gmu.edu/moretrue.pdf))\n\nSee also\n--------\n\n*   [Anti-epistemology](https://wiki.lesswrong.com/wiki/Anti-epistemology), [Belief in belief](https://wiki.lesswrong.com/wiki/Belief_in_belief)\n*   [Semantic stopsign](https://wiki.lesswrong.com/wiki/Semantic_stopsign), [Compartmentalization](https://wiki.lesswrong.com/wiki/Compartmentalization), [Motivated skepticism](https://wiki.lesswrong.com/wiki/Motivated_skepticism)\n*   [Improper belief](https://wiki.lesswrong.com/wiki/Improper_belief), [Truth](https://wiki.lesswrong.com/wiki/Truth)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E6qP9r9xxM4LCxaFk",
    "name": "Litany of Tarski",
    "core": false,
    "slug": "litany-of-tarski",
    "tableOfContents": {
      "html": "<p><span><span class=\"by_qf77EiaoMw7tH3GSr\">The</span><span class=\"by_ffpYPS4h2Yoaymuwk\"> </span></span><strong><span class=\"by_ffpYPS4h2Yoaymuwk\">Litany of Tarski</span></strong><span><span class=\"by_mPipmBTniuABY5PQy\"> is </span><span class=\"by_QBvPFLFyZyuHcBwFm\">a template to remind oneself that beliefs should stem from reality, from what </span><span class=\"by_mPipmBTniuABY5PQy\">actually </span></span><i><span class=\"by_QBvPFLFyZyuHcBwFm\">is</span></i><span class=\"by_QBvPFLFyZyuHcBwFm\">, as opposed to what we </span><i><span class=\"by_QBvPFLFyZyuHcBwFm\">want</span></i><span><span class=\"by_QBvPFLFyZyuHcBwFm\">, or what would be convenient. For any statement X, the</span><span class=\"by_mPipmBTniuABY5PQy\"> litany</span><span class=\"by_cn4SiEmqWbu7K9em5\"> </span><span class=\"by_QBvPFLFyZyuHcBwFm\">takes the form </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"If</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> X, I desire to believe</span><span class=\"by_cn4SiEmqWbu7K9em5\"> that</span><span class=\"by_ffpYPS4h2Yoaymuwk\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">X\"</span><span class=\"by_QBvPFLFyZyuHcBwFm\">.</span></span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">Quoting </span><a href=\"https://www.lesswrong.com/posts/3nZMgRTfFEfHp34Gb/the-meditation-on-curiosity\"><span class=\"by_QBvPFLFyZyuHcBwFm\">The Meditation on Curiosity</span></a><span class=\"by_QBvPFLFyZyuHcBwFm\">:</span></p><blockquote><p><i><span><span class=\"by_ffpYPS4h2Yoaymuwk\">If the </span><span class=\"by_QBvPFLFyZyuHcBwFm\">box contains a diamond,</span></span></i><br><i><span><span class=\"by_ffpYPS4h2Yoaymuwk\">I desire to believe </span><span class=\"by_mPipmBTniuABY5PQy\">that the</span><span class=\"by_ffpYPS4h2Yoaymuwk\"> </span><span class=\"by_QBvPFLFyZyuHcBwFm\">box contains a diamond;</span></span></i><br><i><span><span class=\"by_ffpYPS4h2Yoaymuwk\">If the </span><span class=\"by_QBvPFLFyZyuHcBwFm\">box does</span><span class=\"by_ffpYPS4h2Yoaymuwk\"> not </span><span class=\"by_QBvPFLFyZyuHcBwFm\">contain a diamond,</span></span></i><br><i><span><span class=\"by_ffpYPS4h2Yoaymuwk\">I desire to believe </span><span class=\"by_mPipmBTniuABY5PQy\">that the</span><span class=\"by_ffpYPS4h2Yoaymuwk\"> </span><span class=\"by_QBvPFLFyZyuHcBwFm\">box does</span><span class=\"by_ffpYPS4h2Yoaymuwk\"> not </span><span class=\"by_QBvPFLFyZyuHcBwFm\">contain a diamond;</span></span></i><br><i><span class=\"by_QBvPFLFyZyuHcBwFm\">Let me not become attached to beliefs I may not want.</span></i></p></blockquote><p><span><span class=\"by_RyiDJDCG6R7xyAXzp\">The </span><span class=\"by_QBvPFLFyZyuHcBwFm\">name refers</span><span class=\"by_qf77EiaoMw7tH3GSr\"> to </span></span><a href=\"https://en.wikipedia.org/wiki/Alfred_Tarski\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Alfred Tarski</span></a><span class=\"by_QBvPFLFyZyuHcBwFm\">, who sought to define </span><a href=\"https://plato.stanford.edu/entries/tarski-truth/\"><span><span class=\"by_QBvPFLFyZyuHcBwFm\">what, exactly, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">\"truth\"</span><span class=\"by_QBvPFLFyZyuHcBwFm\"> means</span></span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><p><span class=\"by_QBvPFLFyZyuHcBwFm\">See also: </span><a href=\"https://www.lesswrong.com/tag/map-and-territory\"><span><span class=\"by_QBvPFLFyZyuHcBwFm\">Map</span><span class=\"by_h48TMtPzfimsEobTm\"> and </span><span class=\"by_QBvPFLFyZyuHcBwFm\">Territory</span></span></a><span class=\"by_QBvPFLFyZyuHcBwFm\">, </span><a href=\"https://www.lesswrong.com/tag/litanies-and-mantras\"><span class=\"by_QBvPFLFyZyuHcBwFm\">Litanies &amp; Mantras</span></a><span class=\"by_QBvPFLFyZyuHcBwFm\">.</span></p><h2 id=\"Blog_posts\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">Blog posts</span></h2><ul><li><a href=\"http://lesswrong.com/lw/jz/the_meditation_on_curiosity/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Meditation on Curiosity</span></u></a></li><li><a href=\"http://lesswrong.com/lw/go/why_truth_and/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Why truth? And...</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> —</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> You have an instrumental motive to care about the truth of your </span></span><i><span class=\"by_6Fx2vQtkYSZkaCvAg\">beliefs about</span></i><span class=\"by_6Fx2vQtkYSZkaCvAg\"> anything you care about.</span></li><li><a href=\"http://lesswrong.com/lw/s/belief_in_selfdeception/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Belief in Self-Deception</span></u></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> —</span><span class=\"by_6Fx2vQtkYSZkaCvAg\"> Deceiving yourself is harder than it seems. What looks like a successively adopted false belief may actually be just a </span></span><a href=\"https://wiki.lesswrong.com/wiki/Belief_in_belief\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">belief in false belief</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\">.</span></li><li><a href=\"http://lesswrong.com/lw/js/the_bottom_line/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">The Bottom Line</span></u></a></li><li><a href=\"http://lesswrong.com/lw/jw/a_rational_argument/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">A Rational Argument</span></u></a></li><li><a href=\"http://lesswrong.com/lw/39/tarski_statements_as_rationalist_exercise/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Tarski Statements as Rationalist Exercise</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> by </span><a href=\"https://wiki.lesswrong.com/wiki/Vladimir_Nesov\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Vladimir Nesov</span></u></a></li><li><a href=\"http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">A Fable of Science and Politics</span></u></a><span class=\"by_6Fx2vQtkYSZkaCvAg\"> -- characters discover the color of the sky, with political implications.</span></li></ul><h2 id=\"See_also\"><span class=\"by_6Fx2vQtkYSZkaCvAg\">See also</span></h2><ul><li><a href=\"https://wiki.lesswrong.com/wiki/Truth\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Truth</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Litany_of_Gendlin\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Litany of Gendlin</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Epistemic_hygiene\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Epistemic hygiene</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Rationalization\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Rationalization</span></u></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Self-deception\"><u><span class=\"by_6Fx2vQtkYSZkaCvAg\">Self-deception</span></u></a></li></ul>",
      "sections": [
        {
          "title": "Blog posts",
          "anchor": "Blog_posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 7,
    "description": {
      "markdown": "The **Litany of Tarski** is a template to remind oneself that beliefs should stem from reality, from what actually *is*, as opposed to what we *want*, or what would be convenient. For any statement X, the litany takes the form \"If X, I desire to believe that X\".\n\nQuoting [The Meditation on Curiosity](https://www.lesswrong.com/posts/3nZMgRTfFEfHp34Gb/the-meditation-on-curiosity):\n\n> *If the box contains a diamond,*  \n> *I desire to believe that the box contains a diamond;*  \n> *If the box does not contain a diamond,*  \n> *I desire to believe that the box does not contain a diamond;*  \n> *Let me not become attached to beliefs I may not want.*\n\nThe name refers to [Alfred Tarski](https://en.wikipedia.org/wiki/Alfred_Tarski), who sought to define [what, exactly, \"truth\" means](https://plato.stanford.edu/entries/tarski-truth/).\n\nSee also: [Map and Territory](https://www.lesswrong.com/tag/map-and-territory), [Litanies & Mantras](https://www.lesswrong.com/tag/litanies-and-mantras).\n\nBlog posts\n----------\n\n*   [The Meditation on Curiosity](http://lesswrong.com/lw/jz/the_meditation_on_curiosity/)\n*   [Why truth? And...](http://lesswrong.com/lw/go/why_truth_and/) — You have an instrumental motive to care about the truth of your *beliefs about* anything you care about.\n*   [Belief in Self-Deception](http://lesswrong.com/lw/s/belief_in_selfdeception/) — Deceiving yourself is harder than it seems. What looks like a successively adopted false belief may actually be just a [belief in false belief](https://wiki.lesswrong.com/wiki/Belief_in_belief).\n*   [The Bottom Line](http://lesswrong.com/lw/js/the_bottom_line/)\n*   [A Rational Argument](http://lesswrong.com/lw/jw/a_rational_argument/)\n*   [Tarski Statements as Rationalist Exercise](http://lesswrong.com/lw/39/tarski_statements_as_rationalist_exercise/) by [Vladimir Nesov](https://wiki.lesswrong.com/wiki/Vladimir_Nesov)\n*   [A Fable of Science and Politics](http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/) \\-\\- characters discover the color of the sky, with political implications.\n\nSee also\n--------\n\n*   [Truth](https://wiki.lesswrong.com/wiki/Truth)\n*   [Litany of Gendlin](https://wiki.lesswrong.com/wiki/Litany_of_Gendlin)\n*   [Epistemic hygiene](https://wiki.lesswrong.com/wiki/Epistemic_hygiene)\n*   [Rationalization](https://wiki.lesswrong.com/wiki/Rationalization)\n*   [Self-deception](https://wiki.lesswrong.com/wiki/Self-deception)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "HAFdXkW4YW4KRe2Gx",
    "name": "Utility Functions",
    "core": false,
    "slug": "utility-functions",
    "tableOfContents": {
      "html": "<p><span class=\"by_cn4SiEmqWbu7K9em5\">A </span><strong><span class=\"by_cn4SiEmqWbu7K9em5\">utility function</span></strong><span><span class=\"by_cn4SiEmqWbu7K9em5\"> assigns numerical values </span><span class=\"by_Xn6ACr6Cua8upALWQ\">(\"utilities\"</span><span class=\"by_cn4SiEmqWbu7K9em5\">) to outcomes, in such a way that outcomes with higher utilities are always </span></span><u><a href=\"http://lesswrong.com/tag/preference\"><span class=\"by_cn4SiEmqWbu7K9em5\">preferred</span></a></u><span class=\"by_cn4SiEmqWbu7K9em5\"> to outcomes with lower utilities.</span></p><p><em><span class=\"by_qgdGA4ZEyW7zNdK84\">See also: </span></em><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Complexity of Value</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/decision-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/game-theory?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Game Theory</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/orthogonality-thesis/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Orthogonality Thesis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"http://lesswrong.com/tag/utilitarianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utilitarianism</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/preference\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Preference</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/utility\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\">, </span><a href=\"https://www.lesswrong.com/tag/vnm-theorem\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">VNM Theorem</span></a></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility Functions</span><span class=\"by_HoGziwmhpMGqGeWZy\"> do not work very well in practice for individual humans. Human drives are not coherent nor is there any reason to think they would be (</span></span><a href=\"https://www.lesswrong.com/lw/l3/thou_art_godshatter/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Thou Art Godshatter</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">), and even people with a strong interest in the concept have trouble working out what their utility function actually is even slightly (</span><a href=\"https://www.lesswrong.com/lw/zv/post_your_utility_function/\"><span class=\"by_HoGziwmhpMGqGeWZy\">Post Your Utility Function</span></a><span><span class=\"by_HoGziwmhpMGqGeWZy\">). Furthermore, humans appear to calculate utility and disutility separately - adding one to the other does not predict their </span><span class=\"by_qgdGA4ZEyW7zNdK84\">behavior</span><span class=\"by_HoGziwmhpMGqGeWZy\"> accurately. This makes humans highly exploitable.</span></span></p><p><a href=\"https://www.lesswrong.com/users/pjeby\"><span class=\"by_HoGziwmhpMGqGeWZy\">pjeby</span></a><span class=\"by_HoGziwmhpMGqGeWZy\"> posits humans' difficulty in understanding their own utility functions as the root of </span><a href=\"https://www.lesswrong.com/tag/akrasia\"><span class=\"by_HoGziwmhpMGqGeWZy\">akrasia</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">However, utility functions can be a useful model for dealing with humans in groups, </span><em><span class=\"by_HoGziwmhpMGqGeWZy\">e.g.</span></em><span class=\"by_HoGziwmhpMGqGeWZy\"> in economics.</span></p><p><span class=\"by_Q7NW4XaWQmfPfdcFj\">The </span><a href=\"https://www.lesswrong.com/tag/vnm-theorem\"><span class=\"by_Q7NW4XaWQmfPfdcFj\">VNM Theorem</span></a><span class=\"by_Q7NW4XaWQmfPfdcFj\"> tag is likely to be a strict subtag of the Utility Functions tag, because the VNM theorem establishes when preferences can be represented by a utility function, but a post discussing utility functions may or may not discuss the VNM theorem/axioms.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 119,
    "description": {
      "markdown": "A **utility function** assigns numerical values (\"utilities\") to outcomes, in such a way that outcomes with higher utilities are always [preferred](http://lesswrong.com/tag/preference) to outcomes with lower utilities.\n\n_See also:_ [Complexity of Value](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true), [Decision Theory](https://www.lesswrong.com/tag/decision-theory), [Game Theory](https://www.lesswrong.com/tag/game-theory?showPostCount=true&useTagName=true), [Orthogonality Thesis](https://www.lesswrong.com/tag/orthogonality-thesis/), [Utilitarianism](http://lesswrong.com/tag/utilitarianism), [Preference](https://www.lesswrong.com/tag/preference), [Utility](https://www.lesswrong.com/tag/utility), [VNM Theorem](https://www.lesswrong.com/tag/vnm-theorem)\n\nUtility Functions do not work very well in practice for individual humans. Human drives are not coherent nor is there any reason to think they would be ([Thou Art Godshatter](https://www.lesswrong.com/lw/l3/thou_art_godshatter/)), and even people with a strong interest in the concept have trouble working out what their utility function actually is even slightly ([Post Your Utility Function](https://www.lesswrong.com/lw/zv/post_your_utility_function/)). Furthermore, humans appear to calculate utility and disutility separately - adding one to the other does not predict their behavior accurately. This makes humans highly exploitable.\n\n[pjeby](https://www.lesswrong.com/users/pjeby) posits humans' difficulty in understanding their own utility functions as the root of [akrasia](https://www.lesswrong.com/tag/akrasia).\n\nHowever, utility functions can be a useful model for dealing with humans in groups, _e.g._ in economics.\n\nThe [VNM Theorem](https://www.lesswrong.com/tag/vnm-theorem) tag is likely to be a strict subtag of the Utility Functions tag, because the VNM theorem establishes when preferences can be represented by a utility function, but a post discussing utility functions may or may not discuss the VNM theorem/axioms."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "LhX3F2SvGDarZCuh6",
    "name": "Bayes' Theorem",
    "core": false,
    "slug": "bayes-theorem",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_8bJycwyZpC4XbwPCF\">Bayes'</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> Theorem</span></span></strong><span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> (also known as </span><span class=\"by_8bJycwyZpC4XbwPCF\">Bayes'</span><span class=\"by_Q7NW4XaWQmfPfdcFj\"> Law)</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> is a</span><span class=\"by_e8voDq2aJLGcdWuw6\"> law of probability </span><span class=\"by_RyiDJDCG6R7xyAXzp\">that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate. </span><span class=\"by_EQNTWXLKMeWMp2FQS\">It</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> is</span><span class=\"by_EQNTWXLKMeWMp2FQS\"> commonly</span><span class=\"by_e8voDq2aJLGcdWuw6\"> regarded as the foundation of consistent rational reasoning under uncertainty.</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_EQNTWXLKMeWMp2FQS\">Bayes Theorem is named after Reverend Thomas Bayes who proved the theorem in 1763.</span></span></p><p><em><span class=\"by_qgdGA4ZEyW7zNdK84\">See also: </span></em><a href=\"https://www.lesswrong.com/tag/bayesian-probability\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bayesian probability</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/priors\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Priors</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/likelihood-ratio\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Likelihood ratio</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/belief-update\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Belief update</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/probability-and-statistics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Probability and statistics</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/epistemology\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Epistemology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://www.lesswrong.com/tag/bayesianism\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Bayesianism</span></a></p><p><span><span class=\"by_8bJycwyZpC4XbwPCF\">Bayes'</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> theorem commonly takes the form:</span></span></p><div><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"P(A|B)={\\frac{P(B|A)\\,P(A)}{P(B)}}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">=</span></span></span><span class=\"mjx-texatom MJXc-space3\"><span class=\"mjx-mrow\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 5.962em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 5.962em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span><span class=\"mjx-mspace\" style=\"width: 0.167em; height: 0px;\"></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 5.962em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 5.962em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></span></span></div><p><span class=\"by_RyiDJDCG6R7xyAXzp\">where A is the proposition of interest, B is the observed evidence, P(A) and P(B) are prior probabilities, and P(A|B) is the posterior probability of A.</span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">With the posterior odds, the prior odds and the </span><a href=\"https://www.lesswrong.com/tag/likelihood-ratio\"><span class=\"by_qf77EiaoMw7tH3GSr\">likelihood ratio</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> written explicitly, the theorem reads:</span></p><div><span class=\"mjx-chtml MJXc-display\" style=\"text-align: center;\"><span class=\"mjx-math\" aria-label=\"\\frac{P(A|B)}{P(\\neg{} A|B)}=\\frac{P(A)}{P(\\neg{} A)}\\cdot\\frac{P(B|A)}{P(B|\\neg{} A)}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mfrac\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.183em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.183em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.183em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_8bJycwyZpC4XbwPCF\">¬</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.183em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">=</span></span></span><span class=\"mjx-mfrac MJXc-space3\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 3.146em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 3.146em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 3.146em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_8bJycwyZpC4XbwPCF\">¬</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 3.146em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\"><span class=\"by_8bJycwyZpC4XbwPCF\">⋅</span></span></span><span class=\"mjx-mfrac MJXc-space2\"><span class=\"mjx-box MJXc-stacked\" style=\"width: 4.183em; padding: 0px 0.12em;\"><span class=\"mjx-numerator\" style=\"width: 4.183em; top: -1.59em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span class=\"mjx-denominator\" style=\"width: 4.183em; bottom: -1.09em;\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">B</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_8bJycwyZpC4XbwPCF\">¬</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.519em; padding-bottom: 0.298em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">A</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_EQNTWXLKMeWMp2FQS\">)</span></span></span></span></span><span style=\"border-bottom: 1.3px solid; top: -0.296em; width: 4.183em;\" class=\"mjx-line\"></span></span><span style=\"height: 2.68em; vertical-align: -1.09em;\" class=\"mjx-vsize\"></span></span></span></span></span></div><h2 id=\"_Visualization_of_Bayes__Rule\"><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Visualization</span><span class=\"by_e8voDq2aJLGcdWuw6\"> of </span><span class=\"by_8bJycwyZpC4XbwPCF\">Bayes'</span><span class=\"by_e8voDq2aJLGcdWuw6\"> </span><span class=\"by_Co2dGXQxHAf92LHea\">Rule</span></span></h2><span><figure><img src=\"https://wiki.lesswrong.com/images/7/74/Bayes.png\" class=\"draft-image \" style=\"width:40%\"></figure></span><h2 id=\"External_links\"><span class=\"by_8bJycwyZpC4XbwPCF\">External links</span></h2><ul><li><a href=\"https://arbital.com/p/bayes_rule_guide/\"><span class=\"by_8bJycwyZpC4XbwPCF\">Arbital Guide to Bayes' Rule</span></a></li><li><a href=\"http://yudkowsky.net/rational/bayes\"><span class=\"by_8bJycwyZpC4XbwPCF\">An Intuitive Explanation of Bayes' Theorem</span></a><span class=\"by_8bJycwyZpC4XbwPCF\"> by Eliezer Yudkowsky</span></li><li><a href=\"http://blog.oscarbonilla.com/2009/05/visualizing-bayes-theorem/\"><span class=\"by_8bJycwyZpC4XbwPCF\">Visualizing Bayes' theorem</span></a><span class=\"by_8bJycwyZpC4XbwPCF\"> by Oscar Bonilla</span></li><li><a href=\"http://oracleaide.wordpress.com/2012/12/26/a-venn-pie/\"><span class=\"by_8bJycwyZpC4XbwPCF\">Using Venn pies to illustrate Bayes' theorem</span></a><span class=\"by_8bJycwyZpC4XbwPCF\"> by </span><a href=\"https://www.lesswrong.com/users/oracleaide\"><span class=\"by_8bJycwyZpC4XbwPCF\">oracleaide</span></a></li><li><a href=\"http://kruel.co/2010/02/27/a-guide-to-bayes-theorem-a-few-links/\"><span class=\"by_8bJycwyZpC4XbwPCF\">A Guide to Bayes’ Theorem – A few links</span></a><span class=\"by_8bJycwyZpC4XbwPCF\"> by Alexander Kruel</span></li><li><a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\"><span class=\"by_8bJycwyZpC4XbwPCF\">Bayes' Theorem</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, Wikipedia</span></li></ul>",
      "sections": [
        {
          "title": " Visualization of Bayes' Rule",
          "anchor": "_Visualization_of_Bayes__Rule",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 137,
    "description": {
      "markdown": "**Bayes' Theorem** (also known as Bayes' Law) is a law of probability that describes the proper way to incorporate new evidence into prior probabilities to form an updated probability estimate. It is commonly regarded as the foundation of consistent rational reasoning under uncertainty. Bayes Theorem is named after Reverend Thomas Bayes who proved the theorem in 1763.\n\n_See also:_ [Bayesian probability](https://www.lesswrong.com/tag/bayesian-probability), [Priors](https://www.lesswrong.com/tag/priors), [Likelihood ratio](https://www.lesswrong.com/tag/likelihood-ratio), [Belief update](https://www.lesswrong.com/tag/belief-update), [Probability and statistics](https://www.lesswrong.com/tag/probability-and-statistics), [Epistemology](https://www.lesswrong.com/tag/epistemology), [Bayesianism](https://www.lesswrong.com/tag/bayesianism)\n\nBayes' theorem commonly takes the form:\n\n P(A|B)=P(B|A)P(A)P(B)\n\nwhere A is the proposition of interest, B is the observed evidence, P(A) and P(B) are prior probabilities, and P(A|B) is the posterior probability of A.\n\nWith the posterior odds, the prior odds and the [likelihood ratio](https://www.lesswrong.com/tag/likelihood-ratio) written explicitly, the theorem reads:\n\nP(A|B)P(¬A|B)=P(A)P(¬A)⋅P(B|A)P(B|¬A)\n\nVisualization of Bayes' Rule\n----------------------------\n\n![](https://wiki.lesswrong.com/images/7/74/Bayes.png)\n\nExternal links\n--------------\n\n*   [Arbital Guide to Bayes' Rule](https://arbital.com/p/bayes_rule_guide/)\n*   [An Intuitive Explanation of Bayes' Theorem](http://yudkowsky.net/rational/bayes) by Eliezer Yudkowsky\n*   [Visualizing Bayes' theorem](http://blog.oscarbonilla.com/2009/05/visualizing-bayes-theorem/) by Oscar Bonilla\n*   [Using Venn pies to illustrate Bayes' theorem](http://oracleaide.wordpress.com/2012/12/26/a-venn-pie/) by [oracleaide](https://www.lesswrong.com/users/oracleaide)\n*   [A Guide to Bayes’ Theorem – A few links](http://kruel.co/2010/02/27/a-guide-to-bayes-theorem-a-few-links/) by Alexander Kruel\n*   [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem), Wikipedia"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "wBoHTJs9iQzczNtW3",
    "name": "Robust Agents",
    "core": null,
    "slug": "robust-agents",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Robust Agents</span></strong><span><span class=\"by_r38pkCm7wF4M44MDQ\"> are decision-makers who can perform well in a variety of situations. Whereas some humans rely on folk wisdom or </span><span class=\"by_qgdGA4ZEyW7zNdK84\">instinct,</span><span class=\"by_r38pkCm7wF4M44MDQ\"> and some AIs might be designed to achieve a narrow set of goals, a Robust Agent has a coherent set of values and decision-procedures. This enables them to adapt to new circumstances (such as succeeding in a new environment, or responding to a new strategy by a competitor).</span></span></p><h2 id=\"See_also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/agency\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Agency</span></a></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 23,
    "description": {
      "markdown": "**Robust Agents** are decision-makers who can perform well in a variety of situations. Whereas some humans rely on folk wisdom or instinct, and some AIs might be designed to achieve a narrow set of goals, a Robust Agent has a coherent set of values and decision-procedures. This enables them to adapt to new circumstances (such as succeeding in a new environment, or responding to a new strategy by a competitor).\n\nSee also\n--------\n\n*   [Agency](https://www.lesswrong.com/tag/agency)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "XqykXFKL9t38pbSEm",
    "name": "Well-being",
    "core": null,
    "slug": "well-being",
    "tableOfContents": {
      "html": null,
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 96,
    "description": null,
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "E8PHMuf7tsr8teXAe",
    "name": "Betting",
    "core": null,
    "slug": "betting",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Betting</span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is staking money (or some other form</span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">value) on one's beliefs. It</span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">considered rationally virtuous</span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> to bet </span><span class=\"by_qgdGA4ZEyW7zNdK84\">on one's beliefs,</span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> as the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">real stakes force one to actually consider precisely what they</span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> </span></span><a href=\"https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences\"><span class=\"by_qgdGA4ZEyW7zNdK84\">anticipate</span></a><span><span class=\"by_Tw9etd8rMnHLeSQ9q\"> will </span><span class=\"by_qgdGA4ZEyW7zNdK84\">really happen. LessWrong has a culture of betting.</span></span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">See also:</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prediction Markets, </span></a><a href=\"https://www.lesswrong.com/tag/forecasting-and-prediction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasting &amp; Prediction, </span></a><a href=\"https://www.lesswrong.com/tag/forecasts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Forecasts (Specific Predictions)</span></a></p><h2 id=\"Why_is_betting_important_\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Why is betting important?</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The argument in favor of betting is that one should generally either accept a proposed bet, in order to make money in expectation, or update their beliefs so the bet becomes unprofitable. There are exceptions to this rule, some theoretical, such as the example of </span><a href=\"https://www.lesswrong.com/posts/G7HgP9KTWAMSv6oEJ/bets-and-updating\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Omega and Omicron</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, and some practical, such as uncertainty about whether the bet will be fulfilled. Offering a bet forces someone to think more carefully and share their beliefs more precisely. Losing a bet, even small, can make it more emotionally visceral in a way that might lead to sharpening belief </span><a href=\"https://www.lesswrong.com/tag/calibration\"><span class=\"by_qgdGA4ZEyW7zNdK84\">calibration</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> more. Bets can be made about beliefs that can be immediately verified or about beliefs that will only be verifiable in the future.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In popular culture, this idea is often referred to as \"putting one's money where one's mouth is\".</span></p><p><a href=\"https://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A Bet is a Tax on Bullshit</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> mentions that:</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In fact, the NYTimes should require that Silver, and other pundits, bet their beliefs. Furthermore, to remove any possibility of manipulation, the NYTimes should escrow a portion of Silver’s salary in a blind trust bet. In other words, the NYTimes should bet a portion of Silver’s salary, at the odds implied by Silver’s model, randomly choosing which side of the bet to take, only revealing to Silver the bet and its outcome after the election is over. A blind trust bet creates incentives for Silver to be disinterested in the outcome but very interested in the accuracy of the forecast.</span></p></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In </span><a href=\"https://www.econlib.org/archives/2009/03/what_does_the_b.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">What Does the Betting Norm Tax?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Bryan Caplan says that such a norm should also be present among scholars.</span></p><h2 id=\"Operationalization_for_Bets\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Operationalization for Bets</span></h2><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Operationalizing a belief</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> is the practice of transforming a belief into a bet with a clear, unambiguous resolution criteria. Sometimes this can be difficult, but there can be ways around some difficulties as explained in </span><a href=\"https://www.lesswrong.com/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Tricky Bets and Truth-Tracking Fields</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. The same challenges are present for prediction markets.</span></p><h2 id=\"Prediction_Markets\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prediction Markets</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A</span><a href=\"https://www.lesswrong.com/tag/prediction-markets\"><span class=\"by_qgdGA4ZEyW7zNdK84\"> prediction market</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a way for everyone to participate in betting on a particular question. A positive externality of prediction markets, and to a lesser extent bets, is providing a reliable probability on its questions. It can also act as an insurer. </span><a href=\"https://www.lesswrong.com/posts/ts4KmAR8aJoGMawLb/link-bets-do-not-necessarily-reveal-beliefs\"><span class=\"by_qgdGA4ZEyW7zNdK84\">3</span></a><a href=\"https://www.lesswrong.com/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself\"><span class=\"by_qgdGA4ZEyW7zNdK84\">4</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><a href=\"http://www.truthcoin.info/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Truthcoin</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, an idea for a decentralized prediction market, has the slogan \"Making cheap talk expensive\".</span></p><p><a href=\"http://longbets.org/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Long Bets</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> is also a useful platform to make certain bets.</span></p><blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The purpose of Long Bets is to improve long–term thinking. Long Bets is a public arena for enjoyably competitive predictions, of interest to society, with philanthropic money at stake. The Long Now Foundation furnishes the continuity to see even the longest bets through to public resolution. This website provides a forum for discussion about what may be learned from the bets and their eventual outcomes.</span></p></blockquote><p><span class=\"by_qgdGA4ZEyW7zNdK84\">However, Long Bets hasn't good incentives to make long term bets as explained by Jeff Kaufman in </span><a href=\"https://www.jefftk.com/p/long-bets-by-confidence-level\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Long Bets by Confidence Level</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><span class=\"by_Tw9etd8rMnHLeSQ9q\">See also:</span></p><ul><li><a href=\"https://www.lesswrong.com/tag/bets-registry\"><span class=\"by_qgdGA4ZEyW7zNdK84\">LessWiki Bets Registry</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> (outdated)</span></li></ul><p><span class=\"by_qgdGA4ZEyW7zNdK84\">External links:</span></p><ul><li><a href=\"https://www.lesswrong.com/posts/msf7BHMrWTczbQckh/risk-aversion-does-not-explain-people-s-betting-behaviours\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Risk aversion does not explain people's betting behaviours</span></a></li><li><a href=\"https://www.lesswrong.com/posts/ABMMQ5gSGHwRgExJk/a-method-for-fair-bargaining-over-odds-in-2-player-bets\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A method for fair bargaining over odds in 2 player bets!</span></a></li></ul>",
      "sections": [
        {
          "title": "Why is betting important?",
          "anchor": "Why_is_betting_important_",
          "level": 1
        },
        {
          "title": "Operationalization for Bets",
          "anchor": "Operationalization_for_Bets",
          "level": 1
        },
        {
          "title": "Prediction Markets",
          "anchor": "Prediction_Markets",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 66,
    "description": {
      "markdown": "**Betting** is staking money (or some other form of value) on one's beliefs. It is considered rationally virtuous to bet on one's beliefs, as the real stakes force one to actually consider precisely what they [anticipate](https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences) will really happen. LessWrong has a culture of betting.\n\n*See also:* [Prediction Markets,](https://www.lesswrong.com/tag/prediction-markets) [Forecasting & Prediction,](https://www.lesswrong.com/tag/forecasting-and-prediction) [Forecasts (Specific Predictions)](https://www.lesswrong.com/tag/forecasts)\n\nWhy is betting important?\n-------------------------\n\nThe argument in favor of betting is that one should generally either accept a proposed bet, in order to make money in expectation, or update their beliefs so the bet becomes unprofitable. There are exceptions to this rule, some theoretical, such as the example of [Omega and Omicron](https://www.lesswrong.com/posts/G7HgP9KTWAMSv6oEJ/bets-and-updating), and some practical, such as uncertainty about whether the bet will be fulfilled. Offering a bet forces someone to think more carefully and share their beliefs more precisely. Losing a bet, even small, can make it more emotionally visceral in a way that might lead to sharpening belief [calibration](https://www.lesswrong.com/tag/calibration) more. Bets can be made about beliefs that can be immediately verified or about beliefs that will only be verifiable in the future.\n\nIn popular culture, this idea is often referred to as \"putting one's money where one's mouth is\".\n\n[A Bet is a Tax on Bullshit](https://marginalrevolution.com/marginalrevolution/2012/11/a-bet-is-a-tax-on-bullshit.html) mentions that:\n\n> In fact, the NYTimes should require that Silver, and other pundits, bet their beliefs. Furthermore, to remove any possibility of manipulation, the NYTimes should escrow a portion of Silver’s salary in a blind trust bet. In other words, the NYTimes should bet a portion of Silver’s salary, at the odds implied by Silver’s model, randomly choosing which side of the bet to take, only revealing to Silver the bet and its outcome after the election is over. A blind trust bet creates incentives for Silver to be disinterested in the outcome but very interested in the accuracy of the forecast.\n\nIn [What Does the Betting Norm Tax?](https://www.econlib.org/archives/2009/03/what_does_the_b.html), Bryan Caplan says that such a norm should also be present among scholars.\n\nOperationalization for Bets\n---------------------------\n\n*Operationalizing a belief* is the practice of transforming a belief into a bet with a clear, unambiguous resolution criteria. Sometimes this can be difficult, but there can be ways around some difficulties as explained in [Tricky Bets and Truth-Tracking Fields](https://www.lesswrong.com/posts/LzyN9wzEdfS3j5SmT/tricky-bets-and-truth-tracking-fields). The same challenges are present for prediction markets.\n\nPrediction Markets\n------------------\n\nA [prediction market](https://www.lesswrong.com/tag/prediction-markets) is a way for everyone to participate in betting on a particular question. A positive externality of prediction markets, and to a lesser extent bets, is providing a reliable probability on its questions. It can also act as an insurer. [3](https://www.lesswrong.com/posts/ts4KmAR8aJoGMawLb/link-bets-do-not-necessarily-reveal-beliefs)[4](https://www.lesswrong.com/posts/JDKfPsHvBwgq4Knn9/buy-insurance-bet-against-yourself) [Truthcoin](http://www.truthcoin.info/), an idea for a decentralized prediction market, has the slogan \"Making cheap talk expensive\".\n\n[Long Bets](http://longbets.org/) is also a useful platform to make certain bets.\n\n> The purpose of Long Bets is to improve long–term thinking. Long Bets is a public arena for enjoyably competitive predictions, of interest to society, with philanthropic money at stake. The Long Now Foundation furnishes the continuity to see even the longest bets through to public resolution. This website provides a forum for discussion about what may be learned from the bets and their eventual outcomes.\n\nHowever, Long Bets hasn't good incentives to make long term bets as explained by Jeff Kaufman in [Long Bets by Confidence Level](https://www.jefftk.com/p/long-bets-by-confidence-level).\n\nSee also:\n\n*   [LessWiki Bets Registry](https://www.lesswrong.com/tag/bets-registry) (outdated)\n\nExternal links:\n\n*   [Risk aversion does not explain people's betting behaviours](https://www.lesswrong.com/posts/msf7BHMrWTczbQckh/risk-aversion-does-not-explain-people-s-betting-behaviours)\n*   [A method for fair bargaining over odds in 2 player bets!](https://www.lesswrong.com/posts/ABMMQ5gSGHwRgExJk/a-method-for-fair-bargaining-over-odds-in-2-player-bets)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "JHzjkFnQgsrRrucqQ",
    "name": "Focusing",
    "core": null,
    "slug": "focusing",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Focusing </span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">refers to a family of introspective techniques </span><span class=\"by_Sp5wM4aRAhNERd4oY\">taught by </span></span><a href=\"https://www.lesswrong.com/tag/center-for-applied-rationality-cfar\"><span class=\"by_Sp5wM4aRAhNERd4oY\">CFAR</span></a><span><span class=\"by_Sp5wM4aRAhNERd4oY\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">whose aim is to access one's \"gut\" or </span><span class=\"by_Sp5wM4aRAhNERd4oY\">\"</span></span><a href=\"https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2?useTagName=false\"><span class=\"by_Sp5wM4aRAhNERd4oY\">System 1</span></a><span><span class=\"by_Sp5wM4aRAhNERd4oY\">\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> feelings. Archetypically, sensations within the body </span><span class=\"by_Sp5wM4aRAhNERd4oY\">are approached with a spirit of gentle curiosity, and possible verbal labels are checked against felt senses. Where successful, this</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> can </span><span class=\"by_Sp5wM4aRAhNERd4oY\">improve internal understanding and allow split off trauma or conflict between </span></span><a href=\"https://www.lesswrong.com/tag/subagents\"><span class=\"by_Sp5wM4aRAhNERd4oY\">subagents</span></a><span><span class=\"by_Sp5wM4aRAhNERd4oY\"> to be processed</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> for </span><span class=\"by_Sp5wM4aRAhNERd4oY\">improved </span></span><a href=\"https://www.lesswrong.com/tag/internal-alignment-human\"><span class=\"by_Sp5wM4aRAhNERd4oY\">internal alignment</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Focusing draws in name from the </span><a href=\"https://www.amazon.com/Focusing-Eugene-T-Gendlin/dp/0553278339\"><span class=\"by_qgdGA4ZEyW7zNdK84\">book and technique</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><span class=\"by_sKAL2jzfkYkDbQmx9\">of the same name </span><span class=\"by_qgdGA4ZEyW7zNdK84\">by </span><span class=\"by_sKAL2jzfkYkDbQmx9\">psychologist</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Eugene </span><span class=\"by_sKAL2jzfkYkDbQmx9\">Gendlin (Who's also known on LessWrong for the </span></span><a href=\"https://www.lesswrong.com/tag/litany-of-gendlin\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Litany of Gendlin</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">),</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> and since his introduction, some have developed their own variations [</span></span><a href=\"https://www.lessestwrong.com/posts/PXqQhYEdbdAYCp88m/focusing-for-skeptics\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">].</span></p><p><span class=\"by_Sp5wM4aRAhNERd4oY\">Related techniques: </span><a href=\"https://www.lesswrong.com/tag/internal-double-crux\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Internal Double Crux</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, </span><a href=\"https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Inner Simulator</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 19,
    "description": {
      "markdown": "**Focusing** refers to a family of introspective techniques taught by [CFAR](https://www.lesswrong.com/tag/center-for-applied-rationality-cfar) whose aim is to access one's \"gut\" or \"[System 1](https://www.lesswrong.com/tag/dual-process-theory-system-1-and-system-2?useTagName=false)\" feelings. Archetypically, sensations within the body are approached with a spirit of gentle curiosity, and possible verbal labels are checked against felt senses. Where successful, this can improve internal understanding and allow split off trauma or conflict between [subagents](https://www.lesswrong.com/tag/subagents) to be processed for improved [internal alignment](https://www.lesswrong.com/tag/internal-alignment-human).\n\nFocusing draws in name from the [book and technique](https://www.amazon.com/Focusing-Eugene-T-Gendlin/dp/0553278339) of the same name by psychologist Eugene Gendlin (Who's also known on LessWrong for the [Litany of Gendlin](https://www.lesswrong.com/tag/litany-of-gendlin)), and since his introduction, some have developed their own variations \\[[1](https://www.lessestwrong.com/posts/PXqQhYEdbdAYCp88m/focusing-for-skeptics)\\].\n\nRelated techniques: [Internal Double Crux](https://www.lesswrong.com/tag/internal-double-crux), [Inner Simulator](https://www.lesswrong.com/tag/inner-simulator-suprise-o-meter)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PvridmTCj2qsugQCH",
    "name": "Goodhart's Law",
    "core": null,
    "slug": "goodhart-s-law",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Goodhart's </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Law </span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">states that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">when</span><span class=\"by_qf77EiaoMw7tH3GSr\"> a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">proxy for some value becomes the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> target of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">optimization pressure, the proxy will cease to be </span><span class=\"by_qf77EiaoMw7tH3GSr\">a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">good proxy. One form of Goodhart is demonstrated by</span><span class=\"by_qf77EiaoMw7tH3GSr\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Soviet story of</span><span class=\"by_qf77EiaoMw7tH3GSr\"> a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">factory graded on how many shoes they produced (a good proxy for productivity) – they soon began producing a higher number</span><span class=\"by_qf77EiaoMw7tH3GSr\"> of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">tiny shoes. Useless, but the numbers look good.</span></span></p><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Goodhart's Law</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is of particular relevance to </span></span><a href=\"https://www.lessestwrong.com/tag/ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AI Alignment</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. Suppose you have something which is generally a good proxy for \"the stuff that humans care about\", it would be dangerous to have a powerful AI optimize for the proxy, in accordance with Goodhart's law, the proxy will breakdown. &nbsp;</span></p><h2 id=\"Goodhart_Taxonomy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart Taxonomy</span></h2><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">In</span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span></span><a href=\"https://www.lessestwrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Goodhart Taxonomy</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Scott Garrabrant identifies four kinds of Goodharting:</span></p><ul><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.</span></li><li><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Causal Goodhart - When there is a non-causal correlation between</span><span class=\"by_qf77EiaoMw7tH3GSr\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">proxy and the goal, intervening on the proxy may fail to intervene on the goal.</span></span></li><li><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Extremal Goodhart - Worlds</span><span class=\"by_qf77EiaoMw7tH3GSr\"> in </span><span class=\"by_qgdGA4ZEyW7zNdK84\">which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.</span></span></li><li><span class=\"by_qgdGA4ZEyW7zNdK84\">Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.</span></li></ul><h2 id=\"See_Also\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/groupthink\"><span class=\"by_qf77EiaoMw7tH3GSr\">Groupthink</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/information-cascades\"><span class=\"by_qf77EiaoMw7tH3GSr\">Information cascade</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/affective-death-spiral\"><span class=\"by_qf77EiaoMw7tH3GSr\">Affective death spiral</span></a></li><li><a href=\"https://wiki.lesswrong.com/wiki/Adaptation_executers\"><span class=\"by_qf77EiaoMw7tH3GSr\">Adaptation executers</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/superstimuli\"><span class=\"by_qf77EiaoMw7tH3GSr\">Superstimulus</span></a></li><li><a href=\"https://lessestwrong.com/tag/signaling\"><span class=\"by_qf77EiaoMw7tH3GSr\">Signaling</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/filtered-evidence\"><span class=\"by_qf77EiaoMw7tH3GSr\">Filtered evidence</span></a></li><li><a href=\"https://lessestwrong.com/tag/cached-thought\"><span class=\"by_qf77EiaoMw7tH3GSr\">Cached thought</span></a></li><li><a href=\"https://lessestwrong.com/tag/modesty-argument\"><span class=\"by_qf77EiaoMw7tH3GSr\">Modesty argument</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/egalitarianism\"><span class=\"by_qf77EiaoMw7tH3GSr\">Egalitarianism</span></a></li><li><a href=\"https://lessestwrong.com/tag/rationalization\"><span class=\"by_qf77EiaoMw7tH3GSr\">Rationalization</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">, </span><a href=\"https://lessestwrong.com/tag/dark-arts\"><span class=\"by_qf77EiaoMw7tH3GSr\">Dark arts</span></a></li><li><a href=\"https://lessestwrong.com/tag/epistemic-hygiene\"><span class=\"by_qf77EiaoMw7tH3GSr\">Epistemic hygiene</span></a></li><li><a href=\"https://lessestwrong.com/tag/scoring-rule\"><span class=\"by_qf77EiaoMw7tH3GSr\">Scoring rule</span></a></li></ul>",
      "sections": [
        {
          "title": "Goodhart Taxonomy",
          "anchor": "Goodhart_Taxonomy",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 70,
    "description": {
      "markdown": "**Goodhart's Law** states that when a proxy for some value becomes the target of optimization pressure, the proxy will cease to be a good proxy. One form of Goodhart is demonstrated by the Soviet story of a factory graded on how many shoes they produced (a good proxy for productivity) – they soon began producing a higher number of tiny shoes. Useless, but the numbers look good.\n\nGoodhart's Law is of particular relevance to [AI Alignment](https://www.lessestwrong.com/tag/ai). Suppose you have something which is generally a good proxy for \"the stuff that humans care about\", it would be dangerous to have a powerful AI optimize for the proxy, in accordance with Goodhart's law, the proxy will breakdown.  \n\nGoodhart Taxonomy\n-----------------\n\nIn [Goodhart Taxonomy](https://www.lessestwrong.com/posts/EbFABnst8LsidYs5Y/goodhart-taxonomy), Scott Garrabrant identifies four kinds of Goodharting:\n\n*   Regressional Goodhart - When selecting for a proxy measure, you select not only for the true goal, but also for the difference between the proxy and the goal.\n*   Causal Goodhart - When there is a non-causal correlation between the proxy and the goal, intervening on the proxy may fail to intervene on the goal.\n*   Extremal Goodhart - Worlds in which the proxy takes an extreme value may be very different from the ordinary worlds in which the correlation between the proxy and the goal was observed.\n*   Adversarial Goodhart - When you optimize for a proxy, you provide an incentive for adversaries to correlate their goal with your proxy, thus destroying the correlation with your goal.\n\nSee Also\n--------\n\n*   [Groupthink](https://lessestwrong.com/tag/groupthink), [Information cascade](https://lessestwrong.com/tag/information-cascades), [Affective death spiral](https://lessestwrong.com/tag/affective-death-spiral)\n*   [Adaptation executers](https://wiki.lesswrong.com/wiki/Adaptation_executers), [Superstimulus](https://lessestwrong.com/tag/superstimuli)\n*   [Signaling](https://lessestwrong.com/tag/signaling), [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)\n*   [Cached thought](https://lessestwrong.com/tag/cached-thought)\n*   [Modesty argument](https://lessestwrong.com/tag/modesty-argument), [Egalitarianism](https://lessestwrong.com/tag/egalitarianism)\n*   [Rationalization](https://lessestwrong.com/tag/rationalization), [Dark arts](https://lessestwrong.com/tag/dark-arts)\n*   [Epistemic hygiene](https://lessestwrong.com/tag/epistemic-hygiene)\n*   [Scoring rule](https://lessestwrong.com/tag/scoring-rule)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Z38PqJbRyfwCxKvvL",
    "name": "Research Agendas",
    "core": false,
    "slug": "research-agendas",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_Sp5wM4aRAhNERd4oY\">Research Agendas</span></strong><span class=\"by_Sp5wM4aRAhNERd4oY\"> lay out the areas of research which individuals or groups are working on, or those that they believe would be valuable for others to work on. They help make research more legible and encourage discussion of priorities.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 70,
    "description": {
      "markdown": "**Research Agendas** lay out the areas of research which individuals or groups are working on, or those that they believe would be valuable for others to work on. They help make research more legible and encourage discussion of priorities."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "tZsfB6WfpRy6kFb6q",
    "name": "Conservation of Expected Evidence",
    "core": null,
    "slug": "conservation-of-expected-evidence",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">Conservation of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Expected Evidence </span></span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\">is a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">consequence of probability theory which states</span><span class=\"by_qf77EiaoMw7tH3GSr\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">for</span><span class=\"by_qf77EiaoMw7tH3GSr\"> every expectation of evidence, there is an equal and opposite expectation of counterevidence</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> [</span></span><a href=\"https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span><span class=\"by_qf77EiaoMw7tH3GSr\">.</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> Conservation of Expected Evidence is about </span><span class=\"by_qf77EiaoMw7tH3GSr\">both</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> the direction of the </span><span class=\"by_qf77EiaoMw7tH3GSr\">update</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> and its </span><span class=\"by_qf77EiaoMw7tH3GSr\">magnitude:</span><span class=\"by_sKAL2jzfkYkDbQmx9\"> a low probability of seeing strong evidence in one direction must be balanced by a high probability of observing weak counterevidence in the other </span><span class=\"by_qf77EiaoMw7tH3GSr\">direction [</span></span><a href=\"https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/zTfSXQracE7TW8x4w#1___You_can_t_predict_that_you_ll_update_in_a_particular_direction__\"><span class=\"by_sKAL2jzfkYkDbQmx9\">2</span></a><span><span class=\"by_sKAL2jzfkYkDbQmx9\">].</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> The mere </span></span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">expectation </span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">of encountering evidence–before you've actually seen it–should not shift your prior beliefs.</span><span class=\"by_rordgt937kcgXxfKP\"> It also goes by other names, including </span></span><i><span class=\"by_rordgt937kcgXxfKP\">the </span></i><a href=\"https://en.wikipedia.org/wiki/Law_of_total_expectation\"><i><span class=\"by_rordgt937kcgXxfKP\">law of total expectation</span></i></a><span class=\"by_rordgt937kcgXxfKP\"> and </span><i><span class=\"by_rordgt937kcgXxfKP\">the law of iterated expectations</span></i><span class=\"by_rordgt937kcgXxfKP\">.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">A consequence of this principle is that </span><a href=\"https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence\"><span><span class=\"by_qgdGA4ZEyW7zNdK84\">absence of evidence is evidence of </span><span class=\"by_sKAL2jzfkYkDbQmx9\">absence</span></span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">.</span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">Consider a hypothesis H and evidence (observation) E. </span><a href=\"https://wiki.lesswrong.com/wiki/Prior\"><span class=\"by_qf77EiaoMw7tH3GSr\">Prior</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><a href=\"https://wiki.lesswrong.com/wiki/probability\"><span class=\"by_qf77EiaoMw7tH3GSr\">probability</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> of the hypothesis is P(H); </span><a href=\"https://wiki.lesswrong.com/wiki/posterior\"><span class=\"by_qf77EiaoMw7tH3GSr\">posterior</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> probability is either P(H|E) or P(H|¬E), depending on whether you observe E or not-E (evidence or counterevidence). The probability of observing E is P(E), and probability of observing not-E is P(¬E). Thus, </span><a href=\"https://lessestwrong.com/tag/expected-value\"><span class=\"by_qf77EiaoMw7tH3GSr\">expected value</span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\"> of the posterior probability of the hypothesis </span><span class=\"by_gkwy8MPPGrupNdMdw\">is:</span></span></p><p><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">H</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">|</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">E</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">) ⋅ </span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">E</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">) </span><span class=\"by_qf77EiaoMw7tH3GSr\">+ </span></span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">H</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">|</span><span class=\"by_qf77EiaoMw7tH3GSr\">¬</span></span><i><span class=\"by_qf77EiaoMw7tH3GSr\">E</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">) ⋅ </span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><span class=\"by_qf77EiaoMw7tH3GSr\">¬</span></span><i><span class=\"by_qf77EiaoMw7tH3GSr\">E</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">)</span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">But the prior probability of the hypothesis itself can be trivially broken up the same way:</span></p><p><span><span class=\"mjpage\"><span class=\"mjx-chtml\"><span class=\"mjx-math\" aria-label=\"\\begin{alignat}{2}P(H) &amp; = P(H,E) + P(H,\\neg{E}) \\\\&amp; = P(H|E) \\cdot P(E) + P(H|\\neg{E}) \\cdot P(\\neg{E})\\end{alignat}\"><span class=\"mjx-mrow\" aria-hidden=\"true\"><span class=\"mjx-mtable\" style=\"vertical-align: -0.975em; padding: 0px 0.167em;\"><span class=\"mjx-table\"><span class=\"mjx-mtr\" style=\"height: 1.225em;\"><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: right; width: 2.417em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">H</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></span></span><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0px 0px 0px 0px; text-align: left; width: 16.839em;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_qf77EiaoMw7tH3GSr\">=</span></span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">H</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">E</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\"><span class=\"by_qf77EiaoMw7tH3GSr\">+</span></span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">H</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"margin-top: -0.144em; padding-bottom: 0.519em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span></span></span><span class=\"mjx-mi MJXc-space1\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">¬</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">E</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_gkwy8MPPGrupNdMdw\">)</span></span></span><span class=\"mjx-strut\"></span></span></span></span><span class=\"mjx-mtr\" style=\"height: 1.225em;\"><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: right;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-strut\"></span></span></span><span class=\"mjx-mtd\" style=\"padding: 0.15em 0px 0px 0px; text-align: left;\"><span class=\"mjx-mrow\" style=\"margin-top: -0.2em;\"><span class=\"mjx-mi\"></span><span class=\"mjx-mo MJXc-space3\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_qf77EiaoMw7tH3GSr\">=</span></span></span><span class=\"mjx-mi MJXc-space3\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">H</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">E</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">⋅</span></span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">E</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.298em; padding-bottom: 0.446em;\"><span class=\"by_qf77EiaoMw7tH3GSr\">+</span></span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.057em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">H</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">|</span></span></span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">¬</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">E</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_gkwy8MPPGrupNdMdw\">)</span></span></span><span class=\"mjx-mo MJXc-space2\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.004em; padding-bottom: 0.298em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">⋅</span></span></span><span class=\"mjx-mi MJXc-space2\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.109em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">P</span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">(</span></span></span><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.077em; padding-bottom: 0.298em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">¬</span></span></span><span class=\"mjx-texatom\"><span class=\"mjx-mrow\"><span class=\"mjx-mi\"><span class=\"mjx-char MJXc-TeX-math-I\" style=\"padding-top: 0.446em; padding-bottom: 0.298em; padding-right: 0.026em;\"><span class=\"by_qgdGA4ZEyW7zNdK84\">E</span></span></span></span></span><span class=\"mjx-mo\"><span class=\"mjx-char MJXc-TeX-main-R\" style=\"padding-top: 0.446em; padding-bottom: 0.593em;\"><span class=\"by_gkwy8MPPGrupNdMdw\">)</span></span></span><span class=\"mjx-strut\"></span></span></span></span></span></span></span></span><style>.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}\n.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}\n.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}\n.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}\n.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}\n.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}\n.mjx-numerator {display: block; text-align: center}\n.mjx-denominator {display: block; text-align: center}\n.MJXc-stacked {height: 0; position: relative}\n.MJXc-stacked > * {position: absolute}\n.MJXc-bevelled > * {display: inline-block}\n.mjx-stack {display: inline-block}\n.mjx-op {display: block}\n.mjx-under {display: table-cell}\n.mjx-over {display: block}\n.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}\n.mjx-stack > .mjx-sup {display: block}\n.mjx-stack > .mjx-sub {display: block}\n.mjx-prestack > .mjx-presup {display: block}\n.mjx-prestack > .mjx-presub {display: block}\n.mjx-delim-h > .mjx-char {display: inline-block}\n.mjx-surd {vertical-align: top}\n.mjx-surd + .mjx-box {display: inline-flex}\n.mjx-mphantom * {visibility: hidden}\n.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}\n.mjx-annotation-xml {line-height: normal}\n.mjx-menclose > svg {fill: none; stroke: currentColor; overflow: visible}\n.mjx-mtr {display: table-row}\n.mjx-mlabeledtr {display: table-row}\n.mjx-mtd {display: table-cell; text-align: center}\n.mjx-label {display: table-row}\n.mjx-box {display: inline-block}\n.mjx-block {display: block}\n.mjx-span {display: inline}\n.mjx-char {display: block; white-space: pre}\n.mjx-itable {display: inline-table; width: auto}\n.mjx-row {display: table-row}\n.mjx-cell {display: table-cell}\n.mjx-table {display: table; width: 100%}\n.mjx-line {display: block; height: 0}\n.mjx-strut {width: 0; padding-top: 1em}\n.mjx-vsize {width: 0}\n.MJXc-space1 {margin-left: .167em}\n.MJXc-space2 {margin-left: .222em}\n.MJXc-space3 {margin-left: .278em}\n.mjx-test.mjx-test-display {display: table!important}\n.mjx-test.mjx-test-inline {display: inline!important; margin-right: -1px}\n.mjx-test.mjx-test-default {display: block!important; clear: both}\n.mjx-ex-box {display: inline-block!important; position: absolute; overflow: hidden; min-height: 0; max-height: none; padding: 0; border: 0; margin: 0; width: 1px; height: 60ex}\n.mjx-test-inline .mjx-left-box {display: inline-block; width: 0; float: left}\n.mjx-test-inline .mjx-right-box {display: inline-block; width: 0; float: right}\n.mjx-test-display .mjx-right-box {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}\n.MJXc-TeX-unknown-R {font-family: monospace; font-style: normal; font-weight: normal}\n.MJXc-TeX-unknown-I {font-family: monospace; font-style: italic; font-weight: normal}\n.MJXc-TeX-unknown-B {font-family: monospace; font-style: normal; font-weight: bold}\n.MJXc-TeX-unknown-BI {font-family: monospace; font-style: italic; font-weight: bold}\n.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}\n.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}\n.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}\n.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}\n.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}\n.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}\n.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}\n.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}\n.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}\n.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}\n.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}\n.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}\n.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}\n.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}\n.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}\n.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}\n.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}\n.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}\n.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}\n.MJXc-TeX-vec-R {font-family: MJXc-TeX-vec-R,MJXc-TeX-vec-Rw}\n.MJXc-TeX-vec-B {font-family: MJXc-TeX-vec-B,MJXc-TeX-vec-Bx,MJXc-TeX-vec-Bw}\n@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}\n@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}\n@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}\n@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}\n@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}\n@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}\n@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}\n@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}\n@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}\n@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}\n@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}\n@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}\n@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}\n@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}\n@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}\n@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}\n@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}\n@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}\n@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}\n@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}\n@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}\n@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}\n@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}\n@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-R; src: local('MathJax_Vector'), local('MathJax_Vector-Regular')}\n@font-face {font-family: MJXc-TeX-vec-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Regular.otf') format('opentype')}\n@font-face {font-family: MJXc-TeX-vec-B; src: local('MathJax_Vector Bold'), local('MathJax_Vector-Bold')}\n@font-face {font-family: MJXc-TeX-vec-Bx; src: local('MathJax_Vector'); font-weight: bold}\n@font-face {font-family: MJXc-TeX-vec-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/eot/MathJax_Vector-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/woff/MathJax_Vector-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/fonts/HTML-CSS/TeX/otf/MathJax_Vector-Bold.otf') format('opentype')}\n</style></span></span></span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">Thus, expectation of posterior probability is equal to the prior probability.</span></p><p><span><span class=\"by_mPipmBTniuABY5PQy\">In </span><span class=\"by_qf77EiaoMw7tH3GSr\">other way, if you expect the </span><span class=\"by_mPipmBTniuABY5PQy\">probability </span><span class=\"by_qf77EiaoMw7tH3GSr\">of a hypothesis to change as a result of observing some evidence, the amount of this change if the evidence is positive is</span></span></p><p><i><span class=\"by_gkwy8MPPGrupNdMdw\">D</span></i><sub><span class=\"by_gkwy8MPPGrupNdMdw\">1</span></sub><span><span class=\"by_gkwy8MPPGrupNdMdw\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">= </span></span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">H</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">|</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">E</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">) − </span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">H</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">)</span><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">If the evidence is negative, the change is</span></p><p><i><span class=\"by_qf77EiaoMw7tH3GSr\">D</span></i><sub><span class=\"by_qf77EiaoMw7tH3GSr\">2</span></sub><span class=\"by_qf77EiaoMw7tH3GSr\"> = </span><i><span class=\"by_qf77EiaoMw7tH3GSr\">P</span></i><span class=\"by_qf77EiaoMw7tH3GSr\">(</span><i><span class=\"by_qf77EiaoMw7tH3GSr\">H</span></i><span class=\"by_qf77EiaoMw7tH3GSr\">|¬</span><i><span class=\"by_qf77EiaoMw7tH3GSr\">E</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">)</span><span class=\"by_qf77EiaoMw7tH3GSr\"> − </span></span><i><span class=\"by_qf77EiaoMw7tH3GSr\">P</span></i><span class=\"by_qf77EiaoMw7tH3GSr\">(</span><i><span class=\"by_qf77EiaoMw7tH3GSr\">H</span></i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">)</span><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></span></p><p><span class=\"by_qf77EiaoMw7tH3GSr\">Expectation of the change given positive evidence is equal to negated expectation of the change given counterevidence:</span></p><p><i><span class=\"by_gkwy8MPPGrupNdMdw\">D</span></i><sub><span class=\"by_gkwy8MPPGrupNdMdw\">1</span></sub><span class=\"by_gkwy8MPPGrupNdMdw\"> ⋅ </span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><i><span class=\"by_gkwy8MPPGrupNdMdw\">E</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">) </span><span class=\"by_qf77EiaoMw7tH3GSr\">=  </span><span class=\"by_gkwy8MPPGrupNdMdw\">− </span></span><i><span class=\"by_gkwy8MPPGrupNdMdw\">D</span></i><sub><span class=\"by_gkwy8MPPGrupNdMdw\">2</span></sub><span class=\"by_gkwy8MPPGrupNdMdw\"> ⋅ </span><i><span class=\"by_gkwy8MPPGrupNdMdw\">P</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">(</span><span class=\"by_qf77EiaoMw7tH3GSr\">¬</span></span><i><span class=\"by_qf77EiaoMw7tH3GSr\">E</span></i><span><span class=\"by_gkwy8MPPGrupNdMdw\">)</span><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></span></p><p><span class=\"by_mPipmBTniuABY5PQy\">If you can </span><i><span class=\"by_mPipmBTniuABY5PQy\">anticipate in advance</span></i><span><span class=\"by_mPipmBTniuABY5PQy\"> updating your belief in a particular direction, then you should just go ahead and update now.</span><span class=\"by_qf77EiaoMw7tH3GSr\"> Once you know your destination, you are already there.&nbsp;</span></span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/ii/conservation_of_expected_evidence/\"><span class=\"by_mPipmBTniuABY5PQy\">Conservation of Expected Evidence</span></a></li><li><a href=\"https://lessestwrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence-1\"><span class=\"by_Tw9etd8rMnHLeSQ9q\">Mistakes with Conservation of Expected Evidence</span></a></li></ul><h2 id=\"See_Also\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/filtered-evidence\"><span class=\"by_9c2mQkLQq6gQSksMs\">Filtered evidence</span></a></li></ul>",
      "sections": [
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 13,
    "description": {
      "markdown": "**Conservation of Expected Evidence** is a consequence of probability theory which states that for every expectation of evidence, there is an equal and opposite expectation of counterevidence \\[[1](https://www.lesswrong.com/posts/jiBFC7DcCrZjGmZnJ/conservation-of-expected-evidence)\\]. Conservation of Expected Evidence is about both the direction of the update and its magnitude: a low probability of seeing strong evidence in one direction must be balanced by a high probability of observing weak counterevidence in the other direction \\[[2](https://www.lesswrong.com/s/uLEjM2ij5y3CXXW6c/p/zTfSXQracE7TW8x4w#1___You_can_t_predict_that_you_ll_update_in_a_particular_direction__)\\]. The mere *expectation* of encountering evidence–before you've actually seen it–should not shift your prior beliefs. It also goes by other names, including *the* [*law of total expectation*](https://en.wikipedia.org/wiki/Law_of_total_expectation) and *the law of iterated expectations*.\n\nA consequence of this principle is that [absence of evidence is evidence of absence](https://www.lesswrong.com/posts/mnS2WYLCGJP2kQkRn/absence-of-evidence-is-evidence-of-absence).\n\nConsider a hypothesis H and evidence (observation) E. [Prior](https://wiki.lesswrong.com/wiki/Prior) [probability](https://wiki.lesswrong.com/wiki/probability) of the hypothesis is P(H); [posterior](https://wiki.lesswrong.com/wiki/posterior) probability is either P(H|E) or P(H|¬E), depending on whether you observe E or not-E (evidence or counterevidence). The probability of observing E is P(E), and probability of observing not-E is P(¬E). Thus, [expected value](https://lessestwrong.com/tag/expected-value) of the posterior probability of the hypothesis is:\n\n*P*(*H*|*E*) ⋅ *P*(*E*) + *P*(*H*|¬*E*) ⋅ *P*(¬*E*)\n\nBut the prior probability of the hypothesis itself can be trivially broken up the same way:\n\n\\\\(\\\\begin{alignat}{2}P(H) & = P(H,E) + P(H,\\\\neg{E}) \\\\\\& = P(H|E) \\\\cdot P(E) + P(H|\\\\neg{E}) \\\\cdot P(\\\\neg{E})\\\\end{alignat}\\\\)\n\nThus, expectation of posterior probability is equal to the prior probability.\n\nIn other way, if you expect the probability of a hypothesis to change as a result of observing some evidence, the amount of this change if the evidence is positive is\n\n*D*~1~ = *P*(*H*|*E*) − *P*(*H*).\n\nIf the evidence is negative, the change is\n\n*D*~2~ = *P*(*H*|¬*E*) − *P*(*H*).\n\nExpectation of the change given positive evidence is equal to negated expectation of the change given counterevidence:\n\n*D*~1~ ⋅ *P*(*E*) =  − *D*~2~ ⋅ *P*(¬*E*).\n\nIf you can *anticipate in advance* updating your belief in a particular direction, then you should just go ahead and update now. Once you know your destination, you are already there. \n\nNotable Posts\n-------------\n\n*   [Conservation of Expected Evidence](https://lessestwrong.com/lw/ii/conservation_of_expected_evidence/)\n*   [Mistakes with Conservation of Expected Evidence](https://lessestwrong.com/posts/zTfSXQracE7TW8x4w/mistakes-with-conservation-of-expected-evidence-1)\n\nSee Also\n--------\n\n*   [Filtered evidence](https://lessestwrong.com/tag/filtered-evidence)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "xcBbcAJrvTEkxikW9",
    "name": "Bucket Errors",
    "core": null,
    "slug": "bucket-errors",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">A </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Bucket Error </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is when multiple different concepts or variables are incorrectly lumped together in one's mind as a single concept/variable, potentially leading to distortions of one's thinking. Bucket Errors are related to </span><a href=\"https://www.lesswrong.com/posts/y5MxoeacRKKM3KQth/fallacies-of-compression\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fallacies of Compression</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span><br><br><span class=\"by_qgdGA4ZEyW7zNdK84\">The term</span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">,</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Bucket Error</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">, was introduced in&nbsp;</span><a href=\"https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the\"><span class=\"by_qgdGA4ZEyW7zNdK84\">\"Flinching away from truth” is often about *protecting* the epistemology</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> where an example is given of a child who refuses to believe that they made a spelling mistake. The child is unwilling to believe this fact because they believe having made a spelling mistake is incompatible with becoming a writer which is their dream. \"Becoming a writer\" and \"never making spelling mistakes\" are lumped in the same bucket despite in fact being separate variables.</span></p><p><span class=\"by_efKySALtaLcvtp3jW\">Bucket Errors are similar to the concepts of </span><a href=\"https://en.wikipedia.org/wiki/Equivocation\"><span class=\"by_efKySALtaLcvtp3jW\">equivocation</span></a><span class=\"by_efKySALtaLcvtp3jW\">, identification in Buddhism, or fusion/defusion in modern psychotherapy. See: </span><a href=\"https://www.lesswrong.com/posts/RQrWd5jPZQtpH8f4v/fusion-and-equivocation-in-korzybski-s-general-semantics\"><span class=\"by_efKySALtaLcvtp3jW\">Fusion and Equivocation in Korzybski's General Semantics</span></a><span class=\"by_efKySALtaLcvtp3jW\">.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/compartmentalization\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Compartmentalization</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/distinctions\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Distinctions</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 12,
    "description": {
      "markdown": "A **Bucket Error** is when multiple different concepts or variables are incorrectly lumped together in one's mind as a single concept/variable, potentially leading to distortions of one's thinking. Bucket Errors are related to [Fallacies of Compression](https://www.lesswrong.com/posts/y5MxoeacRKKM3KQth/fallacies-of-compression).  \n  \nThe term*,* *Bucket Error*, was introduced in [\"Flinching away from truth” is often about \\*protecting\\* the epistemology](https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the) where an example is given of a child who refuses to believe that they made a spelling mistake. The child is unwilling to believe this fact because they believe having made a spelling mistake is incompatible with becoming a writer which is their dream. \"Becoming a writer\" and \"never making spelling mistakes\" are lumped in the same bucket despite in fact being separate variables.\n\nBucket Errors are similar to the concepts of [equivocation](https://en.wikipedia.org/wiki/Equivocation), identification in Buddhism, or fusion/defusion in modern psychotherapy. See: [Fusion and Equivocation in Korzybski's General Semantics](https://www.lesswrong.com/posts/RQrWd5jPZQtpH8f4v/fusion-and-equivocation-in-korzybski-s-general-semantics).\n\n**Related Pages:** [Compartmentalization](https://www.lesswrong.com/tag/compartmentalization), [Distinctions](https://www.lesswrong.com/tag/distinctions)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BhrpjXqGuke5GnF6g",
    "name": "Hamming Questions",
    "core": false,
    "slug": "hamming-questions",
    "tableOfContents": {
      "html": "<blockquote><p><span class=\"by_sKAL2jzfkYkDbQmx9\">Mathematician Richard Hamming used to ask scientists in other fields \"What are the most important problems in your field?\" partly so he could troll them by asking \"Why aren't you working on them?\" and partly because getting asked this question is really useful for focusing people's attention on what matters.&nbsp;</span><br><br><span class=\"by_sKAL2jzfkYkDbQmx9\">CFAR developed the technique of \"Hamming Questions\" as different prompts to get your brain to (actually) think about the biggest problems, bottlenecks, and unspoken desires in </span><i><span class=\"by_sKAL2jzfkYkDbQmx9\">your</span></i><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;life.</span><br><br><a href=\"https://www.lesswrong.com/posts/rnFLc3E5Y4FP8TSGC/the-biggest-problem-in-your-life\"><span class=\"by_sKAL2jzfkYkDbQmx9\">(Taken from here)</span></a></p></blockquote><p><br><span class=\"by_sKAL2jzfkYkDbQmx9\">See also </span><a href=\"https://www.lesswrong.com/tag/open-problems\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Open Problems</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 23,
    "description": {
      "markdown": "> Mathematician Richard Hamming used to ask scientists in other fields \"What are the most important problems in your field?\" partly so he could troll them by asking \"Why aren't you working on them?\" and partly because getting asked this question is really useful for focusing people's attention on what matters.   \n>   \n> CFAR developed the technique of \"Hamming Questions\" as different prompts to get your brain to (actually) think about the biggest problems, bottlenecks, and unspoken desires in *your* life.  \n>   \n> [(Taken from here)](https://www.lesswrong.com/posts/rnFLc3E5Y4FP8TSGC/the-biggest-problem-in-your-life)\n\n  \nSee also [Open Problems](https://www.lesswrong.com/tag/open-problems)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "yt9Z7xdQrofW7fCN8",
    "name": "Epistemic Review",
    "core": false,
    "slug": "epistemic-review",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Epistemic Reviews</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> take a closer look at an existing publication – such as a book, paper or blogpost – and evaluate whether its claims are true.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 32,
    "description": {
      "markdown": "**Epistemic Reviews** take a closer look at an existing publication – such as a book, paper or blogpost – and evaluate whether its claims are true."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "hNFdS3rRiYgqqD8aM",
    "name": "Humor",
    "core": null,
    "slug": "humor",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Humor</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">. This tag is a joke.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 118,
    "description": {
      "markdown": "**Humor**. This tag is a joke."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EnFKSZYiDHqMJuvJL",
    "name": "Social Reality",
    "core": false,
    "slug": "social-reality",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Social Reality</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> is \"that which exists, proportional to how much people believe in it\" (contrasted with \"regular reality\", which is \"that which exists, whether you believe in it or not\"). It includes both the rules that govern social interaction, and \"beliefs\" that people adopt as part of tribal membership.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 39,
    "description": {
      "markdown": "**Social Reality** is \"that which exists, proportional to how much people believe in it\" (contrasted with \"regular reality\", which is \"that which exists, whether you believe in it or not\"). It includes both the rules that govern social interaction, and \"beliefs\" that people adopt as part of tribal membership."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "8XiMxJaWbjNtWLsEj",
    "name": "Cost Disease",
    "core": null,
    "slug": "cost-disease",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Cost Disease</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> or </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Baumol's cost disease </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is a name for the rise of salaries in jobs that have experienced no or low increase of labor productivity [</span><a href=\"https://en.wikipedia.org/wiki/Baumol%27s_cost_disease\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]. Some use the term generally to refer to rising costs in general [</span><a href=\"https://www.lesswrong.com/posts/BBQ5HEnL3ShefQxEj/considerations-on-cost-disease\"><span class=\"by_qgdGA4ZEyW7zNdK84\">2</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">].</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Often the questions being asked in Cost Disease discussion are why the cost healthcare and education have increased many, many times over.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">External Posts:</span></strong><br><a href=\"https://slatestarcodex.com/2019/06/10/book-review-the-prices-are-too-dmn-high/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Book Review: Why Are The Prices So D*mn High? </span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">- Scott Alexander</span><br><span class=\"by_sKAL2jzfkYkDbQmx9\">&nbsp;</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 8,
    "description": {
      "markdown": "**Cost Disease** or **Baumol's cost disease** is a name for the rise of salaries in jobs that have experienced no or low increase of labor productivity \\[[1](https://en.wikipedia.org/wiki/Baumol%27s_cost_disease)\\]. Some use the term generally to refer to rising costs in general \\[[2](https://www.lesswrong.com/posts/BBQ5HEnL3ShefQxEj/considerations-on-cost-disease)\\].\n\nOften the questions being asked in Cost Disease discussion are why the cost healthcare and education have increased many, many times over.\n\n**External Posts:**  \n[Book Review: Why Are The Prices So D*mn High?](https://slatestarcodex.com/2019/06/10/book-review-the-prices-are-too-dmn-high/) \\- Scott Alexander"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "h96z2Xt4h6zt2wiw2",
    "name": "GreaterWrong Meta",
    "core": false,
    "slug": "greaterwrong-meta",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">GreaterWrong</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is an alternative front-end/viewer for the LessWrong site. The viewer is accessible at </span><a href=\"https://www.greaterwrong.com\"><span class=\"by_qgdGA4ZEyW7zNdK84\">www.greaterwrong.com</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">. The project uses the API of the main LessWrong project but is maintained independently by LessWrong users, </span><a href=\"https://www.lessestwrong.com/users/clone-of-saturn\"><span class=\"by_qgdGA4ZEyW7zNdK84\">clone of saturn</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"https://www.lessestwrong.com/users/saidachmiz\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Said Achmiz</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 10,
    "description": {
      "markdown": "**GreaterWrong** is an alternative front-end/viewer for the LessWrong site. The viewer is accessible at [www.greaterwrong.com](https://www.greaterwrong.com). The project uses the API of the main LessWrong project but is maintained independently by LessWrong users, [clone of saturn](https://www.lessestwrong.com/users/clone-of-saturn) and [Said Achmiz](https://www.lessestwrong.com/users/saidachmiz)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ALwRRZqvhaop8gxkT",
    "name": "Groupthink",
    "core": false,
    "slug": "groupthink",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_ChXHsXmDQFWZH638i\">Groupthink</span></strong><span class=\"by_ChXHsXmDQFWZH638i\"> is a very well-documented source of </span><a href=\"https://www.lesswrong.com/tag/bias\"><span class=\"by_ChXHsXmDQFWZH638i\">bias</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\"> in cognitive psychology. It refers to the tendency of humans to tend to agree with each other, and hold back objections or dissent even when the group is </span><span class=\"by_LoykQRMTxJFxwwdPy\">wrong.</span></span></p><p><span class=\"by_ChXHsXmDQFWZH638i\">There seems to be a balance of tensions between evaporative cooling of beliefs leading to groupthink, and extremely biased </span><a href=\"https://www.lesswrong.com/tag/blues-and-greens-metaphor\"><span class=\"by_ChXHsXmDQFWZH638i\">color politics</span></a><span class=\"by_ChXHsXmDQFWZH638i\">.</span></p><h2 id=\"Echo_chamber\"><span class=\"by_ChXHsXmDQFWZH638i\">Echo chamber</span></h2><p><span class=\"by_ChXHsXmDQFWZH638i\">An </span><strong><span class=\"by_ChXHsXmDQFWZH638i\">echo chamber</span></strong><span><span class=\"by_ChXHsXmDQFWZH638i\"> is a group of media sources that parrot each other by constantly and unquestioningly reporting a single, biased point of view, thus creating the illusion of </span><span class=\"by_LoykQRMTxJFxwwdPy\">consensus.</span></span></p><h2 id=\"_Virtual_community_\"><span class=\"by_ChXHsXmDQFWZH638i\">\"Virtual community\"</span></h2><p><span><span class=\"by_ChXHsXmDQFWZH638i\">In computer-mediated </span><span class=\"by_LoykQRMTxJFxwwdPy\">deliberation,</span><span class=\"by_ChXHsXmDQFWZH638i\"> the ideology of so-called </span></span><strong><span class=\"by_ChXHsXmDQFWZH638i\">\"virtual community\"</span></strong><span><span class=\"by_ChXHsXmDQFWZH638i\"> has been implicated as </span><span class=\"by_LoykQRMTxJFxwwdPy\">a</span><span class=\"by_ChXHsXmDQFWZH638i\"> damaging source of groupthink. In the real world, a \"community\" is an ethical and political </span></span><i><span class=\"by_ChXHsXmDQFWZH638i\">compromise</span></i><span class=\"by_ChXHsXmDQFWZH638i\"> of values due to a shared need for protection from bodily harm or harm to one's surrounding environment. A \"virtual community\" has no such natural values but still retains the </span><a href=\"https://www.lesswrong.com/tag/mind-killer\"><span class=\"by_ChXHsXmDQFWZH638i\">mind-killing</span></a><span><span class=\"by_ChXHsXmDQFWZH638i\"> illusion of protection, and is thus liable to turn into a </span><span class=\"by_LoykQRMTxJFxwwdPy\">cult.</span></span></p><h2 id=\"See_also\"><span class=\"by_LoykQRMTxJFxwwdPy\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/cached-thought\"><span class=\"by_LoykQRMTxJFxwwdPy\">Cached thought</span></a></li><li><a href=\"https://www.lesswrong.com/tag/affective-death-spiral\"><span class=\"by_LoykQRMTxJFxwwdPy\">Affective death spiral</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/information-cascades\"><span class=\"by_LoykQRMTxJFxwwdPy\">information cascade</span></a></li><li><a href=\"https://www.lesswrong.com/tag/in-group-bias\"><span class=\"by_LoykQRMTxJFxwwdPy\">In-group bias</span></a><span class=\"by_LoykQRMTxJFxwwdPy\">, </span><a href=\"https://www.lesswrong.com/tag/conformity-bias\"><span class=\"by_LoykQRMTxJFxwwdPy\">conformity bias</span></a></li><li><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_LoykQRMTxJFxwwdPy\">Goodhart's law</span></a></li><li><a href=\"https://www.lesswrong.com/tag/availability-heuristic\"><span class=\"by_LoykQRMTxJFxwwdPy\">Availability heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/group-rationality\"><span class=\"by_LoykQRMTxJFxwwdPy\">Group rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/problem-of-verifying-rationality\"><span class=\"by_LoykQRMTxJFxwwdPy\">Problem of verifying rationality</span></a></li><li><a href=\"https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor\"><span class=\"by_LoykQRMTxJFxwwdPy\">Death Spirals and the Cult Attractor</span></a></li></ul>",
      "sections": [
        {
          "title": "Echo chamber",
          "anchor": "Echo_chamber",
          "level": 1
        },
        {
          "title": "\"Virtual community\"",
          "anchor": "_Virtual_community_",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 25,
    "description": {
      "markdown": "**Groupthink** is a very well-documented source of [bias](https://www.lesswrong.com/tag/bias) in cognitive psychology. It refers to the tendency of humans to tend to agree with each other, and hold back objections or dissent even when the group is wrong.\n\nThere seems to be a balance of tensions between evaporative cooling of beliefs leading to groupthink, and extremely biased [color politics](https://www.lesswrong.com/tag/blues-and-greens-metaphor).\n\nEcho chamber\n------------\n\nAn **echo chamber** is a group of media sources that parrot each other by constantly and unquestioningly reporting a single, biased point of view, thus creating the illusion of consensus.\n\n\"Virtual community\"\n-------------------\n\nIn computer-mediated deliberation, the ideology of so-called **\"virtual community\"** has been implicated as a damaging source of groupthink. In the real world, a \"community\" is an ethical and political *compromise* of values due to a shared need for protection from bodily harm or harm to one's surrounding environment. A \"virtual community\" has no such natural values but still retains the [mind-killing](https://www.lesswrong.com/tag/mind-killer) illusion of protection, and is thus liable to turn into a cult.\n\nSee also\n--------\n\n*   [Cached thought](https://www.lesswrong.com/tag/cached-thought)\n*   [Affective death spiral](https://www.lesswrong.com/tag/affective-death-spiral), [information cascade](https://www.lesswrong.com/tag/information-cascades)\n*   [In-group bias](https://www.lesswrong.com/tag/in-group-bias), [conformity bias](https://www.lesswrong.com/tag/conformity-bias)\n*   [Goodhart's law](https://www.lesswrong.com/tag/goodhart-s-law)\n*   [Availability heuristic](https://www.lesswrong.com/tag/availability-heuristic)\n*   [Group rationality](https://www.lesswrong.com/tag/group-rationality)\n*   [Problem of verifying rationality](https://www.lesswrong.com/tag/problem-of-verifying-rationality)\n*   [Death Spirals and the Cult Attractor](https://www.lesswrong.com/tag/death-spirals-and-the-cult-attractor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "PJKgSRkXkCqXmCk3M",
    "name": "Mind Projection Fallacy",
    "core": false,
    "slug": "mind-projection-fallacy",
    "tableOfContents": {
      "html": "<p><span class=\"by_qgdGA4ZEyW7zNdK84\">The </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Mind Projection Fallacy </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is the error of projecting the properties of your own mind onto the external world. For example, one might erroneously think that because they enjoy the taste of chocolate, the chocolate has the inherent property of tastiness, and therefore everyone else must like its taste too.</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Overcoming the mind </span><span class=\"by_kLkGphm6uk86weEzx\">projection</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> fallacy requires realizing that our</span><span class=\"by_qf77EiaoMw7tH3GSr\"> minds are</span><span class=\"by_mPipmBTniuABY5PQy\"> not </span><span class=\"by_qf77EiaoMw7tH3GSr\">transparent windows unto veridical</span><span class=\"by_mPipmBTniuABY5PQy\"> reality; </span><span class=\"by_qf77EiaoMw7tH3GSr\">when</span><span class=\"by_mPipmBTniuABY5PQy\"> you look at a rock, you </span><span class=\"by_qf77EiaoMw7tH3GSr\">experience</span><span class=\"by_mPipmBTniuABY5PQy\"> not the the rock itself, but your mind's </span></span><i><span class=\"by_mPipmBTniuABY5PQy\">representation</span></i><span class=\"by_mPipmBTniuABY5PQy\"> of the rock, reconstructed from photons bouncing off its surface. Sugar in and of itself is not </span><i><span class=\"by_mPipmBTniuABY5PQy\">inherently</span></i><span class=\"by_mPipmBTniuABY5PQy\"> sweet; the sugar itself only has the chemical properties that it does, which your brain </span><i><span class=\"by_qf77EiaoMw7tH3GSr\">interprets</span></i><span class=\"by_mPipmBTniuABY5PQy\"> as sweet.</span></p><h2 id=\"History\"><span class=\"by_qgdGA4ZEyW7zNdK84\">History</span></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Physicist</span><span class=\"by_mPipmBTniuABY5PQy\"> and </span></span><a href=\"https://lessestwrong.com/tag/bayesianism\"><span class=\"by_mPipmBTniuABY5PQy\">Bayesian</span></a><span class=\"by_mPipmBTniuABY5PQy\"> philosopher </span><a href=\"https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes\"><span class=\"by_mPipmBTniuABY5PQy\">E.T. Jaynes</span></a><span class=\"by_mPipmBTniuABY5PQy\"> coined the term </span><i><span><span class=\"by_mPipmBTniuABY5PQy\">mind</span><span class=\"by_qf77EiaoMw7tH3GSr\"> projection fallacy</span></span></i><span><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><span class=\"by_mPipmBTniuABY5PQy\">to refer to this kind of failure to distinguish between epistemological claims (statements about belief, about</span><span class=\"by_qf77EiaoMw7tH3GSr\"> your </span><span class=\"by_mPipmBTniuABY5PQy\">map, about what we can </span></span><i><span class=\"by_mPipmBTniuABY5PQy\">say</span></i><span><span class=\"by_mPipmBTniuABY5PQy\"> about reality) and ontological claims (statements about reality, about </span><span class=\"by_qf77EiaoMw7tH3GSr\">the </span><span class=\"by_mPipmBTniuABY5PQy\">territory, about how things </span></span><i><span class=\"by_mPipmBTniuABY5PQy\">are</span></i><span><span class=\"by_mPipmBTniuABY5PQy\">).</span><span class=\"by_qf77EiaoMw7tH3GSr\"> In particular, the concept was applied in the critique of </span></span><a href=\"https://en.wikipedia.org/wiki/Frequentist_inference\"><span class=\"by_qf77EiaoMw7tH3GSr\">frequentist</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> interpretation of the notion of </span><a href=\"https://wiki.lesswrong.com/wiki/probability\"><span class=\"by_qf77EiaoMw7tH3GSr\">probability</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> as a property of physical systems rather than an epistemic device concerned with levels of certainty, </span><a href=\"https://lessestwrong.com/tag/bayesian-probability\"><span class=\"by_qf77EiaoMw7tH3GSr\">Bayesian probability</span></a><span class=\"by_qf77EiaoMw7tH3GSr\">.</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><a href=\"https://lessestwrong.com/lw/oi/mind_projection_fallacy/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Mind Projection Fallacy</span></a></li><li><a href=\"https://lessestwrong.com/lw/oj/probability_is_in_the_mind/\"><span class=\"by_qf77EiaoMw7tH3GSr\">Probability is in the Mind</span></a></li><li><a href=\"https://lessestwrong.com/lw/8tv/examples_of_the_mind_projection_fallacy/\"><span class=\"by_BtbwfsEyeT4P2eqXu\">Examples of the Mind Projection Fallacy?</span></a></li></ul><h2 id=\"See_Also\"><span><span class=\"by_qf77EiaoMw7tH3GSr\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/bayesian-probability\"><span class=\"by_qf77EiaoMw7tH3GSr\">Bayesian probability</span></a></li><li><a href=\"https://lessestwrong.com/tag/magic\"><span class=\"by_qf77EiaoMw7tH3GSr\">Magic</span></a></li><li><a href=\"https://lessestwrong.com/tag/the-map-is-not-the-territory\"><span class=\"by_qf77EiaoMw7tH3GSr\">The map is not the territory</span></a></li><li><a href=\"https://lessestwrong.com/tag/2-place-and-1-place-words\"><span class=\"by_9c2mQkLQq6gQSksMs\">2-place and 1-place words</span></a></li></ul>",
      "sections": [
        {
          "title": "History",
          "anchor": "History",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 21,
    "description": {
      "markdown": "The **Mind Projection Fallacy** is the error of projecting the properties of your own mind onto the external world. For example, one might erroneously think that because they enjoy the taste of chocolate, the chocolate has the inherent property of tastiness, and therefore everyone else must like its taste too.\n\nOvercoming the mind projection fallacy requires realizing that our minds are not transparent windows unto veridical reality; when you look at a rock, you experience not the the rock itself, but your mind's *representation* of the rock, reconstructed from photons bouncing off its surface. Sugar in and of itself is not *inherently* sweet; the sugar itself only has the chemical properties that it does, which your brain *interprets* as sweet.\n\nHistory\n-------\n\nPhysicist and [Bayesian](https://lessestwrong.com/tag/bayesianism) philosopher [E.T. Jaynes](https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes) coined the term *mind projection fallacy* to refer to this kind of failure to distinguish between epistemological claims (statements about belief, about your map, about what we can *say* about reality) and ontological claims (statements about reality, about the territory, about how things *are*). In particular, the concept was applied in the critique of [frequentist](https://en.wikipedia.org/wiki/Frequentist_inference) interpretation of the notion of [probability](https://wiki.lesswrong.com/wiki/probability) as a property of physical systems rather than an epistemic device concerned with levels of certainty, [Bayesian probability](https://lessestwrong.com/tag/bayesian-probability).\n\nNotable Posts\n-------------\n\n*   [Mind Projection Fallacy](https://lessestwrong.com/lw/oi/mind_projection_fallacy/)\n*   [Probability is in the Mind](https://lessestwrong.com/lw/oj/probability_is_in_the_mind/)\n*   [Examples of the Mind Projection Fallacy?](https://lessestwrong.com/lw/8tv/examples_of_the_mind_projection_fallacy/)\n\nSee Also\n--------\n\n*   [Bayesian probability](https://lessestwrong.com/tag/bayesian-probability)\n*   [Magic](https://lessestwrong.com/tag/magic)\n*   [The map is not the territory](https://lessestwrong.com/tag/the-map-is-not-the-territory)\n*   [2-place and 1-place words](https://lessestwrong.com/tag/2-place-and-1-place-words)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "GQyPQcdEQF4zXhJBq",
    "name": "List of Links",
    "core": null,
    "slug": "list-of-links",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">Some posts contain a </span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">list of links</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">, either to other Less Wrong posts or to external websites</span><strong><span class=\"by_HoGziwmhpMGqGeWZy\">.</span></strong><span class=\"by_HoGziwmhpMGqGeWZy\"> These posts may make it easy to find many resources on a topic, or offer a variety of interesting things to read.</span></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/repository-1\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Repository</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 80,
    "description": {
      "markdown": "Some posts contain a **list of links**, either to other Less Wrong posts or to external websites**.** These posts may make it easy to find many resources on a topic, or offer a variety of interesting things to read.\n\n**Related Pages:** [Repository](https://www.lesswrong.com/tag/repository-1)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YWzByWvtXunfrBu5b",
    "name": "GPT",
    "core": null,
    "slug": "gpt",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">GPT</span></strong><span><span class=\"by_EQNTWXLKMeWMp2FQS\"> (Generative Pretrained Transformer)</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> is a </span><span class=\"by_6Nitmdekyr6gWxzKy\">family of </span><span class=\"by_qgdGA4ZEyW7zNdK84\">large transformer-based language </span><span class=\"by_6Nitmdekyr6gWxzKy\">models</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> created by OpenAI. Its ability to generate remarkably human-like responses has relevance to discussions on AGI.</span></span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 136,
    "description": {
      "markdown": "**GPT** (Generative Pretrained Transformer) is a family of large transformer-based language models created by OpenAI. Its ability to generate remarkably human-like responses has relevance to discussions on AGI."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5hpGj9nDLgokfghvR",
    "name": "Confirmation Bias",
    "core": null,
    "slug": "confirmation-bias",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Confirmation</span><span class=\"by_qf77EiaoMw7tH3GSr\"> bias</span></span></strong><span><span class=\"by_8bJycwyZpC4XbwPCF\"> (also known as positive bias)</span><span class=\"by_qf77EiaoMw7tH3GSr\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> tendency to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">search for, interpret, favor, and recall information in a way that confirms or strengthens </span><span class=\"by_8bJycwyZpC4XbwPCF\">one'</span><span class=\"by_qgdGA4ZEyW7zNdK84\">s prior personal beliefs or</span><span class=\"by_qf77EiaoMw7tH3GSr\"> hypotheses </span><span class=\"by_qgdGA4ZEyW7zNdK84\">[</span></span><a href=\"https://en.wikipedia.org/wiki/Confirmation_bias\"><span class=\"by_Ge4nyWoTAx8EsJCMS\">1</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\">]</span><span class=\"by_qf77EiaoMw7tH3GSr\">. &nbsp;</span><span class=\"by_8bJycwyZpC4XbwPCF\">For example, one might test hypotheses with positive rather than negative examples, thus missing obvious disconfirming tests.</span></span></p><p><em><span class=\"by_8bJycwyZpC4XbwPCF\">See also: </span></em><a href=\"https://www.lesswrong.com/tag/motivated-skepticism\"><span class=\"by_8bJycwyZpC4XbwPCF\">Motivated skepticism</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://lesswrong.com/tag/availability-heuristic\"><span class=\"by_8bJycwyZpC4XbwPCF\">Availability heuristic</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/surprise\"><span class=\"by_8bJycwyZpC4XbwPCF\">Surprise</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/narrative-fallacy\"><span class=\"by_8bJycwyZpC4XbwPCF\">Narrative fallacy</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/privileging-the-hypothesis\"><span class=\"by_8bJycwyZpC4XbwPCF\">Privileging the hypothesis</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, </span><a href=\"https://www.lesswrong.com/tag/heuristics-and-biases\"><span class=\"by_8bJycwyZpC4XbwPCF\">Heuristics and Biases</span></a></p><h2 id=\"External_Links\"><span class=\"by_8bJycwyZpC4XbwPCF\">External Links</span></h2><ul><li><a href=\"https://www.edge.org/conversation/kevin_kelly-speculations-on-the-future-of-science\"><span class=\"by_8bJycwyZpC4XbwPCF\">Speculations on the Future of Science </span></a><span class=\"by_8bJycwyZpC4XbwPCF\">by Kevin Kelly</span></li><li><a href=\"http://psy2.ucsd.edu/~mckenzie/Wason1960QJEP.pdf\"><span class=\"by_8bJycwyZpC4XbwPCF\">On the Failure to Eliminate Hypotheses in a Conceptual Task</span></a><span class=\"by_8bJycwyZpC4XbwPCF\"> by P.C. Wason</span></li><li><a href=\"https://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html\"><span class=\"by_8bJycwyZpC4XbwPCF\">Write Your Hypothetical Apostasy</span></a><span class=\"by_8bJycwyZpC4XbwPCF\"> by </span><a href=\"https://www.lesswrong.com/tag/nick-bostrom\"><span class=\"by_8bJycwyZpC4XbwPCF\">Nick Bostrom</span></a></li><li><a href=\"https://en.wikipedia.org/wiki/Confirmation_bias\"><span class=\"by_8bJycwyZpC4XbwPCF\">Confirmation Bias</span></a><span class=\"by_8bJycwyZpC4XbwPCF\">, Wikipedia</span></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 25,
    "description": {
      "markdown": "**Confirmation bias** (also known as positive bias) is the tendency to search for, interpret, favor, and recall information in a way that confirms or strengthens one's prior personal beliefs or hypotheses \\[[1](https://en.wikipedia.org/wiki/Confirmation_bias)\\].  For example, one might test hypotheses with positive rather than negative examples, thus missing obvious disconfirming tests.\n\n_See also:_ [Motivated skepticism](https://www.lesswrong.com/tag/motivated-skepticism), [Availability heuristic](https://lesswrong.com/tag/availability-heuristic), [Surprise](https://www.lesswrong.com/tag/surprise), [Narrative fallacy](https://www.lesswrong.com/tag/narrative-fallacy), [Privileging the hypothesis](https://www.lesswrong.com/tag/privileging-the-hypothesis), [Heuristics and Biases](https://www.lesswrong.com/tag/heuristics-and-biases)\n\nExternal Links\n--------------\n\n*   [Speculations on the Future of Science](https://www.edge.org/conversation/kevin_kelly-speculations-on-the-future-of-science) by Kevin Kelly\n*   [On the Failure to Eliminate Hypotheses in a Conceptual Task](http://psy2.ucsd.edu/~mckenzie/Wason1960QJEP.pdf) by P.C. Wason\n*   [Write Your Hypothetical Apostasy](https://www.overcomingbias.com/2009/02/write-your-hypothetical-apostasy.html) by [Nick Bostrom](https://www.lesswrong.com/tag/nick-bostrom)\n*   [Confirmation Bias](https://en.wikipedia.org/wiki/Confirmation_bias), Wikipedia"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "YQW2DxpZFTrqrxHBJ",
    "name": "Inferential Distance",
    "core": null,
    "slug": "inferential-distance",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_XgYW5s8njaYrtyP7q\">Inferential </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Distance</span></span></strong><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> between </span><span class=\"by_Sp5wM4aRAhNERd4oY\">two people with respect to an item of knowledge</span><span class=\"by_XgYW5s8njaYrtyP7q\"> is </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the </span><span class=\"by_Sp5wM4aRAhNERd4oY\">amount</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> of steps </span><span class=\"by_Sp5wM4aRAhNERd4oY\">or concepts a person needs to share before they can successfully communicate the </span></span><a href=\"https://www.lesswrong.com/tag/object-level-and-meta-level\"><span class=\"by_Sp5wM4aRAhNERd4oY\">object level</span></a><span><span class=\"by_Sp5wM4aRAhNERd4oY\"> point. This can be thought </span><span class=\"by_qgdGA4ZEyW7zNdK84\">of </span><span class=\"by_Sp5wM4aRAhNERd4oY\">as the missing foundation or building block concepts needed</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to </span><span class=\"by_Sp5wM4aRAhNERd4oY\">think clearly about a specific thing.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">In </span><a href=\"https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Expecting Short Inferential Distances</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, Eliezer Yudkowsky posits that humans systematically underestimate inferential distances.</span></p><blockquote><p><i><span><span class=\"by_qgdGA4ZEyW7zNdK84\">And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just </span><span class=\"by_XgYW5s8njaYrtyP7q\">a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">few words, I’ve got some sad news for you&nbsp;.&nbsp;.&nbsp;. – </span></span></i><a href=\"https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Expecting Short Inferential Distances</span></a></p></blockquote><h2 id=\"Example__Evidence_for_Evolution\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Example: Evidence for Evolution</span></h2><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Explaining</span><span class=\"by_XgYW5s8njaYrtyP7q\"> the </span></span><a href=\"https://lessestwrong.com/tag/evidence\"><span class=\"by_XgYW5s8njaYrtyP7q\">evidence</span></a><span class=\"by_XgYW5s8njaYrtyP7q\"> for the theory of </span><a href=\"https://lessestwrong.com/tag/evolution\"><span class=\"by_XgYW5s8njaYrtyP7q\">evolution</span></a><span class=\"by_XgYW5s8njaYrtyP7q\"> to a physicist would be easy; even if the physicist didn't already know about evolution, they would understand the concepts of evidence, </span><a href=\"https://lessestwrong.com/tag/occam-s-razor\"><span><span class=\"by_XgYW5s8njaYrtyP7q\">Occam's </span><span class=\"by_qf77EiaoMw7tH3GSr\">razor</span></span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">,</span><span class=\"by_XgYW5s8njaYrtyP7q\"> naturalistic explanations, and the general orderly nature of the universe. Explaining the evidence for the theory of evolution to someone without a science background would be much harder. Before even mentioning the specific evidence for evolution, you would have to explain the concept of evidence, why some </span><span class=\"by_qf77EiaoMw7tH3GSr\">kinds of </span><span class=\"by_XgYW5s8njaYrtyP7q\">evidence </span><span class=\"by_qf77EiaoMw7tH3GSr\">are</span><span class=\"by_XgYW5s8njaYrtyP7q\"> more valuable than others, what does and doesn't count as evidence, and so on. This would be unlikely to work during a short conversation.</span></span></p><p><span class=\"by_XgYW5s8njaYrtyP7q\">There is a short inferential distance between you and the physicist; there is a very long inferential distance between you and the person without any science background. Many members of Less Wrong believe that expecting short inferential distances is a classic error. It is also a very difficult problem to solve, since most people will feel </span><a href=\"https://wiki.lesswrong.com/wiki/offence\"><span class=\"by_XgYW5s8njaYrtyP7q\">offended</span></a><span class=\"by_XgYW5s8njaYrtyP7q\"> if you explicitly say that there is too great an inferential distance between you to explain a theory properly. Some people have attempted to explain this through </span><a href=\"https://lessestwrong.com/tag/evolutionary-psychology\"><span><span class=\"by_XgYW5s8njaYrtyP7q\">evolutionary </span><span class=\"by_qf77EiaoMw7tH3GSr\">psychology</span></span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">:</span><span class=\"by_XgYW5s8njaYrtyP7q\"> in the ancestral environment, there was minimal difference in knowledge between people, and therefore no need to worry about inferential distances.</span></span></p><h2 id=\"External_Links\"><span><span class=\"by_LoykQRMTxJFxwwdPy\">External </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Links</span></span></h2><ul><li><a href=\"http://everydayutilitarian.com/essays/why-its-hard-to-explain-things-inferential-distance/\"><span><span class=\"by_efbmXA2tiKgY9qRTD\">Why It's Hard to Explain Things:</span><span class=\"by_LoykQRMTxJFxwwdPy\"> Inferential Distance</span></span></a><span class=\"by_LoykQRMTxJFxwwdPy\"> by Peter Hurford</span></li><li><a href=\"https://jkorpela.fi/wiio.html\"><span class=\"by_LoykQRMTxJFxwwdPy\">How all human communication fails, except by accident, or a commentary of Wiio's laws</span></a></li></ul><h2 id=\"See_Also\"><span><span class=\"by_9c2mQkLQq6gQSksMs\">See </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/general-knowledge\"><span class=\"by_9c2mQkLQq6gQSksMs\">General knowledge</span></a></li><li><a href=\"https://lessestwrong.com/tag/modesty-argument\"><span class=\"by_9c2mQkLQq6gQSksMs\">Modesty argument</span></a></li><li><a href=\"https://lessestwrong.com/tag/illusion-of-transparency\"><span class=\"by_9c2mQkLQq6gQSksMs\">Illusion of transparency</span></a></li><li><a href=\"https://lessestwrong.com/tag/absurdity-heuristic\"><span class=\"by_LoykQRMTxJFxwwdPy\">Absurdity heuristic</span></a></li><li><a href=\"https://www.lesswrong.com/tag/common-knowledge\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Common Knowledge</span></a></li></ul>",
      "sections": [
        {
          "title": "Example: Evidence for Evolution",
          "anchor": "Example__Evidence_for_Evolution",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 46,
    "description": {
      "markdown": "**Inferential Distance** between two people with respect to an item of knowledge is the amount of steps or concepts a person needs to share before they can successfully communicate the [object level](https://www.lesswrong.com/tag/object-level-and-meta-level) point. This can be thought of as the missing foundation or building block concepts needed to think clearly about a specific thing.\n\nIn [Expecting Short Inferential Distances](https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances), Eliezer Yudkowsky posits that humans systematically underestimate inferential distances.\n\n> *And if you think you can explain the concept of “systematically underestimated inferential distances” briefly, in just a few words, I’ve got some sad news for you . . . –* [Expecting Short Inferential Distances](https://www.lessestwrong.com/posts/HLqWn5LASfhhArZ7w/expecting-short-inferential-distances)\n\nExample: Evidence for Evolution\n-------------------------------\n\nExplaining the [evidence](https://lessestwrong.com/tag/evidence) for the theory of [evolution](https://lessestwrong.com/tag/evolution) to a physicist would be easy; even if the physicist didn't already know about evolution, they would understand the concepts of evidence, [Occam's razor](https://lessestwrong.com/tag/occam-s-razor), naturalistic explanations, and the general orderly nature of the universe. Explaining the evidence for the theory of evolution to someone without a science background would be much harder. Before even mentioning the specific evidence for evolution, you would have to explain the concept of evidence, why some kinds of evidence are more valuable than others, what does and doesn't count as evidence, and so on. This would be unlikely to work during a short conversation.\n\nThere is a short inferential distance between you and the physicist; there is a very long inferential distance between you and the person without any science background. Many members of Less Wrong believe that expecting short inferential distances is a classic error. It is also a very difficult problem to solve, since most people will feel [offended](https://wiki.lesswrong.com/wiki/offence) if you explicitly say that there is too great an inferential distance between you to explain a theory properly. Some people have attempted to explain this through [evolutionary psychology](https://lessestwrong.com/tag/evolutionary-psychology): in the ancestral environment, there was minimal difference in knowledge between people, and therefore no need to worry about inferential distances.\n\nExternal Links\n--------------\n\n*   [Why It's Hard to Explain Things: Inferential Distance](http://everydayutilitarian.com/essays/why-its-hard-to-explain-things-inferential-distance/) by Peter Hurford\n*   [How all human communication fails, except by accident, or a commentary of Wiio's laws](https://jkorpela.fi/wiio.html)\n\nSee Also\n--------\n\n*   [General knowledge](https://lessestwrong.com/tag/general-knowledge)\n*   [Modesty argument](https://lessestwrong.com/tag/modesty-argument)\n*   [Illusion of transparency](https://lessestwrong.com/tag/illusion-of-transparency)\n*   [Absurdity heuristic](https://lessestwrong.com/tag/absurdity-heuristic)\n*   [Common Knowledge](https://www.lesswrong.com/tag/common-knowledge)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "bTeiZr6YAEaSPQTC8",
    "name": "Solomonoff Induction",
    "core": null,
    "slug": "solomonoff-induction",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Solomonoff induction </span></strong><span class=\"by_HoGziwmhpMGqGeWZy\">is an inference system defined by </span><a href=\"https://en.wikipedia.org/wiki/Ray_Solomonoff\"><span class=\"by_mPipmBTniuABY5PQy\">Ray Solomonoff</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> that will learn to correctly predict any computable sequence with only the absolute minimum amount of data. This system, in a certain sense, is the perfect universal prediction algorithm.&nbsp;</span></p><p><span><span class=\"by_mPipmBTniuABY5PQy\">To summarize it very informally, Solomonoff induction works </span><span class=\"by_qf77EiaoMw7tH3GSr\">by:</span></span></p><ul><li><span><span class=\"by_qf77EiaoMw7tH3GSr\">Starting</span><span class=\"by_mPipmBTniuABY5PQy\"> with all possible hypotheses</span><span class=\"by_qf77EiaoMw7tH3GSr\"> (sequences)</span><span class=\"by_mPipmBTniuABY5PQy\"> as represented by computer programs </span><span class=\"by_qf77EiaoMw7tH3GSr\">(that generate those sequences),</span><span class=\"by_mPipmBTniuABY5PQy\"> weighted by their simplicity </span><span class=\"by_qf77EiaoMw7tH3GSr\">(2</span></span><sup><span class=\"by_mPipmBTniuABY5PQy\">-</span></sup><strong><sup><span class=\"by_mPipmBTniuABY5PQy\">n</span></sup></strong><span class=\"by_mPipmBTniuABY5PQy\">, where </span><strong><span class=\"by_mPipmBTniuABY5PQy\">n</span></strong><span><span class=\"by_mPipmBTniuABY5PQy\"> is the program length)</span><span class=\"by_qf77EiaoMw7tH3GSr\">;</span></span></li><li><span><span class=\"by_qf77EiaoMw7tH3GSr\">Discarding</span><span class=\"by_mPipmBTniuABY5PQy\"> those hypotheses that are inconsistent with the data.</span></span></li></ul><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">Weighting hypotheses</span><span class=\"by_mPipmBTniuABY5PQy\"> by simplicity, the system automatically incorporates a form of </span></span><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><span><span class=\"by_mPipmBTniuABY5PQy\">Occam's </span><span class=\"by_qf77EiaoMw7tH3GSr\">razor</span></span></a><span><span class=\"by_qf77EiaoMw7tH3GSr\">,</span><span class=\"by_mPipmBTniuABY5PQy\"> which is why it has been playfully referred to as </span></span><i><span class=\"by_mPipmBTniuABY5PQy\">Solomonoff's lightsaber</span></i><span class=\"by_mPipmBTniuABY5PQy\">.</span></p><p><span class=\"by_HSANMQBsHiGrZzwTB\">Solomonoff induction gets off the ground with a solution to the \"problem of the priors\". Suppose that you stand before a universal </span><a href=\"http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_Turing_machine\"><span class=\"by_HSANMQBsHiGrZzwTB\">prefix Turing machine</span></a><span class=\"by_HSANMQBsHiGrZzwTB\"> </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">. You are interested in a certain finite output string </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">. In particular, you want to know the probability that </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> will produce the output </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\"> given a random input tape. This probability is the </span><strong><span class=\"by_HSANMQBsHiGrZzwTB\">Solomonoff </span></strong><i><strong><span class=\"by_HSANMQBsHiGrZzwTB\">a priori</span></strong></i><strong><span class=\"by_HSANMQBsHiGrZzwTB\"> probability</span></strong><span class=\"by_HSANMQBsHiGrZzwTB\"> of </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">.</span></p><p><span class=\"by_HSANMQBsHiGrZzwTB\">More precisely, suppose that a particular infinite input string </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\"> is about to be fed into </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">. However, you know nothing about </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span><span class=\"by_HSANMQBsHiGrZzwTB\"> other than that each term of the string is either 0 or </span><span class=\"by_HoGziwmhpMGqGeWZy\">1.</span><span class=\"by_HSANMQBsHiGrZzwTB\"> As far as your state of knowledge is concerned, the </span></span><i><span class=\"by_HSANMQBsHiGrZzwTB\">i</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">th digit of </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span><span class=\"by_HSANMQBsHiGrZzwTB\"> is as likely to be 0 as it is to be </span><span class=\"by_HoGziwmhpMGqGeWZy\">1,</span><span class=\"by_HSANMQBsHiGrZzwTB\"> for all </span></span><i><span class=\"by_HSANMQBsHiGrZzwTB\">i</span></i><span><span class=\"by_HSANMQBsHiGrZzwTB\"> = 1, 2, </span><span class=\"by_HoGziwmhpMGqGeWZy\">….</span><span class=\"by_HSANMQBsHiGrZzwTB\"> You want to find the </span></span><i><span class=\"by_HSANMQBsHiGrZzwTB\">a priori</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> probability </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">m</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">(</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">) of the following proposition:</span></p><p><span class=\"by_HSANMQBsHiGrZzwTB\">(*) If </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> takes in </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\"> as input, then </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> will produce output </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\"> and then halt.</span></p><p><span class=\"by_HSANMQBsHiGrZzwTB\">Unfortunately, computing the exact value of </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">m</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">(</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">) would require solving the halting problem, which is undecidable. Nonetheless, it is easy to derive an expression for </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">m</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">(</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">). If </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> halts on an infinite input string </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">, then </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> must read only a finite initial segment of </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">, after which </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> immediately halts. We call a finite string </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> a </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">self-delimiting program</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> if and only if there exists an infinite input string </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> beginning with </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> such that </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> halts on </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> immediately after reading to the end of </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span><span class=\"by_HSANMQBsHiGrZzwTB\">. The set </span><span class=\"by_HoGziwmhpMGqGeWZy\">𝒫</span><span class=\"by_HSANMQBsHiGrZzwTB\"> of self-delimiting programs is the </span></span><i><span class=\"by_HSANMQBsHiGrZzwTB\">prefix code</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> for </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span><span class=\"by_HSANMQBsHiGrZzwTB\">. It is the determination of the elements of </span><span class=\"by_HoGziwmhpMGqGeWZy\">𝒫</span><span class=\"by_HSANMQBsHiGrZzwTB\"> that requires a solution to the halting problem.</span></span></p><p><span class=\"by_HSANMQBsHiGrZzwTB\">Given </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span><span class=\"by_HSANMQBsHiGrZzwTB\"> ∈ 𝒫, we write </span><span class=\"by_HoGziwmhpMGqGeWZy\">\"prog</span><span class=\"by_HSANMQBsHiGrZzwTB\"> (</span></span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">) = </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">\" to express the proposition that </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\"> begins with </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">, and we write \"</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">(</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">) = </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">\" to express the proposition that </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> produces output </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">, and then halts, when fed any input beginning with </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">. Proposition (*) is then equivalent to the exclusive disjunction</span></p><p><br><span class=\"by_HSANMQBsHiGrZzwTB\">⋁</span><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\"> ∈ 𝒫: </span></sub><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">(</span></sub><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">) = </span></sub><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">(prog (</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">) = </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">).</span><br><span class=\"by_HSANMQBsHiGrZzwTB\">Since </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span><span class=\"by_HSANMQBsHiGrZzwTB\"> was chosen at random from </span><span class=\"by_HoGziwmhpMGqGeWZy\">{0,</span><span class=\"by_HSANMQBsHiGrZzwTB\"> 1}</span></span><i><sup><span class=\"by_HSANMQBsHiGrZzwTB\">ω</span></sup></i><span class=\"by_HSANMQBsHiGrZzwTB\">, we take the probability of prog (</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">x</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">) = </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> to be 2</span><sup><span class=\"by_HSANMQBsHiGrZzwTB\"> − ℓ(</span></sup><i><sup><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></sup></i><sup><span class=\"by_HSANMQBsHiGrZzwTB\">)</span></sup><span><span class=\"by_HSANMQBsHiGrZzwTB\">, where </span><span class=\"by_HoGziwmhpMGqGeWZy\">ℓ(</span></span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">) is the length of </span><i><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></i><span class=\"by_HSANMQBsHiGrZzwTB\"> as a bit string. Hence, the probability of (*) is</span></p><p><br><i><span class=\"by_HSANMQBsHiGrZzwTB\">m</span></i><span class=\"by_HSANMQBsHiGrZzwTB\">(</span><i><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">) := ∑</span><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\"> ∈ 𝒫: </span></sub><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">U</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">(</span></sub><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">) = </span></sub><i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">y</span></sub></i><sub><span class=\"by_HSANMQBsHiGrZzwTB\">0</span></sub><span class=\"by_HSANMQBsHiGrZzwTB\">2</span><sup><span class=\"by_HSANMQBsHiGrZzwTB\"> − ℓ(</span></sup><i><sup><span class=\"by_HSANMQBsHiGrZzwTB\">p</span></sup></i><sup><span class=\"by_HSANMQBsHiGrZzwTB\">)</span></sup><span class=\"by_HSANMQBsHiGrZzwTB\">.</span><br><span class=\"by_HoGziwmhpMGqGeWZy\">&nbsp;</span></p><h2 id=\"See_also\"><span class=\"by_qf77EiaoMw7tH3GSr\">See also</span></h2><ul><li><a href=\"https://www.lesswrong.com/tag/kolmogorov-complexity\"><span class=\"by_qf77EiaoMw7tH3GSr\">Kolmogorov complexity</span></a></li><li><a href=\"https://www.lesswrong.com/tag/aixi\"><span class=\"by_qf77EiaoMw7tH3GSr\">AIXI</span></a></li><li><a href=\"https://www.lesswrong.com/tag/occam-s-razor\"><span class=\"by_qf77EiaoMw7tH3GSr\">Occam's razor</span></a></li></ul><h2 id=\"References\"><span class=\"by_qf77EiaoMw7tH3GSr\">References</span></h2><ul><li><a href=\"http://www.scholarpedia.org/article/Algorithmic_probability\"><span class=\"by_qf77EiaoMw7tH3GSr\">Algorithmic probability</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> on Scholarpedia</span></li></ul>",
      "sections": [
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "title": "References",
          "anchor": "References",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 43,
    "description": {
      "markdown": "**Solomonoff induction** is an inference system defined by [Ray Solomonoff](https://en.wikipedia.org/wiki/Ray_Solomonoff) that will learn to correctly predict any computable sequence with only the absolute minimum amount of data. This system, in a certain sense, is the perfect universal prediction algorithm. \n\nTo summarize it very informally, Solomonoff induction works by:\n\n*   Starting with all possible hypotheses (sequences) as represented by computer programs (that generate those sequences), weighted by their simplicity (2^-^**^n^**, where **n** is the program length);\n*   Discarding those hypotheses that are inconsistent with the data.\n\nWeighting hypotheses by simplicity, the system automatically incorporates a form of [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor), which is why it has been playfully referred to as *Solomonoff's lightsaber*.\n\nSolomonoff induction gets off the ground with a solution to the \"problem of the priors\". Suppose that you stand before a universal [prefix Turing machine](http://www.scholarpedia.org/article/Algorithmic_complexity#Prefix_Turing_machine) *U*. You are interested in a certain finite output string *y*~0~. In particular, you want to know the probability that *U* will produce the output *y*~0~ given a random input tape. This probability is the **Solomonoff** ***a priori*** **probability** of *y*~0~.\n\nMore precisely, suppose that a particular infinite input string *x*~0~ is about to be fed into *U*. However, you know nothing about *x*~0~ other than that each term of the string is either 0 or 1. As far as your state of knowledge is concerned, the *i*th digit of *x*~0~ is as likely to be 0 as it is to be 1, for all *i* = 1, 2, …. You want to find the *a priori* probability *m*(*y*~0~) of the following proposition:\n\n(*) If *U* takes in *x*~0~ as input, then *U* will produce output *y*~0~ and then halt.\n\nUnfortunately, computing the exact value of *m*(*y*~0~) would require solving the halting problem, which is undecidable. Nonetheless, it is easy to derive an expression for *m*(*y*~0~). If *U* halts on an infinite input string *x*, then *U* must read only a finite initial segment of *x*, after which *U* immediately halts. We call a finite string *p* a *self-delimiting program* if and only if there exists an infinite input string *x* beginning with *p* such that *U* halts on *x* immediately after reading to the end of *p*. The set 𝒫 of self-delimiting programs is the *prefix code* for *U*. It is the determination of the elements of 𝒫 that requires a solution to the halting problem.\n\nGiven *p* ∈ 𝒫, we write \"prog (*x*~0~) = *p*\" to express the proposition that *x*~0~ begins with *p*, and we write \"*U*(*p*) = *y*~0~\" to express the proposition that *U* produces output *y*~0~, and then halts, when fed any input beginning with *p*. Proposition (*) is then equivalent to the exclusive disjunction\n\n  \n⋁*~p~*~ ∈ 𝒫: ~*~U~*~(~*~p~*~) = ~*~y~*~0~(prog (*x*~0~) = *p*).  \nSince *x*~0~ was chosen at random from {0, 1}*^ω^*, we take the probability of prog (*x*~0~) = *p* to be 2^ − ℓ(^*^p^*^)^, where ℓ(*p*) is the length of *p* as a bit string. Hence, the probability of (*) is\n\n  \n*m*(*y*~0~) := ∑*~p~*~ ∈ 𝒫: ~*~U~*~(~*~p~*~) = ~*~y~*~0~2^ − ℓ(^*^p^*^)^.  \n \n\nSee also\n--------\n\n*   [Kolmogorov complexity](https://www.lesswrong.com/tag/kolmogorov-complexity)\n*   [AIXI](https://www.lesswrong.com/tag/aixi)\n*   [Occam's razor](https://www.lesswrong.com/tag/occam-s-razor)\n\nReferences\n----------\n\n*   [Algorithmic probability](http://www.scholarpedia.org/article/Algorithmic_probability) on Scholarpedia"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "5BvRW4FxdD8DFhiew",
    "name": "Pica",
    "core": null,
    "slug": "pica",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">Pica</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> is the name of a medical condition where people with iron deficiency eat ice cubes. There is no iron in ice cubes, but iron and crunchiness sort of go together. By analogy, </span><strong><span class=\"by_nLbwLhBaQeG6tCNDN\">pica</span></strong><span class=\"by_nLbwLhBaQeG6tCNDN\"> is also when people are missing something, and respond by doing something which doesn't provide it, or doesn't provide much of it, like watching sitcoms because they're lonely or playing Minecraft because they feel unproductive.</span></p><p><a href=\"https://www.lesswrong.com/posts/L6Ktf952cwdMJnzWm/motive-ambiguity?commentId=QLS75v2wdDHpo9CX3\"><span class=\"by_sKAL2jzfkYkDbQmx9\">AllAmericanBreakfast suggests</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> that pica is like </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Goodhart's Law</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, with the added </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_sKAL2jzfkYkDbQmx9\">failure</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> that the metric being maximized isn't even clearly related to the problem you're trying to solve, giving two examples:</span></p><ul><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Evaluate your startup by the sheer effort you're putting in? That's Goodhart's Law. Evaluate it by </span><a href=\"https://www.youtube.com/watch?v=zbKaPN-0NcM&amp;ab_channel=LeslieKnopeRocks\"><span class=\"by_sKAL2jzfkYkDbQmx9\">how cool the office looks</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">? That's pica.</span></li><li><span class=\"by_sKAL2jzfkYkDbQmx9\">Evaluate your relationship by the sheer amount of physical affection? That's Goodhart's Law. Evaluate it by how much misery you put each other through \"for love?\" That's pica</span></li></ul><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages:</span></strong><span class=\"by_sKAL2jzfkYkDbQmx9\"> </span><a href=\"https://www.lesswrong.com/tag/akrasia\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Akrasia</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/goodhart-s-law\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Goodhart's Law</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 6,
    "description": {
      "markdown": "**Pica** is the name of a medical condition where people with iron deficiency eat ice cubes. There is no iron in ice cubes, but iron and crunchiness sort of go together. By analogy, **pica** is also when people are missing something, and respond by doing something which doesn't provide it, or doesn't provide much of it, like watching sitcoms because they're lonely or playing Minecraft because they feel unproductive.\n\n[AllAmericanBreakfast suggests](https://www.lesswrong.com/posts/L6Ktf952cwdMJnzWm/motive-ambiguity?commentId=QLS75v2wdDHpo9CX3) that pica is like [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law), with the added [failure](https://www.lesswrong.com/tag/goodhart-s-law) that the metric being maximized isn't even clearly related to the problem you're trying to solve, giving two examples:\n\n*   Evaluate your startup by the sheer effort you're putting in? That's Goodhart's Law. Evaluate it by [how cool the office looks](https://www.youtube.com/watch?v=zbKaPN-0NcM&ab_channel=LeslieKnopeRocks)? That's pica.\n*   Evaluate your relationship by the sheer amount of physical affection? That's Goodhart's Law. Evaluate it by how much misery you put each other through \"for love?\" That's pica\n\n**Related Pages:** [Akrasia](https://www.lesswrong.com/tag/akrasia), [Goodhart's Law](https://www.lesswrong.com/tag/goodhart-s-law)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "eamWQNQ2dPYWEwhqr",
    "name": "Goal Factoring",
    "core": null,
    "slug": "goal-factoring",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Goal Factoring </span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\">is a rationality technique for planning which proceeds by first identifying the underlying goals motivating one or more behaviors and then searching for alternative sets of behaviors that better accomplish the goals.&nbsp;</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">For example, someone might refactor the two behaviors of </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">going to the gym </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">and </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">browsing Facebook </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">into the single behavior of </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">play tennis with my friend </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">which more efficiently and effectively accomplishes the underlying goals of </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">have social interaction </span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">and </span><i><span class=\"by_qgdGA4ZEyW7zNdK84\">get exercise</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 13,
    "description": {
      "markdown": "**Goal Factoring** is a rationality technique for planning which proceeds by first identifying the underlying goals motivating one or more behaviors and then searching for alternative sets of behaviors that better accomplish the goals. \n\nFor example, someone might refactor the two behaviors of *going to the gym* and *browsing Facebook* into the single behavior of *play tennis with my friend* which more efficiently and effectively accomplishes the underlying goals of *have social interaction* and *get exercise*."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "stnsBEmuGpnSfQ5vj",
    "name": "Sunk-Cost Fallacy",
    "core": null,
    "slug": "sunk-cost-fallacy",
    "tableOfContents": {
      "html": "<p><span class=\"by_HoGziwmhpMGqGeWZy\">The </span><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Sunk Cost Fallacy</span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> is the tendency to consider costs that have already been paid and cannot be reclaimed when deciding whether to continue</span><span class=\"by_LoykQRMTxJFxwwdPy\"> a </span><span class=\"by_HoGziwmhpMGqGeWZy\">project. Thinking you have to keep going because </span><span class=\"by_qgdGA4ZEyW7zNdK84\">you'</span><span class=\"by_HoGziwmhpMGqGeWZy\">ve already put in so much. In reality, it is usually much more important to consider whether the benefits of finishing the project are worth more than the </span></span><i><span class=\"by_HoGziwmhpMGqGeWZy\">remaining</span></i><span class=\"by_HoGziwmhpMGqGeWZy\"> costs.</span></p><h2 id=\"External_Links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External Links</span></h2><ul><li><a href=\"http://www.gwern.net/Sunk%20cost\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Are sunk costs fallacies?</span></a></li></ul><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/status-quo-bias\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Status quo bias</span></a></li><li><a href=\"https://lessestwrong.com/tag/loss-aversion\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Loss aversion</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span><a href=\"https://lessestwrong.com/tag/prospect-theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prospect theory</span></a></li><li><a href=\"https://lessestwrong.com/tag/cached-thought\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Cached thought</span></a></li><li><a href=\"https://lessestwrong.com/tag/narrative-fallacy\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Narrative fallacy</span></a></li></ul>",
      "sections": [
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 3
    },
    "postCount": 9,
    "description": {
      "markdown": "The **Sunk Cost Fallacy** is the tendency to consider costs that have already been paid and cannot be reclaimed when deciding whether to continue a project. Thinking you have to keep going because you've already put in so much. In reality, it is usually much more important to consider whether the benefits of finishing the project are worth more than the *remaining* costs.\n\nExternal Links\n--------------\n\n*   [Are sunk costs fallacies?](http://www.gwern.net/Sunk%20cost)\n\nSee Also\n--------\n\n*   [Status quo bias](https://lessestwrong.com/tag/status-quo-bias)\n*   [Loss aversion](https://lessestwrong.com/tag/loss-aversion), [Prospect theory](https://lessestwrong.com/tag/prospect-theory)\n*   [Cached thought](https://lessestwrong.com/tag/cached-thought)\n*   [Narrative fallacy](https://lessestwrong.com/tag/narrative-fallacy)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "Zwc2JcT5az4e5YpJy",
    "name": "Rationality Quotes",
    "core": null,
    "slug": "rationality-quotes",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_HoGziwmhpMGqGeWZy\">Rationality Quotes </span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\">was a series of posts and threads</span><span class=\"by_qf77EiaoMw7tH3GSr\"> on</span><span class=\"by_9c2mQkLQq6gQSksMs\"> </span><span class=\"by_HoGziwmhpMGqGeWZy\">Overcoming Bias and </span><span class=\"by_9c2mQkLQq6gQSksMs\">Less </span><span class=\"by_HoGziwmhpMGqGeWZy\">Wrong, where users would post</span><span class=\"by_qf77EiaoMw7tH3GSr\"> quotes </span><span class=\"by_HoGziwmhpMGqGeWZy\">that had some connection to rationality or other popular </span><span class=\"by_nmk3nLpQE89dMRzzN\">Less Wrong </span><span class=\"by_HoGziwmhpMGqGeWZy\">discussion topics.</span></span></p><p><span><span class=\"by_HoGziwmhpMGqGeWZy\">It started as a series of posts by Eliezer Yudkowsky on Overcoming Bias, posting from his files of collected </span><span class=\"by_nmk3nLpQE89dMRzzN\">quotes </span><span class=\"by_HoGziwmhpMGqGeWZy\">on days when he </span><span class=\"by_qgdGA4ZEyW7zNdK84\">didn'</span><span class=\"by_HoGziwmhpMGqGeWZy\">t have a normal post ready. When</span><span class=\"by_nmk3nLpQE89dMRzzN\"> </span><span class=\"by_qf77EiaoMw7tH3GSr\">Less Wrong </span><span class=\"by_HoGziwmhpMGqGeWZy\">was created, there was a new</span><span class=\"by_qf77EiaoMw7tH3GSr\"> Rationality Quotes </span><span class=\"by_HoGziwmhpMGqGeWZy\">thread each month, where all users could post quotes.</span></span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">Rationality Quotes threads got fewer and fewer comments during the </span><a href=\"https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong\"><span class=\"by_HoGziwmhpMGqGeWZy\">2015-2017 Less Wrong decline</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, and the last thread was for April-June 2017.</span></p><p><span class=\"by_HoGziwmhpMGqGeWZy\">See also: </span><a href=\"https://www.lesswrong.com/tag/open-thread\"><span class=\"by_HoGziwmhpMGqGeWZy\">Open Threads</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/history-of-rationality\"><span class=\"by_HoGziwmhpMGqGeWZy\">History of Rationality</span></a><span class=\"by_HoGziwmhpMGqGeWZy\">, </span><a href=\"https://www.lesswrong.com/tag/humor\"><span class=\"by_HoGziwmhpMGqGeWZy\">Humor</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 134,
    "description": {
      "markdown": "**Rationality Quotes** was a series of posts and threads on Overcoming Bias and Less Wrong, where users would post quotes that had some connection to rationality or other popular Less Wrong discussion topics.\n\nIt started as a series of posts by Eliezer Yudkowsky on Overcoming Bias, posting from his files of collected quotes on days when he didn't have a normal post ready. When Less Wrong was created, there was a new Rationality Quotes thread each month, where all users could post quotes.\n\nRationality Quotes threads got fewer and fewer comments during the [2015-2017 Less Wrong decline](https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong), and the last thread was for April-June 2017.\n\nSee also: [Open Threads](https://www.lesswrong.com/tag/open-thread), [History of Rationality](https://www.lesswrong.com/tag/history-of-rationality), [Humor](https://www.lesswrong.com/tag/humor)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "R6dqPii4cyNpuecLt",
    "name": "Prediction Markets",
    "core": null,
    "slug": "prediction-markets",
    "tableOfContents": {
      "html": "<p><strong><span><span class=\"by_RyiDJDCG6R7xyAXzp\">Prediction </span><span class=\"by_qgdGA4ZEyW7zNdK84\">markets</span></span></strong><span><span class=\"by_HoGziwmhpMGqGeWZy\"> are </span><span class=\"by_qgdGA4ZEyW7zNdK84\">speculative </span><span class=\"by_RyiDJDCG6R7xyAXzp\">markets </span><span class=\"by_qgdGA4ZEyW7zNdK84\">created for the purpose of making predictions. Assets are created whose final cash value is tied to a particular event or parameter. The current market prices</span><span class=\"by_HoGziwmhpMGqGeWZy\"> can </span><span class=\"by_qgdGA4ZEyW7zNdK84\">then</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> be interpreted as </span><span class=\"by_qgdGA4ZEyW7zNdK84\">predictions of </span><span class=\"by_RyiDJDCG6R7xyAXzp\">the probability </span><span class=\"by_qgdGA4ZEyW7zNdK84\">of the</span><span class=\"by_HoGziwmhpMGqGeWZy\"> event </span><span class=\"by_qgdGA4ZEyW7zNdK84\">or</span><span class=\"by_RyiDJDCG6R7xyAXzp\"> the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">expected value of the parameter. Prediction markets are thus structured as betting exchanges, without any risk for the bookmaker. </span></span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> was the first to run a corporate prediction market - at Project Xanadu -, and has made several contributions</span><span class=\"by_2YpRin5m5vBJu8Tg9\"> to </span><span class=\"by_woC2b5rav5sGrAo3E\">the </span><span class=\"by_qgdGA4ZEyW7zNdK84\">field such as: conditional predictions, accuracy issues and market and media manipulation.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">People who buy low and sell high are rewarded for improving the market prediction, while those who buy high and sell low are punished for degrading the market prediction. Evidence so far suggests that prediction markets are at least as accurate as other institutions predicting the same events with a similar pool of participants.</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Predictions markets have been used by organizations such as Google, General Electric, and Microsoft; several online and commercial prediction markets are also in operation. Historically, prediction markets have often been used to predict election outcomes.</span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/forecasting-and-prediction\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prediction</span></a></li><li><a href=\"https://lessestwrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Economic consequences of AI and whole brain emulation</span></a></li><li><a href=\"https://lessestwrong.com/tag/group-rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Group rationality</span></a></li><li><a href=\"https://lessestwrong.com/tag/making-beliefs-pay-rent\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Making beliefs pay rent</span></a></li><li><a href=\"https://www.lesswrong.com/tag/quri\"><span class=\"by_sKAL2jzfkYkDbQmx9\">QURI</span></a></li></ul><h2 id=\"External_Posts\"><span><span class=\"by_sKAL2jzfkYkDbQmx9\">External</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> Posts</span></span></h2><ul><li><a href=\"http://www.overcomingbias.com/2006/11/first_known_bus.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">A 1990 Corporate Prediction Market</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://lessestwrong.com/tag/robin-hanson\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Robin Hanson</span></a></li><li><a href=\"http://www.overcomingbias.com/2006/12/leamers_1986_id.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Leamer's 1986 Idea Futures Proposal</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Robin Hanson</span></li><li><a href=\"http://www.overcomingbias.com/2006/12/should_predicti.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Should Prediction Markets be Charities?</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Peter McCluskey</span></li><li><a href=\"http://www.overcomingbias.com/2006/12/the_future_of_o_1.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">The Future of Oil Prices 2: Option Probabilities</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by </span><a href=\"https://en.wikipedia.org/wiki/Hal_Finney_(cipherpunk)\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Hal Finney</span></a></li><li><a href=\"http://www.overcomingbias.com/2009/09/prediction-markets-as-collective-inteligence.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Prediction Markets As Collective Intelligence</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Robin Hanson</span></li><li><a href=\"http://www.overcomingbias.com/2011/11/conditional-close-election-markets.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Fixing Election Markets</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Robin Hanson</span></li><li><a href=\"http://www.gwern.net/Prediction%20markets\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Prediction Markets</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\"> at gwern.net</span></li><li><a href=\"http://hanson.gmu.edu/ideafutures.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Idea Futures (a.k.a. Prediction Markets)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> by Robin Hanson</span></li></ul><h3 id=\"External_Links\"><span class=\"by_sKAL2jzfkYkDbQmx9\">External Links</span></h3><ul><li><a href=\"http://dl.dropbox.com/u/5317066/2011-graefe.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Comparing face-to-face meetings, nominal groups, Delphi and prediction markets on an estimation task</span></a></li><li><a href=\"http://videolectures.net/uai08_hanson_cpm/\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Video of Robin Hanson's Combinatorial Prediction Markets lecture at the Uncertainty in Artificial Intelligence conference in Helsinki, 2008</span></a></li></ul>",
      "sections": [
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "External Posts",
          "anchor": "External_Posts",
          "level": 1
        },
        {
          "title": "External Links",
          "anchor": "External_Links",
          "level": 2
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 4
    },
    "postCount": 69,
    "description": {
      "markdown": "**Prediction markets** are speculative markets created for the purpose of making predictions. Assets are created whose final cash value is tied to a particular event or parameter. The current market prices can then be interpreted as predictions of the probability of the event or the expected value of the parameter. Prediction markets are thus structured as betting exchanges, without any risk for the bookmaker. [Robin Hanson](https://lessestwrong.com/tag/robin-hanson) was the first to run a corporate prediction market - at Project Xanadu -, and has made several contributions to the field such as: conditional predictions, accuracy issues and market and media manipulation.\n\nPeople who buy low and sell high are rewarded for improving the market prediction, while those who buy high and sell low are punished for degrading the market prediction. Evidence so far suggests that prediction markets are at least as accurate as other institutions predicting the same events with a similar pool of participants.\n\nPredictions markets have been used by organizations such as Google, General Electric, and Microsoft; several online and commercial prediction markets are also in operation. Historically, prediction markets have often been used to predict election outcomes.\n\nSee Also\n--------\n\n*   [Prediction](https://lessestwrong.com/tag/forecasting-and-prediction)\n*   [Economic consequences of AI and whole brain emulation](https://lessestwrong.com/tag/economic-consequences-of-ai-and-whole-brain-emulation)\n*   [Group rationality](https://lessestwrong.com/tag/group-rationality)\n*   [Making beliefs pay rent](https://lessestwrong.com/tag/making-beliefs-pay-rent)\n*   [QURI](https://www.lesswrong.com/tag/quri)\n\nExternal Posts\n--------------\n\n*   [A 1990 Corporate Prediction Market](http://www.overcomingbias.com/2006/11/first_known_bus.html) by [Robin Hanson](https://lessestwrong.com/tag/robin-hanson)\n*   [Leamer's 1986 Idea Futures Proposal](http://www.overcomingbias.com/2006/12/leamers_1986_id.html) by Robin Hanson\n*   [Should Prediction Markets be Charities?](http://www.overcomingbias.com/2006/12/should_predicti.html) by Peter McCluskey\n*   [The Future of Oil Prices 2: Option Probabilities](http://www.overcomingbias.com/2006/12/the_future_of_o_1.html) by [Hal Finney](https://en.wikipedia.org/wiki/Hal_Finney_(cipherpunk))\n*   [Prediction Markets As Collective Intelligence](http://www.overcomingbias.com/2009/09/prediction-markets-as-collective-inteligence.html) by Robin Hanson\n*   [Fixing Election Markets](http://www.overcomingbias.com/2011/11/conditional-close-election-markets.html) by Robin Hanson\n*   [Prediction Markets](http://www.gwern.net/Prediction%20markets) at gwern.net\n*   [Idea Futures (a.k.a. Prediction Markets)](http://hanson.gmu.edu/ideafutures.html) by Robin Hanson\n\n### External Links\n\n*   [Comparing face-to-face meetings, nominal groups, Delphi and prediction markets on an estimation task](http://dl.dropbox.com/u/5317066/2011-graefe.pdf)\n*   [Video of Robin Hanson's Combinatorial Prediction Markets lecture at the Uncertainty in Artificial Intelligence conference in Helsinki, 2008](http://videolectures.net/uai08_hanson_cpm/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "BXL4riEJvJJHoydjG",
    "name": "Orthogonality Thesis",
    "core": null,
    "slug": "orthogonality-thesis",
    "tableOfContents": {
      "html": "<p><span class=\"by_5wu9jG4pm9q6xjZ9R\">The </span><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Orthogonality Thesis</span></strong><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">states</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> that </span><span class=\"by_qgdGA4ZEyW7zNdK84\">an </span><span class=\"by_3g4QCMdSAxvKPzdBx\">agent</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> can have any combination of intelligence level and</span><span class=\"by_3g4QCMdSAxvKPzdBx\"> final</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> goal, that is, its</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span></span><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">final goals</span></a><span class=\"by_5wu9jG4pm9q6xjZ9R\"> and </span><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><span class=\"by_qgdGA4ZEyW7zNdK84\">intelligence levels</span></a><span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> </span><span class=\"by_qgdGA4ZEyW7zNdK84\">can vary independently of each other. This is in contrast</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> to </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the belief that, because of their intelligence, AIs will all converge to</span><span class=\"by_5wu9jG4pm9q6xjZ9R\"> a </span><span class=\"by_qgdGA4ZEyW7zNdK84\">common goal.</span></span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">The thesis was originally defined by </span><a href=\"https://lessestwrong.com/tag/nick-bostrom\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Nick Bostrom</span></a><span><span class=\"by_qgdGA4ZEyW7zNdK84\"> in the paper </span><span class=\"by_3g4QCMdSAxvKPzdBx\">\"</span></span><a href=\"https://nickbostrom.com/superintelligentwill.pdf\"><span class=\"by_3g4QCMdSAxvKPzdBx\">Superintelligent Will</span></a><span><span class=\"by_3g4QCMdSAxvKPzdBx\">\"</span><span class=\"by_qgdGA4ZEyW7zNdK84\">, (along with the </span></span><a href=\"https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">instrumental convergence thesis</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">). For his purposes, Bostrom defines intelligence to be </span><a href=\"https://wiki.lesswrong.com/wiki/instrumental_rationality\"><span class=\"by_qgdGA4ZEyW7zNdK84\">instrumental rationality</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p><p><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Related: </span></i><a href=\"https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&amp;useTagName=true\"><i><span class=\"by_Xn6ACr6Cua8upALWQ\">Complexity of Value</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span></i><a href=\"https://www.lesswrong.com/tag/decision-theory?showPostCount=true&amp;useTagName=true\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Decision Theory</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span></i><a href=\"https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&amp;useTagName=true\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">General Intelligence</span></i></a><i><span class=\"by_qgdGA4ZEyW7zNdK84\">, </span></i><a href=\"https://www.lesswrong.com/tag/utility-functions?showPostCount=true&amp;useTagName=true\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Utility Functions</span></i></a></p><h2 id=\"Defense_of_the_thesis\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Defense of the thesis</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">It has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,</span></p><p><span><span class=\"by_qgdGA4ZEyW7zNdK84\">One reason many researchers assume </span><span class=\"by_3g4QCMdSAxvKPzdBx\">superintelligent agents</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> to converge to the same goals may be because </span></span><a href=\"https://lessestwrong.com/tag/human-universal\"><span class=\"by_qgdGA4ZEyW7zNdK84\">most humans</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as </span><a href=\"https://lessestwrong.com/tag/aixi\"><span class=\"by_qgdGA4ZEyW7zNdK84\">AIXI</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> and </span><a href=\"https://lessestwrong.com/tag/g%C3%B6del-machine\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Gödel machines</span></a><span><span class=\"by_Xn6ACr6Cua8upALWQ\">, </span><span class=\"by_qgdGA4ZEyW7zNdK84\">the thesis is known to be true. Furthermore, if the thesis was false, then </span></span><a href=\"https://lessestwrong.com/tag/oracle-ai\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Oracle AIs</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.</span></p><h2 id=\"Pathological_Cases\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Pathological Cases</span></h2><p><span class=\"by_qgdGA4ZEyW7zNdK84\">There are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.</span></p><h2 id=\"See_Also\"><span class=\"by_qgdGA4ZEyW7zNdK84\">See Also</span></h2><ul><li><a href=\"https://lessestwrong.com/tag/instrumental-convergence\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Instrumental Convergence</span></a></li></ul><h2 id=\"External_links\"><span class=\"by_qgdGA4ZEyW7zNdK84\">External links</span></h2><ul><li><span><span class=\"by_qgdGA4ZEyW7zNdK84\">Definition of the orthogonality thesis from Bostrom's</span><span class=\"by_Xn6ACr6Cua8upALWQ\"> </span></span><a href=\"http://www.nickbostrom.com/superintelligentwill.pdf\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Superintelligent Will</span></a></li><li><a href=\"https://arbital.com/p/orthogonality/\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Arbital orthogonality thesis article&nbsp;</span></a></li><li><a href=\"http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Critique</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\"> of the thesis by John Danaher</span></li><li><span class=\"by_3g4QCMdSAxvKPzdBx\">Superintelligent Will paper by Nick Bostrom</span></li></ul>",
      "sections": [
        {
          "title": "Defense of the thesis",
          "anchor": "Defense_of_the_thesis",
          "level": 1
        },
        {
          "title": "Pathological Cases",
          "anchor": "Pathological_Cases",
          "level": 1
        },
        {
          "title": "See Also",
          "anchor": "See_Also",
          "level": 1
        },
        {
          "title": "External links",
          "anchor": "External_links",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 20,
    "description": {
      "markdown": "The **Orthogonality Thesis** states that an agent can have any combination of intelligence level and final goal, that is, its [final goals](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true) and [intelligence levels](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true) can vary independently of each other. This is in contrast to the belief that, because of their intelligence, AIs will all converge to a common goal.\n\nThe thesis was originally defined by [Nick Bostrom](https://lessestwrong.com/tag/nick-bostrom) in the paper \"[Superintelligent Will](https://nickbostrom.com/superintelligentwill.pdf)\", (along with the [instrumental convergence thesis](https://wiki.lesswrong.com/wiki/instrumental_convergence_thesis)). For his purposes, Bostrom defines intelligence to be [instrumental rationality](https://wiki.lesswrong.com/wiki/instrumental_rationality).\n\n*Related:* [*Complexity of Value*](https://www.lesswrong.com/tag/complexity-of-value?showPostCount=true&useTagName=true)*,* [*Decision Theory*](https://www.lesswrong.com/tag/decision-theory?showPostCount=true&useTagName=true)*,* [*General Intelligence*](https://www.lesswrong.com/tag/general-intelligence?showPostCount=true&useTagName=true)*,* [*Utility Functions*](https://www.lesswrong.com/tag/utility-functions?showPostCount=true&useTagName=true)\n\nDefense of the thesis\n---------------------\n\nIt has been pointed out that the orthogonality thesis is the default position, and that the burden of proof is on claims that limit possible AIs. Stuart Armstrong writes that,\n\nOne reason many researchers assume superintelligent agents to converge to the same goals may be because [most humans](https://lessestwrong.com/tag/human-universal) have similar values. Furthermore, many philosophies hold that there is a rationally correct morality, which implies that a sufficiently rational AI will acquire this morality and begin to act according to it. Armstrong points out that for formalizations of AI such as [AIXI](https://lessestwrong.com/tag/aixi) and [Gödel machines](https://lessestwrong.com/tag/g%C3%B6del-machine), the thesis is known to be true. Furthermore, if the thesis was false, then [Oracle AIs](https://lessestwrong.com/tag/oracle-ai) would be impossible to build, and all sufficiently intelligent AIs would be impossible to control.\n\nPathological Cases\n------------------\n\nThere are some pairings of intelligence and goals which cannot exist. For instance, an AI may have the goal of using as little resources as possible, or simply of being as unintelligent as possible. These goals will inherently limit the degree of intelligence of the AI.\n\nSee Also\n--------\n\n*   [Instrumental Convergence](https://lessestwrong.com/tag/instrumental-convergence)\n\nExternal links\n--------------\n\n*   Definition of the orthogonality thesis from Bostrom's [Superintelligent Will](http://www.nickbostrom.com/superintelligentwill.pdf)\n*   [Arbital orthogonality thesis article ](https://arbital.com/p/orthogonality/)\n*   [Critique](http://philosophicaldisquisitions.blogspot.com/2012/04/bostrom-on-superintelligence-and.html) of the thesis by John Danaher\n*   Superintelligent Will paper by Nick Bostrom"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "EY623WWCXKTvN3kmj",
    "name": "Factored Cognition",
    "core": null,
    "slug": "factored-cognition",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qgdGA4ZEyW7zNdK84\">Factored cognition</span></strong><span class=\"by_qgdGA4ZEyW7zNdK84\"> is an approach to artificial intelligence where sophisticated learning and reasoning is broken down (or factored) into many small and mostly independent tasks [</span><a href=\"https://ought.org/research/factored-cognition\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">].</span></p><p><span class=\"by_qgdGA4ZEyW7zNdK84\">Factored Cognition is related to </span><a href=\"https://www.lesswrong.com/tag/iterated-amplification\"><i><span class=\"by_qgdGA4ZEyW7zNdK84\">Iterated Amplification</span></i><span class=\"by_qgdGA4ZEyW7zNdK84\"> (IDA)</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 29,
    "description": {
      "markdown": "**Factored cognition** is an approach to artificial intelligence where sophisticated learning and reasoning is broken down (or factored) into many small and mostly independent tasks \\[[1](https://ought.org/research/factored-cognition)\\].\n\nFactored Cognition is related to [*Iterated Amplification* (IDA)](https://www.lesswrong.com/tag/iterated-amplification)."
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9mShmhRFzBat3523A",
    "name": "Perceptual Control Theory",
    "core": null,
    "slug": "perceptual-control-theory",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_qf77EiaoMw7tH3GSr\">Perceptual control theory (PCT)</span></strong><span><span class=\"by_qf77EiaoMw7tH3GSr\"> is a psychological theory of animal and human behavior. PCT postulates that an organism's behavior is a means of controlling its perceptions.</span><span class=\"by_qgdGA4ZEyW7zNdK84\"> The model is based on the principles of negative feedback [</span></span><a href=\"https://en.wikipedia.org/wiki/Perceptual_control_theory\"><span class=\"by_qgdGA4ZEyW7zNdK84\">1</span></a><span class=\"by_qgdGA4ZEyW7zNdK84\">]. It is to some extent an application of the ideas used in the engineering discipline of control theory to the modeling of the human mind and behavior.</span><br><br><span class=\"by_qKdS7bw9McaSeMNbY\">PCT postulates that layers of control systems, which have access to a metric to optimize and some set of policies or actions, can maintain balancing-acts for difficult, high-abstraction things without developing any explicit model for how those actions relate to the metric being tracked. The brain is postulated to be one of these multi-layered PCTs.</span><br><br><span class=\"by_qKdS7bw9McaSeMNbY\">Physical movements are a favorite case-study, since they're relatively easy to break down into this these sorts of layered control theory sub-problems. An important subtlety is that the control systems are optimizing for the perception of a state, rather than for a concrete environmental state itself.</span><br><br><span class=\"by_qKdS7bw9McaSeMNbY\">Actions and behaviors develop because they do something to reduce the mismatch between internal perception, and the stimulus readings received.</span></p><h2 id=\"Controversy\"><span class=\"by_qf77EiaoMw7tH3GSr\">Controversy</span></h2><p><span><span class=\"by_qf77EiaoMw7tH3GSr\">It's unclear whether PCT is a valid </span><span class=\"by_qKdS7bw9McaSeMNbY\">theory. It doesn't significantly constrain the space of possible minds</span><span class=\"by_qf77EiaoMw7tH3GSr\"> that </span><span class=\"by_qKdS7bw9McaSeMNbY\">could be built from it, and the</span><span class=\"by_qf77EiaoMw7tH3GSr\"> advocates of the theory on the blog were unable to make a clear case for it. </span><span class=\"by_qKdS7bw9McaSeMNbY\">Experimental results for its quality as an algorithm seemed lackluster;</span><span class=\"by_qf77EiaoMw7tH3GSr\"> see </span></span><a href=\"https://lessestwrong.com/lw/14v/the_usefulness_of_correlations/11iu/\"><span class=\"by_qf77EiaoMw7tH3GSr\">these</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> </span><a href=\"https://lessestwrong.com/lw/14v/the_usefulness_of_correlations/11j6/\"><span class=\"by_qf77EiaoMw7tH3GSr\">critical comments</span></a><span class=\"by_qf77EiaoMw7tH3GSr\"> about the paper version of </span><a href=\"http://www.rand.org/pubs/drafts/DRU2751/\"><span class=\"by_qf77EiaoMw7tH3GSr\">this technical report</span></a><span class=\"by_qKdS7bw9McaSeMNbY\">, which claim that the correct results may have been achieved more through parameter-fitting than PCT.</span><br><br><span class=\"by_qKdS7bw9McaSeMNbY\">Some anecdotally found it more useful for explaining bugs in human behavior, than for modeling what would be ideal behavior.</span></p><h2 id=\"Under_Characterized_Information_Storage\"><span class=\"by_qKdS7bw9McaSeMNbY\">Under-Characterized Information Storage</span></h2><p><span class=\"by_qKdS7bw9McaSeMNbY\">This seems to be a common category of complaints about Perceptual Control Theory.</span><br><br><span class=\"by_qKdS7bw9McaSeMNbY\">This </span><a href=\"http://psychsciencenotes.blogspot.com/2016/01/a-quick-review-and-analysis-of.html\"><span class=\"by_qKdS7bw9McaSeMNbY\">blog post</span></a><span class=\"by_qKdS7bw9McaSeMNbY\"> called out that PCT \"has no theory of information or how that information comes to be made,\" and </span><a href=\"https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it\"><span class=\"by_qKdS7bw9McaSeMNbY\">this post</span></a><span class=\"by_qKdS7bw9McaSeMNbY\"> grappled with a similar problem: struggling to find a place for implicit models, priors, and updates when working with a PCT framework. (</span><a href=\"https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it?commentId=JzphwDyjg6YXBHkAc\"><span class=\"by_qKdS7bw9McaSeMNbY\">This</span></a><span class=\"by_qKdS7bw9McaSeMNbY\"> comment may have made a case for at least some embedded implicit priors.)</span></p><h2 id=\"Notable_Posts\"><span class=\"by_qgdGA4ZEyW7zNdK84\">Notable Posts</span></h2><ul><li><span class=\"by_qKdS7bw9McaSeMNbY\">Kennaway's </span><a href=\"https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it\"><span><span class=\"by_qKdS7bw9McaSeMNbY\">What is</span><span class=\"by_9c2mQkLQq6gQSksMs\"> control </span><span class=\"by_qKdS7bw9McaSeMNbY\">theory</span></span></a><span class=\"by_qKdS7bw9McaSeMNbY\">, and his prior post </span><a href=\"https://www.lesswrong.com/posts/Ba6buPA3u2btdKS82/without-models\"><span class=\"by_qKdS7bw9McaSeMNbY\">Without Models</span></a></li><li><span class=\"by_qKdS7bw9McaSeMNbY\">SSC book reviews of </span><a href=\"https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/\"><span class=\"by_qKdS7bw9McaSeMNbY\">Surfing Uncertainty</span></a><span class=\"by_qKdS7bw9McaSeMNbY\"> and </span><a href=\"https://slatestarcodex.com/2017/03/06/book-review-behavior-the-control-of-perception/\"><span class=\"by_qKdS7bw9McaSeMNbY\">Behavior: The Control of Perception</span></a><ul><li><span class=\"by_qKdS7bw9McaSeMNbY\">Scott Alexander theorizes that it's analogous to Friston's </span><a href=\"https://en.wikipedia.org/wiki/Free_energy_principle\"><span class=\"by_qKdS7bw9McaSeMNbY\">Free Energy Principle</span></a><span class=\"by_qKdS7bw9McaSeMNbY\">, </span><a href=\"https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/\"><span class=\"by_qKdS7bw9McaSeMNbY\">here</span></a><span class=\"by_qKdS7bw9McaSeMNbY\"> and </span><a href=\"https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/\"><span class=\"by_qKdS7bw9McaSeMNbY\">here</span></a></li></ul></li><li><span class=\"by_qKdS7bw9McaSeMNbY\">Vaniver's </span><a href=\"https://www.lesswrong.com/posts/dcRY7XSnuARkHkA5D/an-introduction-to-control-theory\"><span class=\"by_qKdS7bw9McaSeMNbY\">Introduction to Control Theory</span></a><span class=\"by_qKdS7bw9McaSeMNbY\">, and review of </span><a href=\"https://www.lesswrong.com/posts/nPs63hpijnQs37jme/behavior-the-control-of-perception\"><span class=\"by_qKdS7bw9McaSeMNbY\">Behavior: The Control of Perception</span></a><ul><li><span class=\"by_qKdS7bw9McaSeMNbY\">His personal-blog thoughts on the topic </span><a href=\"https://www.lesswrong.com/posts/cMKNFE8hWKNhnEXtM/control-theory-commentary\"><span class=\"by_qKdS7bw9McaSeMNbY\">here</span></a></li></ul></li></ul><h2 id=\"See_also\"><span><span class=\"by_nLbwLhBaQeG6tCNDN\">See </span><span class=\"by_qf77EiaoMw7tH3GSr\">also</span></span></h2><ul><li><a href=\"https://lessestwrong.com/tag/control-theory\"><span><span class=\"by_nLbwLhBaQeG6tCNDN\">Control </span><span class=\"by_qgdGA4ZEyW7zNdK84\">Theory</span></span></a></li></ul>",
      "sections": [
        {
          "title": "Controversy",
          "anchor": "Controversy",
          "level": 1
        },
        {
          "title": "Under-Characterized Information Storage",
          "anchor": "Under_Characterized_Information_Storage",
          "level": 1
        },
        {
          "title": "Notable Posts",
          "anchor": "Notable_Posts",
          "level": 1
        },
        {
          "title": "See also",
          "anchor": "See_also",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 5
    },
    "postCount": 7,
    "description": {
      "markdown": "**Perceptual control theory (PCT)** is a psychological theory of animal and human behavior. PCT postulates that an organism's behavior is a means of controlling its perceptions. The model is based on the principles of negative feedback \\[[1](https://en.wikipedia.org/wiki/Perceptual_control_theory)\\]. It is to some extent an application of the ideas used in the engineering discipline of control theory to the modeling of the human mind and behavior.  \n  \nPCT postulates that layers of control systems, which have access to a metric to optimize and some set of policies or actions, can maintain balancing-acts for difficult, high-abstraction things without developing any explicit model for how those actions relate to the metric being tracked. The brain is postulated to be one of these multi-layered PCTs.  \n  \nPhysical movements are a favorite case-study, since they're relatively easy to break down into this these sorts of layered control theory sub-problems. An important subtlety is that the control systems are optimizing for the perception of a state, rather than for a concrete environmental state itself.  \n  \nActions and behaviors develop because they do something to reduce the mismatch between internal perception, and the stimulus readings received.\n\nControversy\n-----------\n\nIt's unclear whether PCT is a valid theory. It doesn't significantly constrain the space of possible minds that could be built from it, and the advocates of the theory on the blog were unable to make a clear case for it. Experimental results for its quality as an algorithm seemed lackluster; see [these](https://lessestwrong.com/lw/14v/the_usefulness_of_correlations/11iu/) [critical comments](https://lessestwrong.com/lw/14v/the_usefulness_of_correlations/11j6/) about the paper version of [this technical report](http://www.rand.org/pubs/drafts/DRU2751/), which claim that the correct results may have been achieved more through parameter-fitting than PCT.  \n  \nSome anecdotally found it more useful for explaining bugs in human behavior, than for modeling what would be ideal behavior.\n\nUnder-Characterized Information Storage\n---------------------------------------\n\nThis seems to be a common category of complaints about Perceptual Control Theory.  \n  \nThis [blog post](http://psychsciencenotes.blogspot.com/2016/01/a-quick-review-and-analysis-of.html) called out that PCT \"has no theory of information or how that information comes to be made,\" and [this post](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it) grappled with a similar problem: struggling to find a place for implicit models, priors, and updates when working with a PCT framework. ([This](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it?commentId=JzphwDyjg6YXBHkAc) comment may have made a case for at least some embedded implicit priors.)\n\nNotable Posts\n-------------\n\n*   Kennaway's [What is control theory](https://www.lesswrong.com/posts/fJKbCXrCPwAR5wjL8/what-is-control-theory-and-why-do-you-need-to-know-about-it), and his prior post [Without Models](https://www.lesswrong.com/posts/Ba6buPA3u2btdKS82/without-models)\n*   SSC book reviews of [Surfing Uncertainty](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/) and [Behavior: The Control of Perception](https://slatestarcodex.com/2017/03/06/book-review-behavior-the-control-of-perception/)\n    *   Scott Alexander theorizes that it's analogous to Friston's [Free Energy Principle](https://en.wikipedia.org/wiki/Free_energy_principle), [here](https://slatestarcodex.com/2017/09/06/predictive-processing-and-perceptual-control/) and [here](https://slatestarcodex.com/2019/03/20/translating-predictive-coding-into-perceptual-control/)\n*   Vaniver's [Introduction to Control Theory](https://www.lesswrong.com/posts/dcRY7XSnuARkHkA5D/an-introduction-to-control-theory), and review of [Behavior: The Control of Perception](https://www.lesswrong.com/posts/nPs63hpijnQs37jme/behavior-the-control-of-perception)\n    *   His personal-blog thoughts on the topic [here](https://www.lesswrong.com/posts/cMKNFE8hWKNhnEXtM/control-theory-commentary)\n\nSee also\n--------\n\n*   [Control Theory](https://lessestwrong.com/tag/control-theory)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "9DNZfxFvY5iKoZQbz",
    "name": "Interviews",
    "core": null,
    "slug": "interviews",
    "tableOfContents": {
      "html": "<p><strong id=\"Interviews\"><span class=\"by_HoGziwmhpMGqGeWZy\">Interviews</span></strong></p><p><strong><span class=\"by_sKAL2jzfkYkDbQmx9\">Related Pages: </span></strong><a href=\"https://www.lesswrong.com/tag/interview-series-on-risks-from-ai\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Interview Series On Risks From AI</span></a><span class=\"by_sKAL2jzfkYkDbQmx9\">, </span><a href=\"https://www.lesswrong.com/tag/dialogue-format\"><span class=\"by_sKAL2jzfkYkDbQmx9\">Dialogue (format)</span></a></p>",
      "sections": [
        {
          "title": "Interviews",
          "anchor": "Interviews",
          "level": 1
        },
        {
          "divider": true,
          "level": 0,
          "anchor": "postHeadingsDivider"
        }
      ],
      "headingsCount": 2
    },
    "postCount": 60,
    "description": {
      "markdown": "**Interviews**\n\n**Related Pages:** [Interview Series On Risks From AI](https://www.lesswrong.com/tag/interview-series-on-risks-from-ai), [Dialogue (format)](https://www.lesswrong.com/tag/dialogue-format)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "khReijeucXJTnsyMT",
    "name": "Bounties (closed)",
    "core": false,
    "slug": "bounties-closed",
    "tableOfContents": {
      "html": "<p><span class=\"by_gXeEWGjTWyqgrQTzR\">A </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">bounty </span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">or is a monetary payment for accomplishing some one-off task. (If the winner is selected in a competition, it is often referred to as a </span><strong><span class=\"by_gXeEWGjTWyqgrQTzR\">prize</span></strong><span class=\"by_gXeEWGjTWyqgrQTzR\">.) On LessWrong, bounties have historically been paid out for things like providing useful information, doing a novel piece of research, or changing someone's mind about a topic.&nbsp;</span></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">This tag is for bounties that have already been paid out. It has a sister tag for active bounties. &nbsp;</span></p><p><i><span class=\"by_gXeEWGjTWyqgrQTzR\">If you're hosting a bounty, please make sure to change the tags to indicate the status of the bounty.&nbsp;</span></i></p><p><span class=\"by_gXeEWGjTWyqgrQTzR\">See also: </span><a href=\"https://www.lesswrong.com/tag/bounties-active\"><span class=\"by_gXeEWGjTWyqgrQTzR\">Bounties (active)</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, </span><a href=\"https://www.lesswrong.com/tag/grants-and-fundraising-opportunities\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Grants and Fundraising Opportunities</span></a><span class=\"by_Sp5wM4aRAhNERd4oY\">, </span><a href=\"https://www.facebook.com/groups/1781724435404945/\"><span class=\"by_Sp5wM4aRAhNERd4oY\">Bountied Rationality Facebook group</span></a></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 40,
    "description": {
      "markdown": "A **bounty** or is a monetary payment for accomplishing some one-off task. (If the winner is selected in a competition, it is often referred to as a **prize**.) On LessWrong, bounties have historically been paid out for things like providing useful information, doing a novel piece of research, or changing someone's mind about a topic. \n\nThis tag is for bounties that have already been paid out. It has a sister tag for active bounties.  \n\n*If you're hosting a bounty, please make sure to change the tags to indicate the status of the bounty. *\n\nSee also: [Bounties (active)](https://www.lesswrong.com/tag/bounties-active), [Grants and Fundraising Opportunities](https://www.lesswrong.com/tag/grants-and-fundraising-opportunities), [Bountied Rationality Facebook group](https://www.facebook.com/groups/1781724435404945/)"
    },
    "parentTag": null,
    "subTags": []
  },
  {
    "_id": "ABG8vt87eW4FFA6gD",
    "name": "Open Threads",
    "core": false,
    "slug": "open-threads",
    "tableOfContents": {
      "html": "<p><strong><span class=\"by_r38pkCm7wF4M44MDQ\">Open Threads</span></strong><span class=\"by_r38pkCm7wF4M44MDQ\"> are informal discussion areas, where users are welcome to post comments that didn't quite feel big enough to warrant a top-level post, nor fit in other posts.</span></p><p><span class=\"by_r38pkCm7wF4M44MDQ\">Sometimes an Open Thread focuses on a specific topic. The most common Open Threads are the monthly Open and Welcome Threads, which serve as a general focal point of discussion, as well as a place for new users to introduce themselves.</span></p>",
      "sections": [],
      "headingsCount": 0
    },
    "postCount": 456,
    "description": {
      "markdown": "**Open Threads** are informal discussion areas, where users are welcome to post comments that didn't quite feel big enough to warrant a top-level post, nor fit in other posts.\n\nSometimes an Open Thread focuses on a specific topic. The most common Open Threads are the monthly Open and Welcome Threads, which serve as a general focal point of discussion, as well as a place for new users to introduce themselves."
    },
    "parentTag": null,
    "subTags": []
  }
]