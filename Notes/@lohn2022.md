---
title: "AI and Compute: How Much Longer Can Computing Power Drive Artificial Intelligence Progress"
href: https://cset.georgetown.edu/wp-content/uploads/AI-and-Compute-How-Much-Longer-Can-Computing-Power-Drive-Artificial-Intelligence-Progress.pdf
---

Authors project slowdown soon because:
1. Training is expensive
2. Limited supply of AI chips
3. Training large models generates processor traffic jams that are difficult to manage.

The authors also predict a shift in focus to narrower techniques (cf. AlphaFold).

Policy recommendations
- Focus on researchers & improvements to algorithms / hardware designs. %% Don't do this you irresponsible maniacs %%
- Computing power is not the only way to support researchers
- Watch out for oligopolistic markets if research becomes compute-constrained 

---

Before 2012: Moore's Law-like growth.
2012-2018: Compute demands double every 3.4 months.

![[Pasted image 20220924215722.png]]


![[Pasted image 20220924220542.png]]

> Overall, 123 million GPUs shipped in the second quarter of 2021, with Nvidia accounting for 15.23 percent of the total, which suggests Nvidia sells approximately 75 million GPU units per year.21 Thirty-seven percent of Nvidia’s revenue came from the datacenter market, and if we likewise assume that approximately 37 percent of its units went to datacenters, this translates to about 28 million Nvidia GPUs going to datacenters annually.22 Nvidia GPUs are not the only AI accelerators going into datacenters, but they reportedly make up 80 percent of the market.23 Based on all these figures, we estimate the total number of accelerators reaching datacenters annually to be somewhere in the ballpark of 35 million. 

**Extrapolated GPU Usage will soon become infeasible**
Extrapolated GPU usage predicts a single model will require the use of every GPU in every datacenter for three years to train by 2025

**Extrapolated compute demand already slowing**
- Megatron-Turing NLG is a year behind the prediction. 

**There's room for algorithmic improvement**

> The number of computations required to reach AlexNet’s level of performance in 2018 was a mere 1/25th the number of computations that were required to reach the same level of performance in 2012