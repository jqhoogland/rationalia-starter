# Is Power-Seeking AI an Existential Risk?
## Outline
1. **Backdrop**: "intelligent agency is an extremely powerful force, and creating agents much more intelligent than us is playing with fire"
2. **Formal**:
	1. It'll be possible (technically, financially) to build powerful, agentic AI systems.
	2. There will be strong incentives to do so.
	3. True alignment is more difficult than misalignment / deceptive alignment.
	4. Misaligned systems will seek power over humans in high impact ways
	5. This disempowerment = X-risk

# Introduction
- "**Intelligence**" as some loose cluster of abilities that has given us our advantage over the natural world.
	- Carlsmith asserts "Our abilities in these respects are nowhere near any sort of hard limit." The constraints include: "cell count, energy, communication speed, signaling frequency, memory capacity, component reliability, input/output bandwidth, and so forth" %% I think this confidence may be too high. We see regularly that biological systems can sit very closely to information-theoretic bounds on processing capacity (e.g., stem cell differentiation factor gradients, bat sensitivity to $O(10 \text{ns})$ disturbances) %%


# Timelines

## Key Properties — APS
1. **Advanced capabilities**: "outperform the best humans on some set of tasks which when performed at advanced levels grant significant power in today’s world" (e.g., scientific research, business/military strategy)
2. **Agentic planning**: "mak[ing] and execut[ing] plans, in pursuit of objectives, on the basis of models of the world"
3. **Strategic awareness**: "the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining power over humans and the real-world environment."

%%
START
type: Basic
What does Carlsmith identify as the key properties of potentially dangerous AI systems?
Back: **APS** (Advanced capabilities, planning, strategic awareness)
Tags: LessWrong
END
%%

# Incentives


# Alignment

# Deployment

# Correction

# Catastrophe