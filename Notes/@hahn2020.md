---
title: Theoretical Limitations of Self-Attention in Neural Sequence Models
href: https://arxiv.org/pdf/1906.06755.pdf
---

> Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length

^ab8b85

> Dehghani et al. (2019) suggested that transformers cannot compute functions that require sequential processing of input, without providing further details or proofs. Similarly, Shen et al. (2018a); Chen et al. (2018); Hao et al. (2019) have introduced extensions of transformers with recurrence, citing similar intuitions about limitations of transformers.

Examples:
- Closing brackets (`2DYCK`)
- Evaluating iterated negation (`PARITY`)
- LSTMs appear better at learning hierarchies (Tran et al. 2018)

Under assumptions of even bounded recurrent computations, RNNs & Transformers are Turing-complete (Siegelman & Sontag 1995; [[@perez2020]]).

%%
NOTE:
 - These results are partially contradicted by Chiang and Cholak (2022):
	 - > Weâ€™ve seen that the questions of (a) whether a neural network can recognize a language, (b) whether it can achieve low cross-entropy on a language, and (c) whether it can learn to recognize a language are three separate questions, because we have given examples of (a) without (b) and (b) without (c).
	 - It is possible to construct transformers that recognize `PARITY`
	 - You can modify a transformer to get arbitrary low cross-entropy, but it does not learn well.
	 - You can get around the limited influence of a single input symbol by scaling attention logits appropriately
%%