---
href: https://arxiv.org/pdf/2109.13916.pdf
---

We present four problems ready for research, namely withstanding hazards (“**Robustness**”), identifying hazards (“**Monitoring**”), steering ML systems (“**Alignment**”), and reducing deployment hazards (“**Systemic Safety**”)

%%
START
type: Cloze
What are Hendrycks et al.'s (2022) four major **problem areas** in **AI safety**?
1. {{c1::Robustness: withstanding hazards (black swans & adversaries)}}
2. {{c2::Monitoring: identifying hazards (anomalies, hidden functionalities, representative outputs)}}
3. {{c3::Alignment: steering ML systems}}
4. {{c4::Systemic Safety: reducing deployment hazards}}
Tags: LessWrong
END

%%

![[Pasted image 20220929225300.png]]

**ML Safety Research**: "ML research aimed at making the adoption of ML more beneficial, with emphasis on long-term and long-tail risks"

- **Robustness**: black swans & adversaries
- **Monitoring**: anomalies, representative outputs, & hidden functionalities
- **Alignment**: specification (=> value learning), brittleness (=> proxy gaming, value clarification), optimization (& mesalignment), & unintended consequences (=> reversible actions)
- **Systemic Safety**:  for cybersecurity, informed decision making (forecasting, raising considerations)

![[Pasted image 20220929225309.png]]

![[Pasted image 20220929225324.png]]

![[Pasted image 20220929225337.png]]

![[Pasted image 20220929225348.png


# Other
- Privacy
- Fairness
- Ethics

![[Pasted image 20220929225429.png]]