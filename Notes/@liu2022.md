---
title: OMNIGROK: GROKKING BEYOND ALGORITHMIC DATA
href: https://arxiv.org/pdf/2210.01117.pdf
aliases:
  - Omnigrok
  - LU Mechanism
---

**The origin of grokking**: Why is generalization much delayed after overfitting?
> Grokking is caused by the mismatch between training and test loss landscapes. Specifically, (reduced) training and test losses plotted against model weight norm resemble "L" and "U", respectively, as shown in Figure 1b. We refer to this phenomenon as the "LU mechanism", which we elaborate on in Section 2 and 3.
	
**The prevalence of grokking**: Can grokking occur on datasets other than algorithmic datasets?
> Yes. Indeed, we demonstrate grokking for a wide range of machine learning tasks in Section 4, including image classification, sentiment analysis and molecule property prediction. Grokking signals observed for these tasks are usually less dramatic than for algorithmic datasets, which we attribute to representation learning in Section 5.

![[Pasted image 20221006135653.png]]

> It is well known in statistics that generalization error has a "U" shape against model capacity, which is usually attributed to the bias-variance trade-off. Although this common wisdom was challenged by the observation of double descent (Nakkiran et al., 2021), the "U" curve can be recovered from a double descent simply by changing the x-axis from the number of model parameters N to the 2-norm of model parameters w â‰¡ ||w||2 (Ng and Ma, 2022).