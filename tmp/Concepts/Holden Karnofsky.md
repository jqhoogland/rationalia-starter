---
tags: ['LessWrong', 'Portal', 'Concept']
href: https://www.lesswrong.com/tag/holden-karnofsky
---

Holden Karnofsky is a co-founder of [effective altruism](https://www.lesswrong.com/tag/effective-altruism) pioneers [GiveWell](https://www.lesswrong.com/tag/givewell).

In May 2012, Karnofsky posted [Thoughts on the Singularity Institute (SI)](http://lesswrong.com/lw/cbs/thoughts_on_the_singularity_institute_si/), which became the most-upvoted article ever on Less Wrong. It offered a detailed critique of what is now the [Machine Intelligence Research Institute](https://www.lesswrong.com/tag/machine-intelligence-research-institute-miri), and spawned a great deal of discussion.

MIRI staff posted two replies:

- [Eliezer Yudkowsky](https://www.lesswrong.com/tag/eliezer-yudkowsky)
- , 
- [Reply to Holden on 'Tool AI'](http://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/)
- [Luke Muehlhauser](https://www.lesswrong.com/tag/luke-muehlhauser)
- , 
- [Reply to Holden on The Singularity Institute](http://lesswrong.com/lw/di4/reply_to_holden_on_the_singularity_institute/)

Paul Crowley ("ciphergoth") posted discussion articles for each point raised:

- [Objection 1: it seems to me that any AGI that was set to maximize a "Friendly" utility function would be extraordinarily dangerous.](http://lesswrong.com/lw/cck/holden_karnofskys_singularity_institute_objection/)
- [Objection 2: SI appears to neglect the potentially important distinction between "tool" and "agent" AI.](http://lesswrong.com/lw/ccl/holden_karnofskys_singularity_institute_objection/)
- [Objection 3: SI's envisioned scenario is far more specific and conjunctive than it appears at first glance, and I believe this scenario to be highly unlikely.](http://lesswrong.com/lw/ccm/holden_karnofskys_singularity_institute_objection/)
- [Is SI the kind of organization we want to bet on?](http://lesswrong.com/lw/cco/holden_karnofskys_singularity_institute_critique/)
- [Other objections to SI's views](http://lesswrong.com/lw/ccn/holden_karnofskys_singularity_institute_critique/)

Other discussion:

- Phil Goetz, 
- [Holden's Objection 1: Friendliness is dangerous](http://lesswrong.com/lw/chk/holdens_objection_1_friendliness_is_dangerous/)

## Related pages
- [Tool AI](https://www.lesswrong.com/tag/tool-ai)



---

