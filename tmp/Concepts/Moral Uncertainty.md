---
tags: ['LessWrong', 'Concept']
href: http://www.lesswrong.com/tag/moral-uncertainty
---

# Moral Uncertainty
Moral uncertainty (or normative uncertainty) is uncertainty about what we ought, morally, to do given the diversity of moral doctrines. For example, suppose that we knew for certain that new technology would enable more humans to live on another planet with slightly less well-being than on Earth [[1 Projects/Learning/Rationalism/Concepts/Moral Uncertainty|1]]. An average [[Utilitarianism|utilitarian]] would consider these consequences bad, while a total utilitarian would endorse such technology. If we are uncertain about which of these two theories are right, what should we do?

Moral uncertainty includes a level of uncertainty above the more usual uncertainty of [[Decision Theory|what to do given incomplete information]] since it deals also with uncertainty about which moral theory is right. Even with complete information about the world, this kind of uncertainty would still remain [[1 Projects/Learning/Rationalism/Concepts/Moral Uncertainty|1]]. In one level of uncertainty, one can have doubts on how to act because all the relevant empirical information isn’t available, for example, choosing whether to implement or not a new technology (e.g.: [[1 Projects/Learning/Rationalism/Concepts/Artificial General Intelligence|AGI]], [[Nootropics & Other Cognitive Enhancement|Biological Cognitive Enhancement]], [[1 Projects/Learning/Rationalism/Concepts/Whole Brain Emulation|Mind Uploading]]) not fully knowing about its consequences and nature. But even if we ideally get to know each and every consequence of new technology, we would still need to know which is the right ethical perspective for analyzing these consequences.

One approach is to follow only the most probable theory. This has its own problems. For example, what if the most probable theory points only weakly in one way, and other theories point strongly the other way? A better approach is to “perform the action with the highest expected moral value. We get the expected moral value of an action by multiplying the subjective probability that some theory is true by the value of that action if it is true, doing the same for all of the other theories, and adding up the results.” [[1 Projects/Learning/Rationalism/Concepts/Moral Uncertainty|2]] However, we would still need a method of comparing value intertheories, an [[Utility|utilon]] in one theory may not be the same with an utilon in another theory. Outside [[Consequentialism|consequentialism]], many ethical theories don’t use utilions or even any quantifiable values. This is still an open problem....[(Read More)]()

